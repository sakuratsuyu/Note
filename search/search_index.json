{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"NOTES","text":"<p>A note book by @sakuratsuyu.</p> <p>\u5076\u5c14\u5199\u70b9\u4e1c\u897f\uff0c\u5e0c\u671b\u4f60\u559c\u6b22\u3002</p> <p>Note</p> <ul> <li>Currently updating Theory of Computation weekly.</li> </ul> <p>About me</p> <ul> <li>All ids I used: sakuratsuyu, \u685c\u3064\u3086, \u66e6\u9732 and Mslease. Maybe you can find me somewhere else! </li> <li>Junior as a ZJU CS Undergraduate.</li> <li>MADer.</li> </ul> Hope you can find anything useful and interesting here! \u2728\u30ad\u30e9\u30ad\u30e9\u76ee\u2728"},{"location":"comming_soon/","title":"Waiting...","text":"<p>Failure</p> <p>Things there are comming soon.</p>"},{"location":"discussion/","title":"Discussion","text":"<p>Free talk here about anything!</p> <p>Maybe I can put some blogrolls there ... </p> <p>Blogrolls</p> <p>Wants more friends! </p> <p> Isshiki\u4fee's Notebook \u545c\u545c\u4fee\u4f6c\u6559\u6559. </p> <p> Azily's Blog \u88ab\u4f6c\u8584\u7eb1. </p>"},{"location":"tasklist/","title":"Task List","text":"<ul> <li> Make a logo!</li> <li> Mathematics Basis<ul> <li> Discrete Mathematics | DM</li> <li> Numerical Analysis | NA</li> <li> Abstract Algrebra</li> </ul> </li> <li> Computer Science Courses<ul> <li> The Missing Semester of your CS education \u2192 Cheat Sheets / Tools</li> <li> Introduction to Computer Systems | ICS</li> <li> Introduction to Computer Vision | ICV</li> <li> Fundemental of Data Structure | FDS</li> </ul> </li> <li> Cheat Sheets / Tools<ul> <li> Git</li> <li> Markdown</li> <li> Latex</li> <li> Vim / Neovim</li> <li> Shell</li> <li> Data Wrangling</li> <li> Profiling</li> </ul> </li> <li> Pot-pourri<ul> <li> Fundamentals of Quantum Information</li> <li> Simple Cryptography</li> <li> Guidance to Configure Manjaro + i3wm</li> </ul> </li> </ul>"},{"location":"Computer_Science_Courses/","title":"Title Page","text":"<p>Abstract</p> <p>This section stores the notes of the courses that I've learned about computer scicence (CS) from ZJU or other platforms like MIT and Standford.</p> <p>It helps me to build my knowledge system and hope it helps you too. </p> <p>Welcome to point out anything wrong and careless mistakes!</p>"},{"location":"Computer_Science_Courses/FDS/Algorithm_Analysis/","title":"Algorithm Analysis","text":"<p>Definition</p> <p>An algorithm is a finite set of instructions that, if followed, accomplishes a particular task. In addtion, all algorithms must satisfy the following criteria:</p> <ul> <li>Input (zero or more) and Output (at least one).</li> <li>Definiteness: clear and unambiguous.</li> <li>Finiteness: termination after finite number of steps.</li> <li>Effectiveness.</li> </ul> <p>NOTE: A program does not have to be finite. e.g. an operating system.</p>"},{"location":"Computer_Science_Courses/FDS/Algorithm_Analysis/#running-time","title":"Running Time","text":"<p>The most important resource to analyze is the running time. It consists of two parts</p> <ul> <li>Machine and Compiler-Dependent run times.</li> <li>Time and Space Complexities (Machine and Complier-Independent).</li> </ul> <p>In FDS, we mainly consider the average time complexity \\(T_\\text{avg}(N)\\) and the worst time complexity \\(T_\\text{worst}(N)\\) as functions of input size \\(N\\).</p> <p>Genernal Rules</p> <ul> <li>FOR LOOPS the statements inside the loop times the number of iterations.</li> <li>NESTED FOR LOOPS the statements multiplied by the product of the size.</li> <li>CONSECUTIVE STATEMENTS just add.</li> <li> <p>IF/ELSE running time of the test plus the larger of the running time of S1 and S2.</p> <pre><code>if (Condition) {\n    S1;\n} else {\n    S2;\n}\n</code></pre> </li> </ul>"},{"location":"Computer_Science_Courses/FDS/Algorithm_Analysis/#asymptotic-notation","title":"Asymptotic Notation","text":"<p>Definition</p> <ul> <li>\\(T(N) = O(f(N))\\) if there are positive constants \\(c\\) and \\(n_0\\) s.t.</li> </ul> \\[     \\forall\\ N \\ge n_0,\\ \\ T(N) \\le c \\cdot f(N). \\] <ul> <li>\\(T(N) = \\Omega(g(N))\\) if there are positive constants \\(c\\) and \\(n_0\\) s.t.</li> </ul> \\[     \\forall\\ N \\ge n_0,\\ \\ T(N) \\ge c \\cdot g(N). \\] <ul> <li>\\(T(N) = \\Theta(h(N))\\) iff </li> </ul> \\[     T(N) = O(h(N)) = \\Omega(h(N)). \\] <ul> <li>\\(T(N) = o(p(N))\\) if</li> </ul> \\[     T(N) = O(p(N)),\\ \\ T(N) \\ne \\Theta(p(N)). \\] <p>Rules</p> <ul> <li> <p>If \\(T_1(N) = O(f(N))\\) and \\(T_2(N) = O(g(N))\\), then</p> \\[ \\begin{aligned}     T_1(N) + T_2(N) &amp;= \\max(O(f(N)), O(g(N))), \\\\     T_1(N) \\cdot T_2(N) &amp;= O(f(N) \\cdot g(N)). \\\\ \\end{aligned} \\] </li> <li> <p>If \\(T(N)\\) is a polynomial of degree \\(k\\), then \\(T(N) = \\Theta(N^k)\\).</p> </li> </ul> <p>Example</p> <p>The recurrent equations for the time complexities of programs \\(P1\\) and \\(P2\\) are:</p> \\[ \\begin{aligned}     &amp; P1: T(1)=1,T(N)=T(N/3)+1, \\\\     &amp; P2: T(1)=1,T(N)=3T(N/3)+1. \\end{aligned} \\] <p>Find \\(T(N)\\) for \\(P1\\) and \\(P2\\) respectively.</p> <p>Solution.</p> <p>For \\(P1\\),</p> \\[ \\begin{aligned}     T(N) &amp;= T(N / 3) + 1 = T(N / 3^k) + k \\overset{k = \\log N}{=\\!=\\!=\\!=\\!=} T(1) + \\log N = O(\\log N). \\end{aligned} \\] <p>For \\(P2\\),</p> \\[ \\begin{aligned}     T(N) &amp;= 3T(N / 3) + 1 = 3^k T(N / 3^k) + 1 + k + \\cdots + 3^{k - 1}     \\\\ &amp; = 3^kT(N / 3^k) + O(3^k) \\overset{k = \\log N}{=\\!=\\!=\\!=\\!=} N \\cdot T(1) + O(N) = O(N). \\end{aligned} \\]"},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/","title":"Disjoint Set","text":"Definition (Recap) <p>A relation \\(R\\) is defined on a set \\(S\\) if for every pair of elements \\((a, b)\\), \\(a, b \\in S\\), \\(aRb\\) is either true of false. If \\(aRb\\) is true, then \\(a\\) is related to \\(b\\).</p> <p>A relation \\(\\sim\\) over a set \\(S\\) is said to be an equivalence relation over \\(S\\) iff it's symmetric, reflexive and transitive over \\(S\\).</p> <p>Two members \\(x\\) and \\(y\\) of a set \\(S\\) are said to be in the same equivalence class iff \\(x \\sim y\\).</p> <p>ADT</p> <p>Objects:</p> <ul> <li>Elements of the sets: \\(1, 2, 3, \\dots, N\\).</li> <li>Disjoint sets: \\(S_1, S_2, \\dots\\).</li> </ul> <p>Operations:</p> <ul> <li>Union two sets \\(S_i\\) and \\(S_j\\) to \\(S\\), i.e. \\(S = S_i \\cup S_j\\).</li> <li>Find the set \\(S_k\\) which contains the element \\(i\\).</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/#implementation","title":"Implementation","text":""},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/#method-1-tree","title":"Method 1 | Tree","text":"<p>The sets is represented by a forest. Each set \\(S_i\\) is represented by a tree, and the root of the tree is the representation element. And a list <code>name</code> stores the roots of the trees.</p> <p>For Union operation, it sets the parent of the root of one tree to the other root. The following picture shows</p> \\[     S_2 \\cup S1 \\Rightarrow S_2. \\] <p>For Find operation, it finds the root of \\(i\\) and its corresponding <code>name</code>, which is <code>'S'</code> in the following picture.</p>"},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/#method-2-array","title":"Method 2 | Array","text":"<p>Another method is to maintain an array \\(S\\) of size \\(N\\) to reprsent the trees in Method 1.</p> \\[     S[\\text{element}] = \\left\\{     \\begin{aligned}         &amp; 0, &amp;&amp; \\text{if the element is root}, \\\\         &amp; \\text{the element's parent}, &amp;&amp; \\text{if the element isn't root}.     \\end{aligned}     \\right. \\] <p>And the name of each set is the index of the root.</p> <p>For Union operation, we just set</p> <pre><code>S[root_2] = root_1.\n</code></pre> <p>For Find operation, we iteratively or recursively find its parent until <code>S[x] = 0</code>.</p> <pre><code>while (S[x] &gt; 0) {\nx = s[x];\n}\nreturn x;\n</code></pre>"},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/#improvement","title":"Improvement","text":"<p>Motivation: There may be the case that a sequence of special operations that makes the tree degenerate to a list.</p>"},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/#heuristic-union-stategy","title":"Heuristic Union Stategy","text":""},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/#union-by-size","title":"Union-by-Size","text":"<p>When union two sets \\(S_1\\) and \\(S_2\\) (represented by trees), if the size of \\(S_1\\) is smaller, then we union \\(S_1\\) to \\(S_2\\).</p> <p>To indicate the size of a set, suppose we only consider the positive indices, we can let <code>S[root] = -size</code> (initialized with <code>-1</code>) instead of <code>S[root] = 0</code>.</p> Union by Size<pre><code>void Union(int *S, const int root_a, const int root_b) {\nif (root_a &lt; root_b) {\nS[root_a] += S[root_b];\nS[root_b] = root_a;\n} else {\nS[root_b] += S[root_a];\nS[root_a] = root_b;\n}\n}\n</code></pre> <p>Proposition</p> <p>Let \\(T\\) be a tree created by union-by-size with \\(N\\) nodes, then </p> \\[     \\text{height}(T) \\le \\lfloor \\log_2N \\rfloor. \\] <p>Time complexity of \\(N\\) Union and \\(M\\) Find operations is now \\(O(N + M\\log_2N)\\).</p>"},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/#union-by-height-rank","title":"Union-by-Height / Rank","text":"<p>When union two sets \\(S_1\\) and \\(S_2\\) (represented by trees), if the height of tree \\(S_1\\) is smaller, then we union \\(S_1\\) to \\(S_2\\).</p> <p>The height of a tree increases only when two equally deep trees are joined (and then the height goes up by one).</p> Union by Height<pre><code>void Union(int *S, const int root_a, const int root_b) {\nif (S[root_a] &gt; S[root_b]) {\nS[root_a] = root_b;\nreturn;\n}\nif (S[root_a] &lt; S[root_b]) {\nS[root_b] = root_a;\nreturn;\n}\nS[root_a] --;\nS[root_b] = root_a;\n}\n</code></pre>"},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/#path-compression","title":"Path Compression","text":"<p>When we are finding an element, we can simultaneously change the parents of the nodes along the finding path, including itself, to the root, which makes the tree shallower.</p>  When finding the root of the purple node  <p>Union-Find / Disjoint Set</p> <pre><code>int *Initialize(const int n) {\nint *S = (int *)malloc(n * sizeof(int));\nfor (int i = 0; i &lt; n; ++ i) {\nS[i] = -1;\n}\nreturn S;\n}\n\nint Find(int *S, const int a) {\nif (S[a] &lt; 0) {\nreturn a;\n}\nreturn S[a] = Find(S, S[a]);\n}\n\nvoid Union(int *S, const int root_a, const int root_b) {\nif (root_a &lt; root_b) {\nS[root_a] += S[root_b];\nS[root_b] = root_a;\n} else {\nS[root_b] += S[root_a];\nS[root_a] = root_b;\n}\n}\n</code></pre>"},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/#time-complexity","title":"Time Complexity","text":"<p>After heuristic union (union-by-rank) and path compression, the average time complexity of each operation is</p> \\[     O(\\alpha(n)),\\ \\ \\text{ where $\\alpha$ is the inverse function of Ackermann function}. \\] <p>Its growth is very slow, which can be regarded as a constant.</p> <p>Ackermann function $A(m,n) $ is defined by</p> \\[ A(i, j) = \\left\\{ \\begin{aligned}     &amp; 2^j, &amp;&amp; i = 1 \\text{ and } j \\ge 1 \\\\     &amp; A(i - 1, 2), &amp;&amp; i \\ge 2 \\text{ and } j = 1 \\\\     &amp; A(i - 1, A(i, j - 1)), &amp;&amp; i \\ge 2 \\text{ and } j \\ge 2 \\\\ \\end{aligned} \\right. \\] <p>and \\(\\alpha(n)\\) is defined by the maximum \\(m\\) s.t. \\(A(m, m) \\le n\\). \\(\\alpha(n) = O(log^* n)\\) (iterative logarithm).</p> Iterative Logarithm \\[     \\log^* n = \\left\\{     \\begin{aligned}         &amp; 0, &amp;&amp; \\text{if } n \\le 1 \\\\         &amp; 1 + \\log^*(\\log_2 n). &amp;&amp; \\text{if } n &gt; 1 \\\\     \\end{aligned}     \\right. \\] <p>Thus we have</p> \\[ \\begin{aligned}     \\log^* 2 &amp;= 1 + \\log^*(\\log_2 2) = 1 + \\log^*1 = 1, \\\\     \\log^* 4 &amp;= 1 + \\log^*(\\log_2 4) = 1 + \\log^*2 = 2, \\\\     \\log^* 16 &amp;= 1 + \\log^*(\\log_2 16) = 1 + \\log^*4 = 3, \\\\     \\log^* 65536 &amp;= 1 + \\log^*(\\log_2 65536) = 1 + \\log^*16 = 4, \\\\     \\log^* 2^{65536} &amp;= 1 + \\log^*(\\log_2 2^{65536}) = 1 + \\log^*65536 = 5. \\\\ \\end{aligned} \\] Improvement Average Time Complexity Worst Time Complexity No improvement \\(O(\\log n)\\) \\(O(n)\\) Path Compression \\(O(\\alpha (n))\\) \\(O(\\log n)\\) Union by Rank \\(O(\\log n)\\) \\(O(\\log n)\\) Both \\(O(\\alpha(n))\\) \\(O(\\alpha(n))\\)"},{"location":"Computer_Science_Courses/FDS/Graph/","title":"Graph","text":"<p>Definition</p> <p>A Graph consists of vertices and edges. It can be represented by the mark \\(G(V, E)\\), where  </p> <ul> <li>\\(G\\) denotes the graph.</li> <li>\\(V = V(G) = \\{v_1, \\dots, v_n\\}\\) denotes a finite nonempty set of vertices.</li> <li>\\(E = E(G) = \\{e_1, \\dots, e_m\\}\\) denotes a finite set of edges.</li> </ul> <p>When we discuss graph in this course FDS, we have the following restrictions</p> <ul> <li>Self loop is illegal.</li> <li>Multigraph is not considered.</li> </ul> <p>Definition</p> <ul> <li>Undirected graph (\u65e0\u5411\u56fe): the edge from \\(v_i\\) to \\(v_j\\) is the same as that from \\(v_j\\) to \\(v_i\\), denoted by \\((v_i, v_j) = (v_j, v_i)\\).</li> <li>Directed graph (digraph, \u6709\u5411\u56fe): the edge from \\(v_i\\) to \\(v_j\\) is the different from that from \\(v_j\\) to \\(v_i\\), denoted by \\(&lt;v_i, v_j&gt; = &lt;v_j, v_i&gt;\\).</li> <li>Complete graph (\u5b8c\u5168\u56fe):   A graph that has the maximum number of edges.<ul> <li>For a complete undirected graph with \\(n\\) vertices, there are \\(\\dfrac{n(n-1)}{2}\\) edges.  </li> <li>For a complete directed grach with \\(n\\) vertices, there are \\(n(n-1)\\) edges.</li> </ul> </li> <li>Adjacency (\u76f8\u90bb):<ul> <li>For an undirected graph, if there is an edge \\((v_i, v_j)\\), then \\(v_i\\) and \\(v_j\\) are adjacent and \\((v_i, v_j)\\) is incident* on \\(v_i\\) and \\(v_j\\).</li> <li>For a directed graph, if there is an edge \\(&lt;v_i, v_j&gt;\\), then \\(v_i\\) is adjacent to \\(v_j\\), \\(v_j\\) is adjacent from \\(v_i\\) and \\(&lt;v_i, v_j&gt;\\) is incident on \\(v_i\\) and \\(v_j\\).</li> </ul> </li> <li>Subgrach \\(G' \\subset G\\) (\u5b50\u56fe): \\(V(G') \\subseteq V(G)\\) and \\(E(G')\\subseteq E(G)\\).</li> <li>Path from \\(v_p\\) to \\(v_q\\) (\u8def\u5f84): \\(\\{v_p, v_{i_1}, v_{i_2}, \\cdots, v_{i_n}, v_q \\}\\) such that \\((v_p,v_{i1}), (v_{i1}, v_{i2}), \\dots, (v_{in}, v_q)\\) or \\(&lt;v_p,v_{i1}&gt;, &lt;v_{i1}, v_{i2}&gt;, \\dots, &lt;v_{in}, v_q&gt;\\) are in \\(E(G)\\).<ul> <li>Length of a path (\u8def\u5f84\u957f\u5ea6): number of edges on the path.</li> <li>Simple path (\u7b80\u5355\u8def\u5f84): \\(v_{i_1}, v_{i_2}, \\dots, v_{i_n}\\) are distinct.</li> <li>Cycle (\u73af): a simple path with \\(v_p=v_q\\).</li> </ul> </li> <li>Connection (\u8fde\u901a\u6027)<ul> <li>For an undirected graph,<ul> <li>Connected vertices (\u8fde\u901a\u70b9): there is a path from \\(v_i\\) to \\(v_j\\).</li> <li>Connected graph (\u8fde\u901a\u56fe): all pairs of distinct vertices are connected.</li> <li>(Connected) Component (\u8fde\u901a\u5206\u91cf): the maximum connected subgraph.</li> </ul> </li> <li>For a directed graph,<ul> <li>Strongly Connected (\u5f3a\u8fde\u901a): all pairs of distinct vertices are connected.</li> <li>Weakly Connected (\u5f31\u8fde\u901a):  the graph is connected without direction to the edges.</li> <li>Strongly Connected Component (\u5f3a\u8fde\u901a\u5206\u91cf): the maximum subgraph that is strongly connected.</li> </ul> </li> </ul> </li> <li>Biconnection (\u53cc\u8fde\u901a\u6027) for an undirected graph \\(G\\)<ul> <li>\\(v\\) is an articulation point / cut point (\u5272\u70b9) if \\(G'\\), which is \\(G\\) with vertex \\(v\\) deleted has at least 2 connected components.</li> <li>\\((v_i, v_j)\\) is an bridge / cut edge (\u6865 / \u5272\u8fb9) if \\(G'\\), which is \\(G\\) with edge \\((v_i, v_j)\\) deleted has at least 2 connected components.</li> <li>\\(G\\) is a biconnected graph (\u53cc\u8fde\u901a\u56fe) if \\(G\\) is connected and has no articulation points.</li> <li>A biconnected component (\u53cc\u8fde\u901a\u5206\u91cf) is a maximal biconnected subgraph.</li> </ul> </li> <li>A Tree: a graph that is connected and acyclic.</li> <li>A DAG (\u6709\u5411\u65e0\u73af\u56fe): a directed acyclic graph.</li> <li>Degree (\u5ea6): number of edges incident to \\(v\\).<ul> <li>For a directed \\(G\\), we have indegree (\u5165\u5ea6) as the edges to \\(v\\) and outdegree (\u51fa\u5ea6) as the edges from \\(v\\) </li> <li> <p>If there is a graph \\(G\\) with \\(n\\) vertices and \\(e\\) edges, then</p> \\[     e=\\frac12\\sum\\limits_{i=0}^{n-1}degree(v_i). \\] </li> </ul> </li> </ul>"},{"location":"Computer_Science_Courses/FDS/Graph/#representation","title":"Representation","text":"<p>NOTE that all vertices of a graph with \\(n\\) vertices are numbered from \\(0\\) to \\(n - 1\\).</p> <p>Weighted Edges In some cases, we want each edge has a weight \\(w\\). In particular, when weight isn't considered, it's same to treat all weight be \\(1\\).</p> Data Type<pre><code>typedef struct {\nint u; // start vector\nint v; // end vector\nint w; // weight\n} Edge;\n</code></pre>"},{"location":"Computer_Science_Courses/FDS/Graph/#adjacency-matrix","title":"Adjacency Matrix \u90bb\u63a5\u77e9\u9635","text":"<p>Use a matrix to represent the edges of a graph.</p> \\[ \\text{AdjacencyMatrix}[i][j] = \\left\\{ \\begin{aligned} &amp; 1 \\text{ or } \\text{weight}, &amp;&amp; \\text{if } (v_i,v_j) \\text{ or } &lt;v_i,v_j&gt;\\ \\in E(G), \\\\ &amp; 0, &amp;&amp; \\text{otherwise}. \\end{aligned} \\right. \\] Data Type<pre><code>typedef struct {\nint n; // number of vertices\nint m; // number of edges\nint **adjMat;\n} Graph;\n</code></pre> Adjacency Matrix of Digraph <pre><code>Graph *CreateGraph(const int n) {\nGraph *G = (Graph *)malloc(sizeof(Graph));\nG-&gt;n = n;\nG-&gt;m = 0;\nG-&gt;adjMat = (int **)malloc(G-&gt;n * sizeof(int *));\nG-&gt;adjMat[0] = (int *)malloc(G-&gt;n * G-&gt;n * sizeof(int));\nfor (int i = 1; i &lt; G-&gt;n; i ++) {\nG-&gt;adjMat[i] = G-&gt;adjMat[0] + i * G-&gt;n;\n}\nreturn G;\n}\n\nvoid AddEdge(Graph *G, const Edge e) {\nG-&gt;m ++;\nG-&gt;adjMat[e.u][e.v] = e.w;\n}\n\nvoid FreeGraph(Graph *G) {\nfree(G-&gt;adjMat[0]);\nfree(G-&gt;adjMat);\nfree(G);\n}\n</code></pre> <p>Pros</p> <ul> <li>Easy for implementation.</li> <li>Fast to reach.</li> </ul> <p>Cons</p> <ul> <li>Space complexity \\(O(n^2)\\), which is a waste when representing a sparse graph.</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Graph/#adjacency-lists","title":"Adjacency Lists \u90bb\u63a5\u94fe\u8868","text":"<p>Replace each row of the adjacency matrix by a linked list. The order of vertices in each list doesn't matter.</p> Data Type<pre><code>typedef struct _node {\nint to;\nint w;\nstruct _node *next;\n} Node;\n\ntypedef struct {\nint size;\nNode *head;\n} List;\n\ntypedef struct {\nint n;\nint m;\nList **adjList;\n} Graph;\n</code></pre> Adjacency Lists of Digraph <pre><code>void Insert(List *list, const int to, const int w) {\nNode *p = (Node *)malloc(sizeof(Node));\np-&gt;to = to;\np-&gt;w = w;\np-&gt;next = list-&gt;head-&gt;next;\nlist-&gt;head-&gt;next = p;\n}\n\nGraph *CreateGraph(const int n) {\nGraph *G = (Graph *)malloc(sizeof(Graph));\nG-&gt;n = n;\nG-&gt;m = 0;\nG-&gt;adjList = (List **)malloc(G-&gt;n * sizeof(List *));\nfor (int i = 0; i &lt; G-&gt;n; i ++) {\nG-&gt;adjList[i] = (List *)malloc(sizeof(List));\nG-&gt;adjList[i]-&gt;size = 0;\nG-&gt;adjList[i]-&gt;head = (Node *)malloc(sizeof(Node));\nG-&gt;adjList[i]-&gt;head-&gt;next = NULL;\n}\nreturn G;\n}\n\nvoid AddEdge(Graph *G, const Edge e) {\nG-&gt;m ++;\nInsert(G-&gt;adjList[e.u], e.v, e.w);\n}\n\nvoid FreeGraph(Graph *G) {\nfor (int i = 0; i &lt; G-&gt;n; i ++) {\nfor (Node *p = G-&gt;adjList[i]-&gt;head, *temp = NULL; p != NULL;) {\ntemp = p;\np = p-&gt;next;\nfree(temp);\n}\nfree(G-&gt;adjList[i]);\n}\nfree(G-&gt;adjList);\nfree(G);\n}\n</code></pre> <p>Pros</p> <ul> <li>Save space.</li> </ul> <p>Cons</p> <ul> <li>Complicated for implementation.</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Graph/#adjacency-multilists","title":"Adjacency Multilists \u94fe\u5f0f\u524d\u5411\u661f","text":"<p>Use the edges as nodes of multilists, and represent it by something similar with a static list.</p> Data Type<pre><code>typedef struct {\nint to;\nint w;\nint next;\n} EdgeNode;\n\ntypedef struct {\nint n;\nint m;\nEdgeNode *edge;\nint *head;\n} Graph; </code></pre> Adjacency Multilists <pre><code>Graph *CreateGraph(const int n, const int m) { // (1)!\nGraph *G = (Graph *)malloc(sizeof(Graph));\nG-&gt;n = n;\nG-&gt;m = m;\nG-&gt;edge = (EdgeNode *)malloc(G-&gt;m * sizeof(EdgeNode));\nG-&gt;head = (int *)malloc(G-&gt;n * sizeof(int));\nmemset(G-&gt;edge, 0, G-&gt;m * sizeof(EdgeNode));\nfor (int i = 0; i &lt; G-&gt;n; ++ i) {\nG-&gt;head[i] = -1;\n}\nG-&gt;m = 0;\nreturn G;\n}\n\nvoid AddEdge(Graph *G, const Edge e) {\nG-&gt;edge[G-&gt;m].to = e.v;\nG-&gt;edge[G-&gt;m].w = e.w;\nG-&gt;edge[G-&gt;m].next = G-&gt;head[e.u];\nG-&gt;head[e.u] = G-&gt;m;\nG-&gt;m ++;\n}\n\nvoid FreeGraph(Graph *G) {\nfree(G-&gt;edge);\nfree(G-&gt;head);\nfree(G);\n}\n</code></pre> <ol> <li>Note that for undirected graph, <code>m</code> should be pass by <code>2 * m</code>.</li> </ol> <p>Example: How Adjacency Multilists Build</p> <p>  for the left given graph, the corresponding adjancency multilists is the right one  </p>"},{"location":"Computer_Science_Courses/FDS/Graph/#topological-sort","title":"Topological Sort","text":"<p>Definition</p> <p>AOV Network (Activity On Vertex Network) is a digraph \\(G\\) in which \\(V(G)\\) represents activities and \\(E(G)\\) represents precedence relations between activities (vertices).</p> <ul> <li>\\(i\\) is a predecessor of \\(j\\) if there is a path from \\(i\\) to \\(j\\),  </li> <li>\\(i\\) is an immediate predecessor of j if \\(&lt;i,j&gt; \\in E(G)\\), and \\(j\\) is called a successor(immediate sucessor) of \\(i\\).     If we can finish all activities in AOV Network in any order without finishing one before its predecessor finished, the AOV Network is feasible, else it is unfeasible </li> </ul> <p>Patial order is a precedence relation which is both transitive and irrelative.</p> <p>Proposition</p> <p>A project is feasible if it's irreflexive. Otherwise \\(i\\) is a predecessor of \\(i\\), which means that \\(i\\) must be done before \\(i\\) starts.</p> <p>Feasible AOV network must be a DAG.</p> <p>Definition | Topological Order</p> <p>A topological order is a linear ordering of the vertices of a graph such that, for any two vertices \\(i\\) and \\(j\\), if \\(i\\) is a predecessor of \\(j\\) in the network then \\(i\\) precedes \\(j\\) in the linear ordering.</p> Topological Sort <pre><code>void FindIndegree(const Graph *G, int indegree[]) {\nmemset(indegree, 0, G-&gt;n * sizeof(int));\nfor (int i = 0; i &lt; G-&gt;n; i ++) {\nfor (int j = G-&gt;head[i]; j != -1; j = G-&gt;edge[j].next) {\nindegree[G-&gt;edge[j].to] ++;\n}\n}\n}\n\nvoid TopologicalSort(const Graph *G, int topNum[]) { // (1)!\nQueue *queue = CreateQueue(G-&gt;m);\nint indegree[G-&gt;n];\nFindIndegree(G, indegree);\nfor (int i = 0; i &lt; G-&gt;n; i ++) {\nif (indegree[i] == 0) {\nEnqueue(queue, i);\n}\n}\nint cnt = 0;\nwhile (!IsEmptyQueue(queue)) {\nint now = Dequeue(queue);\ntopNum[cnt ++] = now;\nfor (int idx = G-&gt;head[now]; idx != -1; idx = G-&gt;edge[idx].next) { //(2)!\nint to = G-&gt;edge[idx].to;\nindegree[to] --;\nif (indegree[to] == 0) {\nEnqueue(queue, to);\n}\n}\n}\nif (cnt != G-&gt;n) {\nputs(\"Graph has a cycle\");\n}\n}\n</code></pre> <p>1.<code>topNum[]</code> stores the result of topological sort. 2. The graph is represented by adjacent multilists. Other representations are also available.</p> <p>Complexity</p> <p>Suppose \\(E\\) is the number of edges and \\(V\\) is the number of vertices, then</p> <ul> <li>Time complexity \\(O(E + V)\\).</li> <li>Space complexity \\(O(V)\\).</li> </ul> <p>Note</p> <ul> <li>Topological sort can be used to detect whether the graph has a cycle, and it can also be used to detect whether the graph is a chain.</li> <li>Topological sort is not unique.</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Graph/#shortest-paths","title":"Shortest Paths","text":"<p>Definition</p> <p>Given a digraph \\(G=(V,E)\\), and a cost function \\(c(e)\\) for \\(e\\in E(G)\\), the length of a path \\(P\\) (also called weighted path length) from source to destination is \\(\\sum\\limits_{e_i\\subset P}\\ w(e_i)\\).</p> <p>The result of the following algorithms are stored in the following data type.</p> Data Type<pre><code>typedef struct {\nint dist;\nbool vis;\nint prev;\n} Path;\n</code></pre> <ul> <li><code>dist</code> is the distance from <code>source</code> to <code>v_i</code>.</li> <li><code>vis</code> is set to <code>false</code> if <code>v_i</code> is visited or <code>true</code> if not.</li> <li><code>prev</code> record the previous vertex along the path. We can trace back until <code>prev = -1</code> to obtain the path from <code>source</code> to <code>v_i</code>.</li> </ul> Trace Back of Vertex <code>v</code> <pre><code>int now = v;\nwhile (now != -1) {\nprintf(\"%d \", now);\nnow = path[now].prev;\n}\n</code></pre>"},{"location":"Computer_Science_Courses/FDS/Graph/#unweight-shortest-paths","title":"Unweight Shortest Paths","text":"<p>It is implemented by a breadth-first search (BFS) algorithm, which used to find how many vertices from source to destination.</p> Unweighted Shortest Paths <pre><code>#define INF 0x3f3f3f\n\nvoid BFS(const Graph *G, const int source, Path path[]) {\nfor (int i = 0; i &lt; G-&gt;n; i ++) {\npath[i].dist = INF;\npath[i].vis = false;\npath[i].prev = -1;\n}\npath[source].dist = 0;\n\nQueue *queue = CreateQueue(G-&gt;n);\nEnqueue(queue, source);\nwhile (!IsEmptyQueue(queue)) {\nint now = Dequeue(queue);\npath[now].vis = true;\nfor (int idx = G-&gt;head[now]; idx != -1; idx = G-&gt;edge[idx].next) { //(1)!\nint to = G-&gt;edge[idx].to;\nif (!path[to].vis) {\npath[to].dist = path[now].dist + 1;\npath[to].prev = now;\nEnqueue(queue, to);\n}\n}\n}\nFreeQueue(queue);\n\nreturn path;\n}\n</code></pre> <ol> <li>The graph is represented by adjacent multilists. Other representations are also available.</li> </ol> Reason to Choose <code>INF</code> to be <code>0x3f3f3f</code> <ul> <li>We can use <code>memset(a, 0x3f, n * sizeof(int))</code> to initialze.</li> <li>Add two infinities will still result in an infinity.</li> </ul> <p>Complexity</p> <ul> <li>Time complexity \\(O(E + V)\\).</li> <li>Space comlexity \\(O(E)\\).</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Graph/#weighted-shortest-paths","title":"Weighted Shortest Paths","text":""},{"location":"Computer_Science_Courses/FDS/Graph/#floyd","title":"Floyd","text":"<p>Remains</p>"},{"location":"Computer_Science_Courses/FDS/Graph/#dijkstra","title":"Dijkstra","text":"<p>Description of Dijkstra</p> <p>Let \\(S = \\{ s \\text{ and } v_i \\text{'s whose shortest paths have been found}\\}\\).  For any \\(u \\notin S\\), define</p> \\[     \\text{distance}[u] = \\text{minimal length of path} \\{ s \\rightarrow (v_i\\in S)\\rightarrow u \\}. \\] <p>If the paths are generated in non-decreasing order, then  </p> <ol> <li>The shortest path must go through ONLY \\(v_i \\in S\\)</li> <li>(Greedy Method) \\(u\\) is chosen so that \\(\\text{distance}[u] = \\min\\{w \\notin S | \\text{distance}[w]\\}\\) (If \\(u\\) is not unique, then we can choose any of them).</li> <li> <p>If \\(\\text{distance} [u_1] &lt; \\text{distance} [u_2]\\) and we add \\(u_1\\) into \\(S\\), then distance\\([u_2]\\) may change. If so, a shorter path from \\(s\\) to \\(u_2\\) must go through \\(u_1\\) and</p> \\[     \\text{distance}'[u_2] = \\text{distance} [u_1] + \\text{length}(&lt;u_1,u_2&gt;). \\] </li> </ol> <p>Note</p> <p>For finding \\(u\\), we can maintain a priority queue, which can decrease time complexity of finding \\(u\\) from \\(O(V)\\) to \\(O(\\log V)\\).</p> Dijkstra's Algorithm SimpleImproved by Priority Queue <pre><code>void Dijkstra(const Graph *G, const int source, Path path[]) {\nfor (int i = 0; i &lt; G-&gt;n; i ++) {\npath[i].dist = INF;\npath[i].vis = false;\npath[i].prev = -1;\n}\npath[source].dist = 0;\n\nint now = source;\nwhile (!path[now].vis) {\npath[now].vis = true;\n\nfor (int idx = G-&gt;head[now]; idx != -1; idx = G-&gt;edge[idx].next) {\nint to = G-&gt;edge[idx].to;\nint cost = G-&gt;edge[idx].dis;\nif (path[to].dist &gt; path[now].dist + cost) {\npath[to].dist = path[now].dist + cost;\npath[to].prev = now;\n}\n}\n\nint distMin = INF;\nfor (int i = 0; i &lt; G-&gt;n; i ++) {\nif (!path[i].vis &amp;&amp; path[i].dist &lt; distMin) {\ndistMin = path[i].dist;\nnow = i;\n}\n}\n}\n}\n</code></pre> <pre><code>typedef struct {\nint num;\nint dis;\n} Node;\n\n#define MinData -1\ntypedef Node ElementType;\ntypedef struct {\nElementType *element;\nint capacity;\nint size;\n} PriorityQueue;\n\n...\n\nvoid Dijkstra(const Graph *G, const int source, Path path[]) {\nfor (int i = 0; i &lt; G-&gt;n; i ++) {\npath[i].dist = INF;\npath[i].vis = false;\npath[i].prev = -1;\n}\npath[source].dist = 0;\n\nPriorityQueue *Q = Initialize(G-&gt;m);\nInsert(Q, (Node){source, 0});\n\nwhile (!IsEmptyQueue(Q)) {\nNode cur = DeleteMin(Q);\nint now = cur.num;\n\nif (path[now].vis) {\ncontinue;\n}\npath[now].vis = true;\n\nfor (int idx = G-&gt;head[now]; idx != -1; idx = G-&gt;edge[idx].next) {\nint to = G-&gt;edge[idx].to;\nint cost = G-&gt;edge[idx].dis;\nif (path[to].dist &gt; path[now].dist + cost) {\npath[to].dist = path[now].dist + cost;\npath[to].prev = now;\nif (!path[to].vis) {\nInsert(Q, (Node){to, path[to].dist});\n}\n}\n}\n}\n}\n</code></pre> <p>Complexity</p> <ul> <li>Time complexity<ul> <li>Simple \\(O(E + V^2)\\).</li> <li>Improved \\(O((E + V) \\log V)\\).</li> </ul> </li> <li>Space comlexity<ul> <li>Simple \\(O(1)\\).</li> <li>Improved \\(O(E)\\).</li> </ul> </li> </ul> <p>Warning</p> <p>By allowing the used vertices being pushed into the set \\(S\\) again, we can get a algorithm that deals with negative edges well with time complexity \\(T = O(E \\cdot V)\\). However, if the graph has a negative-weight cycle, it will cause infinite loop.</p>"},{"location":"Computer_Science_Courses/FDS/Graph/#spfa","title":"SPFA","text":"<p>Remains</p>"},{"location":"Computer_Science_Courses/FDS/Graph/#acyclic-graph","title":"Acyclic Graph","text":"<p>If the graph is acyclic, vertices may be selected in topological order since when a vertex is selected, its distance can no longer be lowered without any incoming edges from unknown nodes</p> <p>\\(T=O(|E|+|V|)\\) and no priority queue is needed</p>"},{"location":"Computer_Science_Courses/FDS/Graph/#all-pairs-shortest-path-problem","title":"All-Pairs Shortest Path Problem","text":"<ul> <li>Use single-source algorithm for \\(V\\) times with \\(T=O(V^3)\\) - works fast on sparse graph.</li> </ul> Remains <ul> <li>Another \\(O(|V|^3)\\) algorithm in book's Chapter 10 - works faster on dense graphs</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Graph/#network-flow-problems","title":"Network Flow Problems","text":"<p>Definition | Network</p> <p>A network is a digraph \\(G(V, E)\\). Each edge \\((u, v) \\in E(G)\\) has a weight \\(c(u, v)\\) called capacity. In particular, \\(\\forall\\ (u, v) \\notin E(G)\\), \\(c(u, v) = 0\\). Moreover, there are two special points, source (\u6e90\u70b9) \\(s\\) and sink (\u6c47\u70b9) \\(t\\) (\\(s \\ne t\\)).</p> <p>Definition | Flow</p> <p>If \\(f(u, v)\\) is a function that satisfies</p> <ul> <li>Capacity Restriction: \\(f(u, v) \\le c(u, v)\\).</li> <li>Skew Symmetry: \\(f(u, v) = -f(v, u)\\).</li> <li> <p>Conservation of Flow: sum of outflows of source equals to sum of inflows of sink.</p> \\[     \\forall\\ x \\in V - {s, t},\\ \\ \\sum\\limits_{(u, x) \\in E} f(u, x) = \\sum\\limits_{(x, v) \\in E} f(x, v). \\] </li> </ul>"},{"location":"Computer_Science_Courses/FDS/Graph/#maximum-flow","title":"Maximum Flow","text":"<p>Maximum flow probelm want to determine the maximum amount of flow that can pass from source \\(s\\) to sink \\(t\\).</p> <p>We introduce Ford-Fulkerson Augmenting Path Algorithm here.</p> <p>Definition</p> <p>Residual capacity \\(c_f(u, v)\\) is defined by the subtraction between the capacity of the edge and the flow, namely \\(c_f(u, v) = c(u, v) - f(u, v)\\).</p> <p>A residual network \\(G_f\\) is the graph consists of all vertices of \\(G\\) and all edges that residual capacity is larger than \\(0\\), namely \\(G_f = (V_f = V, E_f = \\{(u, v) \\in E | c_f(u, v) &gt; 0\\})\\).</p> <p>An augmenting Path (\u589e\u5e7f\u8def) is a path from \\(s\\) to \\(t\\), with each edge using the minimum residual capacity of edges in the path.</p> <p>Proposition</p> <p>If the edge capabilities are rational numbers, this algorithm always terminate with a maximum flow.</p> <p>Note</p> <p>The algorithm works for \\(G\\) with cycles as well.</p> <p>Basic Idea</p> <ol> <li>Find an augmenting path from \\(s\\) to \\(t\\).</li> <li>Augment it by<ul> <li>subtracting minimum residual capacity along the augmenting path.</li> <li>adding minimum residual capacity along the inverse augmenting path.</li> </ul> </li> </ol> <p>Example</p> <p> </p> <p> </p> <p> </p> Description <ol> <li>\\(G_r\\) is initialized to be the same as \\(G\\) to represent the selected flows. \\(G_f\\) is just a assistant graph of selected augmenting path.</li> <li>First, we find the blue augmenting path from \\(s\\) to \\(t\\) in \\(G_f\\). The minimum residual capacity along tha path is \\(3\\). Then we subtract \\(3\\) along the path, and add \\(3\\) along the inverse path.</li> <li>Second, we find the purple augmenting path from \\(s\\) to \\(t\\) in \\(G_f\\). The minimum residual capacity along tha path is \\(2\\). Then we subtract \\(2\\) along the path, and add \\(2\\) along the inverse path.</li> <li>Since there is no outflows of \\(s\\) in \\(G_f\\), we can't find any augementing path any more. Thus it's the result case.</li> </ol> Ford-Fulkerson Augmenting Path Algorithm DFSEdomnds-Karp / EK (BFS) <pre><code>int dfs(const int now, const int prevCapacity, Graph *G, bool vis[], const int s, const int t) {\nif (now == t) {\nreturn prevCapacity;\n}\n\nvis[now] = true;\nfor (int idx = G-&gt;head[now]; idx; idx = G-&gt;edge[idx].next) {\nint to = G-&gt;edge[idx].to;\nint capacity = G-&gt;edge[idx].capacity;\n\nif (capacity == 0 || vis[to] == true) {\ncontinue;\n}\n\nint cost = dfs(to, min(capacity, prevCapacity), vis, s, t, G);\nif (cost != -1) {\nG-&gt;edge[idx].capacity -= cost;\nG-&gt;edge[idx ^ 1].capacity += cost;\nreturn cost;\n}\n}\nreturn -1;\n}\n\nint FF(Graph *G, const int s, const int t) {\nint ret = 0, cost;\nbool vis[G-&gt;n];\n\nmemset(vis, false, G-&gt;n * sizeof(bool));\nwhile ((cost = dfs(s, 0x3f3f3f, vis, s, t, G)) != -1) {\nmemset(vis, false, G-&gt;n * sizeof(bool));\nret += cost;\n}\nreturn ret;\n}\n</code></pre> <pre><code>\n</code></pre> <p>Remains</p> <p>Complexity</p> <p>Suppose all capacity is integers, we have serval methods to find an augmenting path.</p> <ul> <li> <p>Choose the augmenting path found by an unweighted shortest path algorithm.</p> <ul> <li> <p>Time complexity</p> \\[ \\begin{aligned}     T &amp;= T_{\\text{augmentation}} \\cdot T_{\\text{find a path}} \\\\       &amp;= O(f) \\cdot O(E) \\\\       &amp;= O(f\\cdot E),\\ \\ \\text{ where $f$ is the maximum flow.} \\end{aligned} \\] </li> </ul> </li> <li> <p>Choose the augmenting path that allows the largest increase in flow found by Dijkstra's algorithm.</p> <ul> <li> <p>Time complextiy</p> \\[ \\begin{aligned}     T &amp;= T_{\\text{augmentation}} \\cdot T_{\\text{find a path}} \\\\       &amp;= O(E\\log cap_{\\text{max}}) \\cdot O(E\\log V) \\\\       &amp;= O(E^2\\log V),\\ \\ \\text{if $cap_{\\text{max}} = \\max\\{c(u, v)\\}$ is small}. \\end{aligned} \\] </li> </ul> </li> <li> <p>Choose the augmenting path that has the least number of edges found by unweighted shortest path algorithm.</p> <ul> <li> <p>Time complexity</p> \\[ \\begin{aligned}     T &amp;= T_{augmentation} \\cdot T_{find a path} \\\\       &amp;= O(E) \\cdot O(E \\cdot V) \\\\       &amp;= O(E^2V). \\end{aligned} \\] </li> </ul> </li> </ul> <p>Proposition</p> <p>If every \\(v\\notin {s,t}\\) has either a single incoming edge of capacity \\(1\\) oe a single outgoing edge of capacity \\(1\\), then time bound is reduced to \\(O(E V^{1/2})\\).</p>"},{"location":"Computer_Science_Courses/FDS/Graph/#min-cost-flow","title":"Min Cost Flow","text":"<p>The min-cost flow problem want to find among all maximum flows, the one flow of minimum cost provided that each edge has a cost per unit of flow.</p> <p>Remains</p>"},{"location":"Computer_Science_Courses/FDS/Graph/#minimum-spanning-tree-mst","title":"Minimum Spanning Tree (MST)","text":"<p>Definition</p> <p>A spanning tree of a graph \\(G\\) is a tree which consists of \\(V(G)\\) and a subset of \\(E(G)\\).</p> <p>A minimum spanning tree (MST) is a spanning tree that the total cost of its edges is minimized.</p> <p>Note</p> <ul> <li>MST is a tree since it is acyclic, and thus the number of edges is \\(V-1\\). It is spanning because it covers every vertex.</li> <li>A minimum spanning tree exists iff \\(G\\) is connected (in most case it's undirected).</li> <li>Adding a non-tree edge to a spanning tree, it becomes a cycle.</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Graph/#kruskals-algorithm","title":"Kruskal's Algorithm","text":"<p>It maintains a forest and is implemented by Disjoint Set. Basic idea is adding one edge at one time in an non-decreasing order and it's a Greedy Method.</p> <p>Results of Kruskal are stored in the following data type.</p> Data Type<pre><code>typedef struct {\nint n;          // number of vertices in MST.\nint *vertex;    // vertices in MST.\nint edgeSum;    // sum of the weight of edges in MST.\n} Point;\n</code></pre> Kruskal's Algorithm <pre><code>int cmp(const void *a, const void *b) {\nreturn ((Edge *)a)-&gt;w - ((Edge *)b)-&gt;w;\n}\n\nint Find(int *S, const int a) {\nif (S[a] &lt; 0) {\nreturn a;\n}\nreturn S[a] = Find(S, S[a]);\n}\n\nvoid Union(int *S, const int root_a, const int root_b) {\nif (root_a &lt; root_b) {\nS[root_a] += S[root_b];\nS[root_b] = root_a;\n} else {\nS[root_b] += S[root_a];\nS[root_a] = root_b;\n}\n}\n\nvoid Kruskal(Graph *G, Point *point) {\nEdge edge[G-&gt;m];\nfor (int i = 0, cnt = 0; i &lt; G-&gt;n; i ++) {\nfor (int idx = G-&gt;head[i]; idx != -1; idx = G-&gt;edge[idx].next) {\nedge[cnt ++] = (Edge){i, G-&gt;edge[idx].to, G-&gt;edge[idx].w};\n}\n}\nqsort(edge, G-&gt;m, sizeof(Edge), cmp);\n\nint *S = (int *)malloc(G-&gt;n * sizeof(int));\nfor (int i = 0; i &lt; G-&gt;n; ++ i) {\nS[i] = -1;\n}\n\nint cnt = 0, vertex[G-&gt;n], sum = 0;\nfor (int i = 0; i &lt; G-&gt;m; ++ i) {\nint root_a = Find(S, edge[i].u);\nint root_b = Find(S, edge[i].v);\n\nif (root_a == root_b) {\ncontinue;\n}\n\nUnion(S, root_a, root_b);\nvertex[cnt ++] = i;\nsum += edge[i].w;\n}\n\npoint-&gt;n = cnt;\npoint-&gt;vertex = (int *)malloc(point-&gt;n * sizeof(int));\nmemcpy(point-&gt;vertex, vertex, cnt * sizeof(int));\npoint-&gt;edgeSum = sum;\n\nfree(S);\n}\n</code></pre> <p>Complexity</p> <ul> <li>Time complexity \\(O(E\\log E)\\) (sorting algorithm of \\(O(E \\log E)\\), and union-find of \\(O(E \\alpha(V))\\) or \\(O(E \\log V)\\)).</li> <li>Space complexity \\(O(V)\\).</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Graph/#prims-algorithm","title":"Prim's Algorithm","text":"<p>Basic idea is adding one vertex at one time to grow a tree, very similar to Dijkstra's algorithm. \\(T=O(|E|\\log|V|)\\).</p> Prim's Algorithm SimpleImprove by Priority Queue <pre><code>void Prim(Graph *G, Point *point) {\nint dist[G-&gt;n], prev[G-&gt;n];\nbool vis[G-&gt;n];\n\nmemset(vis, false, sizeof(vis));\nmemset(dist, 0x3f, sizeof(dist));\n\nint cnt = 0, vertex[G-&gt;n], sum = 0;\n\nint now = 0;\ndist[now] = 0;\nfor (int r = 0; r &lt; G-&gt;n; r ++) {\nvis[now] = true;\nif (r != 0) {\nsum += G-&gt;adjMat[prev[now]][now];\n}\nvertex[cnt ++] = now;\n\nfor (int i = 0; i &lt; G-&gt;n; i ++) {\nif (!vis[i] &amp;&amp; G-&gt;adjMat[now][i] &lt; dist[i]) {\ndist[i] = G-&gt;adjMat[now][i];\nprev[i] = now;\n}\n}\n\nint minDist = 0x3f3f3f;\nfor (int i = 0; i &lt; G-&gt;n; i ++) {\nif (!vis[i] &amp;&amp; dist[i] &lt; minDist) {\nnow = i;\nminDist = dist[i];\n}\n}\n}\n\npoint-&gt;n = cnt;\npoint-&gt;vertex = (int *)malloc(cnt * sizeof(int));\nmemcpy(point-&gt;vertex, vertex, cnt * sizeof(int));\npoint-&gt;edgeSum = sum;\n}\n</code></pre> <pre><code>\n</code></pre> <p>Complexity</p> <ul> <li>Time complexity<ul> <li>Simple \\(O(E + V^2)\\).</li> <li>Improved \\(O((E + V) \\log V)\\).</li> </ul> </li> <li>Space complexity \\(O(V)\\).</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Graph/#depth-first-search","title":"Depth-First Search","text":"<p>DFS can be regarded as a generalization of preorder traversal. DFS of a graph can generate a spanning tree, called DFS spanning tree.</p> <pre><code>void DFS(Vertex V) {   vis[V] = true;  /* mark this vertex to avoid cycles */\nfor (each W adjacent to V) {\nif (!vis[W]) {\nDFS(W);\n}\n}\n}\n</code></pre> <p>Complexity</p> <ul> <li>Time complexity \\(O(E + V)\\).</li> <li>Space complexity \\(O(1)\\).</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Graph/#tarjan-algorithm","title":"Tarjan Algorithm","text":"<p>Tarjan algorithm is used to find strongly connected components as well as cut point.</p> <p>Define \\(\\text{Low}(u)\\) by</p> \\[ \\begin{aligned} \\text{Low}(u) = \\min\\{ &amp; \\text{Index}(u),              \\\\ &amp; \\min\\{\\text{Low}(w) | w \\text{ is a child of } u\\},              \\\\ &amp; \\min\\{\\text{Index}(w) | (u, w) \\text{ is a back edge}\\}. \\end{aligned} \\]"},{"location":"Computer_Science_Courses/FDS/Graph/#strongly-connected-components-scc","title":"Strongly Connected Components (SCC)","text":"<p>For finding SCC, tarjan algorithm maintains a DFS spanning tree and a stack.</p> Tarjan Algorithm for Strongly Connected Components <pre><code>void Tarjan(const int now, const Graph *G, int dfn[], int low[], Stack *stack, bool inStack[]) {\nstatic int idx = 0;\n\ndfn[now] = low[now] = ++ idx;\ninStack[now] = true;\nPush(stack, now);\n\nfor (int i = G-&gt;head[now]; i != -1; i = G-&gt;edge[i].next) {\nint to = G-&gt;edge[i].to;\nif (dfn[to] == 0) {\nTarjan(to, G, dfn, low, stack, inStack);\nlow[now] = min(low[now], low[to]);\n} else {\nif (inStack[to]) {\nlow[now] = min(low[now], dfn[to]);\n}\n}\n}\n\nif (dfn[now] == low[now]) {\nint temp;\ndo {\ntemp = Pop(stack);\ninStack[temp] = false;\nprintf(\"%d\", temp)\n} while (now != temp);\nputchar('\\n');\n}\n}\n\nvoid FindStronglyConnectedComponents(const Graph *G) {\nint dfn[G-&gt;n], low[G-&gt;n];\nmemset(dfn, 0, G-&gt;n * sizeof(int));\nmemset(low, 0, G-&gt;n * sizeof(int));\n\nStack *stack = CreateStack(G-&gt;n);\nbool inStack[G-&gt;n];\nmemset(inStack, false, G-&gt;n * sizeof(bool));\n\nfor (int i = 0; i &lt; G-&gt;n; i ++) {\nif (dfn[i] == 0) {\nTarjan(i, G, dfn, low, stack, inStack);\n}\n}\n}\n</code></pre>"},{"location":"Computer_Science_Courses/FDS/Graph/#cut-point","title":"Cut Point","text":"<p>\\(u\\) is an articulation point if</p> <ul> <li>\\(u\\) is the root and has at least 2 children.</li> <li>\\(u\\) is not the root, and has at least 1 child such that \\(\\text{Low}(child) \\ge \\text{Index}(u)\\).</li> </ul> <p>Example</p> <p>Suppose DFS spanning tree starts from vertex \\(3\\). Blue numbers are indices of searching order stored in <code>dfn[]</code> and red points are the cut points.</p> <p>  Tarjan for Cut Point  </p> Tarjan Algorithm for Articulation Point / Cut Point <pre><code>void Tarjan(const int now, const Graph* G, const int root, int dfn[], int low[], bool isCutPoint[]) {\nstatic int idx = 0;\n\nint child = 0;\ndfn[now] = low[now] = ++ idx;\n\nfor (int i = G-&gt;head[now]; i != -1; i = G-&gt;edge[i].next) {\nint to = G-&gt;edge[i].to;\nif (dfn[to] == 0) {\nchild ++;\nTarjan(to, G, root, dfn, low, isCutPoint);\nlow[now] = min(low[now], low[to]);\n\nif (dfn[now] &lt;= low[to] &amp;&amp; now != root) {\nisCutPoint[now] = true;\n}\n}\nelse {\nlow[now] = min(low[now], dfn[to]);\n}\n}\n\nif (child &gt; 1 &amp;&amp; now == root) {\nisCutPoint[now] = true;\n}\n}\n\nvoid FindCutPoint(const Graph *G, bool isCutPoint[]) {\nint dfn[G-&gt;n], low[G-&gt;n];\n\nmemset(dfn, 0, G-&gt;n * sizeof(int));\nmemset(low, 0, G-&gt;n * sizeof(int));\n\nmemset(isCutPoint, false, G-&gt;n * sizeof(bool));\nfor (int i = 0; i &lt; G-&gt;n; i ++) {\nif (dfn[i] == 0) {\nTarjan(i, G, i, dfn, low, isCutPoint);\n}\n}\n}\n</code></pre>"},{"location":"Computer_Science_Courses/FDS/Graph/#euler-paths-and-circuits","title":"Euler Paths and Circuits","text":"<p>Definition</p> <p>Euler tour / Path is a path go through all edges.</p> <p>Euler circuit is a path go through all edges and finish at the starting point.</p> <p>Theorem</p> <p>An Euler circuit is possible only if the graph is connected and each vertex has an even degree  </p> <p>An Euler tour is possible if there are exactly two vertices having odd degree. One must start at one of the odd-degree vertices.</p> <p>Note</p> <ul> <li>The path should be maintained as a linked list</li> <li>For each adjacency list, maintain a pointer to the last edge scanned</li> <li>\\(T=O(|E|+|V|)\\)</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Graph/#hamilton-paths-and-circuits","title":"Hamilton Paths and Circuits","text":"<p>Definition</p> <p>Hamilton tour / Path is a path go through all vertices.</p> <p>Hamilton circuit is a path go through all vertices and finish at the starting point.</p>"},{"location":"Computer_Science_Courses/FDS/Hashing/","title":"Hashing","text":"<p>Hash table (\u6563\u5217\u8868, \u54c8\u5e0c\u8868), or say Symbol table (Dictionary) is an ADT that supports insertions, deletions and finds in constant average time.</p> <p>ADT</p> <p>Objects: A set of key-value pairs, where keys are unique.</p> <p>Operations:</p> <ul> <li>Create a hash table.</li> <li>Check whether a key is already in a hash table.</li> <li>Find the corresponding value of a key in a hash table.</li> <li>Insert a new key-value pair into a hash table.</li> <li>Delete a key and its corresponding value in a hash table.</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Hashing/#hash-function","title":"Hash Function","text":"<p>A hash function \\(f\\) is a function used for building hash table by</p> \\[     hash(key) = value. \\] <p>But sometimes we have \\(key_1 \\ne key_2\\), but \\(hash(key_1) = hash(key_2)\\), we called this a collision. </p> <p>Property</p> <ul> <li>\\(hash(x)\\) is easy to compute and minimize collisions.</li> <li>\\(hash(x)\\) is supposed to be unbiased, or say uniform. That is for any \\(x\\) and \\(i\\), we want that</li> </ul> \\[     P\\{hash(x) = i\\} = \\frac1b,\\ \\ \\text{ where $b$ is some number}. \\] <p>\u00a0\u00a0\u00a0\u00a0 Such kind of a hash function is called a uniform hash function.</p> <p>Common Choice of Hash Function</p> <ul> <li>Remainder. \\(N\\) is the size of hash table, often a prime number.</li> </ul> \\[     hash(x) = x \\text{ mod } N. \\] <ul> <li> <p>Middle Square. Take middle some digits of the square of the key as the address of hash function.</p> </li> <li> <p>Random.</p> </li> </ul>"},{"location":"Computer_Science_Courses/FDS/Hashing/#hash-table","title":"Hash Table","text":"<p>Definition</p> <ul> <li>\\(T\\) is the total number of distinct possible value for \\(x\\).</li> <li>\\(n\\) is the total number of identifers in hash table.</li> <li>Identifier Density \\(d = \\dfrac{n}{T}\\).</li> <li>Loading Density \\(\\lambda = \\dfrac{n}{sb}\\).</li> </ul> <p>An overflow occur when a new identifier are hashed into a full bucket.</p>"},{"location":"Computer_Science_Courses/FDS/Hashing/#collision-resolution","title":"Collision Resolution","text":"<p>Although we try many methods to make a hash function to avoid collision. But it's unfortunately unavoidable. The most important issue of hash table is to deal with collision. We have some strategy for it.</p> <p>Suppose the hash function we use below are all</p> \\[     hash(x) = x \\text{ mod } N,\\ \\ N \\in \\mathbb{P}. \\]"},{"location":"Computer_Science_Courses/FDS/Hashing/#separate-chaining","title":"Separate Chaining \u62c9\u94fe\u6cd5","text":"<p>Separate chaining keeps a list of all keys that hash to the same value.</p> Data Type<pre><code>typedef struct {\nint key;\nint value;\n} Pair;\ntypedef Pair ElementType;\ntypedef struct _node {\nElementType *element;\nstruct _node *next;\n} Node;\ntypedef struct {\nint size;\nNode *head;\n} List;\n\ntypedef struct {\nint tableSize;\nList **list;\nint (*Hash)(const int key, const int N);\n} HashTable;\n</code></pre> Separate Chaining <pre><code>static bool IsPrime(const int x) {\nbool ret = true;\nfor (int i = 2, temp = sqrt(x); i &lt;= temp; i ++) {\nif (x % i == 0) {\nret = false;\nbreak;\n}\n}\nreturn ret;\n}\n\nstatic int NextPrime(int x) {\nwhile (!IsPrime(x)) {\nx ++;\n}\nreturn x;\n}\n\nstatic int Hash(const int key, const int N) {\nreturn key % N;\n}\n\nHashTable *CreateHashTable(const int tableSize) {\nHashTable *H = (HashTable *)malloc(sizeof(HashTable));\nH-&gt;tableSize = NextPrime(tableSize); // (1)!\nH-&gt;list = (List **)malloc(H-&gt;tableSize * sizeof(List *));\nfor (int i = 0; i &lt; H-&gt;tableSize; i ++) {\nH-&gt;list[i] = (List *)malloc(sizeof(List));\nH-&gt;list[i]-&gt;head = (Node *)malloc(sizeof(Node)); // (2)!\nH-&gt;list[i]-&gt;head-&gt;next = NULL;\n}\nH-&gt;Hash = Hash;\nreturn H;\n}\n\nvoid FreeHashTable(HashTable *H) {\nfor (int i = 0; i &lt; H-&gt;tableSize; i ++) {\nfor (Node *p = H-&gt;list[i]-&gt;head, *temp = NULL; p != NULL; ) {\ntemp = p;\np = p-&gt;next;\nfree(temp);\n}\nfree(H-&gt;list[i]);\n}\nfree(H-&gt;list);\nfree(H);\n} Pair *Find(HashTable *H, const int key) {\nPair *ret = NULL;\nList *L = H-&gt;list[H-&gt;Hash(key, H-&gt;tableSize)];\nfor (Node *p = L-&gt;head-&gt;next; p!= NULL; p = p-&gt;next) {\nif (p-&gt;element-&gt;key == key) {\nret = p-&gt;element;\nbreak;\n}\n}\nreturn ret;\n}\n\nvoid Insert(HashTable *H, const ElementType pair) {\nif (Find(H, pair.key) != NULL) {\n// WARNING(\"The key is already in the hash table\");\nreturn;\n}\nList *L = H-&gt;list[H-&gt;Hash(pair.key, H-&gt;tableSize)];\nL-&gt;size ++;\nNode *p = (Node *)malloc(sizeof(Node));\np-&gt;element = (Pair *)malloc(sizeof(Pair));\np-&gt;element-&gt;key = pair.key;\np-&gt;element-&gt;value = pair.value;\np-&gt;next = L-&gt;head-&gt;next;\nL-&gt;head-&gt;next = p;\n}\n\nvoid Delete(HashTable *H, const int key) {\nbool flag = false;\nList *L = H-&gt;list[H-&gt;Hash(key, H-&gt;tableSize)];\nfor (Node *p = L-&gt;head-&gt;next, *q = L-&gt;head; p != NULL; p = p-&gt;next, q = p) {\nif (p-&gt;element-&gt;key == key) {\nL-&gt;size --;\nq-&gt;next = p-&gt;next;\nfree(p);\nflag = true;\nbreak;\n}\n}\nif (flag == false) {\n// WARNING(\"There is no key in the hash table\");\nreturn;\n}\n}\n</code></pre> <ol> <li>Ensure the <code>tableSize</code> is prime.</li> <li>Sentinel.</li> </ol>"},{"location":"Computer_Science_Courses/FDS/Hashing/#open-addressing-closed-hashing","title":"Open Addressing \u5f00\u653e\u5bfb\u5740\u6cd5 / Closed Hashing \u95ed\u6563\u5217\u6cd5","text":"<p>Instead of using pointer and linked lists, if we do not want to expand the hash table, an alternative way is Open Addressing.</p> <p>The basic idea is that if there is a collision where \\(hash(x)\\) is already occupied, we make some offset, which means to take the following function as the address of hash function with \\(i\\) increasing until the address is not occupied.</p> \\[     (hash(key) + f(i)) \\text{ mod } N. \\]"},{"location":"Computer_Science_Courses/FDS/Hashing/#linear-probing","title":"Linear Probing \u7ebf\u6027\u63a2\u6d4b","text":"<p>Linear probing defines \\(f(i)\\) by</p> \\[     f(i) = i. \\] <p>Analysis of linear probing show that the expected number of probes \\(p\\) satisfies</p> \\[ \\begin{aligned} p = &amp;\\left\\{ \\begin{aligned}     &amp; \\frac12\\left(1 + \\frac{1}{(1 - \\lambda)^2}\\right), &amp;&amp; \\text{for insertions and unsuccessful searches,} \\\\     &amp; \\frac12\\left(1 + \\frac{1}{(1 - \\lambda)}\\right), &amp;&amp; \\text{for successful searches,} \\end{aligned} \\right. ,\\\\ &amp; \\text{where $\\lambda$ is the loading density}. \\end{aligned} \\]"},{"location":"Computer_Science_Courses/FDS/Hashing/#quadratic-probing","title":"Quadratic Probing \u5e73\u65b9\u63a2\u6d4b","text":"<p>Quadratic probing defines \\(f(i)\\) by</p> \\[     f(i) = i^2. \\] <p>Different from linear probing, there may be the cases that all addresses of \\(hash(key) + f(i)\\) are occupied even though the hash table is not full. To avoid this case, an important property for quadratic probing is described in the following theroem.</p> <p>Theorem</p> <p>If quadratic probing is used, and the table size is prime, then a new element can always be inserted if the table is at least half empty.</p> <p>Theorem</p> <p>If the table size is a prime of the form \\(4k + 3\\), then the quadratic probing can probe the entire table.</p> <p>Cluster \u805a\u96c6 / \u5806\u79ef</p> <p>Cluster means the ununiformity occupation of the hash table, which forms clusters or blocks. For linear probing, there is primary clustering and for quadratic probing, there is secondary clustering.</p> <p>Since cluster will result in increased search time, we should make efforts to avoid it.</p>"},{"location":"Computer_Science_Courses/FDS/Hashing/#double-hashing","title":"Double Hashing \u53cc\u6563\u5217","text":"<p>Double hashing defines \\(f(i)\\) by</p> \\[     f(i) = i \\cdot hash_2(key), \\] <p>which means that the total hash address is</p> \\[     hash(key) = hash_1(key) + i \\cdot hash_2(key). \\] <p>The second hash function \\(hash_2(x)\\) must satisfy</p> <ul> <li>\\(hash_2(x) \\ne 0\\).</li> <li>Make sure that all addresses can be probed.</li> </ul> <p>A choice of \\(hash_2(x)\\) that relatively works well is</p> \\[     hash_2(x) = R - (x \\text{ mod } R),\\ \\ \\text{ where } R \\in \\mathbb{P} \\text{ and } R &lt; N. \\] <p>NOTE If double hashing is correctly implemented, simulations imply that the expected number of probes is almost the same as for a random collision resolution strategy.</p> Data Type<pre><code>typedef struct {\nint key;\nint value;\n} Pair;\ntypedef Pair ElementType;\n\ntypedef struct {\nint tableSize;\nbool *occupied;\nElementType **table;\nint (*Hash)(const int key, const int N);\nint (*f)(const int i);\n} HashTable;\n</code></pre> Open Addressing <pre><code>static bool IsPrime(const int x) {\nbool ret = true;\nfor (int i = 2, temp = sqrt(x); i &lt;= temp; i ++) {\nif (x % i == 0) {\nret = false;\nbreak;\n}\n}\nreturn ret;\n}\n\nstatic int NextPrime(int x) {\nwhile (!IsPrime(x)) {\nx ++;\n}\nreturn x;\n}\n\nstatic int Hash(const int key, const int N) {\nreturn key % N;\n}\n\nstatic int f(const int i) {\nreturn i * i; // (1)!\n}\n\nHashTable *CreateHashTable(const int tableSize) {\nHashTable *H = (HashTable *)malloc(sizeof(HashTable));\nH-&gt;tableSize = NextPrime(tableSize);\nH-&gt;occupied = (bool *)malloc(H-&gt;tableSize * sizeof(bool));\nmemset(H-&gt;occupied, false, H-&gt;tableSize * sizeof(bool));\nH-&gt;table = (ElementType **)malloc(H-&gt;tableSize * sizeof(ElementType *));\nfor (int i = 0; i &lt; H-&gt;tableSize; i ++) {\nH-&gt;table[i] = (ElementType *)malloc(sizeof(ElementType));\n}\nH-&gt;Hash = Hash;\nH-&gt;f = f;\nreturn H;\n}\n\nvoid FreeHashTable(HashTable *H) {\nfree(H-&gt;occupied);\nfor (int i = 0; i &lt; H-&gt;tableSize; i ++) {\nfree(H-&gt;table[i]);\n}\nfree(H-&gt;table);\nfree(H);\n}\n\nvoid PrintHashTable(const HashTable *H) {\nfor (int i = 0; i &lt; H-&gt;tableSize; i ++) {\nif (H-&gt;occupied[i] == false) {\nprintf(\"0 \");\n} else {\nprintf(\"%d \", H-&gt;table[i]-&gt;key);\n}\n}\nputchar('\\n');\nfor (int i = 0; i &lt; H-&gt;tableSize; i ++) {\nif (H-&gt;occupied[i] == false) {\nprintf(\"0 \");\n} else {\nprintf(\"%d \", H-&gt;table[i]-&gt;value);\n}\n}\nputchar('\\n');\n}\n\nPair *Find(HashTable *H, const int key) {\nPair *ret = NULL;\nint pos = H-&gt;Hash(key, H-&gt;tableSize);\nint pivot = pos, i = 1;\nwhile (H-&gt;occupied[pos] == true) {\nif (H-&gt;table[pos]-&gt;key == key) {\nret = H-&gt;table[pos];\nbreak;\n}\npos = (pivot + H-&gt;f(i)) % H-&gt;tableSize;\ni ++;\n}\nreturn ret;\n}\n\nvoid Insert(HashTable *H, const ElementType pair) {\nbool flag = false;\nint pos = H-&gt;Hash(pair.key, H-&gt;tableSize);\nint pivot = pos, i = 1;\nwhile (H-&gt;occupied[pos] == true) {\nif (H-&gt;table[pos]-&gt;key == pair.key) {\nflag = true;\nbreak;\n}\npos = (pivot + H-&gt;f(i)) % H-&gt;tableSize;\ni ++;\n}\nif (flag == true) {\n// WARNING(\"The key is already in the hash table\");\nreturn;\n}\nH-&gt;occupied[pos] = true;\nH-&gt;table[pos]-&gt;key = pair.key;\nH-&gt;table[pos]-&gt;value = pair.value;\n}\n\nvoid Delete(HashTable *H, const int key) {\nbool flag = false;\nint pos = H-&gt;Hash(key, H-&gt;tableSize);\nint pivot = pos, i = 1;\nwhile (H-&gt;occupied[pos] == true) {\nif (H-&gt;table[pos]-&gt;key == key) {\nH-&gt;occupied[pos] = false;\nflag = true;\nbreak;\n}\npos = (pivot + H-&gt;f(i)) % H-&gt;tableSize;\ni ++;\n}\nif (flag == false) {\n// WARNING(\"There is no key in the hash table\");\nreturn;\n}\n}\n</code></pre> <ol> <li>Here is the quadratic probing function. We can replace it by <code>return i</code> for linear probing. Or we can use double hashing by the following snippet.     <pre><code>typedef struct {\n...\nint R;\nint (*f)(const int i, const int key, const int R);\n} HashTable;\n\nHashTable *CreateHashTable(const int tableSize) {\n...\nH-&gt;R = /* Some prime smaller than `tableSize` */;\nH-&gt;f = f;\nreturn H;\n}\n\nstatic int f(const int i, const int key, const int R) {\nint h = R - (key % R);\nreturn i * h;\n}\n</code></pre></li> </ol> <p>Tip</p> <p>For the probing statement</p> <pre><code>pos = (pivot + H-&gt;f(i)) % H-&gt;tableSize;\n</code></pre> <p>We can replace it by the following more efficient snippet.</p> Linear ProbingQuadratic ProbingDouble Hashing <pre><code>pos = (pos + 1) % H-&gt;tableSize;\n</code></pre> <pre><code>pos = (pos + 2 * i - 1) % H-&gt;tableSize;\n</code></pre> <pre><code>int h = H-&gt;R - (key % H-&gt;R);\nwhile (...) {\n...\npos = (pos + h) % H-&gt;tableSize;\n...\n}\n</code></pre>"},{"location":"Computer_Science_Courses/FDS/Hashing/#rehashing","title":"Rehashing \u518d\u6563\u5217","text":"<p>Quadratic probing does not require the use of a second hash function and is thus likely to be simpler and faster in practice.</p> <p>But when the hash table gets too full, quadratic probing may fail. Thus we need rehashing.</p> <p>When to Rehash?</p> <ul> <li>The hash table is half full.</li> <li>An insertion fails.</li> <li>The hash table reaches a certain load factor.</li> </ul> <p>Process of Rehashing</p> <p>Step.1 Build another table that is about twice as big (Remember it had better be prime).</p> <p>Step.2 Scan down the entire original hash table for non-deleted elements.</p> <p>Step.3 Use a new function to hash those elements into the new table.</p> Rehashing <pre><code>HashTable *Rehash(HashTable *H) {\nHashTable newH = CreateHashTable(2 * H-&gt;tableSize);\nfor (int i = 0; i &lt; H-&gt;tableSize; i ++) {\nif (H-&gt;flag[i] == false) {\nInsert(newH, H-&gt;table[i]);\n}\n}\nfree(H-&gt;flag);\nfree(H-&gt;table);\nreturn newH;\n}\n</code></pre>"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/","title":"Lists, Stacks and Queues","text":""},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#lists-stacks-and-queues","title":"Lists, Stacks and Queues","text":"<p>Definition</p> <p>Data type consists of objects and operations.</p> <p>An Abstract Data Type (ADT) is a data type that is organized in such a way that</p> <ul> <li>the specification on the objects are separated from the representation of the objects.</li> <li>the specification of the operations on the objects are separated from the implementation on the operations</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#list-linear-list","title":"List (Linear List)","text":"<p>ADT</p> <p>Objects: (item<sub>0</sub>, item<sub>1</sub>, ..., item<sub>N<sub>~-</sub>~1</sub>)</p> <p>Operations</p> <ul> <li>Make an empty list.</li> <li>Check whether it's an empty list.</li> <li>Print all the items in a list.</li> <li>Get the length of a list.</li> <li>Find the k-th item from a list.</li> <li>Insert a new item at position k of a list.</li> <li>Delete an item at position k from a list.</li> <li>Find the next item of the current item from a list.</li> <li>Find the previous item of the current item from a list.</li> </ul> <p>Following are some implementations of list.</p>"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#array-sequential-list","title":"Array / Sequential List","text":"<p>We say it a sequential mapping or sequantial storage structure.</p> Data Type<pre><code>typedef int ElementType;\ntypedef struct {\nElementType *array;\nint capacity;\nint size; } List;\n</code></pre> Array / Sequential List <pre><code>List *CreateList(const int capacity) {\nList *list = (List *)malloc(sizeof(List));\nlist-&gt;capacity = capacity;\nlist-&gt;size = 0;\nlist-&gt;array = (ElementType *)malloc(list-&gt;capacity * sizeof(ElementType));\nreturn list;\n}\n\nvoid FreeList(List *list) {\nfree(list-&gt;array);\nfree(list);\n}\n\nvoid PrintList(const List *list) {\nfor (int i = 0; i &lt; list-&gt;size; i ++) {\nprintf(\"%d \", list-&gt;array[i]);\n}\nputchar('\\n');\n}\n\nint GetListLength(const List *list) {\nreturn list-&gt;size;\n}\n\nbool IsEmptyList(const List *list) {\nreturn list-&gt;size == 0;\n}\n\nElementType Find(const List *list, const int position) {\nreturn list-&gt;array[position];\n}\n\nvoid Insert(List *list, const int position, const ElementType element) {\nassert(0 &lt; position &amp;&amp; position &lt;= list-&gt;size &amp;&amp;\n(list-&gt;capacity == -1 || list-&gt;size &lt; list-&gt;capacity));\nlist-&gt;size ++;\nfor (int i = list-&gt;size; i &gt; position; i --) {\nlist-&gt;array[i] = list-&gt;array[i - 1];\n}\nlist-&gt;array[position] = element;\n}\n\nvoid Delete(List *list, const int position) {\nassert(0 &lt;= position &amp;&amp; postion &lt; list-&gt;size &amp;&amp; list-&gt;size &gt; 0);\nlist-&gt;size --;\nfor (int i = position; i &lt; size; i ++ ) {\nlist-&gt;array[i] = list-&gt;array[i + 1];\n}\n}\n</code></pre>"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#pros","title":"Pros","text":"<ul> <li>Finding takes \\(O(1)\\) time.</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#cons","title":"Cons","text":"<ul> <li>Maximum size is limited, or overestimated.</li> <li>Insertion and deletion take \\(O(N)\\) time with plenty of data movements.</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#linked-list","title":"Linked List","text":"<p>We say it a linked storage structure.</p> Data Type<pre><code>typedef int ElementType;\ntypedef struct _node {\nElementType element;\nstruct _node *next;\n} Node;\ntypedef struct {\nNode *head;\nint capacity;\nint size;\n} List;\n</code></pre> Linked List without Sentinel <pre><code>List *CreateList(const int capacity) {\nList *list = (List *)malloc(sizeof(List));\nlist-&gt;capacity = capacity;\nlist-&gt;size = 0;\nlist-&gt;head = NULL;\nreturn list;\n}\n\nvoid FreeList(List *list) {\nfor (Node *p = list-&gt;head, *temp = NULL; p != NULL;) {\ntemp = p;\np = p-&gt;next;\nfree(temp);\n}\nfree(list);\n}\n\nvoid PrintList(const List *list) {\nfor (Node *p = list-&gt;head; p != NULL; p = p-&gt;next) {\nprintf(\"%d \", p-&gt;element);\n}\nputchar('\\n');\n}\n\nint GetListLength(const List *list) {\nreturn list-&gt;size;\n}\n\nbool IsEmptyList(const List *list) {\nreturn list-&gt;size == 0;\n}\n\nElementType Find(const List *list, const int position) {\nNode *p = list-&gt;head;\nfor (int i = 0; i &lt; position; i ++, p = p-&gt;next);\nreturn p-&gt;element;\n}\n\nvoid Insert(List *list, const int position, const ElementType element) {\nassert(0 &lt; position &amp;&amp; position &lt;= list-&gt;size &amp;&amp;\n(list-&gt;capacity == -1 || list-&gt;size &lt; list-&gt;capacity));\nNode *p = (Node *)malloc(sizeof(Node));\np-&gt;element = element;\np-&gt;next = NULL;\n\nlist-&gt;size ++;\nif (position == 0) {\np-&gt;next = list-&gt;head;\nlist-&gt;head = p; } else {\nNode *q = list-&gt;head;\nfor (int i = 0; i &lt; position - 1; i ++, q = q-&gt;next);\np-&gt;next = q-&gt;next;\nq-&gt;next = p;\n}\n}\n\nvoid Delete(List *list, const int position) {\nassert(0 &lt;= position &amp;&amp; postion &lt; list-&gt;size &amp;&amp; list-&gt;size &gt; 0);\nlist-&gt;size --;\nNode *temp = NULL;\nif (position == 0) {\ntemp = list-&gt;head;\nlist-&gt;head = list-&gt;head-&gt;next;\nfree(temp);\n} else {\nNode *q = list-&gt;head;\nfor (int i = 0; i &lt; position - 1; i ++, q = q-&gt;next);\ntemp = q-&gt;next;\nq-&gt;next = temp-&gt;next;\nfree(temp);\n}\n}\n</code></pre> <p>From the implementation, we find it complicated for the special judge of <code>position = 0</code>. Thus, we add a dummy node, or say sentinel (\u54e8\u5175) to simplify the implementation.</p> Linked List with Sentinel <pre><code>List *CreateList(const int capacity) {\nList *list = (List *)malloc(sizeof(List));\nlist-&gt;capacity = capacity;\nlist-&gt;size = 0;\nlist-&gt;head = (Node *)malloc(sizeof(Node)); // (1)!\nlist-&gt;head-&gt;element = 0;\nlist-&gt;head-&gt;next = NULL;\nreturn list;\n}\n\nvoid FreeList(List *list) {\nfor (Node *p = list-&gt;head-&gt;next, *temp = NULL; p != NULL;) {\ntemp = p;\np = p-&gt;next;\nfree(temp);\n}\nfree(list-&gt;head);\nfree(list);\n}\n\nvoid PrintList(const List *list) {\nfor (Node *p = list-&gt;head-&gt;next; p != NULL; p = p-&gt;next) {\nprintf(\"%d \", p-&gt;element);\n}\nputchar('\\n');\n}\n\nint GetListLength(const List *list) {\nreturn list-&gt;size;\n}\n\nbool IsEmptyList(const List *list) {\nreturn list-&gt;size == 0;\n}\n\nElementType Find(const List *list, const int position) {\nNode *p = list-&gt;head-&gt;next;\nfor (int i = 0; i &lt; position; i ++, p = p-&gt;next);\nreturn p-&gt;element;\n}\n\nvoid Insert(List *list, const int position, const ElementType element) {\nassert(0 &lt; position &amp;&amp; position &lt;= list-&gt;size &amp;&amp;\n(list-&gt;capacity == -1 || list-&gt;size &lt; list-&gt;capacity));\nNode *p = (Node *)malloc(sizeof(Node));\np-&gt;element = element;\np-&gt;next = NULL;\n\nlist-&gt;size ++;\nNode *q = list-&gt;head;\nfor (int i = 0; i &lt; position; i ++, q = q-&gt;next);\np-&gt;next = q-&gt;next;\nq-&gt;next = p;\n}\n\nvoid Delete(List *list, const int position) {\nassert(0 &lt;= position &amp;&amp; postion &lt; list-&gt;size &amp;&amp; list-&gt;size &gt; 0);\nlist-&gt;size --;\nNode *temp = NULL;\nNode *q = list-&gt;head;\nfor (int i = 0; i &lt; position; i ++, q = q-&gt;next);\ntemp = q-&gt;next;\nq-&gt;next = temp-&gt;next;\nfree(temp);\n}\n</code></pre> <ol> <li>Dummy Node, or say Sentinel \u54e8\u5175</li> </ol>"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#pros_1","title":"Pros","text":"<ul> <li>No limited size (In the implementation above, <code>capacity = -1</code> means unlimited capacity).</li> <li>Insertion and deletion take \\(O(1)\\) time.</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#cons_1","title":"Cons","text":"<ul> <li>Finding takes \\(O(N)\\) time.</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#doubly-linked-list","title":"Doubly Linked List","text":"Data Type<pre><code>typedef int ElementType;\ntypedef struct _node {\nElementType element;\nstruct _node *next;\nstruct _node *prev;\n} Node;\ntypedef struct {\nNode *head;\nint capacity;\nint size;\n} List;\n</code></pre> <p>Also, it's reasonable to add a field <code>Node *tail</code> to <code>List</code> and maintain <code>tail</code> for some convenient usage.</p> Doubly Linked List with Sentinel <pre><code>List *CreateList(const int capacity) {\nList *list = (List *)malloc(sizeof(List));\nlist-&gt;capacity = capacity;\nlist-&gt;size = 0;\nlist-&gt;head = (Node *)malloc(sizeof(Node));\nlist-&gt;head-&gt;element = 0;\nlist-&gt;head-&gt;next = NULL;\nlist-&gt;head-&gt;prev = NULL;\nreturn list;\n}\n\nvoid FreeList(List *list) {\nfor (Node *p = list-&gt;head-&gt;next, *temp = NULL; p != NULL;) {\ntemp = p;\np = p-&gt;next;\nfree(temp);\n}\nfree(list-&gt;head);\nfree(list);\n}\n\nvoid PrintList(const List *list) {\nfor (Node *p = list-&gt;head-&gt;next; p != NULL; p = p-&gt;next) {\nprintf(\"%d \", p-&gt;element);\n}\nputchar('\\n');\n}\n\nint GetListLength(const List *list) {\nreturn list-&gt;size;\n}\n\nbool IsEmptyList(const List *list) {\nreturn list-&gt;size == 0;\n}\n\nElementType Find(const List *list, const int position) {\nNode *p = list-&gt;head-&gt;next;\nfor (int i = 0; i &lt; position; i ++, p = p-&gt;next);\nreturn p-&gt;element;\n}\n\nvoid Insert(List *list, const int position, const ElementType element) {\nassert(0 &lt; position &amp;&amp; position &lt;= list-&gt;size &amp;&amp;\n(list-&gt;capacity == -1 || list-&gt;size &lt; list-&gt;capacity));\nNode *p = (Node *)malloc(sizeof(Node));\np-&gt;element = element;\np-&gt;next = p-&gt;prev = NULL;\n\nlist-&gt;size ++;\nNode *q = list-&gt;head;\nfor (int i = 0; i &lt; position; i ++, q = q-&gt;next);\np-&gt;next = q-&gt;next;\nq-&gt;next = p;\nq-&gt;next-&gt;prev = q;\np-&gt;next-&gt;prev = p;\n}\n\nvoid Delete(List *list, const int position) {\nassert(0 &lt;= position &amp;&amp; postion &lt; list-&gt;size &amp;&amp; list-&gt;size &gt; 0);\nlist-&gt;size --;\nNode *temp = NULL;\nNode *q = list-&gt;head;\nfor (int i = 0; i &lt; position; i ++, q = q-&gt;next);\ntemp = q-&gt;next;\nq-&gt;next = temp-&gt;next;\ntemp-&gt;next-&gt;prev = q;\nfree(temp);\n}\n</code></pre>"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#circularly-linked-list","title":"Circularly Linked List","text":"<p>If there is a sentinel, then the next node of the last node is the sentinel. And Since it's circular, we can define <code>position</code> iteratively and take the mod of <code>position</code> as its actual <code>pos</code>.</p> <pre><code>int pos = ((position % list-&gt;size) + list-&gt;size) % list-&gt;size;\n</code></pre> Circularly Doubly Linked List with Sentinel <pre><code>List *CreateList(const int capacity) {\nList *list = (List *)malloc(sizeof(List));\nlist-&gt;capacity = capacity;\nlist-&gt;size = 0;\nlist-&gt;head = (Node *)malloc(sizeof(Node));\nlist-&gt;head-&gt;element = 0;\nlist-&gt;head-&gt;next = list-&gt;head;\nlist-&gt;head-&gt;prev = list-&gt;head;\nreturn list;\n}\n\nvoid FreeList(List *list) {\nfor (Node *p = list-&gt;head-&gt;next, *temp = NULL; p != list-&gt;head;) {\ntemp = p;\np = p-&gt;next;\nfree(temp);\n}\nfree(list-&gt;head);\nfree(list);\n}\n\nvoid PrintList(const List *list) {\nfor (Node *p = list-&gt;head-&gt;next; p != list-&gt;head; p = p-&gt;next) {\nprintf(\"%d \", p-&gt;element);\n}\nputchar('\\n');\n}\n\nint GetListLength(const List *list) {\nreturn list-&gt;size;\n}\n\nbool IsEmptyList(const List *list) {\nreturn list-&gt;size == 0;\n}\n\nElementType Find(const List *list, const int position) {\nNode *p = list-&gt;head-&gt;next;\nint pos = ((position % list-&gt;size) + list-&gt;size) % list-&gt;size;\nfor (int i = 0; i &lt; pos; i ++, p = p-&gt;next);\nreturn p-&gt;element;\n}\n\nvoid Insert(List *list, const int position, const ElementType element) {\nassert(0 &lt; position &amp;&amp; position &lt;= list-&gt;size &amp;&amp;\n(list-&gt;capacity == -1 || list-&gt;size &lt; list-&gt;capacity));\n\nNode *p = (Node *)malloc(sizeof(Node));\np-&gt;element = element;\np-&gt;next = p-&gt;prev = NULL;\n\nlist-&gt;size ++;\nint pos = ((position % list-&gt;size) + list-&gt;size) % list-&gt;size;\nNode *q = list-&gt;head;\nfor (int i = 0; i &lt; pos; i ++, q = q-&gt;next);\np-&gt;next = q-&gt;next;\nq-&gt;next = p;\nq-&gt;next-&gt;prev = q;\np-&gt;next-&gt;prev = p;\n}\n\nvoid Delete(List *list, const int position) {\nassert(list-&gt;size &gt; 0);\nint pos = ((position % list-&gt;size) + list-&gt;size) % list-&gt;size;\nlist-&gt;size --;\n\nNode *temp = NULL;\nNode *q = list-&gt;head;\nfor (int i = 0; i &lt; pos; i ++, q = q-&gt;next);\ntemp = q-&gt;next;\nq-&gt;next = temp-&gt;next;\ntemp-&gt;next-&gt;prev = q;\nfree(temp);\n}\n</code></pre> <p>Note</p> <p>Considering</p> <ul> <li>whether there is the field <code>tail</code>,</li> <li>whether there is a sentinel, or two sentinels (one for <code>head</code> and one for <code>tail</code>),</li> <li>whether it's doubly linked,</li> <li>whether it's circularly linked,</li> </ul> <p>we have numerous implementations of various linked lists.</p> <p>Example: Polynomial ADT</p> <ul> <li>Objects: \\(P(x) = a_1x^{e_1} + \\dots + a_nx^{e_n}\\)</li> <li>Operations:<ul> <li>Find degree.</li> <li>Addition, Subtraction, Multiplication, Differentiation.</li> </ul> </li> </ul> <p>Example: Multilists / Sparse Matrix Representation</p> <p>Suppose a university with 40000 students and 2500 courses. We want to print the students' name list for each course and the registered classes list for each student.</p> <p> </p> <p>If we want the students' name of course \\(C3\\), we start from node \\(C3\\) and move to the right. For each node, we sub-move another pointer circularly up and down to find \\(S1\\), which is the student's name of this node.</p>"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#static-list","title":"Static List","text":"<p>Or called Cursor Implementation of Linked Lists without pointer. It's an alternative method to implement linked list in the cases that some languages don't support pointers.</p> <p>Reference: \u9759\u6001\u94fe\u8868\u53ca\u5b9e\u73b0\uff08C\u8bed\u8a00\uff09\u8be6\u89e3.</p> <p>Simply put, we need an array to maintain TWO linked lists, one for data, the other for the unused space. The head of the unused space list is commonly at position <code>0</code>.</p> Data Type<pre><code>#define MAXN 1000\ntypedef int ElementType;\ntypedef struct {\nElementType element;\nint next;\n} Node;\ntypedef struct {\nNode MEM_SPACE[MAXN];\nint head;\nint size;\n} List;\n</code></pre> <p>Also, we can consider the sentinel, the tail, and the double and circular property with different data type defined. The following is an implementation of static list with sentinel.</p> Static List with Sentinel <p>First, we should define our own <code>malloc</code> and <code>free</code> function.</p> <pre><code>int MemAlloc(List *list) {\nint pos = list-&gt;MEM_SPACE[0].next;\nlist-&gt;MEM_SPACE[0].next = list-&gt;MEM_SPACE[pos].next;\nreturn pos;\n}\n\nvoid MemFree(List *list, const int pos) {\nlist-&gt;MEM_SPACE[pos].next = list-&gt;MEM_SPACE[0].next;\nlist-&gt;MEM_SPACE[0].next = pos;\n}\n</code></pre> <p>The rest are quite similar, we mainly make the following modification.</p> <ul> <li>From <code>x-&gt;element</code> to <code>list-&gt;MEM_SPACE[x].element</code>;</li> <li>From <code>x-&gt;next</code> to <code>list-&gt;MEM_SPACE[x].next</code>;</li> </ul> <pre><code>List *CreateList() {\nList *list = (List *)malloc(sizeof(List)); // (1)!\nfor (int i = 0; i &lt; MAXN - 1; i ++) {\nlist-&gt;MEM_SPACE[i].next = i + 1;\n}\nlist-&gt;MEM_SPACE[MAXN - 1].next = 0;\nlist-&gt;size = 0;\n\nlist-&gt;head = MemAlloc(list);\nlist-&gt;MEM_SPACE[list-&gt;head].element = 0;\nlist-&gt;MEM_SPACE[list-&gt;head].next = 0;\nreturn list;\n}\n\nvoid FreeList(List *list) {\nfree(list);\n}\n\nvoid PrintList(const List *list) {\nfor (int p = list-&gt;MEM_SPACE[list-&gt;head].next; p != 0; p = list-&gt;MEM_SPACE[p].next) {\nprintf(\"%d \", list-&gt;MEM_SPACE[p].element);\n}\nputchar('\\n');\n}\n\nint GetListLength(const List *list) {\nreturn list-&gt;size;\n}\n\nbool IsEmptyList(const List *list) {\nreturn list-&gt;size == 0;\n}\n\nElementType Find(const List *list, const int position) {\nint p = list-&gt;MEM_SPACE[list-&gt;head].next;\nfor (int i = 0; i &lt; position; i ++, p = list-&gt;MEM_SPACE[p].next);\nreturn list-&gt;MEM_SPACE[p].element;\n}\n\nvoid Insert(List *list, const int position, const ElementType element) {\nassert(0 &lt; position &amp;&amp; position &lt;= list-&gt;size &amp;&amp;\n(list-&gt;capacity == -1 || list-&gt;size &lt; list-&gt;capacity));\n\nint p = MemAlloc(list);\nlist-&gt;MEM_SPACE[p].element = element;\nlist-&gt;MEM_SPACE[p].next = 0;\n\nlist-&gt;size ++;\nint q = list-&gt;head;\nfor (int i = 0; i &lt; position; i ++, q = list-&gt;MEM_SPACE[q].next);\nlist-&gt;MEM_SPACE[p].next = list-&gt;MEM_SPACE[q].next;\nlist-&gt;MEM_SPACE[q].next = p;\n}\n\nvoid Delete(List *list, const int position) {\nassert(0 &lt;= postion &amp;&amp; position &lt; list-&gt;size &amp;&amp; list-&gt;size &gt; 0);\nlist-&gt;size --;\nint temp = 0;\nint q = list-&gt;head;\nfor (int i = 0; i &lt; position; i ++, q = list-&gt;MEM_SPACE[q].next);\ntemp = list-&gt;MEM_SPACE[q].next;\nlist-&gt;MEM_SPACE[q].next = list-&gt;MEM_SPACE[temp].next;\nMemFree(list, temp);\n}\n</code></pre> <ol> <li>For tidiness and simplicty, I still use <code>malloc</code> here, but in actual cases, all fields of <code>list</code> are often used as a global variable and <code>list</code> will not be a pointer.</li> </ol>"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#stack","title":"Stack","text":"<p>A stack is a Last-In-First-Out (LIFO) list.</p> <p>ADT</p> <p>Objects: (item<sub>0</sub>, item<sub>1</sub>, ..., item<sub>N<sub>~-</sub>~1</sub>)</p> <p>Operations</p> <ul> <li>Make an empty stack.</li> <li>Print all the items in a stack.</li> <li>Get the depth of a stack.</li> <li>Find the top item of a stack.</li> <li>Push a new item onto a stack.</li> <li>Pop an item from a stack.</li> </ul> Data Type<pre><code>typedef int ElementType;\ntypedef struct _node {\nElementType element;\nstruct _node *next;\n} Node;\ntypedef struct {\nNode *top;\nint capacity;\nint size;\n} Stack;\n</code></pre> <p>Similarly, we can use array and linked list to implement a stack. The following is the linked list implementation.</p> <p>NOTE: There is convention of <code>Pop</code> function that <code>Pop</code> will not only delete the element on the top, but return the deleted element.</p> Stack <pre><code>Stack *CreateStack(const int capacity) {\nStack *stack = (Stack *)malloc(sizeof(Stack));\nstack-&gt;capacity = capacity;\nstack-&gt;size = 0;\nstack-&gt;top = NULL;\nreturn stack;\n}\n\nvoid FreeStack(Stack *stack) {\nfor (Node *p = stack-&gt;top, *temp = NULL; p != NULL;) {\ntemp = p;\np = p-&gt;next;\nfree(p);\n}\nfree(stack);\n}\n\nvoid PrintStack(const Stack *stack) {\nfor (Node *p = stack-&gt;top; p != NULL; p = p-&gt;next) {\nprintf(\"%d \", p-&gt;element);\n}\nputchar('\\n');\n}\n\nint GetStackDepth(const Stack *stack) {\nreturn stack-&gt;size;\n}\n\nbool IsEmptyStack(const Stack *stack) {\nreturn stack-&gt;size == 0;\n}\n\nElementType Top(const Stack *stack) {\nreturn stack-&gt;top-&gt;element;\n}\n\nvoid Push(Stack *stack, ElementType element) {\nif (stack-&gt;capacity != -1 &amp;&amp; stack-&gt;size == stack-&gt;capacity) {\n// Warning\nreturn;\n}\n\nstack-&gt;size ++;\nNode *p = (Node *)malloc(sizeof(Node));\np-&gt;element = element;\np-&gt;next = stack-&gt;top;\nstack-&gt;top = p;\n}\n\nElementType Pop(Stack *stack) {\nif (stack-&gt;size == 0) {\n// Warning\nreturn -1;\n}\n\nstack-&gt;size --;\nNode *temp = stack-&gt;top;\nstack-&gt;top = temp-&gt;next;\nElementType element = temp-&gt;element;\nfree(temp);\nreturn element;\n}\n</code></pre>"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#queue","title":"Queue","text":"<p>A queue is a First-In-First-Out (FIFO) list.</p> <p>ADT</p> <p>Objects: (item<sub>0</sub>, item<sub>1</sub>, ..., item<sub>N<sub>~-</sub>~1</sub>)</p> <p>Operations</p> <ul> <li>Make an empty queue.</li> <li>Print all the items in a queue.</li> <li>Get the length of a queue.</li> <li>Find the front item of a queue.</li> <li>Enqueue a new item to a queue.</li> <li>Dequeue an item from a queue.</li> </ul> Data Type<pre><code>typedef int ElementType;\ntypedef struct _node {\nElementType element;\nstruct _node *next;\n} Node;\ntypedef struct {\nNode *front;\nNode *rear;\nint capacity;\nint size;\n} Queue;\n</code></pre> <p>Similarly, we can use array and linked list to implement a queue. The following is the linked list implementation.</p> <p>NOTE: There is convention of <code>Dequeue</code> function that <code>Dequeue</code> will not only delete the element in the front of the queue, but return the deleted element.</p> Queue <pre><code>Queue *CreateQueue(const int capacity) {\nQueue *queue = (Queue *)malloc(sizeof(Queue));\nqueue-&gt;capacity = capacity;\nqueue-&gt;size = 0;\nqueue-&gt;front = (Node *)malloc(sizeof(Node));\nqueue-&gt;front-&gt;element = 0;\nqueue-&gt;front-&gt;next = NULL;\nqueue-&gt;rear = queue-&gt;front;\nreturn queue;\n}\n\nvoid FreeQueue(Queue *queue) {\nfor (Node *p = queue-&gt;front-&gt;next, *temp = NULL; p != NULL;) {\ntemp = p;\np = p-&gt;next;\nfree(p);\n}\nfree(queue);\n}\n\nvoid PrintQueue(const Queue *queue) {\nfor (Node *p = queue-&gt;front-&gt;next; p != NULL; p = p-&gt;next) {\nprintf(\"%d \", p-&gt;element);\n}\nputchar('\\n');\n}\n\nint GetQueueLength(const Queue *queue) {\nreturn queue-&gt;size;\n}\n\nbool IsEmptyQueue(const Queue *queue) {\nreturn queue-&gt;size == 0;\n}\n\nElementType Front(const Queue *queue) {\nreturn queue-&gt;front-&gt;next-&gt;element;\n}\n\nvoid Enqueue(Queue *queue, ElementType element) {\nif (queue-&gt;capacity != -1 &amp;&amp; queue-&gt;size == queue-&gt;capacity) {\n// Warning\nreturn;\n}\n\nqueue-&gt;size ++;\nNode *p = (Node *)malloc(sizeof(Node));\np-&gt;element = element;\np-&gt;next = NULL;\nqueue-&gt;rear-&gt;next = p;\nqueue-&gt;rear = p;\n}\n\nElementType Dequeue(Queue *queue) {\nif (queue-&gt;size == 0) {\n// Warning\nreturn -1;\n}\n\nqueue-&gt;size --;\nif (queue-&gt;size == 0) {\nqueue-&gt;rear = queue-&gt;front;\n}\nNode *temp = queue-&gt;front-&gt;next;\nqueue-&gt;front-&gt;next = temp-&gt;next;\nElementType element = temp-&gt;element;\nfree(temp);\nreturn element;\n}\n</code></pre>"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#other-queues","title":"Other Queues","text":""},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#deque","title":"Deque","text":"<p>A deque is a queue that we can enqueue and dequeue from both sides. The interfaces are like below.</p> <pre><code>ElementType Front(const Deque dequue);\nElementType Back(const Deque dequue);\nvoid PushFront(Deque dequue, const ElementType element);\nvoid PushBack(Deque dequue, const ElementType element);\nElementType PopFront(Deque dequue);\nElementType PopBack(Deque dequue);\n</code></pre>"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#circular-queue","title":"Circular Queue","text":"<p>A circular queue is a queue that store elements circularly. It can be seen in the array implementation of queue.</p> <p>To distinguish whether is empty or full, we stipulate that when it's empty, it has \\(N - 1\\) elements. Then</p> <ul> <li>If a queue is empty, then <code>front = rear</code>.</li> <li>If a queue is full, then <code>front = (rear + 1) % N</code>.</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/","title":"Priority Queues","text":"<p>Priority queue (PQ, pq) is a First-In-Largest-Out queue. The elements must be comparable, which enables priority. Each element is added to the rear of the queue, and the prior element is deleted in the front of the queue. The implementation of priority queue is Heap.</p> <p>ADT</p> <p>Objects: A finite ordered list with zero or more elements.</p> <p>Operations:</p> <ul> <li>Initialize a priority queue with a set of elements.</li> <li>Find the prior element of a priority queue.</li> <li>Insert a new element into a priority queue.</li> <li>Delete the prior element of a priority queue.</li> </ul> Comparison <p>Let's first consider some data structure to implement the operations we need above.</p> <ul> <li>Array<ul> <li>Insertion (Add one element at the end) - \\(\\Theta(1)\\).</li> <li>Deletion (Find the prior key, remove and shift array) - \\(\\Theta(n) + O(n)\\)</li> </ul> </li> <li>Linked List<ul> <li>Insertion (Add to the front of the list) - \\(\\Theta(1)\\)</li> <li>Deletion (Find the prior key and remove it) - \\(\\Theta(n) + \\Theta(1)\\).</li> </ul> </li> <li>Ordered Array<ul> <li>Insertion (Find the proper position, shift array and add the element) - \\(O(n) + O(n)\\)</li> <li>Deletion (Remove the first element) - \\(\\Theta(1)\\)</li> </ul> </li> <li>Ordered Linked List<ul> <li>Insertion (Find the proper position and add the element) - \\(O(n) + \\Theta(1)\\)</li> <li>Deletion (Remove the first element) - \\(\\Theta(1)\\)</li> </ul> </li> <li>BST / AVL tree<ul> <li>Both Insertion and Deletion are \\(O(\\log n)\\)</li> <li>Many operations that we don't need</li> <li>Thus we modify it to a binary heap</li> </ul> </li> </ul>"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#binary-heap","title":"Binary Heap \u4e8c\u53c9\u5806","text":"<p>Definition</p> <p>A binary tree with \\(n\\) nodes and height \\(h\\) is called a complete binary tree iff its notes correspond to the nodes numbers from \\(1\\) to \\(n\\) (up to down and left to right) in a perfect binary tree of height \\(h\\).</p> <p>Property</p> <ul> <li>A complete binary tree of height \\(h\\) has between \\(2^h\\) and \\(2^{h + 1} - 1\\) nodes, and thus</li> </ul> \\[     h = \\lfloor \\log n \\rfloor. \\] <ul> <li>A complete binary tree with \\(n\\) nodes can be represented by an array with \\(n + 1\\) items. (The zeroth is not used).<ul> <li>The array is built by the levelorder of the tree.</li> <li> <p>For any node with index \\(i\\), we have</p> \\[ \\begin{aligned} \\text{Parent}(i) &amp;= \\left\\{     \\begin{aligned}         &amp; \\lfloor i / 2 \\rfloor, &amp; i \\ne 1 \\\\         &amp; \\text{None}, &amp; i = 1.     \\end{aligned} \\right. \\\\ \\text{LeftChild}(i) &amp;= \\left\\{     \\begin{aligned}         &amp; 2i, &amp; 2i \\le n \\\\         &amp; \\text{None}, &amp; 2i &gt; n.     \\end{aligned} \\right. \\\\ \\text{RightChild}(i) &amp;= \\left\\{     \\begin{aligned}         &amp; 2i + 1, &amp; 2i + 1 \\le n \\\\         &amp; \\text{None}, &amp; 2i + 1 &gt; n.     \\end{aligned} \\right. \\end{aligned} \\] </li> </ul> </li> </ul> <p> </p> <p>Definition</p> <p>A min tree is a tree in which the key value in each node is no larger than the key values in its children (if any).  A min heap is a complete binary tree that is also a min tree.</p> <p>A max tree is a tree in which the key value in each node is no smaller than the key values in its children (if any).  A max heap is a complete binary tree that is also a max tree.</p> <p>Thus Heap consists of max heap and min heap. And a heap is a complete binary tree and a max / min tree.</p>"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#implementation","title":"Implementation","text":"Data Type<pre><code>#define MinData -1\ntypedef int ElementType;\ntypedef struct {\nElementType *element;\nint capacity;\nint size;\n} PriorityQueue;\n</code></pre> <p>NOTE: The heap is stored in an array since it's a complete binary tree. And <code>element[0]</code> is a sentinel. If it's a min heap, then <code>element[0]</code> must store the smallest value <code>MinData</code>.</p> Priority Queue<pre><code>static void SwapElement(ElementType *a, ElementType *b) {\nElementType temp = *a;\n*a = *b;\n*b = temp;\n}\n\nPriorityQueue *CreateQueue(const int capacity) {\nPriorityQueue *Q = (PriorityQueue *)malloc(sizeof(PriorityQueue));\nQ-&gt;capacity = capacity;\nQ-&gt;size = 0;\nQ-&gt;element = (ElementType *)malloc((Q-&gt;capacity + 1) * sizeof(ElementType));\nQ-&gt;element[0] = MinData; // Sentinel\nreturn Q;\n}\n\nbool IsEmptyQueue(PriorityQueue *Q) {\nreturn Q-&gt;size == 0;\n}\n\nstatic void PercolateUp(PriorityQueue *Q, int p) {\nwhile (p &gt;&gt; 1 &gt; 0) {\nint parent = p &gt;&gt; 1;\nif (Q-&gt;element[p] &lt; Q-&gt;element[parent]) {\nSwapElement(Q-&gt;element + p, Q-&gt;element + parent);\n} else {\nbreak;\n}\np = parent;\n}\n}\n\nstatic void PercolateDown(PriorityQueue *Q, int p) {\nwhile (p &lt;&lt; 1 &lt;= Q-&gt;size) {\nint child = p &lt;&lt; 1;\nif (child != Q-&gt;size &amp;&amp; Q-&gt;element[child + 1] &lt; Q-&gt;element[child]) {\nchild ++;\n}\nif (Q-&gt;element[p] &gt; Q-&gt;element[child]) {\nSwapElement(Q-&gt;element + p, Q-&gt;element + child);\n} else {\nbreak;\n}\np = child;\n}\n}\n\nvoid Insert(PriorityQueue *Q, ElementType x) {\nint p = ++ Q-&gt;size;\nQ-&gt;element[p] = x;\nPercolateUp(Q, p);\n}\n\nElementType DeleteMin(PriorityQueue *Q) {\nElementType minElement = Q-&gt;element[1];\nQ-&gt;element[1] = Q-&gt;element[Q-&gt;size --];\nPercolateDown(Q, 1);\nreturn minElement;\n}\n</code></pre> <p>Time Complexity</p> <p>Time complexities of both <code>Insert</code> and <code>DeleteMin</code> are \\(O(\\log n)\\).</p>"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#other-heap-operations","title":"Other Heap Operations","text":""},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#build-heap","title":"Build Heap","text":"<p>Suppose now there is an array of data and we want to build a heap based on the data. Instead of creating an empty heap and inserting one by one, we have the following faster implementation.</p> <pre><code>PriorityQueue *BuildHeap(const int n, const ElementType a[]) {\nPriorityQueue *Q = (PriorityQueue *)malloc(sizeof(PriorityQueue));\nQ-&gt;size = Q-&gt;capacity = n;\nQ-&gt;element = (ElementType *)malloc((Q-&gt;size + 1) * sizeof(ElementType));\nmemcpy(Q-&gt;element + 1, a, Q-&gt;size * sizeof(ElementType));\n\nint pos = 1;\nwhile (pos &lt;&lt; 1 &lt; Q-&gt;size) {\npos &lt;&lt; = 1;\n}\nfor (int i = pos; i &gt;= 1; i --) {\nPercolateDown(Q, i);\n}\n\nreturn Q;\n}\n</code></pre> <p>Time Complexity</p> <p>Theorem</p> <p>For the perfect binary tree of heigh \\(h\\) containing \\(2^{h + 1} - 1\\) nodes, the sum of the heights of the node is \\(2^{h + 1} - 1 - (h + 1)\\).</p> <p>Thus the time complexity of building a heap is \\(O(n)\\), which is linear.</p>"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#increase-decrease-value","title":"Increase / Decrease Value","text":"<p>Increase or decrease some value <code>delta</code> (<code>delta &gt; 0</code>) at position <code>pos</code> of a heap.</p> <pre><code>void IncreaseValue(Priority Queue *Q, const int pos, const ElementType delta) {\nassert(delta &gt; 0);\nQ-&gt;element[pos] += delta;\nPercolateUp(Q, pos);\n}\n</code></pre> <pre><code>void DecreaseValue(Priority Queue *Q, const int pos, const ElementType delta) {\nassert(delta &gt; 0);\nQ-&gt;element[pos] -= delta;\nPercolateDown(Q, pos);\n}\n</code></pre>"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#delete","title":"Delete","text":"<p>Delete the element at position <code>pos</code> of a heap. We just decrease it to the minimum and then delete it.</p> <pre><code>#define inf 2147483647\nvoid Delete(PriorityQueue *Q, const int pos) {\nDecreaseValue(Q, pos, inf);\nDeleteMin(Q);\n}\n</code></pre>"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#d-heap","title":"d-Heap","text":"<p>A d-Heap is a heap like binary heap except that all nodes have \\(d\\) children.</p>  3-Heap  <p>Note</p> <ul> <li><code>DeleteMin</code> will take \\(d - 1\\) comparisons to find the smallest child and thus totally take \\(O(d \\log_d n)\\) time.</li> <li><code>Insert</code> will take \\(O(\\log_d n)\\) time.</li> <li>If a d-heap is stored as an array, for an entry of index \\(i\\),<ul> <li>the parent is at \\(\\lfloor (i + d - 2) / d \\rfloor\\).</li> <li>the first child is at \\((i - 1)d + 2\\).</li> <li>the last child is at \\(id + 1\\).</li> </ul> </li> </ul>"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#leftist-tree-heap","title":"Leftist Tree / Heap \u5de6\u504f\u6811","text":"<p>Remains</p>"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#skew-heap","title":"Skew Heap \u659c\u5806","text":"<p>Remains</p>"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#binomial-queues","title":"Binomial Queues \u4e8c\u9879\u961f\u5217 / \u4e8c\u9879\u5806","text":"<p>Remains</p>"},{"location":"Computer_Science_Courses/FDS/Sort/","title":"Sort","text":"<p>Suppose there is an array of comparable elements \\(x_0, \\dots x_{n - 1}\\) and sort the array.</p>"},{"location":"Computer_Science_Courses/FDS/Sort/#property","title":"Property","text":"<p>When discussing various sort algorithms, we concerns the following properties.</p>"},{"location":"Computer_Science_Courses/FDS/Sort/#stability","title":"Stability","text":"<p>Stability measures whether the algorithm change the relative order of the same elements.</p>"},{"location":"Computer_Science_Courses/FDS/Sort/#time-complexity","title":"Time Complexity","text":"<p>We consider the best time complexity, worst time complexity and average complexity.</p> <p>Definition</p> <p>An inversion in an array of numbers is any ordered pair \\((i, j)\\) with the property that</p> \\[     i &lt; j,\\ \\ A[i] &gt; A[j]. \\] <p>Theorem</p> <p>The average number of inversions in an array of \\(N\\) distinct number is \\(\\dfrac{N(N - 1)}{4}\\).</p> <p>Theorem</p> <ul> <li>Any algorithm that sorts by exchanging adjacent elements requires \\(\\Omega(N^2)\\) time on average.</li> <li>Any algorithm that sorts by comparison requires \\(\\Omega(N \\log N)\\) time on average.</li> </ul> <p>Also, there are some sort algorithms not based on exchange.</p>"},{"location":"Computer_Science_Courses/FDS/Sort/#space-complexity","title":"Space Complexity","text":"<p>Space complexity considers the extra space the algorithm need.</p>"},{"location":"Computer_Science_Courses/FDS/Sort/#bubble-sort","title":"Bubble Sort \u5192\u6ce1\u6392\u5e8f","text":"IterationIteration (Improved)Recursion <pre><code>void BubbleSort(const int n, int a[]) {\nfor (int i = n - 1; i &gt;= 0; i --) {\nfor (int j = 0; j &lt; i; j ++) {\nif (a[j] &gt; a[j + 1]) {\nSwap(a + j, a + j + 1);\n}\n}\n}\n}\n</code></pre> <pre><code>void BubbleSort(const int n, int a[]) {\nfor (int i = n - 1, end = 0; i &gt; 0; i = end, end = 0) {\nfor (int j = 0; j &lt; i; j ++) {\nif (a[j] &gt; a[j + 1]) {\nSwap(a + j, a + j + 1);\nend = j;\n}\n}\n}\n}\n</code></pre> <pre><code>void BubbleSort(const int n, int a[]) {\nif (n == 0) {\nreturn;\n}\nfor (int i = 0; i &lt; n - 1; i ++) {\nif (a[i] &gt; a[i + 1]) {\nSwap(a + i, a + i + 1);\n}\n}\nBubbleSort(n - 1, a);\n}\n</code></pre> <p>Analysis</p> <ul> <li>Stable.</li> <li>Time complexity \\(O(n^2)\\).</li> <li>Space complexity \\(O(1)\\).</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Sort/#insertion-sort","title":"Insertion Sort \u63d2\u5165\u6392\u5e8f","text":"IterationRecursion <pre><code>void InsertionSort(const int n, int a[]) {\nint j;\nfor (int i = 1; i &lt; n; i ++) {\nint x = a[i];\nfor (j = i - 1; j &gt;= 0 &amp;&amp; a[j] &gt; x; j --) {\na[j + 1] = a[j];\n}\na[j + 1] = x;\n}\n}\n</code></pre> <pre><code>void InsertionSort(const int n, int a[]) {\nif (n == 0) {\nreturn;\n}\nInsertionSort(n - 1, a);\nint x = a[n - 1], j;\nfor (j = n - 2; j &gt;= 0 &amp;&amp; a[j] &gt; x; j --) {\na[j + 1] = a[j];\n}\na[j + 1] = x;\n}\n</code></pre> <p>Analysis</p> <ul> <li>Stable.</li> <li>Time complexity<ul> <li>Avearge case \\(O(n^2)\\).</li> <li>Best case \\(O(n)\\) (High Efficiency when it's almost in order).</li> <li>When \\(n \\le 20\\), insertion sort is the fastest.</li> </ul> </li> <li>Space complexity \\(O(1)\\).</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Sort/#selection-sort","title":"Selection Sort \u9009\u62e9\u6392\u5e8f","text":"IterationRecursion <pre><code>void InsertionSort(const int n, int a[]) {\nfor (int i = 0; i &lt; n - 1; i ++) {\nint idx = i;\nfor (int j = i + 1; j &lt; n; j ++) {\nidx = a[j] &lt; a[idx] ? j : idx;\n}\nSwap(a + i, a + idx);\n}\n}\n</code></pre> <pre><code>int FindMin(int a[], const int begin, const int end) {\nint ret = begin;\nfor (int i = begin + 1; i &lt;= end; i++) {\nret = a[i] &lt; a[ret] ? i : ret;\n}\nreturn ret;\n}\n\nvoid Sort(int a[], const int begin, const int end) {\nif (begin == end) {\nreturn;\n}\nSwap(a + begin, a + FindMin(a, begin, end));\nSort(a, begin + 1, end);\n}\n\nvoid SelectionSort(const int n, int a[]) {\nSort(a, 0, n - 1);\n}\n</code></pre> <p>Analysis</p> <ul> <li>Unstable.</li> <li>Time complexity \\(O(n^2)\\).</li> <li>Space complexity \\(O(1)\\).</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Sort/#counting-sort","title":"Counting Sort \u8ba1\u6570\u6392\u5e8f","text":"<pre><code>#define m 10010 // (1)!\nvoid CountingSort(const int n, int a[]) {\nint cnt[m], b[n];\nmemset(cnt, 0, sizeof(cnt));\nfor (int i = 0; i &lt; n; i ++) {\ncnt[a[i]] ++;\n}\nfor (int i = 0; i &lt; m; i ++) {\ncnt[i] += cnt[i - 1];\n}\nfor (int i = n - 1; i &gt;= 0; i --) {\nb[cnt[a[i] --] = a[i];\n}\nmemcpy(a, b, sizeof(a));\n}\n</code></pre> <ol> <li><code>m</code> is the upper bound of <code>a[i]</code>.</li> </ol> <p>Analysis</p> <ul> <li>Stable.</li> <li>Time complexity \\(O(n + m)\\), where \\(m\\) is the upper bound of \\(a[i]\\).</li> <li>Space complexity \\(O(n + m)\\).</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Sort/#radix-sort","title":"Radix Sort \u57fa\u6570\u6392\u5e8f","text":"<pre><code>typedef struct {\nint element;\nint key[K]; // (1)!\n} ElementType;\n\nvoid CountingSort(const int n, ElementType a[], int p, int cnt[], ElementType b[]) {\nmemset(cnt, 0, sizeof(cnt));\nfor (int i = 0; i &lt; n; i ++) {\ncnt[a[i].key[p]] ++;\n}\nfor (int i = 0; i &lt; m[p]; i ++) {\ncnt[i] += cnt[i - 1];\n}\nfor (int i = n - 1; i &gt;= 0; i --) {\nb[cnt[a[i].key[p] --] = a[i];\n}\nmemcpy(a, b, sizeof(a));\n} void RadixSort(const int n, ElementType a[]) {\nint cnt[M]; // (2)!\nElementType b[n];\nfor (int i = k - 1; i &gt;= 0; i --) {\nCountingSort(n, a, i, cnt, b);\n}\n}\n</code></pre> <ol> <li><code>K</code> is the number of keys.</li> <li><code>M</code> is the maximum of <code>m[i]</code>, where <code>m[i]</code> is the upper bound of each key.</li> </ol> <p>The most common keys for sorting integers are</p> <ul> <li>the digit number from high to low (left to right), which is called Most Significant Digit (MSD) sort.</li> <li>the digit number from low to high (right to left), which is called Least Significant Digit (LSD) sort.</li> </ul> <p>In general, LSD is better than MSD.</p> Keys of LSDKeys of MSD <pre><code>for (int i = 0; i &lt; n; i ++) {\nfor (int j = 0; j &lt; K; j ++) {\na[i].key[j] = (int)(a[i].element / pow(10, j)) % 10;\n}\n}\n</code></pre> <pre><code>for (int i = 0; i &lt; n; i ++) {\nfor (int j = 0; j &lt; K; j ++) {\na[i].key[j] = (int)(a[i].element / (pow(10, K - 1) / pow(10, j))) % 10;\n}\n}\n</code></pre> <p>Analysis</p> <ul> <li>Stable unless the inner sort is unstable.</li> <li>Time complexity<ul> <li>If \\(\\max\\{m_i\\}\\) is small, we can use Counting Sort as the inner sort, then the time complexity is \\(O\\left(nk + \\sum\\limits_{i = 1}^k m_i\\right)\\), where \\(k\\) is the number of keys, \\(m_i\\) is the upper bound of each key.</li> <li>If \\(\\max\\{m_i\\}\\) is large, we use the sort based on comparison of \\(O(nk \\log n)\\) instead of Radix Sort.</li> </ul> </li> <li>Space complexity \\(O(n + \\max\\{m_i\\})\\).</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Sort/#quick-sort","title":"Quick Sort \u5feb\u901f\u6392\u5e8f","text":"<pre><code>void Sort(int a[], const int L, const int R) {\nif (L &gt;= R) {\nreturn;\n}\n\nint rnd = L + rand() % (R - L); // (1)!\nint pos = L, cnt = 0;\nSwap(&amp;a[L], &amp;a[rnd]);\nfor (int i = L + 1; i &lt;= R; i ++) { // (2)!\nif (a[i] &lt; a[L]) {\npos ++;\nif (pos + cnt &lt; i) {\nSwap(&amp;a[pos], &amp;a[cnt + pos]);\n}\nSwap(&amp;a[i], &amp;a[pos]);\ncontinue;\n}\nif (a[i] == a[L]) {\ncnt ++;\nSwap(&amp;a[i], &amp;a[cnt + pos]);\n}\n}\nSwap(&amp;a[L], &amp;a[pos]);\n\nSort(a, L, pos - 1);\nSort(a, pos + 1 + cnt, R);\n}\n\nvoid QuickSort(const int n, int a[]) {\nsrand(time(NULL));\nSort(a, 0, n - 1);\n}\n</code></pre> <ol> <li>Different pivot picking strategies are discussed below.</li> <li>This is a 3-way radix quicksort.</li> </ol> <p>Analysis</p> <ul> <li>Unstable.</li> <li>Time complexity<ul> <li>Best and average cases \\(O(n \\log n)\\).</li> <li>Worst cases \\(O(n^2)\\).<ul> <li>Each pivot <code>a[pos]</code> is the maxima / minima.</li> <li>Degenerate to list.</li> </ul> </li> </ul> </li> <li>Space complexity \\(O(1)\\).</li> </ul> Time Complexity \\[     T(N) = T(i) + T(N - i - 1) + cN. \\] <ul> <li>Worst Case</li> </ul> \\[     T(N) = T(N - 1) + cN \\Rightarrow T(N) = O(N^2), \\] <ul> <li>Best Case</li> </ul> \\[     T(N) = 2T\\left(\\frac{N}{2}\\right) + cN \\Rightarrow T(N) = O(N\\log N), \\] <ul> <li>Average Case</li> </ul> \\[     T(N) = \\frac2N\\sum\\limits_{j = 0}^{N - 1}T(j) + cN \\Rightarrow T(N) = O(N\\log N). \\] Pivot Picking Strategy <ul> <li>A simple way is <code>a[0]</code>.</li> <li>A random way, which is used above, is <code>a[rnd]</code>, where <code>rnd</code> is a random number between <code>L</code> and <code>R</code>.</li> <li> <p>Median-of-Three Paritioning <code>a[mid]</code>,  where <code>mid = median(left, center, right)</code>.</p> Code <pre><code>int MediamThree(int a[], const int L, const int R) {\nint mid = L + ((R - L) &gt;&gt; 1);\nif (a[L] &gt; a[mid]) {\nSwap(a + L, a + mid);\n}\nif (a[L] &gt; a[R]) {\nSwap(a + L, a + R);\n}\nif (a[mid] &gt; a[R]) {\nSwap(a + mid, a + R);\n}\nassert(a[L] &lt;= a[mid] &amp;&amp; a[mid] &lt;= a[R]);\nSwap(a[mid], a[R - 1]);\nreturn a[R - 1];\n}\n</code></pre> </li> </ul> Improvement <p>Suppose the value of pivot is <code>v</code>.</p> <p>2-way Radix Quicksort</p> <p>Scan from left and right respectively by two index <code>(i, j)</code>, make the elements <code>&lt;= v</code> on the left of <code>i</code> and the elements <code>&gt;= v</code> on the right of <code>j</code>.</p> <p>3-way Radix Quicksort</p> <p>Partition the series into THREE parts: <code>&gt; m</code>, <code>&lt; m</code> and <code>= m</code>. It's efficient to deal with the cases of multiple identical values.</p>"},{"location":"Computer_Science_Courses/FDS/Sort/#merge-sort","title":"Merge Sort \u5f52\u5e76\u6392\u5e8f","text":"RecursionIteration <pre><code>void Merge(int a[], const int L, const int mid, const int R, int temp[]) {\nint i = L, j = mid + 1;\nfor (int k = L; k &lt;= R; k ++) {\nif (i &lt;= mid &amp;&amp; (j &gt; R || a[i] &lt;= a[j])) {\ntemp[k] = a[i];\ni ++;\n} else {\ntemp[k] = a[j];\nj ++;\n}\n}\nmemcpy(a + L, temp + L, (R - L + 1) * sizeof(int));\n}\n\nvoid Sort(int a[], const int L, const int R, int temp[]) {\nif (L == R) {\nreturn;\n}\n\nint mid = L + ((R - L) &gt;&gt; 1);\nSort(a, L, mid, temp);\nSort(a, mid + 1, R, temp);\nMerge(a, L, mid, R, temp);\n}\n\nvoid MergeSort(const int n, int a[]) {\nint temp[n];\nSort(a, 0, n - 1, temp);\n}\n</code></pre> <pre><code>#define min(a, b) ((a) &lt; (b) ? (a) : (b))\n\nvoid Merge(int a[], const int L, const int mid, const int R, int temp[]) {\nint i = L, j = mid + 1;\nfor (int k = L; k &lt;= R; k ++) {\nif (i &lt;= mid &amp;&amp; (j &gt; R || a[i] &lt;= a[j])) {\ntemp[k] = a[i];\ni ++;\n} else {\ntemp[k] = a[j];\nj ++;\n}\n}\nmemcpy(a + L, temp + L, (R - L + 1) * sizeof(int));\n}\n\nvoid MergeSort(const int n, int a[]) {\nint temp[n];\nfor (int width = 1; width &lt; n; width &lt;&lt;= 1) {\nfor (int i = 0; i &lt; n; i += width &lt;&lt; 1) {\nMerge(a, i, min(n, i + width) - 1, min(n, i + (width &lt;&lt; 1) - 1), temp);\n}\n}\n}\n</code></pre> <p>Analysis</p> <ul> <li>Stable.</li> <li>Time complexity \\(O(n \\log n)\\).</li> <li>Space complexity \\(O(n)\\).</li> <li>Better performance at parallel sort.</li> </ul> Time Complexity \\[ \\begin{aligned}     T(1) &amp;= 1, \\\\     T(N) &amp;= 2T\\left(\\frac{N}{2}\\right) + O(N) \\\\          &amp;= 2^k T\\left(\\frac{N}{2^k}\\right) + k \\cdot O(N) \\\\          &amp;= N \\cdot O(1) + \\log N \\cdot O(N) \\\\          &amp;= O(N + N \\log N). \\end{aligned} \\]"},{"location":"Computer_Science_Courses/FDS/Sort/#bucket-sort","title":"Bucket Sort \u6876\u6392\u5e8f","text":"<pre><code>#define m 10010 // (1)!\n\ntypedef struct _node {\nint element;\nstruct _node *next;\n} Node;\ntypedef struct {\nint size;\nNode *head;\n} List;\n\nvoid InsertionSort(List *list) { // (2)!\nfor (Node *p = list-&gt;head-&gt;next, *q = list-&gt;head; p != NULL;) {\nbool flag = false;\nfor (Node *r = list-&gt;head-&gt;next, *s = list-&gt;head; r != p; s = r, r = r-&gt;next) {\nif (r-&gt;element &gt; p-&gt;element) {\nNode *temp = p;\np = p-&gt;next;\nq-&gt;next = p;\nflag = true;\n\ns-&gt;next = temp;\ntemp-&gt;next = r;\nbreak;\n}\n}\nif (flag == false) {\nq = p;\np = p-&gt;next;\n}\n}\n}\n\nvoid BucketSort(const int n, int a[]) {\nint bucketNum = 6; // (3)!\nint bucketSize = (m + bucketNum - 1) / bucketNum;\nList **bucket = (List **)malloc(bucketNum * sizeof(List *));\nfor (int i = 0; i &lt; bucketNum; i ++) {\nbucket[i] = (List *)malloc(sizeof(List));\nbucket[i]-&gt;size = 0;\nbucket[i]-&gt;head = (Node *)malloc(sizeof(Node));\nbucket[i]-&gt;head-&gt;next = NULL;\n}\n\nfor (int i = 0; i &lt; n; i ++) {\nint pos = a[i] / bucketSize;\nbucket[pos]-&gt;size ++;\nNode *p = (Node *)malloc(sizeof(Node));\np-&gt;element = a[i];\np-&gt;next = bucket[pos]-&gt;head-&gt;next;\nbucket[pos]-&gt;head-&gt;next = p;\n}\n\nfor (int i = 0, cnt = 0; i &lt; bucketNum; i ++) {\nInsertionSort(bucket[i]);\nfor (Node *p = bucket[i]-&gt;head-&gt;next; p != NULL; p = p-&gt;next) {\na[cnt ++] = p-&gt;element;\n}\n}\n\nfor (int i = 0; i &lt; bucketNum; i ++) {\nfor (Node *p = bucket[i]-&gt;head, *temp = NULL; p != NULL;) {\ntemp = p;\np = p-&gt;next;\nfree(temp);\n}\nfree(bucket[i]);\n}\nfree(bucket);\n}\n</code></pre> <ol> <li><code>m</code> is the upper bound of <code>a[i]</code>.</li> <li>The inner sort is used insertion sort, and here is a list implementation of Insertion Sort.</li> <li><code>bucketNum</code> is the number of the bucket.</li> </ol> <p>Analysis</p> <ul> <li>Similar to Radix Sort.</li> <li>Stable unless the inner sort is unstable.</li> <li>Time complexity<ul> <li>Average case \\(O\\left(n + k \\cdot \\left(\\dfrac{n}{k}\\right)^2 + k\\right)\\), where \\(k\\) is the number of the bucket.</li> <li>Worst case \\(O(n^2)\\).</li> <li>Large constant of time complexity.</li> </ul> </li> <li>Space complexity \\(O(n)\\).</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Sort/#heap-sort","title":"Heap Sort \u5806\u6392\u5e8f","text":"<pre><code>void MaxHeapify(int a[], int p, int size) { // (1)!\nwhile (p &lt;&lt; 1 &lt;= size) {\nint child = p &lt;&lt; 1;\nif (child != size &amp;&amp; a[child] &lt; a[child + 1]) {\nchild ++;\n}\nif (a[p] &lt; a[child]) {\nSwap(a + p, a + child);\n} else {\nbreak;\n}\np = child;\n}\n}\n\nvoid HeapSort(const int n, int a[]) {\nfor (int i = n / 2 - 1; i &gt;= 0; i --) { // (2)!\nMaxHeapify(a, i, n - 1);\n}\nfor (int i = n - 1; i &gt; 0; i --) {\nSwap(a, a + i);\nMaxHeapify(a, 0, i - 1);\n}\n}\n</code></pre> <ol> <li>The same as <code>PercolateDown()</code> of implementation of heap.</li> <li>The same as Build Heap.</li> </ol> <p>Analysis</p> <ul> <li>Improvement of Selection Sort.</li> <li>Unstable.</li> <li>Time complexity \\(O(n \\log n)\\).</li> <li>Space complexity \\(O(1)\\).</li> <li>Much cache miss and not based on partition, which means hard to parallel.</li> </ul> Theorem <p>The average number of comparisons used to heapsort a random permutation of N distinct items is  \\(2N \\log N - O(N\\log \\log N)\\).</p>"},{"location":"Computer_Science_Courses/FDS/Sort/#shell-sort","title":"Shell Sort \u5e0c\u5c14\u6392\u5e8f","text":"Shell's IncrementShell's Increment (Simplified)Hibbard's Increment <pre><code>void ShellSort(const int n, int a[]) {\nfor (int step = n &gt;&gt; 1; step &gt; 0; step &gt;&gt;= 1) {\nfor (int start = 0; start &lt; step; start ++) {\nfor (int i = start; i &lt; n; i += step) {\nint temp = a[i], j;\nfor (j = i; j &gt;= step; j -= step) {\nif (temp &lt; a[j - step]) {\na[j] = a[j - step];\n} else {\nbreak;\n}\n}\na[j] = temp;\n}\n}\n}\n}\n</code></pre> <pre><code>void ShellSort(const int n, int a[]) {\nfor (int step = n &gt;&gt; 1; step &gt; 0; step &gt;&gt;= 1) {\nfor (int i = step; i &lt; n; i += step) {\nint temp = a[i], j;\nfor (j = i; j &gt;= step; j -= step) {\nif (temp &lt; a[j - step]) {\na[j] = a[j - step];\n} else {\nbreak;\n}\n}\na[j] = temp;\n}\n}\n}\n</code></pre> <pre><code>void ShellSort(const int n, int a[]) {\nint t = 1, step;\nwhile (t &lt;&lt; 1 &lt; n) {\nt &lt;&lt;= 1;\n}\nstep = t - 1;\n\nwhile (step &gt; 0) {\nfor (int i = step; i &lt; n; i += step) {\nint temp = a[i], j;\nfor (j = i; j &gt;= step; j -= step) {\nif (temp &lt; a[j - step]) {\na[j] = a[j - step];\n} else {\nbreak;\n}\n}\na[j] = temp;\n}\nt &gt;&gt;= 1;\nstep = t - 1;\n}\n}\n</code></pre> <p>Analysis</p> <ul> <li>Improvement of Insertion Sort.</li> <li>Unstable.</li> <li>Time complexity<ul> <li>Depend on the increment sequence (example are discussed below).</li> <li>Best case \\(O(n)\\).</li> <li>Worst case \\(\\Theta(n^2)\\).</li> <li>Best worst case that are known is \\(O(n \\log^2 n)\\).</li> </ul> </li> <li>Space complexity \\(O(1)\\).</li> <li>Much cache miss and not based on partition, which means hard to parallel.</li> </ul> Choice of Increment Sequence <p>Shell's Increment Sequence</p> \\[     h_t = \\left\\lfloor \\frac{N}{2} \\right\\rfloor,\\ \\     h_k = \\left\\lfloor \\frac{h_{k + 1}}{2} \\right\\rfloor. \\] <p>Theorem</p> <p>The worst case running time of Shell Sort using Shell's increment sequence is \\(\\Theta(N^2)\\).</p> <p>  Worst Case of Shell's Increment Sequence  </p> <p>Hibbard's Increment Sequence</p> \\[     h_k = 2^k - 1. \\] <p>Theorem</p> <p>The worst case running time of Shell Sort using Hibbard's increment sequence is \\(\\Theta\\left(N^{\\frac{3}{2}}\\right)\\).</p> <p>Conjectures</p> \\[     T_{avg-Hibbard}(N) = O\\left(N^{\\frac54}\\right). \\] Conjectures: Sedgewick's Best Sequence <p>Sedgewick\u2019s best sequence is \\(\\{1, 5, 19, 41, 109, \\dots\\}\\) in which the terms are either of the form \\(9 \\times 4^i - 9 \\times 2^i + 1\\) or \\(4^i \u2013 3 \\times 2^i + 1\\). </p> \\[     T_{avg}(N) = O\\left(N^{\\frac76}\\right),\\ \\      T_{worst}(N) = O\\left(N^{\\frac43}\\right). \\]"},{"location":"Computer_Science_Courses/FDS/Trees/","title":"Trees","text":"<p>Definition</p> <p>A tree is a collection of nodes. The collection can be empty; otherwise, a tree consists of</p> <ul> <li>a distinguished node \\(r\\), called the root.</li> <li>zero or more nonempty subtrees \\(T_1, \\dots, T_k\\), each of whose roots are connected by a directed edge from \\(r\\).</li> </ul> <p>Definition</p> <ul> <li>Degree of a node:         number of subtrees of the node.</li> <li>Degree of a tree:         maximum among the degrees of all nodes.</li> <li>Parent:                   a node that has subtrees.</li> <li>Children:                 the roots of the subtrees of parent.</li> <li>Siblings:                 children of the same parent.</li> <li>Leaf (terminal node):     a node with degree \\(0\\) (no children).</li> <li>Path from \\(n_1\\) to \\(n_k\\): a unique sequence of nodes \\(n_1,n_2,\\dots,n_k\\) such that \\(n_i\\) is the parent of \\(n_{i+1}\\).</li> <li>Length of path:           number of edges on the path.</li> <li>Ancestor of a node:       all the nodes along the path from the node up to the root.</li> <li>Descendants of a node:    all the nodes in its subtrees.</li> <li>Depth of \\(n_k\\):           length of the unique path from the root to \\(n_i\\).</li> <li>Height of \\(n_k\\):          length of the longest path from \\(n_i\\) to a leaf.</li> <li>Height / Depth of a tree: the height of the root or the maximum depth among all leaves.</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Trees/#representation","title":"Representation","text":"<p>Consider the following tree.</p>"},{"location":"Computer_Science_Courses/FDS/Trees/#list-representation","title":"List Representation","text":"<ul> <li>Array Representation</li> </ul> <ul> <li>Linked List Representation</li> </ul>"},{"location":"Computer_Science_Courses/FDS/Trees/#firstchild-nextsibling-representation","title":"FirstChild-NextSibling Representation","text":"<p>This representation is not unique with different <code>FirstChild</code> chosen.</p>"},{"location":"Computer_Science_Courses/FDS/Trees/#binary-tree","title":"Binary Tree","text":"<p>Definition</p> <p>A binary tree is a tree in which no node can have more than two children.</p> <p>Since it has the same structure of FirstChild-NextSibling Representation, thus all trees can be represented by binary trees.</p> <p>Property</p> <ul> <li>In a binary tree, left child and right child are different.</li> <li>The maximum number of nodes of level \\(i\\) is \\(2^{i - 1}\\). The maximum number of nodes in a binary tree of depth \\(k\\) is \\(2^k - 1\\).</li> <li>\\(n_0 = n_2 + 1\\), where \\(n_0\\) is the number of leaf nodes and \\(n_2\\) is the number of nodes of degreee 2.</li> </ul> Data Type<pre><code>typedef int ElementType;\ntypedef struct TreeNode {\nElementType element;\nstruct TreeNode *left;\nstruct TreeNode *right;\n} Tree;\n</code></pre>"},{"location":"Computer_Science_Courses/FDS/Trees/#traversals","title":"Traversals","text":"<p>There are four traversals for a binary tree, preorder traversal, inorder traversal, postorder traversal and levelorder traversal (or in graph, breadth first search, BFS).</p> <ul> <li>Pre-, in-, and postorder traversals are implemented by recursion, or alternatively by a stack with iteration.</li> <li>Levelorder traversal is implemented by a queue.</li> </ul> <p>Tip</p> <p>For a general tree, it also has</p> <ul> <li>Preorder Traversal: first itself, then sons.</li> <li>Postorder Traversal: first sons, then itself.</li> <li>Levelorder Traversal.</li> </ul> Preorder Traversal \u524d\u5e8f\u904d\u5386Inorder Traversal \u4e2d\u5e8f\u904d\u5386Postorder Traversal \u540e\u5e8f\u904d\u5386 <pre><code>void Preorder(Tree *tree) {\nif (tree == NULL) {\nreturn;\n}\n\nprintf(\"%d \", tree-&gt;element);\nPreorder(tree-&gt;left);\nPreorder(tree-&gt;right);\n}\n</code></pre> <pre><code>void Inorder(Tree *tree) {\nif (tree == NULL) {\nreturn;\n}\n\nInorder(tree-&gt;left);\nprintf(\"%d \", tree-&gt;element);\nInorder(tree-&gt;right);\n}\n</code></pre> <pre><code>void Postorder(Tree *tree) {\nif (tree == NULL) {\nreturn;\n}\n\nPostorder(tree-&gt;left);\nPostorder(tree-&gt;right);\nprintf(\"%d \", tree-&gt;element);\n}\n</code></pre> <p></p> Levelorder Traversal \u5c42\u5e8f\u904d\u5386 / \u5bbd\u5ea6\u4f18\u5148\u904d\u5386 <pre><code>void Levelorder(Tree *tree) {\nEnqueue(queue, tree);\nwhile (!IsEmptyQueue(queue)) {\nTree *cur = Dequeue(queue);\nprintf(\"%d \", cur-&gt;element);\nEnqueue(queue, cur-&gt;left);\nEnqueue(queue, cur-&gt;right);\n}\n}\n</code></pre>"},{"location":"Computer_Science_Courses/FDS/Trees/#threaded-binary-trees","title":"Threaded Binary Trees","text":"<p>There are \\(n + 1\\) <code>NULL</code> children of the leaf nodes of a binarty tree with \\(n\\) nodes. We want to make these <code>NULL</code> pointers more useful. And here comes the Threaded Binary Trees.</p> Data Type<pre><code>typedef int ElementType;\ntypedef struct ThreadedTreeNode {\nElementType element;\nstruct ThreadedTreeNode *left;\nstruct ThreadedTreeNode *right;\nbool leftThread;    // `true` to indicate `left` is a thread instead of a child pointer.\nbool rightThread;   // `true` to indicate `right` is a thread instead of a child pointer.\n} ThreadedTree;\n</code></pre> <p>Rule for Threaded Binary Trees</p> <p>NOTE: the following \"inorder\"\" can also be replaced by any traversals.</p> <p>Rule 1: If <code>tree-&gt;left == NULL</code>, then replace it with a pointer to its inorder predecessor.</p> <p>Rule 2: If <code>tree-&gt;right == NULL</code>, then replace it with a pointer to its inorder sucessor.</p> <p>Rule 3: A threaded binary tree must have a head node of which the left child points to the first node.</p> Example <p>The following is the infix syntax tree of </p> \\[     A + B * C / D. \\] <p> </p>"},{"location":"Computer_Science_Courses/FDS/Trees/#binary-search-tree-bst","title":"Binary Search Tree (BST)","text":"<p>Definition</p> <p>A binary search tree is a binary tree. It may be empty. If it is not empty, it satisfies the following properties:</p> <ul> <li>Every node has a key which is an integer, and the keys are distinct.</li> <li>The keys in a nonempty left subtree must be smaller than the key in the root of the subtree.</li> <li>The keys in a nonempty right subtree must be larger than the key in the root of the subtree.</li> <li>The left and right subtrees are also binary search trees.</li> </ul> <p>ADT</p> <p>Objects: A finite ordered list with zero or more elements.</p> <p>Operations:</p> <ul> <li>Make an empty BST.</li> <li>Find a key in a BST.</li> <li>Find the minimum key in a BST.</li> <li>Find the maximum key in a BST.</li> <li>Insert a key to a BST.</li> <li>Delete a key in a BST.</li> </ul> <p>BST</p> <pre><code>Tree *Find(Tree *tree, const ElementType element) {\nif (tree == NULL) {\nreturn NULL;\n}\nif (element == tree-&gt;element) {\nreturn tree;\n}\nreturn Find(tree, elemet &lt; tree-&gt;element ? tree-&gt;left : tree-&gt;right);\n}\n\nTree *FindMin(Tree *tree) {\nif (tree == NULL) {\nreturn NULL;\n}\nreturn tree-&gt;left == NULL ? tree : FindMin(tree-&gt;left);\n}\n\nTree *FindMax(Tree *tree) {\nif (tree == NULL) {\nreturn NULL;\n}\nreturn tree-&gt;right == NULL ? tree : FindMax(tree-&gt;right);\n}\n\nTree *Insert(Tree *tree, const ElementType element) {\nif (tree == NULL) {\ntree = (TreeNode *)malloc(sizeof(TreeNode));\ntree-&gt;element = element;\ntree-&gt;left = tree-&gt;right = NULL;\n} else {\nif (element &lt; tree-&gt;element) {\ntree-&gt;left = Insert(tree-&gt;left, element);\n} else {\ntree-&gt;right = Insert(tree-&gt;right, element);\n}\n}\nreturn tree;\n}\n\nTree *Delete(Tree *tree, const ElementType element) {\nif (tree == NULL) {\n// Warning: Element not found.\nreturn NULL;\n}\nif (element == tree-&gt;element) {\nif (tree-&gt;left != NULL &amp;&amp; tree-&gt;right != NULL) {\nTreeNode *temp = FindMin(tree-&gt;right); // (1)!\ntree-&gt;element = temp-&gt;element;\ntree-&gt;right = Delete(tree-&gt;right, temp-&gt;element);\n} else {\nTreeNode *temp = tree;\ntree = tree-&gt;left != NULL ? tree-&gt;left : tree-&gt;right;\nfree(temp);\n}\n}\nif (element &lt; tree-&gt;element) {\ntree-&gt;left = Delete(tree-&gt;left, element);\n} else {\ntree-&gt;right = Delete(tree-&gt;right, element);\n}\n}\n</code></pre> <ol> <li>Here it's replaced with the smallest node in the right subtree. Or alternatively it can be replaced with the largest node in the left subtree.</li> </ol>"},{"location":"Computer_Science_Courses/FDS/Trees/#time-complexity","title":"Time Complexity","text":"<p>The average complexities of all operations are \\(O(d)\\), where \\(d\\) is the depth of the target node. The average depth over all nodes in a tree is \\(O(\\log N)\\).</p> <p>But for the worst cases, when building a tree by inserting a set of oredered elements, it will degenerate to linear list with \\(O(N)\\) complexity of each operation.</p>"},{"location":"Computer_Science_Courses/FDS/Trees/#avl-tree","title":"AVL Tree","text":"<p>Remains</p>"},{"location":"Computer_Science_Courses/FDS/Trees/#splay-tree","title":"Splay Tree","text":"<p>Remains</p>"},{"location":"Computer_Science_Courses/FDS/Trees/#b-tree","title":"B-Tree","text":"<p>Remains</p>"},{"location":"Computer_Science_Courses/FDS/class_notes/","title":"Class Notes","text":"<p>Lecturer        \u5e72\u7ea2\u534e</p>"},{"location":"Computer_Science_Courses/FDS/class_notes/#lecture-grade-100","title":"Lecture Grade (100 %)","text":"<ul> <li>Laboratory Projects (25/30%)<ul> <li>3 labs.</li> <li>Take Programming Ability Test in the first week.<ul> <li>Top 30% can choose to take the HARD MODE.</li> </ul> </li> <li>Peer Review (PR)</li> </ul> </li> <li>Quizzes  (10%)<ul> <li>problems chosen from HW</li> </ul> </li> <li>Homework (10%)</li> <li>Mid-Term Exam (15%)</li> <li>Final exam (40%)</li> </ul>"},{"location":"Computer_Science_Courses/FDS/class_notes/#code-of-academic-honesty","title":"Code of Academic Honesty","text":"<p>To attend the final exam, it's neccessary to pass a test called Code of Academic Honesty.</p> <p>\u5982\u4f55\u770b\u5f85\u6d59\u6c5f\u5927\u5b66\u6570\u636e\u7ed3\u6784\u8bfe\u4e0a\u5de5\u9ad8\u73ed\u4e32\u901a\u821e\u5f0a\u7684\u884c\u4e3a\uff1f</p>"},{"location":"Computer_Science_Courses/ICV/1_Introduction/","title":"Lecture 1 Introduction","text":""},{"location":"Computer_Science_Courses/ICV/1_Introduction/#review-of-linear-algrebra","title":"Review of Linear Algrebra","text":""},{"location":"Computer_Science_Courses/ICV/1_Introduction/#affine-transformations","title":"Affine Transformations \u4eff\u5c04\u53d8\u6362","text":"<p>Affine map combines linear map and translation.</p> \\[ \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix} . \\] <p>Using homogenous coordinates (\u9f50\u6b21\u5750\u6807)</p> \\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a &amp; b &amp; t_x \\\\ c &amp; d &amp; t_y \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}. \\]"},{"location":"Computer_Science_Courses/ICV/1_Introduction/#eigenvectors-and-eigenvalues","title":"Eigenvectors and Eigenvalues","text":"<ul> <li>The eigenvalues of symmetric matrices are real numbers.</li> <li>The eigenvalues of positive definite matrices are positive numbers.</li> </ul>"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/","title":"Lecture 2 Image Formation","text":""},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#camera-and-lens","title":"Camera and Lens","text":""},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#pinhole-camera","title":"Pinhole Camera","text":"<p>A barrier to block off most of the rays. The opening is the aperture \u5149\u5708.</p> <p>Flaws</p> <ul> <li>Less light gets through.</li> <li>Diffraction effects (\u884d\u5c04).</li> </ul>"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#lens","title":"Lens \u900f\u955c","text":"\\[ \\frac{1}{i} + \\frac{1}{o} = \\frac{1}{f}. \\]"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#focal-length-f","title":"Focal length \\(f\\)  \u7126\u8ddd","text":"<p>If \\(o = \\infty\\), then \\(f = i\\).</p>"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#magnification-m","title":"Magnification \\(m\\) \u653e\u5927\u7387","text":"\\[ m = \\frac{h_i}{h_o}. \\]"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#field-of-view-fov","title":"Field of View (FOV) \u89c6\u91ce","text":"<p>Factor</p> <ul> <li>Focal Length.<ul> <li>Longer focal length, Narrower angle of view. Vice versa.</li> </ul> </li> </ul> <p>Note</p> <p>50mm / 46\u00b0 (and full frame) is the most similar FOV with human eyes. Thus 50mm lens is called standard lens (\u6807\u51c6\u955c\u5934).</p> <ul> <li>telephoto lens (\u957f\u7126\u955c\u5934\uff0c\u671b\u8fdc\u955c\u5934): \u89c6\u91ce\u5c0f, \u653e\u5927\u7387\u5927.</li> <li>short focal lens (\u77ed\u7126\u955c\u5934\uff0c\u5e7f\u89d2\u955c\u5934); \u89c6\u91ce\u5927, \u653e\u5927\u7387\u5c0f.</li> </ul> <ul> <li>Sensor Size<ul> <li>Bigger sensor size, Wider angle of view. Vice versa.</li> </ul> </li> </ul>"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#aperture","title":"Aperture \u5149\u5708","text":"<p>The representation of aperture is its Diameter \\(D\\).</p> <p>F-Number</p> \\[ N = \\frac{f}{D}\\ \\ \\text{ (mostly greater than $1$, around $1.8 \\sim 22$)}. \\]"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#lens-defocus","title":"Lens Defocus","text":"<p>Blur Circle Diameter (\u5149\u6591\u534a\u5f84)</p> \\[ b = \\frac{D}{i'}|i' -i|,\\ \\ b \\propto D \\propto \\frac{1}{N}. \\]"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#focusing","title":"Focusing \u5bf9\u7126","text":""},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#depth-of-field-dof","title":"Depth of Field (DoF) \u666f\u6df1","text":"\\[ {\\tt DoF} = o_2 - o_1 = \\frac{2of^2cN(o-f)}{f^4-c^2N^2(o-f)^2}. \\] <p>From the equation above, we can find that DoF is almost proportional to \\(N\\), and thus the Larger aperture, the Smaller F-Number and the Smaller DoF.</p>"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#how-to-blur-the-background","title":"How to blur the background","text":"<ul> <li>Large aperture.</li> <li>Long focal length.</li> <li>Near foreground.</li> <li>Far background.</li> </ul>"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#geometric-image-formation","title":"Geometric Image Formation\uff08\u5b9a\u4f4d\uff09","text":"<p>Camera model</p> <p> </p> \\[ \\begin{bmatrix} u \\\\ v \\end{bmatrix} = \\begin{bmatrix} f \\cfrac{x}{z} \\\\ f \\cfrac{y}{z} \\end{bmatrix}. \\]"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#homogeneous-coordinates-projective-coordinates","title":"Homogeneous Coordinates / Projective Coordinates","text":"<p>Suppose that \\(\\begin{bmatrix} x \\\\ y \\\\ w \\end{bmatrix}\\) is the same as \\(\\begin{bmatrix} x/w \\\\ y/w \\\\ 1 \\end{bmatrix}\\), then we get</p> \\[ \\begin{bmatrix} f &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; f &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} =  \\begin{bmatrix} fx \\\\ fy \\\\ z \\\\ \\end{bmatrix} \\cong \\begin{bmatrix} f\\cfrac{x}{z} \\\\ f\\cfrac{y}{z} \\\\ 1 \\end{bmatrix}. \\] <p>For convenience of dicussion, we can also put the image plane in front of the camera (opposite to the previous picture).</p>"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#perspective-projection","title":"Perspective Projection","text":"<ul> <li>What are Preserevd: Straight lines are still straight.</li> <li>What are Lost: Length and Angle.</li> </ul>"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#vanishing-points","title":"Vanishing Points","text":"<p>Properties</p> <ul> <li>Any two parallel lines have the same vanishing point \\(\\bf v\\).<ul> <li>\\(\\bf v\\) can be outside the image frame or at infinity.</li> </ul> </li> <li>Line from \\(\\bf C\\) to \\(\\bf v\\) is parallel to the lines.</li> </ul>"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#vanishing-lines","title":"Vanishing Lines","text":"<p>Multiple vanishing points compose the vanishing lines.</p> <p>The direction of the vanishing line tells us the orientation of the plane.</p>"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#distortion","title":"Distortion","text":"<ul> <li>Converging verticals.</li> </ul> <p> Problem and Solution (View Camera \u79fb\u8f74\u76f8\u673a)</p> <ul> <li> <p>Exterior columns appear bigger. (Due to lens flaws.)</p> <p> </p> </li> <li> <p>Radial distortion. (Due to imperfect lens.)</p> </li> </ul> \\[ \\begin{aligned} r^2 &amp;= {x'}_n^{2} + {y'}_n^{2}, \\\\ {x'}_d &amp;= {x'}_n(1 + \\kappa_1r^2 + \\kappa_2r^4), \\\\ {y'}_d &amp;= {y'}_n(1 + \\kappa_1r^2 + \\kappa_2r^4). \\\\ \\end{aligned} \\] <p>Solution</p> <p>Take a photo of a grid at the same point and then use the mathematics to calculate and correct radial distortion.</p>"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#orthographic-projection","title":"Orthographic Projection","text":"\\[ \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} =  \\begin{bmatrix} x \\\\ y \\\\ 1 \\\\ \\end{bmatrix} \\Rightarrow (x, y). \\]"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#photometric-image-formation","title":"Photometric Image Formation\uff08\u5b9a\u989c\u8272/\u4eae\u5ea6\uff09","text":""},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#image-sensor","title":"Image Sensor \u4f20\u611f\u5668","text":"<ul> <li>CMOS</li> <li>CCD (Charge Coupled Device)</li> </ul>"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#shutter","title":"Shutter \u5feb\u95e8","text":"<p>Shutter speed controls exposure time.</p>"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#color-sensing","title":"Color Sensing","text":"<ul> <li> <p>Color Spaces</p> <ul> <li>RGB</li> <li>HSV</li> </ul> </li> <li> <p>Practical Color Sensing: Bayer Filter</p> </li> </ul>"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#shading","title":"Shading \u7740\u8272","text":""},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#brdf-bidirectional-reflectance-distribution-function","title":"BRDF (Bidirectional Reflectance Distribution Function)","text":"\\[ L_r(\\hat{\\textbf{v}_r};\\lambda) = \\int L_i(\\hat{\\textbf{i}_r};\\lambda)f_r(\\hat{\\textbf{v}_r}, \\hat{\\textbf{v}_i}, \\hat{\\textbf{n}}; \\lambda)\\cos^+\\theta_i\\ d\\hat{\\textbf{v}_i}. \\]"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#diffuse-lambertian-reflection","title":"Diffuse (Lambertian) Reflection","text":"<ul> <li>Shading independent of view direction.</li> </ul>"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#specular-term","title":"Specular Term","text":"<ul> <li>Intensity depends on view direction.</li> </ul>"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#blinn-phong-reflection-model","title":"Blinn-Phong Reflection Model","text":"\\[ L = L_a + L_d + L_s = k_aI_a + k_d(I/r^2)\\max(0, \\textbf{n}\\cdot\\textbf{l}) + k_s(I/r^2)\\max(0, \\textbf{n}\\cdot\\textbf{h})^p. \\]"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/","title":"Lecture 3 Image Processing","text":""},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#image-processing","title":"Image Processing","text":"<p>Common Processs of Images:</p> <p>Incresing contrast, Invert, Blur, Sharpen, Edge Detection ...</p>"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#convolution","title":"Convolution","text":"<ul> <li>Continous 1-D form,</li> </ul> \\[ (f * g)(x) = \\int_{-\\infty}^\\infty f(y)g(x-y)dy. \\] <ul> <li>Discrete 2D form,</li> </ul> \\[ (f * g)(x) = \\sum_{i, j =-\\infty}^\\infty f(i, j)g(x - i, y - j). \\]"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#gaussian-blur","title":"Gaussian Blur","text":"<p>We define thb 2D Gaussian function as below,</p> \\[ \\large f(i, j) = \\frac{1}{2\\pi\\sigma^2}e^{-\\frac{i^2 + j^2}{2\\sigma^2}}. \\] <p>Process of Gaussian Blur</p> <ol> <li>Define a window size (commmly a square, say \\(n \\times n\\), and let \\(r = \\lfloor n / 2 \\rfloor\\)).</li> <li>Select a point, say \\((x, y)\\) and then put a window around it.</li> <li> <p>Apply the Gaussian function at each point in the window, and sum them up, namely </p> \\[ G(x, y) = \\sum\\limits_{i = x - r}^{x + r}\\sum\\limits_{j = y - r}^{y + r}f(i,j). \\] </li> <li> <p>Then the \u300cblurred\u300d value of point \\((x, y)\\) is \\(G(x, y)\\).</p> </li> </ol>"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#sharpen","title":"Sharpen","text":"<p>An example of kernel matrix</p> \\[     f = \\begin{bmatrix}     0 &amp; -1 &amp; 0 \\\\     -1 &amp; 5 &amp; -1 \\\\     0 &amp; -1 &amp; 0     \\end{bmatrix} \\] <p>An insight</p> <p>Let \\(I\\) be the original image, then the sharpen image can be consider as</p> \\[     I' = I + (I - \\text{blur}(I)), \\] <p>where \\(I - \\text{blur}(I)\\) can be regarded as the high frequency part of content.</p>"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#extract-gradients","title":"Extract Gradients","text":"<p>Two examples of kernel matrices</p> <ul> <li>\\(f = \\begin{bmatrix} -1 &amp; 0 &amp; 1 \\\\ -2 &amp; 0 &amp; 2 \\\\ -1 &amp; 0 &amp; 1\\end{bmatrix}\\) extracts horizontal gradients.</li> <li>\\(f = \\begin{bmatrix}  -1 &amp; -2 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\end{bmatrix}\\) extracts vertical gradients.</li> </ul>"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#bilateral-filters","title":"Bilateral Filters","text":"<p>Feature</p> <ul> <li>Kernel depends on image content.</li> <li>Better performance but lower efficiency.</li> </ul> <p>A trick: Separable Filter</p> <p>Definition</p> <p>A filter is separable if it can be wriiten as the outer product of two other filters.</p> <p>Example</p> \\[ \\frac19 \\begin{bmatrix} 1 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix} =  \\frac13 \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\times \\frac13 \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\end{bmatrix}  \\] <p>Purpose / Advantages: speed up the calculation.</p>"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#image-sampling","title":"Image Sampling","text":"<p>Basic Idea</p> <p>Down-sampling \u2192 Reducing image size.</p>"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#aliasing","title":"Aliasing","text":"<ul> <li>Aliasing is the artifacts due to sampling.</li> <li>Higher frequencies need faster sampling.</li> <li>Undersampling creates frequency aliases.</li> </ul>"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#fourier-transform","title":"Fourier Transform","text":"<p>Simly put, fourier transform represents a function as a weighted sum of sines and cosines, where the sines and consines are in various frequencies.</p> <p></p> <p>Convolution Theorem</p> <p> </p> <p>Now we can consider sampling and aliasing from the view of Fourier Transform (FT) ...</p>"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#sampling-and-aliasing-from-ft","title":"Sampling and Aliasing From FT","text":"<p>Sampling is repeating frequency contents. Aliasing is mixed frequency contents.</p> <p>Method to reduce aliasing</p> <ul> <li> <p>Increase sampling rate</p> <p>Nyquist-Shannon theorem<p>the signal can be perfectly reconstructed if sampled with a frequency larger than \\(2f_0\\).  </p> </p> </li> <li> <p>Anti-aliasing</p> <ul> <li> <p>Filtering \u2192 Sampling.</p> <p> </p> </li> </ul> </li> </ul> <p>Example</p> <p> </p>"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#image-magnification","title":"Image Magnification","text":"<p>Basic Idea</p> <p>Up-sampling is Inverse of down-sampling.</p> <p>An important method: Interpolation.</p>"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#1d-interpolation","title":"1D Interpolation","text":"<ul> <li> <p>Nearest-neighbour Interpolation</p> <ul> <li> <p>Feature: Not continuous; Not smooth.</p> <p> </p> </li> </ul> </li> <li> <p>Linear Interpolation</p> <ul> <li> <p>Feature: Continuous; Not smooth.</p> <p> </p> </li> </ul> </li> <li> <p>Cubic Interpolation</p> <ul> <li> <p>Feature: Continuous; Smooth.</p> <p> </p> </li> </ul> </li> </ul>"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#2d-interpolation","title":"2D Interpolation","text":"<p>(similar to 1D cases)</p> <ul> <li>Nearest-neighbour Interpolation</li> <li>Bilinear Interpolation<ul> <li>define \\(\\text{lerp}(x, v_0,v_1) = v_0 + x(v_1 - v_0)\\).</li> <li>Suppose the point in the rectangle surrounded by four points are \\(u_{00},u_{01},u_{10},u_{11}\\).</li> <li>then \\(f(x, y) = \\text{lerp}(t, u_0, u_1)\\), where \\(u_0 = \\text{lerp}(s, u_{00}, u_{10})\\) and \\(u_1 = \\text{lerp}(s, u_{01}, u_{11})\\).</li> </ul> </li> <li>Bicubic Interpolation          </li> </ul>"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#super-resolution","title":"Super-Resolution","text":"<p>Remains</p>"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#change-aspect-ratio","title":"Change Aspect Ratio","text":"<p>Remains</p>"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/","title":"Lecture 4 Model Fitting and Optimization","text":""},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#optimization","title":"Optimization","text":""},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#target","title":"Target","text":"\\[ \\begin{aligned} &amp; \\text{Minimize } &amp; f_0(x), \\\\ &amp; \\text{Subject to } &amp;f_i(x) &amp; \\le 0,\\ i = 1, \\dots, m, \\\\ &amp; &amp; g_i(x) &amp; = 0,\\ i = 1,\\dots, p. \\end{aligned} \\] <ul> <li>\\(x \\in \\mathbb{R}^n\\) is a vector variable to be chosen.</li> <li>\\(f_0\\) is the objective function to be minimized.</li> <li>\\(f_1, \\dots, f_m\\) are the inequality constraint functions.</li> <li>\\(g_1, \\dots, g_p\\) are the equality constraint functions.</li> </ul>"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#model-fitting","title":"Model Fitting","text":"<p>A typical approach: Minimize the Mean Square Error (MSE)</p> \\[ \\hat x = \\mathop{\\arg\\min}\\limits_x \\sum\\limits_i(b_i - a_i^Tx)^2. \\]"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#reasons-to-choose-mse","title":"Reasons to choose MSE","text":"<p>Key Assumptions: MSE is MLE with Gaussian noise.</p> <p>From Maximum Likelihood Estimation (MLE), the data is assumed to be with Gaussian noise.</p> \\[ b_i = a_i^Tx + n,\\ \\ n \\sim G(0, \\sigma). \\] <p>The likelihood of observing \\((a_i, b_i)\\) is </p> \\[ P[(a_i, b_i)|x] = P[b_i - a_i^Tx] \\propto \\exp\\left(-\\frac{(b_i - a_i^Tx)^2}{2\\sigma^2}\\right). \\] <p>If the data points are independent,</p> \\[ \\begin{aligned} P[(a_1, b_1)(a_2, b_2)\\dots|x] &amp; = \\prod\\limits_iP[(a_i, b_i)|x] = \\prod\\limits_iP[b_i - a_{i^T}x] \\\\ &amp; \\propto \\exp\\left(-\\frac{\\sum_i(b_i - a_i^Tx)^2}{2\\sigma^2}\\right) = \\exp\\left(-\\frac{||Ax-b||^2_2}{2\\sigma^2}\\right). \\end{aligned} \\] \\[ \\begin{aligned} \\hat x &amp;= \\mathop{\\arg\\max}\\limits_x P[(a_1, b_1)(a_2, b_2)\\dots|x] \\\\ &amp;= \\mathop{\\arg\\max}\\limits_x \\exp\\left(-\\frac{||Ax-b||^2_2}{2\\sigma^2}\\right)  = \\mathop{\\arg\\min}\\limits_x||Ax - b||_2^2. \\end{aligned} \\]"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#numerical-methods","title":"Numerical Methods","text":""},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#analytical-solution","title":"Analytical Solution \u89e3\u6790\u89e3","text":"<p>The derivative of \\(||Ax-b||^2_2\\) is \\(2A^T(Ax - b)\\), let it be \\(0\\). Then we get \\(\\hat x\\) satisfying</p> \\[ A^TAx = A^Tb. \\] <p>But, if no analytical solution, we should only consider the approximate solution.</p>"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#approximate-solution","title":"Approximate Solution \u8fd1\u4f3c\u89e3","text":"<p>Method</p> <ol> <li>\\(x \\leftarrow x_0\\) (Initialization),</li> <li>while not converge,<ol> <li>\\(p \\leftarrow \\text{descending\\_direction(x)}\\),</li> <li>\\(\\alpha \\leftarrow \\text{descending\\_step(x)}\\),</li> <li>\\(x \\leftarrow x + \\alpha p\\).</li> </ol> </li> </ol>"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#gradient-descent-gd","title":"Gradient Descent (GD)","text":"<p>Local Minimum and Global Minimum</p> <p> </p>"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#steepest-descent-method","title":"Steepest Descent Method","text":"\\[     F(x_0 + \\Delta x) \\approx F(x_0) + J_F\\Delta x. \\] <ul> <li>Direction \\(\\Delta x = -J^T_F\\).</li> <li> <p>Step</p> <ul> <li> <p>To minimize \\(\\phi(a)\\).</p> <p> </p> <p>Backtracking Algorithm</p> <ul> <li>Initialize \\(\\alpha\\) with a large value.</li> <li>Decrease \\(\\alpha\\) until \\(\\phi(\\alpha)\\le\\phi(0) + \\gamma\\phi'(0)\\alpha\\).</li> </ul> <p> </p> </li> </ul> </li> <li> <p>Advantage</p> <ul> <li>Easy to implement.</li> <li>Perform well when far from the minimum.</li> </ul> </li> <li>Disadvantage<ul> <li>Converge slowly when near the minimum, which wastes a lot of computation.</li> </ul> </li> </ul>"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#newton-method","title":"Newton Method","text":"\\[     F(x_0 + \\Delta x) \\approx F(x_0) + J_F\\Delta x + \\frac12\\Delta x^TH_F\\Delta x. \\] <ul> <li>Direction \\(\\Delta x = -H_F^{-1}J^T_F\\).</li> <li>Advantage<ul> <li>Faster convergence.</li> </ul> </li> <li>Disadvantage<ul> <li>Hessian matrix requires a lot of computation.</li> </ul> </li> </ul>"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#gauss-newton-method","title":"Gauss-Newton Method","text":"<p>For \\(\\hat x = \\mathop{\\arg\\min}\\limits_x ||Ax-b||^2_2 \\overset{\\Delta}{=\\!=}\\mathop{\\arg\\min}\\limits_x||R(x)||^2_2\\), expand \\(R(x)\\).</p> \\[     \\begin{aligned}     ||R(x_k+\\Delta x)||^2_2 &amp;\\approx ||R(x_k) + J_R\\Delta x||^2_2 \\newline      &amp;= ||R(x_k)||^2_2 + 2R(x_k)^TJ_R\\Delta x + \\Delta x^TJ^T_RJ_R\\Delta x.     \\end{aligned} \\] <ul> <li> <p>Direction \\(\\Delta x = -(J_R^TJ_R)^{-1}J_R^TR(x_k) = -(J_R^TJ_R)^{-1}J_F^T\\)</p> <ul> <li>Compared to Newton Method, \\(J_R^TJ_R\\) is used to approximate \\(H_F\\).</li> </ul> </li> <li> <p>Advantage</p> <ul> <li>No need to compute Hessian matrix.</li> <li>Faster to converge.</li> </ul> </li> <li> <p>Disadvantage</p> <ul> <li>\\(J^T_RJ_R\\) may be singular.</li> </ul> </li> </ul>"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#levenberg-marquardt-lm","title":"Levenberg-Marquardt (LM)","text":"<ul> <li> <p>Direction \\(\\Delta x = -(J_R^TJ_R + \\lambda I)^{-1}J_R^TR(x_k)\\).</p> <ul> <li>\\(\\lambda \\rightarrow \\infty\\) Steepest Descent.</li> <li>\\(\\lambda \\rightarrow 0\\) Gauss-Newton.</li> </ul> </li> <li> <p>Advantage</p> <ul> <li>Start and converge quickly.</li> </ul> </li> </ul>"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#convex-optimization","title":"Convex Optimization","text":"<p>Remains</p>"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#robust-estimation","title":"Robust Estimation","text":"<p>Inlier obeys the model assumption. Outlier differs significantly rom the assumption.</p> <p>Outlier makes MSE fail. To reduce its effect, we can use other loss functions, called robust functions.</p>"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#ransac-random-sample-concensus","title":"RANSAC (RANdom SAmple Concensus)","text":"<p>The most powerful method to handle outliers.</p> <p>Key ideas</p> <ul> <li>The distribution of inliers is similar while outliers differ a lot.</li> <li>Use data point pairs to vote.</li> </ul> <p> </p>"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#ill-posed-problem","title":"ill-posed Problem \u75c5\u6001\u95ee\u9898 / \u591a\u89e3\u95ee\u9898","text":"<p>Ill-posed problems are the problems that the solution is not unique. To make it unique:</p> <ul> <li> <p>L2 regularization</p> <ul> <li>Suppress redundant variables.</li> <li>\\(\\min_x||Ax-b||^2_2,\\ s.t. ||x||_2 \\le 1\\). (the same as \\(\\min_x||Ax-b||^2_2 + \\lambda||x||_2\\))</li> </ul> </li> <li> <p>L1 regularization</p> <ul> <li>Make \\(x\\) sparse.</li> <li>\\(\\min_x||Ax-b||^2_2,\\ s.t. ||x||_1 \\le 1\\). (the same as \\(\\min_x||Ax-b||^2_2 + \\lambda||x||_1\\))</li> </ul> </li> </ul>"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#graphcut-and-mrf","title":"Graphcut and MRF","text":"<p>A key idea: Neighboring pixels tend to take the same label.</p> <p>Images as Graphs</p> <ul> <li>A vertex for each pixel.</li> <li>An edge between each pair, weighted by the affinity or similarity between its two vertices.<ul> <li>Pixel Dissimilarity \\(S(\\textbf{f}_i, \\textbf{f}_j) = \\sqrt{\\sum_k(f_{ik} - f_{jk})^2}\\).</li> <li>Pixel Affinity \\(w(i, j) = A(\\textbf{f}_i, \\textbf{f}_j) = \\exp\\left(-\\dfrac{1}{2\\sigma^2}S(\\textbf{f}_i, \\textbf{f}_j)\\right)\\).</li> </ul> </li> <li>Graph Notation \\(G = (V, E)\\).<ul> <li>\\(V\\): a set of vertices.</li> <li>\\(E\\): a set of edges.</li> </ul> </li> </ul>"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#graph-cut","title":"Graph Cut","text":"<p>Cut \\(C=(V_A, V_B)\\) is a parition of vertices \\(V\\) of a graph \\(G\\) into two disjoint subsets \\(V_A\\) and \\(V_B\\).</p> <p>Cost of Cut: \\(\\text{cut}(V_A, V_B) = \\sum\\limits_{u\\in V_A, v\\in V_B} w(u, v)\\).</p> <p>Problem with Min-Cut</p> <p>Min-cut is bias to cut small and isolated segments.</p> <p> </p> <p>Solution: Normalized Cut</p> <p>Compute how strongly verices \\(V_A\\) are associated with vertices \\(V\\).</p> \\[     \\text{assoc}(V_A, V) =  \\sum\\limits_{u\\in V_A, v\\in V} w(u, v), \\] \\[ \\text{NCut}(V_A, V_B) = \\frac{\\text{cut}(V_A, V_B)}{\\text{assoc}(V_A, V)} + \\frac{\\text{cut}(V_A, V_B)}{\\text{assoc}(V_B, V)}. \\]"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#markow-random-field-mrf","title":"Markow Random Field (MRF)","text":"<p>Graphcut is an exception of MRF.</p> <p>Question</p>"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/","title":"Lecture 5 Image Matching and Motion Estimation","text":""},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#image-matching","title":"Image Matching","text":"<p>Abstract</p> <ol> <li>Dectection</li> <li>Description</li> <li>Matching</li> <li>Learned based matching</li> </ol>"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#1-detection","title":"1. Detection","text":"<p>We want uniqueness.</p>"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#corner-detection","title":"Corner Detection","text":"<p>Local measures of uniqueness</p> <p> shifting the window in any direction causes a big change</p> <p> distribution of gradients</p> <p>Principal Component Analysis (PCA) \u4e3b\u6210\u5206\u5206\u6790</p> <p>To describe the distribution of gradients by two eigenvectors.</p> <p> </p> <p>Method of Corner Detection</p> <ol> <li>Compute the covariance matrix \\(H\\) at each point and compute its eigenvalues \\(\\lambda_1\\), \\(\\lambda_2\\).</li> <li>Classification<ol> <li>Flat - \\(\\lambda_1\\) and \\(lambda_2\\) are both small.</li> <li>Edge - \\(\\lambda_1 \\gg \\lambda_2\\) or \\(\\lambda_1 \\ll \\lambda_2\\).</li> <li>Corner - \\(\\lambda_1\\) and \\(\\lambda_2\\) are both large.</li> </ol> </li> </ol> <p>Harris Corner Detector</p> <p>However, computing eigenvalues are expensive. Instead, we use Harris corner detector to indicate it.</p> <ol> <li>Compute derivatives at each pixel.</li> <li>Compute covariance matrix \\(H\\) in a Gaussian window around each pixel.</li> <li> <p>Compute corner response function (Harris corner detector) \\(f\\), given by</p> \\[     f = \\frac{\\lambda_1\\lambda_2}{\\lambda_1 + \\lambda_2} = \\frac{\\det(H)}{\\text{tr}(H)}. \\] </li> <li> <p>Threshold \\(f\\).</p> </li> <li>Find local maxima of response function.</li> </ol> <p>Invariance Properties</p> <ul> <li>Invariant to translation, rotation and intensity shift.</li> <li>Not invariant to scaling and intensity scaling.<ul> <li>How to find the correct scale?<ul> <li>Try each scale and find the scale of maximum of \\(f\\).</li> </ul> </li> </ul> </li> </ul> <p> </p> <p>Implementation</p> <p>image pyramid with a fixed window size.</p> <p> </p>"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#blob-detection","title":"Blob Detection","text":"<p>convolution!</p>"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#laplacian-of-gaussian-log-filter","title":"Laplacian of Gaussian (LoG) Filter","text":"<p>Example</p> \\[     f = \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; -4 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\end{bmatrix}. \\] <p>Laplacian is sensitive to noise. To solve this flaw, we often</p> <ol> <li>Smooth image with a Gaussian filter</li> <li>Compute Laplacian</li> </ol> \\[ \\nabla^2(f*g) = f*\\nabla^2g, \\text{ where $f$ is the Laplacian and $g$ is the Gaussian} \\]"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#scale-selection-the-same-problem-as-corner-detection","title":"Scale Selection (the same problem as corner detection)","text":"<p>The same solution as corner detection - Try and find maximum.</p>"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#implementation-difference-of-gaussian-dog","title":"Implementation: Difference of Gaussian (DoG)","text":"<p>A way to acclerate computation, since LoG can be approximated by DoG.</p> \\[ \\nabla^2G_\\sigma \\approx G_{\\sigma_1} - G_{\\sigma_2} \\]"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#2-description","title":"2. Description","text":"<p>We mainly focus on the SIFT (Scale Invariant Feature Transform) descriptor.</p>"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#orientation-normalization","title":"Orientation Normalization","text":"<ol> <li>Compute orientation histogram</li> <li>Select dominant orientation</li> <li>Normalize: rotate to fixed orientation</li> </ol>"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#properites-of-sift","title":"Properites of SIFT","text":"<ul> <li>Extraordinaraily robust matching technique<ul> <li>handle changes in viewpoint<ul> <li>Theoretically invariant to scale and rotation</li> </ul> </li> <li>handle significant changes in illumination</li> <li>Fast</li> </ul> </li> </ul> Other dectectores and descriptors <ul> <li>HOG</li> <li>SURF</li> <li>FAST</li> <li>ORB</li> <li>Fast Retina Key-point</li> </ul>"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#3-matching","title":"3. Matching","text":""},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#feature-matching","title":"Feature matching","text":"<ul> <li>Define distance function to compare two descriptors.</li> <li>Test all to find the minimum distance.</li> </ul> <p>Problem: repeated elements</p> <ul> <li> <p>To find that the problem happens: Ratio Test</p> </li> <li> <p>Ratio score = \\(\\frac{||f_1 - f_2||}{||f_1 - f_2'||}\\)</p> </li> <li> <p>best match \\(f_2\\), second best match \\(f'_2\\).</p> </li> </ul> <ul> <li>Another strategy: Mutual Nearest Neighbor<ul> <li>\\(f_2\\) is the nearest neighbour of \\(f_1\\) in \\(I_2\\).</li> <li>\\(f_1\\) is the nearest neighbour of \\(f_2\\) in \\(I_1\\).</li> </ul> </li> </ul>"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#4-learning-based-matching","title":"4. Learning based matching","text":"<p>Question</p>"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#motion-estimation","title":"Motion Estimation","text":"<p>Problems</p> <ul> <li>Feature-tracking</li> <li>Optical flow</li> </ul>"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#method-lucas-kanade-method","title":"Method: Lucas-Kanade Method","text":""},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#assumptions","title":"Assumptions","text":"<ol> <li>Small motion</li> <li>Brightness constancy</li> <li>Spatial coherence</li> </ol>"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#brightness-constancy","title":"Brightness constancy","text":"\\[ \\begin{aligned} I(x, y, t) &amp;= I(x + u, y + v, t + 1) \\\\ I(x + u, y + v, t + 1) &amp;\\approx I(x, y, t) + I_x u + I_y v + I_t \\\\ I_x u + I_y v + I_t &amp;\\approx 0 \\\\ \\nabla I \\cdot [u, v]^T + I_t &amp;= 0 \\end{aligned} \\]"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#aperture-problem","title":"Aperture Problem","text":"<p>Idea: To get more equations \u2192 Spatial coherence constraint</p> <p>Assume the pixel's neighbors have the same motion \\([u, v]\\).</p> <p>If we use an \\(n\\times n\\) window, </p> \\[ \\begin{bmatrix} I_x(\\textbf{p}_\\textbf{1}) &amp; I_y(\\textbf{p}_\\textbf{1}) \\\\ \\vdots &amp; \\vdots\\\\ I_x(\\textbf{p}_\\textbf{1}) &amp; I_y(\\textbf{p}_{\\textbf{n}^\\textbf{2}}) \\\\ \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = - \\begin{bmatrix} I_t(\\textbf{p}_\\textbf{1}) \\\\ \\vdots \\\\ I_t(\\textbf{p}_{\\textbf{n}^\\textbf{2}}) \\end{bmatrix} \\Rightarrow Ad=b \\] <p>More equations than variables. Thus we solve \\(\\min_d||Ad-b||^2\\).</p> <p>Solution is given by \\((A^TA)d = A^Tb\\), when solvable, which is \\(A^TA\\) should be invertible and well-conditioned (eigenvalues should not be too small, similarly with criteria for Harris corner detector).</p> <p>Thus it's easier to estimate the motion of a corner, then edge, then low texture region (flat).</p>"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#small-motion-assumption","title":"Small Motion Assumption","text":"<p>For taylor expansion, we want the motion as small as possbile. But probably not - at least larger than one pixel.</p>"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#solution","title":"Solution","text":"<ul> <li>Reduce the resolution</li> </ul> <ul> <li>Use the image pyramid</li> </ul>"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/","title":"Lecture 6 Image Stitching","text":""},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#image-warping","title":"Image Warping","text":"<p>Info</p> <ul> <li>Image filtering changes intensity of image.</li> <li>Image warping (\u7fd8\u66f2) changes shape of image.</li> </ul>"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#2d-transformations","title":"2D Transformations","text":""},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#affine-transformations","title":"Affine Transformations \u4eff\u5c04\u53d8\u6362","text":"\\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a &amp; b &amp; c \\\\ d &amp; e &amp; f \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}  \\] \\[ \\begin{aligned} x' &amp;= ax + by + c, \\\\ y' &amp;= dx + ey + f. \\end{aligned} \\] <p>For these equations, the degree of freedom (\u81ea\u7531\u5ea6) is \\(6\\), so we need at least \\(6\\) equations, which is \\(3\\) pairs of points, to solve out \\(a, b, c, d, e, f\\).</p>"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#projective-transformation-homography","title":"Projective Transformation (Homography) \u5355\u5e94\u53d8\u6362","text":"\\[ \\begin{bmatrix}     x' \\\\     y' \\\\     1 \\end{bmatrix} \\cong \\begin{bmatrix}     h_{00} &amp; h_{01} &amp; h_{02} \\\\     h_{10} &amp; h_{11} &amp; h_{12} \\\\     h_{20} &amp; h_{21} &amp; h_{22} \\\\ \\end{bmatrix} \\begin{bmatrix}     x \\\\     y \\\\     1. \\end{bmatrix}  \\] \\[ \\begin{aligned}     x' = \\frac{h_{00}x + h_{01}y + h_{02}}{h_{20}x + h_{11}y + h_{12}}, \\\\     y' = \\frac{h_{10}x + h_{11}y + h_{12}}{h_{20}x + h_{11}y + h_{12}}. \\end{aligned} \\] <p>By convention, we have a constraint of one of the two options below:</p> <ul> <li>The vector norm (\u5411\u91cf\u8303\u6570) \\(||(h_{00}, h_{01}, \\dots, h_{22})^T||\\) is \\(1\\).</li> <li>\\(h_{22} = 1\\).</li> </ul> <p>Consider the constraint above, for these equation, the degree of freedom is 8, so we need at least \\(8\\) equations, namely \\(4\\) pairs of points, to solve out \\(h_{ij}\\).</p>"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#summary-of-different-transformations","title":"Summary of Different Transformations","text":""},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#implementation","title":"Implementation","text":"<p>For convenience and maneuverability, we need a function that implies the general warping instead of different warping functions at different pixels.</p>"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#forward-warping-not-so-good","title":"Forward Warping (Not so good)","text":"<p>Suppose that an image function \\(f:(x, y) \\rightarrow (r, g, b)\\) transforms with \\(T:(x, y) \\rightarrow (x', y')\\) to a new image function \\(g:(x', y') \\rightarrow (r, g, b)\\).</p> <p>If the transformed pixed lands inside the pixels</p> <p> </p> <p>Problem  It's quite complicated to solve it.</p>"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#inverse-warping","title":"Inverse Warping","text":"<p>Why not consider inversely?</p> <p>If the transformed pixed lands inside the pixels</p> <p> </p> <p>Solution 2D Interpolation!</p>"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#image-stitching","title":"Image Stitching","text":"\\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} \\cong T \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}. \\]"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#image-matching","title":"Image Matching","text":"<p>By image matching we can obtain the matching points. Each pair of matching points can provide a matrix multiplication of the form shown above.</p>"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#find-t-for-image-wraping","title":"Find \\(T\\) for Image Wraping","text":""},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#affine-transformations_1","title":"Affine Transformations","text":"<p>For each matching,</p> \\[ \\begin{aligned}     x' = ax + by + c, \\\\     y' = dx + ey + f. \\end{aligned} \\] <p>The matrix form is</p> \\[     \\begin{bmatrix} x' \\\\ y' \\end{bmatrix}     =      \\begin{bmatrix} x &amp; y &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; x &amp; y &amp; 1 \\end{bmatrix}     \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix}. \\] <p>For \\(n\\) matchs, we have the following equations,</p> \\[ \\begin{bmatrix}     x_1 &amp; y_1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\     0 &amp; 0 &amp; 0 &amp; x_1 &amp; y_1 &amp; 1 \\\\     x_2 &amp; y_2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\     0 &amp; 0 &amp; 0 &amp; x_2 &amp; y_2 &amp; 1 \\\\     \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\     x_n &amp; y_n &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\     0 &amp; 0 &amp; 0 &amp; x_n &amp; y_n &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix} =  \\begin{bmatrix}     x_1' \\\\     y_1' \\\\     x_2' \\\\     y_2' \\\\     \\vdots \\\\     x_n' \\\\     y_n' \\end{bmatrix}, \\] <p>namely,</p> \\[ \\mathbf{A}_{2n \\times 6} \\times \\mathbf{t}_{6 \\times 1} = \\mathbf{b}_{2n \\times 1}. \\] <p>Note</p> <p>Although \\(3\\) pairs are the least number to solve out \\(a, b, c, d, e, f\\), there may be some linear dependent equations, which leads to a nonsingular matrix and degenerates the case that there is no unique solution.</p> <p>Thus, in order to get the value of \\(a, b, c, d, e, f\\), the most used method is sampling more points and calculate by MSE, namely determining \\(t\\) to minimize \\(||\\mathbf{A} \\mathbf{t} - \\mathbf{b}||\\).</p> <p>Method of Least Square</p> \\[ \\begin{aligned}     \\mathbf{A^{T}At} &amp;= \\mathbf{A^{T}b}, \\\\     t &amp;= \\mathbf{A^{T}A^{-1}A^{T}b}. \\end{aligned} \\]"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#projective-transformations","title":"Projective Transformations","text":"<p>Similarly, for each matching,</p> \\[ \\begin{aligned}     x_i'(h_{20}x_i + h_{21} y_i + h_{22}) = h_{00}x_i + h_{01}y_i + h_{02}, \\\\     y_i'(h_{20}x_i + h_{21} y_i + h_{22}) = h_{10}x_i + h_{11}y_i + h_{12}. \\end{aligned} \\] <p>The matrix form is</p> \\[ \\begin{bmatrix}     x_i &amp; y_i &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; -x_i'x_i &amp; -x_i'y_i &amp; -x_i'\\\\     0 &amp; 0 &amp; 0 &amp; x_i &amp; y_i &amp; 1 &amp; -y_i'x_i &amp; -y_i'y_i &amp; -y_i' \\end{bmatrix} \\begin{bmatrix}     h_{00} \\\\ h_{01} \\\\ h_{02} \\\\ h_{10} \\\\ h_{11} \\\\ h_{12} \\\\ h_{20} \\\\ h_{21} \\\\ h_{22} \\end{bmatrix} = \\begin{bmatrix}     0 \\\\ 0 \\end{bmatrix} \\] <p>For \\(n\\) matchs, we have the following equations,</p> \\[ \\begin{bmatrix}     x_1 &amp; y_1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\     0 &amp; 0 &amp; 0 &amp; x_1 &amp; y_1 &amp; 1 \\\\     x_2 &amp; y_2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\     0 &amp; 0 &amp; 0 &amp; x_2 &amp; y_2 &amp; 1 \\\\     \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\     x_n &amp; y_n &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\     0 &amp; 0 &amp; 0 &amp; x_n &amp; y_n &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix} =  \\begin{bmatrix}     x_1' \\\\     y_1' \\\\     x_2' \\\\     y_2' \\\\     \\vdots \\\\     x_n' \\\\     y_n' \\end{bmatrix}, \\] <p>namely,</p> \\[     \\mathbf{A}_{2n \\times 9} \\times \\mathbf{t}_{9 \\times 1} = \\mathbf{b}_{2n \\times 1}. \\] <p>Similarly, we need to minimize \\(||\\mathbf{Ah - 0}|| = \\mathbf{||Ah||}\\).</p> <p>For contraint: \\(||\\mathbf{h}|| = 1\\)</p> <p>If we use the contraint \\(||\\mathbf{h}|| = 1\\), we only need to determine the direction of \\(\\mathbf{h}\\), denoted by \\(\\mathbf{\\hat h}\\).</p> <p>Mathematically, we can prove that \\(\\mathbf{\\hat h}\\) is the corresponding eigenvector of the minimum eigenvalue \\(\\mathbf{A^{T}A}\\).</p>"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#robustness","title":"Robustness","text":"<p>Outliers</p> <p>Image matching is not perfect, and there may be some wrong matching points to be outlier (\u5b64\u7acb\u70b9).</p> <p>Solution: RANSAC</p> <ul> <li>Each time randomly select \\(s\\) sets of matching points.</li> <li>Calculate the transform matrix \\(T\\) by the selected matching points.</li> <li>Among all matching points, count the number of inliers that approximately fit the transformation \\(T\\) in a range of error as its score.</li> <li>Repeat \\(N\\) times and then select the transformation \\(T_0\\) that has the maximum score.</li> <li>(Better Step) For all matching points that approximately fit the transformation \\(T_0\\), calculate an average transform matrix \\(T\\).</li> </ul>"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#summary","title":"Summary","text":"<p>How to make image stitching?</p> <ol> <li>Input Images.</li> <li>Feature Matching.</li> <li>Compute transformation matrices \\(T\\) with RANSAC.</li> <li>Warp image 2 to image 1.</li> </ol> <p>Extension: Panaroma and Cylindrical Projection</p> <p> </p> <p>Problem: Drift</p> <p> </p> <p>Solution</p> <ul> <li>Small vertical errors accumulate over time.</li> <li>Apply correction s.t. the sum of drift is 0.</li> </ul> <p>Namely reduce the up and down shake when capturing panoroma.</p>"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/","title":"Lecture 7 Structure from Motion (SfM)","text":""},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#camera-calibration","title":"Camera Calibration \u76f8\u673a\u6807\u5b9a","text":"<p>The process of camera calibration is the process from world coordinates to image sensor, we focus on the four coordinates:</p> <ul> <li>World Coordinates (\u4e16\u754c\u5750\u6807\u7cfb)</li> <li>Camera Coordinates (\u76f8\u673a\u5750\u6807\u7cfb)</li> <li>Image Plane (\u56fe\u50cf\u5750\u6807\u7cfb)</li> <li>Image Sensor (\u50cf\u7d20\u5750\u6807\u7cfb)</li> </ul> <p>The parameters in the first two transform matrices are called Camera Extrinsics (\u76f8\u673a\u5916\u53c2); the parameters in the last two transform matrices are called Camera Intrinsics (\u76f8\u673a\u5185\u53c2).</p>"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#principle-analysis","title":"Principle Analysis","text":""},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#coordinate-transformation","title":"Coordinate Transformation","text":"<p>Coordinate transformation is from world coordinates to camera coordinates.</p> <p>Feature</p> <p>This transformation is Rigid Body Transformation \u521a\u4f53\u53d8\u6362 (only rotation and translation), corresponding to a rotation matrix \\(R\\) (which is also orthonormal (\u5355\u4f4d\u6b63\u4ea4)) and translation vector \\(c_w\\).</p> \\[ \\mathbf{x}_c = R(\\mathbf{x}_w - \\mathbf{c}_w) = R \\mathbf{x}_w - R \\mathbf{c}_w \\overset{\\Delta}{=\\!=} R \\mathbf{x}_w + \\mathbf{t},\\ \\ \\text{ where } \\mathbf{t } = -R \\mathbf{c}_w. \\] <p>Thus this transformation can be represented by \\(R\\) and \\(\\mathbf{t}\\) to indicate equivalently.</p> \\[ \\mathbf{x}_c = \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\end{bmatrix} = \\begin{bmatrix}     r_{11} &amp; r_{12} &amp; r_{13} \\\\     r_{21} &amp; r_{22} &amp; r_{23} \\\\     r_{31} &amp; r_{32} &amp; r_{33} \\\\ \\end{bmatrix} \\begin{bmatrix}     x_w \\\\ y_w \\\\ z_w \\end{bmatrix} + \\begin{bmatrix}     t_x \\\\ t_y \\\\ t_z \\end{bmatrix}. \\] <p>We also consider the homogenous coordinates,</p> \\[ \\mathbf{\\tilde x}_c = \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1\\end{bmatrix} = \\begin{bmatrix}     r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\\\     r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y\\\\     r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z\\\\     0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix}     x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix}. \\] <p>Extrinsic Matrix \u5916\u53c2\u77e9\u9635</p> \\[ M_{ext} = \\begin{bmatrix}     R_{3\\times 3} &amp; \\mathbf{t} \\\\     \\mathbf{0}_{1\\times 3} &amp; 1 \\end{bmatrix} = \\begin{bmatrix}     r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\\\     r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y\\\\     r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z\\\\     0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}, \\] <p>or omitting the last row,</p> \\[ M_{ext} = \\begin{bmatrix}     R_{3\\times 3} &amp; \\mathbf{t} \\\\ \\end{bmatrix} = \\begin{bmatrix}     r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\\\     r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y\\\\     r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z\\\\ \\end{bmatrix}. \\]"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#perspective-projection","title":"Perspective Projection","text":"<p>Perspective projection is from camera coordinates to image plane.</p> <p>We have discuss this part in the previous Lectures, thus we skip it and just leave the conclusion.</p> \\[ \\begin{bmatrix}     x_i \\\\ y_i \\\\ 1 \\end{bmatrix} \\cong \\begin{bmatrix}     f &amp; 0 &amp; 0 &amp; 0 \\\\     0 &amp; f &amp; 0 &amp; 0 \\\\     0 &amp; 0 &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix}     x_c \\\\     y_c \\\\     z_c \\\\     1 \\end{bmatrix} \\]"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#image-plane-to-image-sensor-mapping","title":"Image Plane to Image Sensor Mapping","text":"<p>pixel (\u50cf\u7d20\u70b9)</p> <p>Note that since the production and demand, each pixel is not necessarily a square, but a rectangle instead. Compare to the origin image, it has a certain degree of scaling. This discussion also appears in video making.</p> <p>In addition, since the origin of image plane is at the center of the image, while the origin of image sensor is at the top left corner, we need some tranlation for the coordinate transformation.</p> \\[ \\begin{aligned}     u &amp;= m_x f \\frac{x_c}{z_c} + c_x, \\\\     v &amp;= m_y f \\frac{y_c}{z_c} + c_y. \\end{aligned} \\] <p>Combining two transformations above, and denoting \\(f_x = m_x f\\), \\(f_y = m_y f\\),</p> <p>then the transformation from camera coordinates to image sensor is</p> \\[ \\mathbf{\\tilde{u}} = \\begin{bmatrix}     u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix}     f_x &amp; 0 &amp; c_x &amp; 0 \\\\     0 &amp; f_y &amp; c_y &amp; 0 \\\\     0 &amp; 0 &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix}     x_c \\\\     y_c \\\\     z_c \\\\     1 \\end{bmatrix}. \\] <p>Intrinsic Matrix \u5185\u53c2\u77e9\u9635</p> \\[ M_{int} =  \\begin{bmatrix}     f_x &amp; 0 &amp; c_x &amp; 0 \\\\     0 &amp; f_y &amp; c_y &amp; 0 \\\\     0 &amp; 0 &amp; 1 &amp; 0 \\end{bmatrix}, \\] <p>or</p> \\[ K =  \\begin{bmatrix}     f_x &amp; 0 &amp; c_x \\\\     0 &amp; f_y &amp; c_y \\\\     0 &amp; 0 &amp; 1 \\end{bmatrix}. \\] <p>Projection Matrix \\(P\\)</p> <p>World to Camera</p> \\[ \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1\\end{bmatrix} = \\begin{bmatrix}     r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\\\     r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y\\\\     r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z\\\\     0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix}     x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix}, \\] \\[     \\mathbf{\\tilde u} = M_{int}\\mathbf{\\tilde x}_c. \\] <p>Camera to Pixel</p> \\[ \\begin{bmatrix}     u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix}     f_x &amp; 0 &amp; c_x &amp; 0 \\\\     0 &amp; f_y &amp; c_y &amp; 0 \\\\     0 &amp; 0 &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix}     x_c \\\\     y_c \\\\     z_c \\\\     1 \\end{bmatrix}, \\] \\[     \\mathbf{\\tilde x}_c = M_{ext}\\mathbf{\\tilde x}_w. \\] <p>Thus</p> \\[     \\mathbf{\\tilde u} = M_{int}M_{ext} \\mathbf{\\tilde x}_w = P \\mathbf{\\tilde x}, \\] <p>where \\(P\\) is called Projection Matrix.</p> \\[ \\begin{bmatrix}     u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix}     p_{11} &amp; p_{12} &amp; p_{13} &amp; p_{14} \\\\     p_{21} &amp; p_{22} &amp; p_{23} &amp; p_{24} \\\\     p_{31} &amp; p_{32} &amp; p_{33} &amp; p_{34} \\\\     p_{41} &amp; p_{42} &amp; p_{43} &amp; p_{44} \\end{bmatrix} \\begin{bmatrix}     x_w \\\\     y_w \\\\     z_w \\\\     1 \\end{bmatrix}. \\]"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#implemetation","title":"Implemetation","text":"<p>Step 1. Capture an image of an object with known geometry.</p> <p>Step 2. Identify correspondences (Image Matching \u56fe\u50cf\u5339\u914d) between 3D scene points and image points.</p> <p>Step 3. For each corresponding point \\(i\\), we get </p> \\[ \\begin{bmatrix}     u^{(i)} \\\\ v^{(i)} \\\\ w^{(i)} \\end{bmatrix} \\cong \\begin{bmatrix}     p_{11} &amp; p_{12} &amp; p_{13} &amp; p_{14} \\\\     p_{21} &amp; p_{22} &amp; p_{23} &amp; p_{24} \\\\     p_{31} &amp; p_{32} &amp; p_{33} &amp; p_{34} \\\\     p_{41} &amp; p_{42} &amp; p_{43} &amp; p_{44} \\end{bmatrix} \\begin{bmatrix}     x_w^{(i)} \\\\     y_w^{(i)} \\\\     z_w^{(i)} \\\\     1 \\end{bmatrix}. \\] \\[ \\begin{aligned}     u^{(i)} &amp;= \\frac{p_{11}x_w^{(i)} + p_{12}y_w^{(i)} + p_{13}z_w^{(i)} + p_{14}}{p_{31}x_w^{(i)} + p_{32}y_w^{(i)} + p_{33}z_w^{(i)} + p_{34}}, \\\\     v^{(i)} &amp;= \\frac{p_{21}x_w^{(i)} + p_{22}y_w^{(i)} + p_{23}z_w^{(i)} + p_{24}}{p_{31}x_w^{(i)} + p_{32}y_w^{(i)} + p_{33}z_w^{(i)} + p_{34}} \\end{aligned}. \\] <p>Step 4. Rearrange.</p> \\[ \\small \\begin{bmatrix}     x_w^{(1)} &amp; y_w^{(1)} &amp; z_w^{(1)} &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; -u_1x_w^{(1)} &amp; -u_1y_w^{(1)} &amp; -u_1z_w^{(1)} &amp; -u_1 \\\\     0 &amp; 0 &amp; 0 &amp; 0 &amp;  x_w^{(1)} &amp; y_w^{(1)} &amp; z_w^{(1)} &amp; 1 &amp; -v_1x_w^{(1)} &amp; -v_1y_w^{(1)} &amp; -v_1z_w^{(1)} &amp; -v_1 \\\\     \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\      x_w^{(i)} &amp; y_w^{(i)} &amp; z_w^{(i)} &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; -u_ix_w^{(i)} &amp; -u_iy_w^{(i)} &amp; -u_iz_w^{(i)} &amp; -u_i \\\\     0 &amp; 0 &amp; 0 &amp; 0 &amp;  x_w^{(i)} &amp; y_w^{(i)} &amp; z_w^{(i)} &amp; 1 &amp; -v_ix_w^{(i)} &amp; -v_iy_w^{(i)} &amp; -v_iz_w^{(i)} &amp; -v_i \\\\     \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\      x_w^{(n)} &amp; y_w^{(n)} &amp; z_w^{(n)} &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; -u_nx_w^{(n)} &amp; -u_ny_w^{(n)} &amp; -u_nz_w^{(n)} &amp; -u_n \\\\     0 &amp; 0 &amp; 0 &amp; 0 &amp;  x_w^{(n)} &amp; y_w^{(n)} &amp; z_w^{(n)} &amp; 1 &amp; -v_nx_w^{(n)} &amp; -v_ny_w^{(n)} &amp; -v_nz_w^{(n)} &amp; -v_n \\\\ \\end{bmatrix} \\begin{bmatrix}     p_{11} \\\\ p_{12} \\\\ p_{13} \\\\ p_{14} \\\\     p_{21} \\\\ p_{22} \\\\ p_{23} \\\\ p_{24} \\\\     p_{31} \\\\ p_{32} \\\\ p_{33} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix}     0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\     0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\     0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\] \\[ i.e.\\ \\ A \\mathbf{p} = 0. \\] <p>Step 5. Solve for \\(P\\).</p> <p>Similarly, we can give some constraints.</p> <ul> <li>Option 1. \\(p_{34} = 1\\).</li> <li>Option 2. \\(||p|| = 1\\).</li> </ul> <p>Thus the degree of freedom is \\(11\\)\uff0cat least \\(6\\) pairs of points.</p> <p>Similarly we need to minimize \\(||\\mathbf{Ap - 0}|| = \\mathbf{||Ap||}\\).</p> <p>And also, if we select the constraint that the vector norm is \\(1\\), then we only need to determine the direction of \\(\\mathbf{p}\\), denoted by \\(\\mathbf{\\hat p}\\), and \\(\\mathbf{\\hat p}\\) is the corresponding eigenvector of the eigenvalue of \\(\\mathbf{A^{T}A}\\).</p> <p>Step 6. From \\(P\\) to Answers</p> \\[ \\begin{bmatrix}     p_{11} \\\\ p_{12} \\\\ p_{13} \\\\ p_{14} \\\\     p_{21} \\\\ p_{22} \\\\ p_{23} \\\\ p_{24} \\\\     p_{31} \\\\ p_{32} \\\\ p_{33} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix}     f_x &amp; 0 &amp; c_x &amp; 0 \\\\     0 &amp; f_y &amp; c_y &amp; 0 \\\\     0 &amp; 0 &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix}     r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\\\     r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y\\\\     r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z\\\\     0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\] <p>Notice that</p> \\[ \\begin{bmatrix}     p_{11} \\\\ p_{12} \\\\ p_{13} \\\\     p_{21} \\\\ p_{22} \\\\ p_{23} \\\\     p_{31} \\\\ p_{32} \\\\ p_{33} \\end{bmatrix} = \\begin{bmatrix}     f_x &amp; 0 &amp; c_x \\\\     0 &amp; f_y &amp; c_y \\\\     0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix}     r_{11} &amp; r_{12} &amp; r_{13} \\\\     r_{21} &amp; r_{22} &amp; r_{23} \\\\     r_{31} &amp; r_{32} &amp; r_{33} \\\\ \\end{bmatrix} = KR. \\] <p>where \\(K\\) is an Upper Right Triangular Matrix, \\(R\\) is an Orthonormal Matrix).</p> <p>From QR Factorization, we know that the solution of \\(K\\) and \\(R\\) is unique.</p> <p>And</p> \\[ \\begin{bmatrix}     p_{14} \\\\ p_{24} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix}     f_x &amp; 0 &amp; c_x \\\\     0 &amp; f_y &amp; c_y \\\\     0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix}     t_x \\\\ t_y \\\\ t_z \\end{bmatrix} = K \\mathbf{t}. \\] <p>Thus we can solve</p> \\[ \\mathbf{t} = K^{-1}  \\begin{bmatrix}     p_{14} \\\\ p_{24} \\\\ p_{34} \\end{bmatrix}. \\] <p>Problem: Distortion</p> <p> </p> <p>In fact, camera itself has some coefficients of distortion, we ignore its effects here.</p> <p>Perspective-n-Point Problem (PnP \u95ee\u9898)</p> <p>Weakning the premises, suppose camera intrinsics are known (namely matrix \\(K\\)), we need to solve camera extrinsics (namely matrix \\(R\\) and \\(\\mathbf{t}\\)), namely camera pose (\u76f8\u673a\u59ff\u6001).</p> <p>We have some methods to solve this problem.</p>"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#direct-linear-transform-dlt","title":"Direct Linear Transform (DLT)","text":"<p>It's the method mentioned above. First solve out \\(P\\), and from \\(K\\) solve out \\(R\\) and \\(\\mathbf{t}\\).</p>"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#p3p","title":"P3P","text":"<p>The question is with the coordinates \\(a, b, c, A, B, C\\) known, find out the coordinate of \\(O\\).</p> <ul> <li>First, we need at least \\(3\\) pairs of points.</li> </ul> Intermediates. <ol> <li>From Cosine Theorem,</li> </ol> \\[ \\begin{aligned}     OA^2 + OB^2 - 2 OA \\cdot OB \\cos \\angle AOB &amp;= AB^2, \\\\     OA^2 + OC^2 - 2 OB \\cdot OC \\cos \\angle BOC &amp;= BC^2, \\\\     OA^2 + OC^2 - 2 OA \\cdot OC \\cos \\angle AOC &amp;= AC^2. \\end{aligned} \\] <ol> <li>Let \\(x = \\dfrac{OA}{OC}\\), \\(y = \\dfrac{OB}{OC}\\), \\(v = \\dfrac{AB^2}{OC^2}\\), \\(u = \\dfrac{BC^2}{AB^2}\\), \\(w = \\dfrac{AC^2}{AB^2}\\), and simplify the equations above. We have</li> </ol> \\[ \\begin{aligned}     (1 - u)y^2 - ux^2 - cos \\angle BOC y + 2uxy \\cos \\angle AOB + 1 &amp;= 0, \\\\     (1 - w)x^2 - wy^2 - cos \\angle AOC y + 2wxy \\cos \\angle AOB + 1 &amp;= 0. \\end{aligned} \\] <ol> <li>The equations above are Binary Quadratic Equation (\u4e8c\u5143\u4e8c\u6b21\u65b9\u7a0b\u7ec4) with FOUR solutions, where two of them are the cases that the coordinate of \\(O\\) is between \\(\\text{Plane} ABC\\) and \\(\\text{Plane} abc\\), thus discarded.</li> </ol> <ul> <li>We need one ore pair of points for the unique solution, thus we need \\(4\\) pairs of points in total.</li> </ul>"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#pnp","title":"PnP","text":"<p>The idea of PnP is to minimize Reprojection Error (\u91cd\u6295\u5f71\u8bef\u5dee).</p> \\[     \\mathop{\\arg\\min\\limits_{R, \\mathbf{t}}} \\sum\\limits_i ||(p_i, K(RP_i + \\mathbf{t})||^2, \\] <p>where, \\(p_i\\) is the 2D coordinates of the projection plane and \\(P_i\\) is the 3D coordinates in space.</p> <p>The method is FIRST initializing by P3P\uff0cand SECOND optimizing by Gauss-Newton method.</p>"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#epnp","title":"EPnP","text":"<ul> <li>one of the most popluar methods.</li> <li>time complexity \\(O(N)\\).</li> <li>high accuracy.</li> </ul> <p>The general method is to solve by the linear combinations of four pairs of control points.</p> <p> </p>"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#two-frame-structure-from-motion","title":"Two-frame Structure from Motion","text":"Stereo vision \u7acb\u4f53\u89c6\u89c9 <p>Compute 3D structure of the scene and camera poses from views (images).</p> <p>Two-frame SfM is aimed at solving a problem: Given two images and the intrinsic matrices (\\(K_l(3 \\times 3)\\) and \\(K_r(3 \\times 3)\\))\uff0cfind camera pose and the coordinates of the captured objects.</p>"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#principle-induction","title":"Principle Induction \u539f\u7406\u63a8\u5bfc","text":"<p>Epipolar Geometry \u5bf9\u6781\u51e0\u4f55</p> <p>Epipolar geometry describes a geometry relation of \\(u_l\\), \\(u_r\\), which are the 2D projections of a 3D point \\(P\\) in two views. And through this relation build up the transformation of the two cameras (namely find \\(R\\) and \\(\\mathbf{t}\\)).</p> <p> </p> <p>Definition</p> <ul> <li>Epipole \u5bf9\u6781\u70b9: the projection of one camera in the other camera, i.e. \\(e_l\\) and \\(e_r\\) in the picture above.<ul> <li>For given two camera, \\(e_l\\) and \\(e_r\\) are unique.</li> </ul> </li> <li>Epipolar Plane of Scene Point P \u5173\u4e8e P \u70b9\u7684\u5bf9\u6781\u9762: the plane formed by Scene Point \\(P\\), \\(O_l\\) and \\(O_r\\). <ul> <li>Each scene point has a unique corresponding epipolar plane.</li> <li>Epipoles are on the epipolar plane.</li> </ul> </li> </ul> <p> </p>"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#essential-matrix-e","title":"Essential Matrix \\(E\\) \u57fa\u672c\u77e9\u9635","text":"<p>For an epipolar plane, we have</p> \\[ \\begin{aligned}     \\mathbf{n} = \\mathbf{t} \\times \\mathbf{x}_l, \\\\     \\mathbf{x}_l \\cdot (\\mathbf{t} \\times \\mathbf{x}_l) = 0. \\end{aligned} \\] <p>where \\(\\mathbf{n}\\) is called Normal Vector.</p> <p>Suppose the right camera transforms with a rigid body transformation to the left camera, namely</p> \\[ \\begin{aligned}     \\mathbf{x}_l &amp;= R \\mathbf{x}_r + \\mathbf{t}, \\\\     \\begin{bmatrix}         x_l \\\\ y_l \\\\ z_l     \\end{bmatrix}     &amp;=     \\begin{bmatrix}         r_{11} &amp; r_{12} &amp; r_{13} \\\\         r_{21} &amp; r_{22} &amp; r_{23} \\\\         r_{31} &amp; r_{32} &amp; r_{33}     \\end{bmatrix}     \\begin{bmatrix}         x_r \\\\ y_r \\\\ y_r     \\end{bmatrix}     +     \\begin{bmatrix}         t_x \\\\ t_y \\\\ t_z     \\end{bmatrix}. \\end{aligned} \\] <p>Moreover, we have</p> \\[ \\begin{aligned}     \\begin{bmatrix}         x_l &amp; y_l &amp; z_l     \\end{bmatrix}     \\begin{bmatrix}         t_yz_l - t_zy_l \\\\         t_zx_l - t_xz_l \\\\         t_xy_l - t_yx_l     \\end{bmatrix}     = 0,     &amp; \\text{ Cross-product definition} \\\\ \\\\     \\begin{bmatrix}         x_l &amp; y_l &amp; z_l     \\end{bmatrix}     \\underbrace{     \\begin{bmatrix}         0 &amp; -t_z &amp; t_y \\\\         t_z &amp; 0 &amp; -t_x \\\\         -t_y &amp; t_x &amp; 0     \\end{bmatrix}     }_{T_\\times}     \\begin{bmatrix}         x_l \\\\ y_l \\\\ z_l     \\end{bmatrix}     = 0.     &amp; \\text{ Matrix-vector form} \\end{aligned} \\] <p>Substitute to the transformation matrix,</p> \\[ \\small \\begin{bmatrix}     x_l &amp; y_l &amp; z_l \\end{bmatrix} \\left( \\underbrace{ \\begin{bmatrix}     0 &amp; -t_z &amp; t_y \\\\     t_z &amp; 0 &amp; -t_x \\\\     -t_y &amp; t_x &amp; 0 \\end{bmatrix} }_{T_\\times} \\underbrace{ \\begin{bmatrix}     r_{11} &amp; r_{12} &amp; r_{13} \\\\     r_{21} &amp; r_{22} &amp; r_{23} \\\\     r_{31} &amp; r_{32} &amp; r_{33} \\end{bmatrix} }_{R} \\begin{bmatrix}     x_r \\\\ y_r \\\\ y_r \\end{bmatrix} + \\underbrace{ \\begin{bmatrix}     0 &amp; -t_z &amp; t_y \\\\     t_z &amp; 0 &amp; -t_x \\\\     -t_y &amp; t_x &amp; 0 \\end{bmatrix} \\begin{bmatrix}     t_x \\\\ t_y \\\\ t_z \\end{bmatrix} }_{\\mathbf{t} \\times \\mathbf{t} = \\mathbf{0}} \\right) = 0. \\] \\[ \\begin{bmatrix}     x_l &amp; y_l &amp; z_l \\end{bmatrix} \\underbrace{ \\begin{bmatrix}     e_{11} &amp; e_{12} &amp; e_{13} \\\\     e_{21} &amp; e_{22} &amp; e_{23} \\\\     e_{31} &amp; e_{32} &amp; e_{33} \\end{bmatrix} }_{\\text{Essential Matrix } E} \\begin{bmatrix}     x_r \\\\ y_r \\\\ y_r \\end{bmatrix} ,\\ \\ \\text{ where } E = T_\\times R. \\] <p>Notice that \\(T_\\times\\) is a skew-symmetric matrix (\u53cd\u5bf9\u79f0\u77e9\u9635) and \\(R\\) an orthonomal matrix\uff0cFrom SVD Decomposition (Singule Value Decomposition), we can decompose \\(E\\) uniquely and get \\(T_\\times\\) and \\(R\\).</p>"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#fundemental-matrix-f","title":"Fundemental Matrix \\(F\\) \u672c\u771f\u77e9\u9635","text":"<p>From projection and intrinsic matrix \\(K\\), we have</p> \\[ \\small \\begin{aligned} \\text{Left Camera} &amp;&amp; \\text{Right Camera} \\\\ z_l  \\begin{bmatrix} u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &amp;= \\underbrace{ \\begin{bmatrix} f_x^{(l)} &amp; 0 &amp; o_x^{(l)} \\\\ 0 &amp; f_y^{(l)} &amp; o_y^{(l)} \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} }_{K_l} \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\end{bmatrix} &amp; z_r  \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} &amp;= \\underbrace{ \\begin{bmatrix} f_x^{(r)} &amp; 0 &amp; o_x^{(r)} \\\\ 0 &amp; f_y^{(r)} &amp; o_y^{(r)} \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} }_{K_r} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\end{bmatrix} \\\\ \\mathbf{x}_l^T &amp;= \\begin{bmatrix} u_l &amp; v_l &amp; 1 \\end{bmatrix} z_l (K_l^{-1})^T &amp; \\mathbf{x}_r &amp;=  K_r^{-1} z_r \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} \\end{aligned} \\] <p>Substitute the following equation to the equations above,</p> \\[ \\mathbf{x}^T_l E \\mathbf{x}_r = 0, \\] <p>we have</p> \\[ \\begin{bmatrix}     u_l &amp; v_l &amp; 1 \\end{bmatrix} z_l (K_l^{-1})^T E K_r^{-1} z_r \\begin{bmatrix}     u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = 0. \\] <p>Discard \\(z_l\\), \\(z_r\\), then</p> \\[ \\begin{bmatrix}     u_l &amp; v_l &amp; 1 \\end{bmatrix} F \\begin{bmatrix}     u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = 0 ,\\ \\ \\text{ where } E = K_l^T F K_r. \\]"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#implementation","title":"Implementation \u5b9e\u73b0","text":"<p>Step 1. Find a set (\\(m\\) pairs) of corresponding features. \u627e\u5230\u4e00\u7ec4\uff08\\(m\\) \u5bf9\uff09\u5339\u914d\u70b9.</p> <p>Least Number of Points</p> <p>At least 8 pairs: \\((u_l^{(i)}, v_l^{(i)})\\) \u2194 \\((u_r^{(i)}, v_r^{(i)})\\)</p> <p>NOTE that one pair only corresponds one equation. (not the same as before)</p> <p>Step 2. Build linear systems and solve for \\(F\\). \u5efa\u7acb\u65b9\u7a0b\u7ec4\u5e76\u6c42\u89e3\u77e9\u9635 \\(F\\).</p> <p>For each matching \\(i\\),</p> \\[ \\begin{bmatrix}     u_l^{(i)} &amp; v_l^{(i)} &amp; 1 \\end{bmatrix} \\begin{bmatrix}     f_{11} &amp; f_{12} &amp; f_{13} \\\\     f_{21} &amp; f_{22} &amp; f_{23} \\\\     f_{31} &amp; f_{32} &amp; f_{33} \\end{bmatrix} \\begin{bmatrix}     u_r^{(i)} \\\\ v_r^{(i)} \\\\ 1 \\end{bmatrix} = 0. \\] <p>Unfold it and we have</p> \\[ \\left( f_{11} u_r^{(i)} + f_{12} v_r^{(i)} + f_13 \\right)u_l^{(i)} + \\left( f_{21} u_r^{(i)} + f_{22} v_r^{(i)} + f_23 \\right)v_l^{(i)} + f_{31} u_r^{(i)} + f_{32} v_r^{(i)} + f_33 = 0. \\] <p>For all selected points, we have</p> \\[ \\begin{bmatrix}     u_l^{(1)}u_r^{(1)} &amp; u_l^{(1)}v_r^{(1)} &amp; u_l^{(1)} &amp; v_l^{(1)}u_r^{(1)} &amp; v_l^{(1)}v_r^{(1)} &amp; v_l^{(1)} &amp; u_r^{(1)} &amp; v_r^{(1)} &amp; 1 \\\\     \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\      u_l^{(i)}u_r^{(i)} &amp; u_l^{(i)}v_r^{(i)} &amp; u_l^{(i)} &amp; v_l^{(i)}u_r^{(i)} &amp; v_l^{(i)}v_r^{(i)} &amp; v_l^{(i)} &amp; u_r^{(i)} &amp; v_r^{(i)} &amp; 1 \\\\     \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\      u_l^{(m)}u_r^{(m)} &amp; u_l^{(m)}v_r^{(m)} &amp; u_l^{(m)} &amp; v_l^{(m)}u_r^{(m)} &amp; v_l^{(m)}v_r^{(m)} &amp; v_l^{(m)} &amp; u_r^{(m)} &amp; v_r^{(m)} &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix}     f_{11} \\\\ f_{12} \\\\ f_{13} \\\\     f_{21} \\\\ f_{22} \\\\ f_{23} \\\\     f_{31} \\\\ f_{32} \\\\ f_{33} \\\\ \\end{bmatrix} = \\begin{bmatrix}     0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}. \\] \\[     A \\mathbf{f} = \\mathbf{0}. \\] <p>Similarly, we can give some constraints.</p> <ul> <li>Option 1. \\(f_{33} = 1\\).</li> <li>Option 2. \\(||f|| = 1\\).</li> </ul> <p>Thus the degree of freedom is \\(8\\)\uff0cat least \\(8\\) pairs of points.</p> <p>Similarly we need to minimize \\(||\\mathbf{Af - 0}|| = \\mathbf{||Af||}\\).</p> <p>And also, if we select the constraint that the vector norm is \\(1\\), then we only need to determine the direction of \\(\\mathbf{p}\\), denoted by \\(\\mathbf{\\hat p}\\), and \\(\\mathbf{\\hat p}\\) is the corresponding eigenvector of the eigenvalue of \\(\\mathbf{A^{T}A}\\).</p> <p>Step 3. Find \\(E\\) and Extract \\(R\\), \\(\\mathbf{t}\\)</p> \\[     E = K_l^T F K_r. \\] <p>By SVD decomposition,  from \\(E = T_\\times R\\) we get \\(T_\\times\\) \u548c \\(R\\). </p> <p>\\(R\\) is the rotation matrix and \\(\\mathbf{t}\\) can be solved directly by \\(T_\\times\\).</p> <p>Step 4. Find 3D Position of Scene Points</p> <p>Now we have</p> \\[ \\begin{aligned} \\text{Left Camera} \\\\ \\begin{bmatrix}     u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &amp;= \\begin{bmatrix}     f_x^{(l)} &amp; 0 &amp; o_x^{(l)} &amp; 0 \\\\     0 &amp; f_y^{(l)} &amp; o_y^{(l)} &amp; 0 \\\\     0 &amp; 0 &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix}     x_l \\\\ y_l \\\\ z_l \\\\ 1 \\end{bmatrix} ,\\ \\  \\begin{bmatrix}     x_l \\\\ y_l \\\\ z_l \\\\ 1 \\end{bmatrix} = \\begin{bmatrix}     r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\\\     r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\\\     r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\\\     0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix}     x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix}, \\\\ \\begin{bmatrix}     u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &amp;= \\begin{bmatrix}     f_x^{(l)} &amp; 0 &amp; o_x^{(l)} &amp; 0 \\\\     0 &amp; f_y^{(l)} &amp; o_y^{(l)} &amp; 0 \\\\     0 &amp; 0 &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix}     r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\\\     r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\\\     r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\\\     0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix}     x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix}. \\\\ \\\\ \\normalsize \\mathbf{\\tilde u}_l &amp;=  \\normalsize P_l \\mathbf{\\tilde x}_r. \\\\ \\\\ \\small \\text{Right Camera} \\\\ \\begin{bmatrix}     u_r \\\\ v_r \\\\ 1 \\end{bmatrix} &amp;= \\begin{bmatrix}     f_x^{(r)} &amp; 0 &amp; o_x^{(r)} &amp; 0 \\\\     0 &amp; f_y^{(r)} &amp; o_y^{(r)} &amp; 0 \\\\     0 &amp; 0 &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix}     x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix}, \\\\ \\\\ \\normalsize \\mathbf{\\tilde u}_l &amp;=  \\normalsize M_{int_r} \\mathbf{\\tilde x}_r. \\end{aligned} \\] <p>From</p> \\[ \\begin{bmatrix}     u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = \\begin{bmatrix}     m_{11} &amp; m_{12} &amp; m_{13} &amp; m_{14} \\\\     m_{21} &amp; m_{22} &amp; m_{23} &amp; m_{24} \\\\     m_{31} &amp; m_{32} &amp; m_{33} &amp; m_{34} \\\\ \\end{bmatrix} \\begin{bmatrix}     x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} ,\\ \\ \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} p_{11} &amp; p_{12} &amp; p_{13} &amp; p_{14} \\\\ p_{21} &amp; p_{22} &amp; p_{23} &amp; p_{24} \\\\ p_{31} &amp; p_{32} &amp; p_{33} &amp; p_{34} \\\\ \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix}, \\] <p>we have</p> \\[ \\underbrace{ \\begin{bmatrix}     u_r m_{31} - m_{11} &amp; u_r m_{32} - m_{12} &amp; u_r m_{33} - m_{13} \\\\      v_r m_{31} - m_{21} &amp; v_r m_{32} - m_{22} &amp; v_r m_{33} - m_{23} \\\\      u_l p_{31} - p_{11} &amp; u_l p_{32} - p_{12} &amp; u_l p_{33} - p_{13} \\\\       v_l p_{31} - p_{21} &amp; v_l p_{32} - p_{22} &amp; v_l p_{33} - p_{23} \\end{bmatrix} }_{A_{4\\times 3}} \\underbrace{ \\begin{bmatrix}     x_r \\\\ y_r \\\\ z_r \\end{bmatrix} }_{\\mathbf{x}_r} = \\underbrace{ \\begin{bmatrix}     m_{14} - m_{34} \\\\     m_{24} - m_{34} \\\\       p_{14} - p_{34} \\\\      p_{24} - p_{34} \\end{bmatrix} }_{\\mathbf{b}} \\] <p>From the uniqueness of analysis, theoretically there are two rows that are linearly dependent. But in actual point selection, there can be some error, thus for maximize the usage of the data, we still use MSE.</p> <p>From MSE, we have</p> \\[     \\mathbf{x}_r = (A^TA)^{-1}A^T \\mathbf{b}. \\] <p>Non-linear Solution</p> <p>The method above is to solve linear system. And another idea is to minimize reprojection error (\u91cd\u6620\u5c04\u8bef\u5dee).</p> \\[     \\text{cost}(P) = \\text{dist}(\\mathbf{u}_l, \\mathbf{\\hat u}_l)^2 + \\text{dist}(\\mathbf{u}_r, \\mathbf{\\hat u}_r)^2. \\] <p> </p> <p>By the way, we can optimize \\(R\\) and \\(\\mathbf{t}\\) by this method.</p>"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#multi-frame-structure-from-motion","title":"Multi-frame Structure from Motion","text":"<p>Similar to Two-frame SfM, given \\(m\\) images, \\(n\\) 3D points and the intrinsic matrices, find the camera pose and the 3D coordinates of the objects.</p> <p>Namely for the equations below,</p> \\[ \\mathbf{u}_j^{(i)} = P_{proj}^{(i)} \\mathbf{P}_j ,\\ \\ \\text{ where } i = 1, \\dots, m, j = 1, \\dots, n. \\] <p>and \\(mn\\) 2D projection points \\(\\mathbf{u}_j^{(i)}\\), find \\(m\\) projection matrices \\(P_{proj}^{(i)}\\) and 3D coordinates of \\(n\\) points \\(P_j\\).</p>"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#solution-sequential-structrue-from-motion","title":"Solution: Sequential Structrue from Motion","text":"<p>Step 1. Initialize camera motion and scene structure. \u521d\u59cb\u5316</p> <p>First, select two images to do Two-frame SfM.</p> <p>Step 2. Deal with an addition view. \u5904\u7406\u4e0b\u4e00\u5f20\u56fe</p> <p>For each additional image,</p> <ul> <li>For the points with built 3D points: PnP Problem. </li> </ul> <ul> <li>For the points without built 3D points: do Two-frame SfM.</li> </ul> <p>Step 3. Refine structure and motion: Bundle Adjustment. \u96c6\u675f\u8c03\u6574</p> <p>After processing all images, we optimize the 3D coordinates and camera parameters by Bundle Adjustment (\u96c6\u675f\u8c03\u6574)\uff08implemented by LM algorithm\uff09, namely optimizing reprojection error (non-linear MSE):</p> \\[     E(P_{proj}, \\mathbf{P}) = \\sum\\limits_{i=1}^m \\sum\\limits_{j=1}^n \\text{dist} \\left( u_j^{(i)}, P_{proj}^{(i)} \\mathbf{P}_j \\right)^2. \\]"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#colmap","title":"COLMAP","text":"<p>COLMAP is a general-purpose Structure-from-Motion (SfM) and Multi-View Stereo (MVS) pipeline with a graphical and commandline interface. It offers a wide range of features for reconstruction of ordered and unordered image collections.</p> <p>Pipeline</p> <p> </p>"},{"location":"Computer_Science_Courses/MISSING/note/","title":"The Missing Semester of Your CS Education","text":"<p>Info</p> <p>Lecturer: Anish, Jon and Jose.</p> <p>Course Website:</p> <p>The Missing Semester of Your CS Education. (Chinese Version) [MIT 2020 version].</p> <p>My Feedback and Things Needed to Know before learning this course</p> <p>Generally, it's a course to teach you many useful tools, mostly based on command line. The environment is Mac OS / Linux based OS, and the shell they use is zsh. These tools help you doing things more efficiently and proficiently. However, I think having got a master of python and JSON will make you a better understanding of this course, which I haven't yet. :-&lt;</p> <p>Records on Youtube are really recomended, which also enables your learning skills through English. Because much content is not in the lecture notes.</p> <p>By the way, Jose had quite an accent so English substitutions are recomended. :-)</p>"},{"location":"Computer_Science_Courses/MISSING/note/#1-course-overview-and-the-shell","title":"1. Course Overview and the Shell","text":""},{"location":"Computer_Science_Courses/MISSING/note/#2-shell-tools-and-scripting","title":"2. Shell Tools and Scripting","text":""},{"location":"Computer_Science_Courses/MISSING/note/#script","title":"Script","text":"<p>Reference : Advanced Bash-Scripting Guide</p> <ul> <li>Assignment <code>foo=bar</code>.</li> <li>Access value <code>$foo</code>, <code>$(foo)</code>, <code>${foo}</code>.</li> </ul> Tip <p>Sometimes <code>${foo}</code> is not allowed depending on shell. Thus <code>$(foo)</code> is usually used.</p> <ul> <li>Space are the delimiters in shell. So <code>foo = bar</code> will be split to three arguments.</li> </ul> <p>Difference between quote <code>'</code> and <code>\"</code></p> <p>Provided that <code>foo=bar</code>, then echo <code>\"$foo\"</code> prints <code>bar</code> and echo <code>'$foo'</code> prints <code>$foo</code>.</p> <ul> <li> <p>Return value</p> <ul> <li>Commands will ofter return output through <code>STDOUT</code>, errors through <code>STDERR</code>.</li> </ul> </li> <li> <p>Special identifiers (selected, not all)</p> <ul> <li><code>$0</code> - Script name (command name).</li> <li><code>$1</code> to <code>$9</code> - Arugments to the script.</li> <li><code>$@</code> - All the arguments.</li> <li><code>$#</code> - Number of arguments.</li> <li><code>$?</code> - Return code of the previous command. If the previous command executes successfully, <code>$?</code> will return <code>1</code>, otherwise <code>0</code>.</li> <li><code>$$</code> - Process identification number (PID) for the script</li> <li><code>!!</code> - Entire last command. It's useful for sudo <code>!!</code>.</li> <li><code>$_</code> - Last argument from the last command.</li> </ul> </li> <li> <p>Short-circuiting operators \u77ed\u8def\u8fd0\u7b97\u7b26</p> <ul> <li><code>&amp;&amp;</code> and <code>||</code>. The same meaning as in C language.</li> </ul> </li> <li> <p>Substitution</p> <ul> <li>Command Substitution<ul> <li><code>$(CMD)</code> will executes command <code>CMD</code> and its return value replaces <code>$(CMD)</code>.</li> <li>e.g. <code>for file in $(ls)</code> to iterate the files.</li> </ul> </li> <li>Process Substitution<ul> <li><code>&lt;(CMD)</code> will executes command <code>CMD</code> and its return value will be stored in a temporary file. That's useful when the return value is passed by file instead of <code>STDIN</code>.</li> <li>e.g. <code>diff &lt;(ls foo) &lt;(ls bar)</code>.</li> </ul> </li> </ul> </li> <li> <p>Globbing \u901a\u914d</p> <ul> <li><code>?</code> match one amount of characters, <code>*</code> match any amount of characters.<ul> <li>e.g. <code>rm foo*</code> will remove <code>foo</code>, <code>foo1</code> and <code>foo123</code>. <code>rm foo?</code> will only remove <code>foo1</code>.</li> </ul> </li> <li><code>{}</code> expand the value in it.<ul> <li>e.g. touch <code>{foo,bar}/{a..c}</code> will expand to <code>touch foo/a foo/b foo/c bar/a bar/b bar/c</code>; <code>convert image.{png,jpg}</code> will exapand to <code>convert image.png image.jpg</code>.</li> </ul> </li> </ul> </li> </ul> <p>Shebang</p> <p>When writing bash script, you will commonly see <code>#!/usr/bin...</code> called shebang at the first line. When executed, it will run the script by the program this line specifies, not necessarily bash.</p> <p>Use command <code>env</code> for shebang is a good pratice. e.g. <code>#!/usr/bin/env python</code> will let shell execute the script using python.</p> Example <p>The following script will iterate through the arguments of files and append <code># foobar</code> if the file has no <code>foobar</code>.</p> <pre><code>#!/bin/bash\n\necho \"Starting program at $(date)\" # Date will be substituted\n\necho \"Running program $0 with $# arguments with pid $$\"\n\nfor file in \"$@\"; do\ngrep foobar \"$file\" &gt; /dev/null 2&gt; /dev/null\n    # When pattern is not found, grep has exit status 1\n# We redirect STDOUT and STDERR to a null register since we do not care about them\nif [[ $? -ne 0 ]]; then\necho \"File $file does not have any foobar, adding one\"\necho \"# foobar\" &gt;&gt; \"$file\"\nfi\ndone\n</code></pre>"},{"location":"Computer_Science_Courses/MISSING/note/#shell-tools","title":"Shell Tools","text":"<p>How to use commands?</p> <ul> <li><code>man</code>, <code>CMD -h</code> or <code>CMD --help</code>.</li> <li><code>tldr</code> (short for Too Long Don't Read) is also a good tool to search for frequently used methods of the command.</li> </ul>"},{"location":"Computer_Science_Courses/MISSING/note/#finding-files","title":"Finding Files","text":"<p><code>find</code></p> <ul> <li>Simple usage<ul> <li><code>find . -name src -type -d</code> finds all the directories named srcin the current directory.</li> <li><code>find . -path '*/test/*.py' -type f</code> finds all  .py files that have a parent folder named testin the current directory.</li> <li><code>find . -mtime -1</code> finds all files modified in the last day.</li> <li><code>find . size +500k -size -10M -name '.tar.gz'</code> finds all <code>.tar.gz</code> files with size ranging 500k to 10M.</li> </ul> </li> <li>Advanced usage</li> <li><code>find . -name '*.tmp' -exec rm {} \\;</code> removes all <code>.tmp</code> files</li> <li><code>find . -name '*.png' -exec convert {} {}.jpg \\;</code> convert all PNG files to JPG files.</li> <li>Generally, <code>PATTERN</code> is used for match like <code>find . -name 'PATTERN'</code>. <code>-iname</code> is optional to ignore the letter case.</li> </ul> <p><code>fd</code>, a better tool than <code>find</code>.</p> <p><code>locate</code></p> <ul> <li><code>locate</code> uses a database updated by <code>updatedb</code>.</li> <li>It's faster to find files in some certain directories.</li> </ul>"},{"location":"Computer_Science_Courses/MISSING/note/#finding-code","title":"Finding Code","text":"<p><code>grep</code></p> <ul> <li><code>-C</code> for getting context around the matching result.</li> <li><code>-v</code> for inverting the result.</li> <li><code>-R</code> for recursive finding.</li> </ul> <p><code>rg</code> (short for <code>ripgrep</code>)</p> Example <ul> <li><code>rg -t py 'import requests'</code> finds all .py files using requestslib.</li> <li><code>rg -u --files-without-match \"^#!\"</code> finds all files (including hidden files) without a shebang line.</li> <li><code>rg foo -A 5</code> finds all match of fooand prints the following 5 lines.</li> <li><code>rg -stats PATTERN</code> print statistics of matches.</li> </ul>"},{"location":"Computer_Science_Courses/MISSING/note/#find-shell-commands","title":"Find Shell Commands","text":"<ul> <li><code>history</code> with <code>grep</code>.</li> <li><code>Ctrl + R</code> performs backwards search through history. It can be used with <code>fzf</code>.</li> </ul> history-based autosuggestions <p>It's first introduced by fish shell. This feature dynamically autocompletes the current shell command with the most recent command.</p>"},{"location":"Computer_Science_Courses/MISSING/note/#directory-navigation","title":"Directory Navigation","text":"<ul> <li><code>alias</code> for command short cut. <code>ln -s</code> creates symbolic links.</li> <li><code>fasd</code> or <code>autojump</code> finds frequent or recent files and directories.</li> <li><code>tree</code> or <code>brootgive</code> an overview of a directory structure.</li> <li><code>nnn</code> or <code>ranger</code> is full fledged file manager. </li> </ul>"},{"location":"Computer_Science_Courses/MISSING/note/#3-editors-vim","title":"3. Editors (Vim)","text":"<p>Since I've learned about vim before. For the detail of basic operations please see the lecture notes and other resources. The following are some supplements.</p>"},{"location":"Computer_Science_Courses/MISSING/note/#vim-mode-in-other-programs","title":"Vim-mode in other programs","text":"<ul> <li>Shell.</li> <li>Readline.</li> <li>Vim in Browsers (Vimium-C), even in Jupyter notebooks.</li> </ul>"},{"location":"Computer_Science_Courses/MISSING/note/#advanced-vim","title":"Advanced Vim","text":""},{"location":"Computer_Science_Courses/MISSING/note/#search-and-replace","title":"Search and Replace","text":"<p><code>:s</code>: <code>%s/foo/bar/g</code> to replace <code>foo</code> with <code>bar</code> globally in file. <code>%s/\\[.*\\](\\(.*\\))/\\1/g</code> is to replace named Markdown links with plain URLs.</p>"},{"location":"Computer_Science_Courses/MISSING/note/#macro","title":"Macro","text":"<ul> <li><code>q{character}</code> to start recording a marcro in register <code>{character}</code>.</li> <li><code>q</code> to stop recording.</li> <li><code>@{character}</code> replays the macro<ul> <li><code>{number}@{character}</code> executes a macro <code>{number}</code> times.</li> </ul> </li> <li>Macros can be recursive.</li> </ul> Example : convert xml to json <p>File</p> <ol> <li>Record a macro stored in register <code>e</code> to format a single element.<ul> <li>Go to line with <code>&lt;name&gt;</code></li> <li><code>qe^r\"f&gt;s\": \"&lt;ESC&gt;f&lt;C\"&lt;ESC&gt;q</code></li> </ul> </li> <li>Record a macro stored in registser <code>p</code> to format a person.<ul> <li>Go to line with <code>&lt;person&gt;</code></li> <li><code>qpS{&lt;ESC&gt;j@eA,&lt;ESC&gt;j@ejS},&lt;ESC&gt;q</code></li> </ul> </li> <li>Record a macro stored in register <code>q</code> to format a person and go to the next person.<ul> <li>Go to line with <code>&lt;person&gt;</code></li> <li><code>qq@pjq</code></li> </ul> </li> <li>Execute macro until end of file.<ul> <li><code>999@q</code></li> </ul> </li> <li>Manually remove last <code>,</code> and add <code>[</code> and <code>]</code> delimiters.</li> </ol>"},{"location":"Computer_Science_Courses/MISSING/note/#4-data-wrangling","title":"4. Data Wrangling","text":"<p>An example runs through this lecture : Wrangle the data from <code>ssh myserver journalctl</code>.</p>"},{"location":"Computer_Science_Courses/MISSING/note/#regular-expressions-regex","title":"Regular Expressions (Regex)","text":""},{"location":"Computer_Science_Courses/MISSING/note/#common-patterns","title":"Common patterns","text":"<ul> <li><code>.</code> matches any singel character except newline <code>\\n</code> or <code>\\r</code>.</li> <li><code>*</code> zero or more of the preceding match.</li> <li><code>+</code> one or more of the preceding match.</li> <li><code>[abc]</code> matches any one character of <code>a</code>, <code>b</code> and <code>c</code>.</li> <li><code>(RX1|RX2)</code> either matches <code>RX1</code> or <code>RX2</code>.</li> <li><code>^</code> the start of the line.</li> <li><code>$</code> the end of the line.</li> <li>By default, <code>*</code> and <code>+</code> are greedy. Suffix <code>*</code> or <code>+</code> with <code>?</code> to take it non-greedy.</li> </ul>"},{"location":"Computer_Science_Courses/MISSING/note/#capture-group","title":"Capture group","text":"<ul> <li>Any match of parenthesis will stored in the capture group (unless <code>?:</code> or other similar patterns are put in the front of the parenthesis). We can use <code>\\1</code>, <code>\\2</code> to refer them.</li> </ul> <p>More patterns and details for regex on Regular expression - Wikipedia or \u6b63\u5219\u8868\u8fbe\u5f0f \u2013 \u6559\u7a0b | \u83dc\u9e1f\u6559\u7a0b.</p> <p>A nice tool to check regex: regex101: build, test, and debug regex.</p> <p><code>sed</code></p> <p><code>sed</code> is a stream editor building on top of the old <code>ed</code> editor. It supports regex for data wrangling.</p> <p>Mostly used command are <code>s</code> (substitution). Its syntax is <code>s/REGEX/SUBSTITUTION</code>. <code>REGEX</code> is the regular expression, <code>SUBSTITUTION</code> is the text to substitute matching text with.</p> <p>Regex of <code>sed</code> is a bit weird. It requires a <code>\\</code> before most of the character above to give them special meaning. Or more intuitively, pass <code>-E</code>.</p> <p>But <code>sed</code> don't support non-greedy mode. Another tool supporting non-greedy is <code>perl -pe</code>.</p> <p>At the last of these part, regex is used to wrangle data with the following command.</p> <pre><code>ssh myserver journalctl\n| grep sshd\n| grep \"Disconnected from\"\n| sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \\[preauth\\])?$/\\2/'\n</code></pre>"},{"location":"Computer_Science_Courses/MISSING/note/#other-command-for-data-wrangling","title":"Other command for data wrangling","text":"<p><code>sort</code></p> <ul> <li>By default, it sorts the data with the first column as a key word and ascending lexicographic order.</li> <li><code>-n</code> will sort in numeric order.</li> <li><code>-r</code> will reverse.</li> <li><code>-u</code> will take the unique value.</li> <li><code>-k</code> specifies the coloum as a key word.</li> </ul> <p><code>uniq</code></p> <ul> <li>filter repetitive lines</li> <li><code>-c</code> or <code>--count</code> will show a count of the number of occurrences.</li> <li><code>-u</code> will show the lines that only occur once.</li> </ul> <p><code>wc</code></p> <ul> <li>count something</li> <li><code>-c</code> or <code>--bytes</code> or <code>--chars</code> will show the count of bytes.</li> <li><code>-l</code> or <code>--lines</code> will  show the count of lines.</li> <li><code>-w</code> or <code>--words</code> will show the count of words.</li> </ul> <p><code>paste</code></p> <ul> <li><code>-d&lt;delimiter&gt;</code> or <code>--delimiters=&lt;delimiter&gt;</code> specifies the delimiter.</li> <li><code>-s</code> or <code>--serial</code> combines multiple lines into one line.</li> </ul> <p><code>awk</code></p> <p><code>awk</code> is a programming language that just happens to be good at processing text streams. As with many other things here, only basic wll be gone through.</p> <ul> <li><code>$0</code> is set to the entire line. <code>$1</code> through <code>$n</code> are set to the n-th field of that line, when separated by awk field separator (white space by default, change with <code>-F</code>).</li> <li>So <code>| awk '{print $2}'</code> will print the second part from <code>STDIN</code>. </li> <li>Compute the number of single used usernames that match some regex pattern (start with <code>c</code> and end with <code>e</code>) from <code>STDIN</code>: <code>| awk '$1 == 1 &amp;&amp; $2 ~ /^c[^ ]*e$/ {print $2}' | wc -l</code>. <code>wc -l</code> is also not needed for it if we rewrite the <code>awk</code> part to <code>awk 'BEGIN {rows = 0} $1 == 1 &amp;&amp; $2 ~ /^c[^ ]*e$/ {rows += $1} END {print rows}'</code>. <code>BEGIN</code> and <code>END</code> are patterns that match the start and the end of the input respectively.</li> </ul> <pre><code>ssh myserver journalctl\n | grep sshd\n | grep \"Disconnected from\"\n| sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \\[preauth\\])?$/\\2/'\n| sort | uniq -c\n | sort -nk1,1 | tail -n10\n | awk '{print $2}' | paste -sd,\n</code></pre>"},{"location":"Computer_Science_Courses/MISSING/note/#analyzing-data","title":"Analyzing Data","text":"<p><code>bc</code></p> <ul> <li>A simple caculator which support <code>+-*/^%</code> and <code>sqrt()</code>.</li> <li>Other feature<ul> <li>set decimal digits: <code>echo 'scale=2; (4.3 - 1.2) / 2' | bc</code> returns <code>1.55</code>.</li> <li>specify base: <code>echo 'ibase=2; 111' | bc</code> returns <code>7</code>.</li> <li>change base : <code>echo 'obase=2; 5 | bc</code> returns <code>101</code>.</li> </ul> </li> </ul> <p>Plot the Data</p> <p>R language or <code>gnuplot</code> for simpler plot.</p> <pre><code>ssh myserver journalctl\n | grep sshd\n | grep \"Disconnected from\"\n| sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \\[preauth\\])?$/\\2/'\n| sort | uniq -c\n | sort -nk1,1 | tail -n10\n | gnuplot -p -e 'set boxwidth 0.5; plot \"-\" using 1:xtic(2) with boxes'\n</code></pre>"},{"location":"Computer_Science_Courses/MISSING/note/#wrangling-binary-data","title":"Wrangling Binary Data","text":"<p>Notice that <code>-</code> takes the output to <code>STDIN</code> or takes the input from <code>STDIN</code>.</p> <pre><code>ffmpeg -loglevel panic -i /dev/video0 -frames 1 -f image2 - | convert - -colorspace gray -\n</code></pre>"},{"location":"Computer_Science_Courses/MISSING/note/#5-command-line-environment","title":"5. Command-line Environment","text":""},{"location":"Computer_Science_Courses/MISSING/note/#job-control","title":"Job Control","text":""},{"location":"Computer_Science_Courses/MISSING/note/#signal","title":"Signal","text":"<p>Shell uses a UNIX communication mechanism called a signal to communicate information to the process. When a process receives a signal, it stops its execution, deals with the signal and potentially changes the flow of execution. For this reason, signals are software interrupts.</p> <p>Use <code>man signal</code> or <code>kill -l</code> to learn more about it.</p>"},{"location":"Computer_Science_Courses/MISSING/note/#killing-a-process","title":"Killing a process","text":"<ul> <li><code>Ctrl + C</code> will deliver a <code>SIGINT</code> signal to the process.</li> <li><code>Ctrl + \\</code> will deliver a <code>SIGNQUIT</code> signal to the process.</li> </ul> <p>Although <code>SIGINT</code> and <code>SIGQUIT</code> are both usually associated with terminal related requests, a more generic signal for asking a process to exit gracefully is the <code>SIGTERM</code> signal. We can use kill command, with the syntax <code>kill -TERM &lt;PID&gt;</code>.</p> <p><code>SIGKILL</code> is a special signal. It cannot be captured by the process and will always terminate it immediately. But it can have bad side effects such as leaving orphaned children processes.</p>"},{"location":"Computer_Science_Courses/MISSING/note/#pausing-and-backgrouding-processes","title":"Pausing and backgrouding processes","text":"<p><code>STIGSTOP</code> pauses a process. In the terminal, <code>Ctrl + Z</code> will prompt the shell to send a <code>SIGTSTP</code> signal, short for Terminal Stop. (i.e. the terminal's version of <code>SIGSTOP</code>)</p> <p><code>fg</code> and <code>bg</code> can continue the paused job in the foreground or in the background respectively.</p> <p>Note</p> <p>To run the program in the background, suffix <code>&amp;</code> with the command. But it will still use the shell's <code>STDOUT</code>, or we can use shell redirections.</p> <p>To background an already running program, we can do <code>Ctrl + Z</code> and then execute <code>bg</code>. Note the background process are still children processes of the terminal and will die if the terminal will closed. Because closing the terminal sends a <code>SIGHUP</code> signal.</p> <p>To prevent it, we can run the program with <code>nohup</code> or use <code>disown</code> for the already running program. Or we can use a terminal multiplexer.</p>"},{"location":"Computer_Science_Courses/MISSING/note/#terminal-multiplexers","title":"Terminal Multiplexers","text":"<p><code>tmux</code></p> <ul> <li>the config file is <code>~/.tmux.conf</code>.</li> <li>a hierarchy of three layers : session, window and pane.</li> <li>detail in <code>tmux</code> and then press <code>&lt;C-b&gt;?</code>.</li> </ul>"},{"location":"Computer_Science_Courses/MISSING/note/#alias-dotfiles-and-ssh","title":"Alias, dotfiles and SSH","text":"<p>I've learn about it before. So this part is just for some supplements.</p>"},{"location":"Computer_Science_Courses/MISSING/note/#dotfile-version-control","title":"Dotfile Version Control","text":"<p>It's nice to put all the dotfiles together in a folder with version control. And symbolically link them to its own place.</p>"},{"location":"Computer_Science_Courses/MISSING/note/#execute-command-directly-from-ssh-device","title":"Execute command directly from ssh device","text":"<p>e.g. <code>ssh foobar@server ls</code> will execute <code>ls</code> in the home folder of <code>foobar</code>.</p>"},{"location":"Computer_Science_Courses/MISSING/note/#copy-files-over-ssh","title":"Copy files over SSH","text":"<ul> <li>ssh + tee</li> <li>scp</li> <li>rsync</li> </ul>"},{"location":"Computer_Science_Courses/MISSING/note/#ssh-config","title":"SSH config","text":"<p>Config file is in <code>~/.ssh/config</code>.</p> <pre><code>Host vm\n    User foobar\n    HostName 172.16.174.141\n    Port 2222\n    IdentityFile ~/.ssh/id_ed25519\n    LocalForward 9999 localhost:8888\n\n# wildcards \u901a\u914d\u7b26 are also OK!\nHost *.mit.edu\n    User foobaz\n</code></pre>"},{"location":"Computer_Science_Courses/MISSING/note/#shell-framework-and-terminal-emulators","title":"Shell &amp; Framework and Terminal Emulators","text":"<p><code>zsh</code> is a superset of bash with the following features.</p> <ul> <li>Smarter globbing</li> <li>Inline globbing</li> <li>Spelling correction</li> <li>Better completion/selection</li> <li>Path expansion</li> </ul> <p>For framework of shell, the following is some attainable features. - Right prompt - Command syntax highlighting - History substring search - Prompt themes</p>"},{"location":"Computer_Science_Courses/MISSING/note/#configuration-of-terminal-emulators","title":"Configuration of terminal emulators","text":"<p>A look at terminal emulators</p> <ul> <li>Font</li> <li>Theme</li> <li>Keyboard shortcuts</li> <li>Scrollback</li> <li>Performance (<code>alacritty</code> of <code>kitty</code> offer GPU acceleration)</li> </ul>"},{"location":"Computer_Science_Courses/MISSING/note/#6-version-control-git","title":"6. Version Control (Git)","text":"<p>Info</p> <p>Snapshot</p> <p>In git terminology, a file is called a blob and a directory is called a tree. A snapshot is the top-level tree that is being tracked.</p> <p>History Model</p> <p>In git, a history is a directed acyclic graph (DAG, \u6709\u5411\u65e0\u73af\u56fe) of snapshots.</p>"},{"location":"Computer_Science_Courses/MISSING/note/#structure-and-model-of-git","title":"Structure and Model of git","text":"<p>All git command line commands are manipulations of either the <code>references</code> data or the <code>objects</code> data.</p> Pseudo Code of git model<pre><code>// pseudocode\n\ntype blob = array&lt;byte&gt;\ntype tree = map&lt;string, tree | blob&gt;\ntype commit = struct {\n    parents: array&lt;commit&gt;\n    author: string\n    message: string\n    snapshot: tree\n}\n\ntype object = blob | tree | commit\nobjects = map&lt;string, object&gt;\n# The key of `objects` is the sha-1 of object, which is a hexadecimal string of 40 characters.\n\ndef store(o):\n    id = sha1(o)\n    object[id] = o\n\ndef load(id):\n    return object[id]\n\nreferences = map&lt;string, string&gt;    # &lt;name, sha-1 string&gt;\n# Name is immutable but sha-1 string not.\n</code></pre>"},{"location":"Computer_Science_Courses/MISSING/note/#7-debugging-and-profiling","title":"7. Debugging and Profiling","text":""},{"location":"Computer_Science_Courses/MISSING/note/#debugging","title":"Debugging","text":""},{"location":"Computer_Science_Courses/MISSING/note/#print-debugging-and-logging","title":"Print debugging and Logging","text":"<p>Here is an example code that logs messages.</p>"},{"location":"Computer_Science_Courses/MISSING/note/#color-the-log","title":"Color the Log","text":"<p>For better readability, we can color them. If the terminal supports true color (\u771f\u5f69\u8272), then <code>echo -e '\\e[38;2;255;0;0mThis is read\\e[0m'</code> will print out a red message <code>This is red</code>.</p> <p>The following script shows how to print many RGB colors into the terminal.</p> <pre><code>#!/usr/bin/env bash\nfor R in $(seq 0 20 255); do\nfor G in $(seq 0 20 255); do\nfor B in $(seq 0 20 255); do\nprintf \"\\e[38;2;${R};${G};${B}m\u2588\\e[0m\";\ndone\ndone\ndone\n</code></pre>"},{"location":"Computer_Science_Courses/MISSING/note/#third-party-logs","title":"Third party logs","text":"<p>System uses a system log, which is increasingly where all log messages go. Most Linux system use <code>systemd</code>. <code>systemd</code> places the logs under <code>/var/log/journal</code> in a specialized format and use <code>journalctl</code> to display the messages.</p> <p><code>logger</code> shell program supports logging under the system logs.</p>"},{"location":"Computer_Science_Courses/MISSING/note/#debugger","title":"Debugger","text":"<p>Debuggers are programs that let you interact with the execution of a program.</p> <p>Many programming languages come with some form of debugger. In Python this is <code>pdb</code>, or <code>ipdb</code> for better performance.</p> <p>For more low level programming, <code>gdb</code> and <code>lldb</code> may be a good choice.</p> <p>Even if trying to debug a black box binary, there are tools to help. <code>strace</code> in LInux can trace the system call the program uses.</p> <p>Static Analysis</p> <p>Static analysis programs take source code as input and analyze it using coding rules to reason about its correctness.</p> <p><code>pyflakes</code> and <code>mypy</code> for python and <code>shellcheck</code> for shell scripts.</p> <p>Most editors and IDEs support displaying the output of these tools within the editor, highlighting the locations of warning and errors. This is called code linting.</p>"},{"location":"Computer_Science_Courses/MISSING/note/#profiling","title":"Profiling","text":""},{"location":"Computer_Science_Courses/MISSING/note/#timing","title":"Timing","text":"<ul> <li>Real time<ul> <li>Wall clcok elapsed time from start to finish of the program, including the time taken by other processes and time taken while blocked ( e.g. waiting for I/O or network ).</li> </ul> </li> <li>User time<ul> <li>Amount of time spent in the CPU running user code.</li> </ul> </li> <li>Sys time<ul> <li>Amount of time spent in the CPU running kernel code.</li> </ul> </li> </ul>"},{"location":"Computer_Science_Courses/MISSING/note/#profiler","title":"Profiler","text":"<ul> <li>CPU<ul> <li>Two main types of CPU profilers : tracing and sampling profilers.<ul> <li>Tracing profilers keep a record of every function call the program makes.</li> <li>Sampling profilers probe the program periodically and record the program's stack.</li> </ul> </li> <li>e.g. <code>cProfiler</code> for python.</li> </ul> </li> <li>Memory<ul> <li>e.g. <code>memory_profiler</code> for python.</li> </ul> </li> <li>Event Profileing</li> </ul> <p>Visualization</p> <p>For CPU sampling profilers is to use a Flame Graph.</p> <p> </p> <p>Resource Monitoring</p> <ul> <li>General Monitoring <code>htop</code></li> <li>I/O operations <code>iotop</code></li> <li>Dis usage <code>df</code></li> <li>Memory Usage <code>free</code></li> <li>Open Files <code>lsof</code></li> <li>Network Connections and Config <code>ss</code></li> <li>Network Usage <code>`nethogs</code> and <code>iftop</code></li> </ul> <p><code>hyperfine</code> can quickly benchmark command line programs.</p>"},{"location":"Computer_Science_Courses/MISSING/note/#8-metaprogramming","title":"8. Metaprogramming","text":""},{"location":"Computer_Science_Courses/MISSING/note/#build-systems","title":"Build systems","text":"<p><code>make</code> is one of the most common build systems. It has its warts, but works quite well for simple-to-moderate projects. When we run <code>make</code>, it consult a file called <code>Makefile</code> in the current directory.</p>"},{"location":"Computer_Science_Courses/MISSING/note/#dependency-management","title":"Dependency Management","text":"<p>Different distros have differnet package manager and repository. But there are some common concepts of them.</p>"},{"location":"Computer_Science_Courses/MISSING/note/#versioning","title":"Versioning","text":"<p>Most projects that other projects depend on issue a version number with every release.</p> <p>The relatively common standard is semantic versioning. For its rules, every version number is of the form: <code>major.minor.patch</code>.</p> <ul> <li>Increase the <code>patch</code> version, if a new release does not change the API, such as security improvement.</li> <li>Increase the <code>minor</code> version, if an API is added in a backwards-compatible (\u5411\u4e0b\u517c\u5bb9) way.</li> <li>Increase the <code>major</code> version, if the API is changed in a non-backwaraeds-compatible way.</li> </ul>"},{"location":"Computer_Science_Courses/MISSING/note/#lock-file","title":"Lock File","text":"<p>A lock file is simply a file that lists the exact version you are currently depending on of each dependency. Usually, an update program should be run explicitly to upgrade the newer versions of the dependencies.</p>"},{"location":"Computer_Science_Courses/MISSING/note/#continuous-integration-systems","title":"Continuous Integration Systems","text":"<p>Continuous, or CI is an umbrella term for stuff that runs whenever your code changes.</p> <p>By far the most common one is a rule like \u300cwhen someone pushes code, run the test suite\u300d.</p>"},{"location":"Computer_Science_Courses/MISSING/note/#9-security-and-cryptography","title":"9. Security and Cryptography","text":"<p>Entropy</p> <p>Entropy is a measure of randomness. It's useful when determining the strength of a password.</p> <p>Entropy is measured in bits. When selecting uniformly at random from a set of possible outcomes (\u5747\u5300\u5206\u5e03\u7684\u968f\u673a\u79bb\u6563\u53d8\u91cf), the entropy is equal to \\(log_2(\\text{\\# of possibilities})\\).</p> <p>For on-line guessing, around 40 bits of entropy is pretty good. To be resistant to off-line guessing, a stronger password would be necessary (e.g. 80 bits or more).</p>"},{"location":"Computer_Science_Courses/MISSING/note/#hash-function","title":"Hash Function","text":"<pre><code>hash(value: array&lt;byte&gt;) -&gt; vector&lt;byte, N&gt; (for some fixed N)\n</code></pre> Example <p>An example is SHA-1, which used in git. It maps arbitrary-sized inputs to 160-bit outputs. We can try out SHA-1 with <code>sha1sum</code> command,</p> <pre><code>$ printf 'hello' | sha1sum\naaf4c61ddcc5e8a2dabede0f3b482cd9aea9434d\n$ printf 'hello' | sha1sum\naaf4c61ddcc5e8a2dabede0f3b482cd9aea9434d\n$ printf 'Hello' | sha1sum\nf7ff9e8b7bb2e09b70935a5d785e0cc5d9d0abf0\n</code></pre> <p>At a high level, a hash function can be thought of as a hard-to-invert random-looking (but deterministic) function.</p> <p>Properties</p> <ul> <li>Deterministic \u786e\u5b9a\u6027 : the same input always generates the same output.</li> <li>Non-invertible \u4e0d\u53ef\u9006\u6027 : it's hard to find an input <code>m</code> such that <code>hash(m) = h</code> for desired output <code>h</code>.</li> <li>Target collision resistant  \u76ee\u6807\u78b0\u649e\u62b5\u6297\u6027\uff08\u5f31\u65e0\u78b0\u649e\uff09: given an input <code>m1</code>, it's hard to find a different input <code>m2</code>, such that <code>hash(m1) = hash(m2)</code>.</li> <li>Collision resistant \u78b0\u649e\u62b5\u6297\u6027\uff08\u5f3a\u65e0\u78b0\u649e\uff09: it's hard to find two inputs <code>m1</code> and <code>m2</code> such that <code>hash(m1) = hash(m2)</code>. (strictly stronger than target collision resistant)</li> </ul> <p>Note</p> <p>Although it  may work for certain purpose (like implementing directory by hash table), SHA-1 is no longer considered a strong cryptographic hash function.</p> <p>Applications</p> <ul> <li>Git, for content-addressed storage \u5185\u5bb9\u5bfb\u5740\u5b58\u50a8. Hash function is not only cryptographic, it's used in git to avoid content-address conflict. </li> <li>Message digest \u6587\u4ef6\u4fe1\u606f\u6458\u8981. The official sites usually post hashes alongside the download links. Software can be downloaded from mirrors and be checked after downloading the file. </li> <li> <p>Commitment schemes \u627f\u8bfa\u673a\u5236. </p> Example <p>Suppose you want to commit to a particular value, but reveal the value itself later. For example, I want to do a fair coin toss \u201cin my head\u201d, without a trusted shared coin that two parties can see. I could choose a value <code>r = random()</code>, and then share <code>h = sha256(r)</code>. Then, you could call heads or tails (we\u2019ll agree that even <code>r</code> means heads, and odd <code>r</code> means tails). After you call, I can reveal my value <code>r</code>, and you can confirm that I haven\u2019t cheated by checking <code>sha256(r)</code> matches the hash I shared earlier.</p> </li> </ul>"},{"location":"Computer_Science_Courses/MISSING/note/#key-derivation-functions-kdfs","title":"Key Derivation Functions (KDFs)","text":"<p>Properties</p> <ul> <li>Slow : To slow down off-line brute-force attacks.</li> </ul> <p>Applications</p> <ul> <li>Storing login credentials. Generate and store a random salt <code>salt = random()</code> for each user, store <code>KDF(password + salt)</code> and verify by <code>KDF(input + salt)</code>. </li> <li>Producing keys from pass-phrases for use in other cryptographic algorithms.</li> </ul> <pre><code>flowchart LR\n    p[passphrase] --&gt; KDF --&gt; k[key] --&gt; encrypt\n    pt[plaintext] --&gt; encrypt\n    encrypt --&gt; ct[ciphertext]\n    style p fill:#FFF, stroke-width:0px\n    style k fill:#FFF, stroke-width:0px\n    style pt fill:#FFF, stroke-width:0px\n    style ct fill:#FFF, stroke-width:0px</code></pre>"},{"location":"Computer_Science_Courses/MISSING/note/#symmetric-cryptography","title":"Symmetric Cryptography","text":"<pre><code>keygen() -&gt; key (this function is randomized)\n\nencrypt(plaintext: array&lt;byte&gt;, key) -&gt; ciphertext: array&lt;byte&gt;\ndecrypt(ciphertext: array&lt;byte&gt;, key) -&gt; plaintext: array&lt;byte&gt;\n</code></pre> <p>An example of a symmetric cryptosystem in wide use today is AES.</p> <pre><code>$ openssl aes-256-cbc -salt -in note.md -out note.enc.md\nenter aes-256-cbc encryption password:\nVerifying - enter aes-256-cbc encryption password:\n$ openssl aes-256-cbc -d -in note.enc.md -out note.dec.md\nenter aes-256-cbc encryption password:\n$ cmp note.md note.dec.md\n$ echo $?\n0\n</code></pre> <p>Properties</p> <ul> <li>given <code>ciphertext</code>, it's hard to figure out <code>plaintext</code> without key.</li> <li><code>decrypt(encrypt(m, k), k) = m</code></li> </ul> <p>Applications</p> <ul> <li>Encrypting files for storage in an untrusted cloud service.</li> </ul>"},{"location":"Computer_Science_Courses/MISSING/note/#asymmetric-cryptography","title":"Asymmetric Cryptography","text":"<pre><code>keygen() -&gt; (public key, private key)\n\nencrypt(plaintext: array&lt;byte&gt;, public key) -&gt; ciphertext: array&lt;byte&gt;\ndecrypt(ciphertext: array&lt;byte&gt;, private key) -&gt; plaintext: array&lt;byte&gt;\n\nsign(message: array&lt;byte&gt;, private key) -&gt; signature: array&lt;byte&gt;\nverify(message: array&lt;byte&gt;, signature: array&lt;byte&gt;, public key) -&gt; bool (the signature is valid or not)\n</code></pre> <p>An example of an asymmetric cryptosystem is RSA.</p> <p>Properties</p> <ul> <li>hard to get plaintext from ciphertext without private key.</li> <li>decrypt(encrypt(m, k), k) = m</li> </ul> <p>Applications</p> <ul> <li>PGP email encryption.</li> <li>Private messaging. Apps like Signal or Keybase.</li> <li>Signing software. Git can have GPG-signed commit and tags. With a posted public key, anyone can verify the authenticity of downloaded software.</li> </ul> <p>Key Distribution</p> <p>It's a big challenge of distributing public keys / mapping public keys to real-world identities.</p> <p>Solutions</p> <ul> <li>for Signal : trust on first use, and support out-of-band \u7ebf\u4e0b public key exchange and safety number \u9762\u5bf9\u9762\u4ea4\u6362\u516c\u94a5.</li> <li>for PGP : web of trust \u4fe1\u4efb\u7f51\u7edc</li> <li>for Keybase : social proof \u793e\u4ea4\u7f51\u7edc\u8bc1\u660e</li> </ul>"},{"location":"Computer_Science_Courses/MISSING/note/#hybrid-encrytion","title":"Hybrid Encrytion","text":""},{"location":"Computer_Science_Courses/MISSING/note/#sender-end","title":"Sender End","text":"<pre><code>flowchart LR\n    m[plaintext] --&gt; se[symmetric encryption]\n    sk[symmetric keygen] --&gt; key --&gt; se --&gt; ct[ciphertext]\n    key --&gt; ae\n    pk[public key] --&gt; ae[asymmetric encryption] --&gt; ek[encrypted key]\n    style m fill:#FFF, stroke-width:0px\n    style key fill:#FFF, stroke-width:0px\n    style ct fill:#FFF, stroke-width:0px\n    style ek fill:#FFF, stroke-width:0px\n    style pk fill:#FFF, stroke-width:0px</code></pre>"},{"location":"Computer_Science_Courses/MISSING/note/#receiver-end","title":"Receiver End","text":"<pre><code>flowchart LR\n    ek[encrypted key] --&gt; ad[asymmetric decryption]\n    pk[private key] --&gt; ad[asymmetric decryption] --&gt; key --&gt; sd[symmetric decryption]\n    ct[ciphertext] --&gt; sd[symmetric decryption] --&gt; m[plaintext]\n    style m fill:#FFF, stroke-width:0px\n    style pk fill:#FFF, stroke-width:0px\n    style ek fill:#FFF, stroke-width:0px\n    style key fill:#FFF, stroke-width:0px\n    style ct fill:#FFF, stroke-width:0px</code></pre> <p>Case Studies</p> <ul> <li>Password managers: KeePassXC, Pass, 1Password</li> <li>Two-factor authentication</li> <li>Full disk encryption<ul> <li>Linux : cryptsetup + LUKS</li> <li>Windows : BitLocker</li> <li>macOS : FileVault</li> </ul> </li> <li>Private messaging</li> <li>Signal and Keybase. End-to-end security is bootstrapped from asymmetric-key encryption.</li> <li>SSH</li> </ul>"},{"location":"Computer_Science_Courses/MISSING/note/#10-11-pot-pourri-and-qa","title":"10 &amp; 11. Pot-pourri and Q&amp;A","text":""},{"location":"Mathematics_Basis/","title":"Title Page","text":"<p>Abstract</p> <p>This section stores the notes of mathematics basis.</p> <p>Algrebra, analysis, and any other courses or topics concerned about mathematics.</p> <p>Welcome to point out anything wrong and careless mistakes!</p>"},{"location":"Mathematics_Basis/DM/Chap_1/","title":"Chapter 1 | The Foundations: Logic and Proofs","text":""},{"location":"Mathematics_Basis/DM/Chap_1/#proposition","title":"Proposition","text":"<p>Definition</p> <p>Proposition (\u63a8\u65ad) is a statement that is either true or false, but not both.</p>"},{"location":"Mathematics_Basis/DM/Chap_1/#logical-operators-connective","title":"Logical Operators / Connective","text":"<ul> <li>Negation (\u5426\u5b9a) \\(\\neg p\\).</li> <li>Conjunction (\u5408\u53d6) \\(p \\wedge q\\).</li> <li>Disjunction (\u6790\u53d6) \\(p \\vee q\\).</li> <li> <p>Implication (\u6761\u4ef6 / \u8574\u542b) \\(p \\rightarrow q\\).</p> <ul> <li>\\(p\\) is called the hypothesis (or antecedent or premise).</li> <li>\\(q\\) is called the conclusion (or consequence).</li> </ul> Common Ways to Express Implication \\(p \\rightarrow q\\) <ul> <li>If \\(p\\) then \\(q\\).</li> <li>\\(p\\) is sufficient for \\(q\\).</li> <li>\\(p\\) implies \\(q\\).</li> <li>\\(q\\) is necessary for \\(p\\).</li> <li>If \\(q\\) whenever \\(p\\)</li> <li>\\(p\\) only if \\(q\\).</li> </ul> </li> <li> <p>Biconditional (\u53cc\u6761\u4ef6) \\(p \\leftrightarrow q\\).</p> </li> </ul> <p>Priorities \\(\\neg\\), then \\(\\wedge\\ \\vee\\), then \\(\\rightarrow\\ \\leftrightarrow\\).</p> <p>Remark</p> <ul> <li>There is no such a sign \\(\\leftarrow\\) in the discussion in Dicrete Mathematics.</li> <li> <p>Disjunction is inclusive or.</p> Example <ul> <li>inclusive or (\u6216\u3001\u517c\u6216). e.g. I passed mathematics or English.</li> <li>exclusive or (\u5f02\u6216). e.g. Paul was born in 1983 or 1984.</li> </ul> </li> <li> <p>\u5408\u53d6\u5426\u5b9a (denoted by Sheffer stroke \u8c22\u8d39\u5c14\u7ad6\u7ebf) \\(p\\uparrow q \\Leftrightarrow \\neg(p \\wedge q)\\)\uff0c\u6790\u53d6\u5426\u5b9a (denoted by Peirce arrow \u76ae\u5c14\u65af\u7bad\u5934) \\(p\\downarrow q \\Leftrightarrow \\neg(p \\vee q)\\).</p> </li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_1/#propositional-formula","title":"Propositional Formula","text":"<p>Definition</p> <p>(Well Formed) Formula (wff) are one of the follows:</p> <ul> <li>Propositional variable / Proposition, \\(T\\), \\(F\\).</li> <li>\\(\\neg A\\), \\((A \\wedge B)\\), \\((A \\vee B)\\), \\((A \\rightarrow B)\\), \\((A \\leftrightarrow B)\\), if \\(A\\) and \\(B\\) are formulae.</li> <li>Finitely many composed applications above.</li> </ul> <p>Definition | Classification</p> <ul> <li>tautology (\u91cd\u8a00\u5f0f / \u6c38\u771f\u5f0f): if its truth table contains only true values for every case.</li> <li>contradiction (\u6c38\u5047\u5f0f): if its truth table contains only false values for every case.</li> <li>contingence: neither a tautology nor a contradiction.</li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_1/#proposition-equivalences","title":"Proposition Equivalences","text":"<p>Definition</p> <p>Formulae \\(A\\) and \\(B\\) are logical equivalent if \\(A \\leftrightarrow B\\) is tautology, denoted by \\(A \\Leftrightarrow B\\).</p> <p>Definition</p> <p>if \\(p \\rightarrow q\\),</p> <ul> <li>inverse (\u5426\u547d\u9898) \\(\\neg p \\rightarrow \\neg q\\)</li> <li>converse (\u9006\u547d\u9898) \\(q \\rightarrow p\\)</li> <li>contrapositive (\u9006\u5426\u547d\u9898) \\(\\neg q \\rightarrow \\neg p\\)</li> </ul> <p>NOTE: \\(p \\rightarrow q \\Leftrightarrow \\neg q \\rightarrow \\neg p\\).</p>"},{"location":"Mathematics_Basis/DM/Chap_1/#predicates-and-quantifiers","title":"Predicates and Quantifiers","text":"<p>Definition</p> <p>Predicates (\u8c13\u8bcd / \u65ad\u8a00) is a  statement of the form \\(P(x_1, x_2, \\dots, x_n)\\), where \\(P\\) is a propositional function at the n-tuple \\((x_1, x_2,\\dots,x_n)\\).</p>"},{"location":"Mathematics_Basis/DM/Chap_1/#quantifier","title":"Quantifier \u91cf\u8bcd","text":"<ul> <li>domain / universe of discourse \u8bba\u57df</li> <li>universal quantifier \u5168\u79f0\u91cf\u8bcd<ul> <li>For all \\(x\\), \\(p(x): \\forall xp(x)\\).</li> </ul> </li> <li>existential quantifier \u5b58\u5728\u91cf\u8bcd<ul> <li>For some \\(x\\), \\(p(x): \\exists xp(x)\\).</li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_1/#banding-variables","title":"Banding Variables","text":"<p>Definition</p> <p>When a quantifier is used on the variable \\(x\\), then the occurrence of \\(x\\) is bound while others are free.</p> <p>The part of a logical expression to which a quantifier is applied is called the scope (\u8f96\u57df) of this quantifier.</p> <p>Remark</p> <ul> <li> <p>If the universe of discourse can be listed, then</p> \\[     \\forall xP(x) \\Leftrightarrow P(x_1) \\wedge \\dots \\wedge P(x_n) \\] \\[     \\exists xP(x) \\Leftrightarrow P(x_1) \\vee\\dots \\vee P(x_n) \\] </li> <li> <p>Properties</p> <ul> <li> <p>De Morgan's Laws</p> \\[     \\neg \\forall x p(x) \\Leftrightarrow \\exists x \\neg p(x),\\ \\ \\neg \\exists x p(x) \\Leftrightarrow \\forall x \\neg p(x). \\] </li> <li> \\[ \\begin{aligned}     \\forall x ((p(x) \\wedge q(x)) &amp; \\Leftrightarrow (\\forall x p(x)) \\wedge (\\forall x q(x)). \\\\     \\exists x ((p(x) \\vee q(x)) &amp; \\Leftrightarrow (\\exists x p(x)) \\vee (\\exists x q(x)). \\end{aligned} \\] </li> <li> \\[ \\begin{aligned}     \\forall x q(x) \\rightarrow A &amp; \\Leftrightarrow \\exists x (q(x) \\rightarrow A). \\\\     \\exists x q(x) \\rightarrow A &amp; \\Leftrightarrow \\forall x (q(x) \\rightarrow A). \\end{aligned} \\] </li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_1/#method-of-proof","title":"Method of Proof","text":"<p>Definition</p> <ul> <li>An axiom (\u516c\u7406) is a proposition accepted as true without proof.</li> <li>An theorem (\u5b9a\u7406) is a statement that can be shown to be true.<ul> <li>A lemma (\u5f15\u7406) is a small theorem used to prove a bigger theorem.</li> <li>A corollary (\u63a8\u8bba) is a theorem proven to be a logical consequnce of another theorem.</li> </ul> </li> <li>A conjecture (\u731c\u60f3) is a statement whose truth value is unknown.</li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_1/#valid-arguments","title":"Valid Arguments","text":"<p>Definition</p> <p>Deductive reasoning the process of reaching a conclusion \\(q\\) from a sequence of propositions \\(p_1,\\dots,p_n\\).</p> <ul> <li>\\(p_1,\\dots,p_n\\) are premises or hypothesis.</li> <li>\\(q\\) is conclusion.</li> </ul> <p>An argument in propositional logic is a sequence of propositions. All but the final proposition in the argument are called premises (\u5047\u8bbe) and the final proposition is called the conclusion (\u7ed3\u8bba).</p> <p>An argument is valid if when all the hypotheses are true, the conclusion is true.  </p>"},{"location":"Mathematics_Basis/DM/Chap_1/#rules-of-inference","title":"Rules of Inference","text":"Glossary <p>Hypothetical Judgement \u5047\u8a00\u63a8\u65ad: Modus ponens \u80af\u5b9a\u524d\u4ef6\u5f0f, Modus tollens \u5426\u5b9a\u540e\u4ef6\u5f0f.</p> <p>Hypothetical syllogism \u5047\u8a00\u4e09\u6bb5\u8bba, Disjunctive syllogism \u6790\u53d6\u4e09\u6bb5\u8bba.</p> <p>Addition \u9644\u52a0\u89c4\u5219, Simplification \u7b80\u5316\u89c4\u5219, Conjuction \u5408\u53d6\u89c4\u5219, Resolution \u6d88\u89e3\u539f\u7406.</p> <p>Remark</p> <ul> <li>\\((p_1 \\wedge p_2 \\wedge \\dots \\wedge p_n) \\rightarrow (p \\rightarrow q) \\Leftrightarrow (p_1 \\wedge p_2 \\wedge \\dots \\wedge p_n \\wedge p) \\rightarrow q\\).</li> </ul> <p>Contradiction \u53cd\u8bc1\u6cd5</p> <p>Principle \\((p_1 \\wedge p_2 \\wedge \\dots \\wedge p_n) \\rightarrow q \\Leftrightarrow \\neg (p_1 \\wedge p_2 \\wedge \\dots \\wedge p_n \\wedge p_n \\wedge \\neg q)\\)</p> <p>Steps for proof \\(p \\rightarrow q\\):</p> <ol> <li>Assume \\(p\\) is true and \\(q\\) is false.</li> <li>Show that \\(\\neg p\\) is also true.</li> <li>\\(p \\wedge \\neg p = F\\), which leads to a contradiction.</li> </ol> <ul> <li>About Resolution rule<ul> <li>Use for automatic theorem proving.</li> <li>\\(q\\vee r\\) is called the resolvent.</li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_1/#normal-form","title":"Normal Form","text":"<p>Definition</p> <ul> <li>A literal is a variable or its negation.</li> <li>Conjunctive clauses (short for clauses) are conjunctions with literals as conjuncts.</li> <li>A formula is in disjunctive normal norm (DNF) if it's wriiten as a disjunction of conjunctions of literals.</li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_1/#disjunctive-normal-form-dnf","title":"Disjunctive Normal Form (DNF)","text":"<p>Quote</p> <p>Similar discussion can be seen in the canonical form and K-map of Digital Design.</p> \\[     \\bigvee\\limits _{i = 1}^{k} \\bigwedge\\limits _{j = 1}^{n_i} A_{ij} = (A_{11} \\wedge A_{12} \\wedge \\dots \\wedge A_{1n_1}) \\vee \\dots \\vee (A_{k1} \\wedge A_{k2} \\wedge \\dots \\wedge A_{kn_1}). \\] <p>Theorem</p> <p>Any formula is tautologically equivalent to some formula in DNF.</p>"},{"location":"Mathematics_Basis/DM/Chap_1/#full-disjunctvie-normal-form","title":"Full Disjunctvie Normal Form","text":"<p>Definition</p> <ul> <li>A minterm is a conjunction of literals in which each variable is represented exactly one.</li> <li>A full disjunctive form is a boolean function expressed as a disjunction of minterms.</li> </ul> <p>Remark</p> <ul> <li>\\(A \\text{ is tautology} \\Leftrightarrow \\left(A \\Leftrightarrow \\bigvee\\limits _{i = 0}^{2^n - 1}m_i\\right)\\)</li> <li>Full disjunctive form can be obtained by using truth table.</li> <li>\\(\\{\\neg, \\wedge, \\vee\\}\\) is functionally complete.</li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_1/#conjuctive-normal-form-cnf-full-conjuctive-normal-form","title":"Conjuctive Normal Form (CNF) &amp; Full Conjuctive Normal Form","text":"<p>Similar to DNF / Full DNF.</p> \\[     \\bigwedge\\limits _{i = 1}^{k} \\bigvee\\limits _{j = 1}^{n_i} A_{ij} = (A_{11} \\vee A_{12} \\vee \\dots \\vee A_{1n_1}) \\wedge \\dots \\wedge (A_{k1} \\vee A_{k2} \\vee \\dots \\vee A_{kn_1}). \\]"},{"location":"Mathematics_Basis/DM/Chap_1/#prenex-normal-form","title":"Prenex Normal Form \u524d\u675f\u8303\u5f0f","text":"<p>Definition</p> <p>A formula is in prenex normal form if it's of the form</p> \\[     Q_1x_1Q_2x_2\\cdots Q_nx_nB, \\] <p>where \\(Q_i(i = 1, 2, \\cdots, n)\\) is \\(\\forall\\) or \\(\\exists\\) and \\(B\\) is quantifier free.</p> <p>Algorithm of prenex normal form</p> <ol> <li>Rename bounded variables.</li> <li>Eliminate all connectives \\(\\rightarrow\\) and \\(\\leftrightarrow\\).</li> <li>Move all negations inward.</li> <li>Move all quantifiers to the front of the formula.</li> </ol> <p>Example</p> <p>Convert</p> \\[     \\forall x (\\forall y P(y, x) \\rightarrow \\exist y Q(x, y)) \\] <p>into prenex normal form.</p> <p>Solution.</p> \\[ \\begin{aligned}     \\forall x &amp; (\\forall y P(y, x) \\rightarrow \\exists y Q(x, y)) \\\\     &amp; \\Leftrightarrow \\forall x (\\neg \\forall y P(y, x) \\vee \\exists y Q(x, y)) \\\\     &amp; \\Leftrightarrow \\forall x (\\exists y \\neg P(y, x) \\vee \\exists y Q(x, y)) \\\\     &amp; \\Leftrightarrow \\forall x (\\exists y \\neg P(y, x) \\vee \\exists z Q(x, z)) \\\\     &amp; \\Leftrightarrow \\forall x \\exists y \\exists z (\\neg P(y, x) \\vee Q(x, z)) \\\\     &amp; \\Leftrightarrow \\forall x \\exists y \\exists z (P(y, x) \\rightarrow Q(x, z)) \\\\ \\end{aligned} \\]"},{"location":"Mathematics_Basis/DM/Chap_10/","title":"Chapter 10 | Graphs","text":"<p>Definition</p> <ul> <li>A simple graph \\(G = (V, E)\\) consists of \\(V\\), a nonempty set of vertices and \\(E\\), a set of unordered pair of distinct elements of \\(V\\) called edges.</li> <li>A multigraph \\(G = (V, E)\\) consists of \\(V\\), \\(E\\) and a function from \\(E\\) to \\(\\{\\{u, v\\} | u, v \\in V, u \\neq v\\}\\). The edges \\(e_1\\) and \\(e_2\\) are called multiple or parallel edges if \\(f(e_1) = f(e_2)\\).</li> <li>A pseudograph \\(G = (V, E)\\) consists of \\(V\\), \\(E\\) and a function from \\(E\\) to \\(\\{\\{u, v\\} | u, v \\in V\\}\\). An edge is a loop if \\(f(e) = \\{u, u\\} = \\{u\\}\\) for some \\(u \\in V\\).</li> <li>A directed graph \\(G = (V, E)\\) consists of \\(V\\), and a set of edges \\(E\\) that are ordered pairs of elemetnes of \\(V\\).</li> <li>A directed multigraph \\(G = (V, E)\\) consists of \\(V\\), \\(E\\) and a function from \\(E\\) to \\(\\{(u, v) | u, v \\in V, u \\neq v\\}\\). The edges \\(e_1\\) and \\(e_2\\) are called multiple or parallel edges if \\(f(e_1) = f(e_2)\\).</li> </ul> Type Edges Multiple Edges? Loop? Simple Graph Undirected No No Multigraph Undirected Yes No Pseudograph Undirected Yes Yes Directed Graph Directed No Yes Directed multigraph Directed Yes Yes <p>Definition</p> <ul> <li>\\(H\\) is a subgraph of \\(G\\), if \\(W \\subseteq V,\\ \\ F \\subseteq E\\).</li> <li>\\(H\\) is a spanning subgraph of \\(G\\), if \\(W = V,\\ \\ F \\subseteq E\\).</li> <li>Union of two simple graph \\(G_1 = (V_1, E_1)\\) and \\(G_2 = (V_2, E_2)\\) is the simple graph with vertex set \\(V_1 \\cup V_2\\) and edge set \\(E_1 \\cup E_2\\), denoted by \\(G_1 \\cup G_2\\).</li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_10/#adjacency-and-degree","title":"Adjacency and Degree","text":"<p>Tip</p> <p>In undirected graph, we use \\(\\{u, v\\}\\) to denote an edge, while in directed graph, we use \\((u, v)\\) to denote an edge.</p> <p>Definition</p> <p>For Undirected graph,</p> <ul> <li>\\(u\\) and \\(v\\) are adjacent or neighbours if \\(e = \\{u, v\\}\\) is an edge of \\(G\\).<ul> <li>\\(e\\) is incident with \\(u\\) and \\(v\\).</li> <li>\\(u\\) and \\(v\\) are endpoints.</li> </ul> </li> <li>The degree of a vertex is the number of edges incident with it, except that a loop at a vertex contribute twice to the degree, denoted by \\(\\text{deg}(v)\\).<ul> <li>Isolated vertex: \\(\\text{deg} = 0\\).</li> <li>Pendant vertex: \\(\\text{deg} = 1\\).</li> </ul> </li> </ul> <p>Theorem | Handshaking Theorem</p> <p>\\(G\\) is a graph with \\(n\\) vertices \\(v_1, v_2, \\cdots, v_m\\) and \\(m\\) edges, then</p> \\[     \\sum\\limits_{i = 1}^{n} \\text{deg}(v_i) = 2m. \\] <p>Definition</p> <p>For a directed graph,</p> <ul> <li>If \\(e = (u, v)\\) is an edge of \\(G\\).<ul> <li>\\(u\\) is the initial vertex and adjecnt to \\(v\\).</li> <li>\\(u\\) is the end vertex or terminal and adjecnt from \\(v\\).</li> </ul> </li> <li>In-degree of \\(v\\), denoted by \\(\\text{deg}^{-}v\\), is the number of edges with \\(v\\) as their end vertex.</li> <li>Out-degree of \\(v\\), denoted by \\(\\text{deg}^{+}v\\), is the number of edges with \\(v\\) as their initial vertex.</li> </ul> <p>Theorem</p> <p>\\(G\\) is a directed graph,</p> \\[     \\sum\\limits_{i = 1}^{n} \\text{deg}^{-}(v_i) = \\sum\\limits_{i = 1}^{n} \\text{deg}^{+}(v_i) = |E|. \\]"},{"location":"Mathematics_Basis/DM/Chap_10/#special-simple-graphs","title":"Special Simple Graphs","text":"<p>Complete Graph \\(K_n\\)</p> <p>The complete graph \\(K_n\\) is the simple graph with \\(n\\) vertices and every pair of vertices is joined by an edge.</p> \\[     K_n = (V, E),\\ \\ \\text{ where } E = \\{\\{u, v\\} | u \\ne v\\}. \\] <p>Remark \\(|E| = C(n ,2)\\).</p> <p> </p> <p>Cycles \\(C_n\\)</p> <p>The cycle \\(C_n\\) (\\(n ge 3\\)) consists of \\(n\\) vertices \\(v_1, v_2, \\dots, v_n\\) and edges \\(\\{v_1, v_2\\}, \\{v_2, v_3\\}, \\dots \\{v_{n - 1}, v_n\\} and \\{v_n, v_1\\}\\).</p> <p> </p> <p>Wheels \\(W_n\\)</p> <p>The wheel \\(W_n\\) is to add an additional vertex to the cycle \\(C_n\\), and connect this new vertex to each of \\(n\\) vertices by new edges.</p> <p> </p> <p>n-Cubes \\(Q_n\\) n \u7ef4\u8d85\u7acb\u65b9\u4f53</p> <p>n-Cubes \\(Q_n\\) is the graph has vertices representing \\(2^n\\) bit strings of length \\(n\\).</p> <p>Bipartite Graph \u4e8c\u5206\u56fe / \u5076\u56fe</p> <p>A bipartite graph \\(G = (V, E)\\) is a graph s.t.</p> \\[     V = V_1 \\cup V_2,\\ \\ V_1 \\cap V_2 = \\emptyset. \\] <p>and no edges between any two vertices in the same subset \\(V_k\\) (\\(k = 1, 2\\)).</p> <p>A bipartite graph is the complete bipartite graph \\(K_{m, n}\\) if every vertex in \\(V_1\\) is joined to a vertex in \\(V_2\\) nad conversely, and \\(|V_1| = m\\), \\(|V_2| = n\\).</p> <p> </p>"},{"location":"Mathematics_Basis/DM/Chap_10/#representation","title":"Representation","text":"<p>There are many ways to represent graphs: Graph, adjacecy lists, adjacency matrices and incidences.</p>"},{"location":"Mathematics_Basis/DM/Chap_10/#adjacency-matrix","title":"Adjacency Matrix \u90bb\u63a5\u77e9\u9635","text":"<p>Definition</p> <p>\\(G\\) is a graph, then its corresponding adjacency matrix \\(A = [a_{ij}]_{n \\times n}\\) where</p> <ul> <li> <p>\\(G\\) is a simple graph, then</p> \\[     a_{ij} = \\left\\{         \\begin{aligned}             &amp; 1, &amp; \\{v_i, v_j\\} \\in E, \\\\             &amp; 0, &amp; \\text{otherwise}.         \\end{aligned}     \\right. \\] </li> <li> <p>\\(G\\) is a pseudograph, then</p> \\[     a_{ij} = \\text{the number of edges that are associated to } \\{v_i, v_j\\}. \\] </li> <li> <p>\\(G\\) is a directed graph, then</p> \\[     a_{ij} = \\left\\{         \\begin{aligned}             &amp; 1, &amp; (v_i, v_j) \\in E, \\\\             &amp; 0, &amp; \\text{otherwise}.         \\end{aligned}     \\right. \\] </li> <li> <p>\\(G\\) is a directed multigraph, then</p> \\[     a_{ij} = \\text{the number of edges that are associated to } (v_i, v_j). \\] </li> </ul> <p>Property</p> <ul> <li>The adjacency matrix of an undirected graph is symmetric, \\(a_{ij} = a_{ji}\\).</li> <li>For simple graph,<ul> <li>\\(a_{ii} = 0\\) since a simple graph has no loops.</li> <li>\\(\\text{deg}(v_i) = \\sum\\limits_{j = 1}^{n} a_{ij}\\).</li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_10/#incidence-matrix","title":"Incidence Matrix \u5173\u8054\u77e9\u9635","text":"<p>Definition</p> <p>\\(G = (V, E)\\) is an undirected graph with vertices \\(v_1, \\dots, v_n\\) and edges \\(e_1, \\dots, e_n\\), then the incidence matrix w.r.t this ordering of \\(V\\) and \\(E\\) is matrix \\(M = [m_{ij}]_{n \\times m}\\), where</p> \\[     m_{ij} = \\left\\{         \\begin{aligned}             &amp; 1, &amp; \\text{when edge $e_j$ is incidence with $v_i$}, \\\\             &amp; 0, &amp; \\text{otherwise}.         \\end{aligned}     \\right. \\] <p>Example</p> <p> </p>"},{"location":"Mathematics_Basis/DM/Chap_10/#isomorphism-of-graphs","title":"Isomorphism of Graphs","text":"<p>Definition</p> <p>Two simple graphs \\(G_1 = (V_1, E_1)\\) and \\(G_2 = (V_2, E_2)\\) are called isomorphic, if there is a bijection \\(f\\) from \\(V_1\\) to \\(V_2\\) with the property</p> \\[     \\forall\\ a, b \\in V_1,\\ \\ (a, b) \\in E_1 \\Leftrightarrow (f(a), f(b)) \\in E_2. \\] <p>Bijection \\(f\\) is called an isomorphism.</p>"},{"location":"Mathematics_Basis/DM/Chap_10/#connectivity","title":"Connectivity","text":"<p>Definition</p> <p>Similarly we defines path, circuit, simple path / circuit, connection, connected components, cut vertices / point, cut edge / bridge, strongly connected and weakly connected.</p> <p>Refer to Graph.</p> <p>Theorem</p> <p>There are a simple path between every pair of distinct vertices of a connected undirected graph.</p> <p>Theorem</p> <p>\\(G\\) is a graph with its adjacency matrix \\(A\\) w.r.t the ordering \\(v_1, \\dots, v_n\\). The number of different paths of length \\(r\\) from \\(v_i\\) to \\(v_j\\) equals to the \\((i, j)\\)th entry of \\(A^r\\).</p> <p>Note</p> <p>We can use this theorem to determine whether a graph is connected.</p>"},{"location":"Mathematics_Basis/DM/Chap_10/#euler-paths-and-circuits","title":"Euler Paths and circuits","text":"<p>Definition</p> <ul> <li>An Euler Path is a simple path containing every edge of \\(G\\).</li> <li>An Euler Circuit is a simple circuit containing every edge of \\(G\\).</li> <li>A graph \\(G\\) is an Euler graph if it has an Euler circuit.</li> </ul> <p>Theorem</p> <p>A connected multigraph \\(G = (V, E)\\) has an Euler curcuit iff every vertex has even degree.</p>"},{"location":"Mathematics_Basis/DM/Chap_10/#hamilton-paths-and-circuits","title":"Hamilton Paths and circuits","text":"<p>Definition</p> <ul> <li>An Hamilton Path is a path go through all vertices.</li> <li>An Hamilton Circuit is a circuit go through all vertices.</li> <li>A connected graph \\(G\\) is an Hamilton graph if it has an Hamilton circuit.</li> </ul> <p>Theorem</p> <p>A connected simple graph \\(G\\) with \\(n\\) (\\(n \\ge 3\\)) vertices has a Hamilton circuit if the degree of each vertex is at least \\(n / 2\\).</p> <p>Theorem</p> <p>A connected simple graph \\(G(V, E)\\) with \\(n\\) (\\(n \\ge 3\\)) vertices has a Hamilton circuit if</p> \\[     \\forall u, v \\in V,\\ \\ d(u) + d(v) \\ge n. \\]"},{"location":"Mathematics_Basis/DM/Chap_10/#shortest-path-problems","title":"Shortest Path Problems","text":"<p>Refer to Shortest Paths.</p>"},{"location":"Mathematics_Basis/DM/Chap_10/#planar-graphs","title":"Planar Graphs","text":"<p>Definition</p> <p>A graph is called planar if it can be drawn in the plane without crossing edges. Such a drawing is called a planar representation of the graph.</p> <p>Example</p> <p>\\(K_4\\) is planar, \\(Q_n\\) is all planar. But \\(K_5\\) and \\(K_{3,3}\\) are not planar.</p> <p>Definition</p> <p>A planar representation of a graph into regions, including an unbounded region.</p> <p>Theorem | Euler's Formula</p> <p>Let \\(G\\) be a connected planar simple graph with \\(e\\) edges and \\(v\\) vertices. Let \\(r\\) be the number of regions in a planar representation of \\(G\\). Then</p> \\[     v - e + r = 2. \\] <p>Further consider the conditions of simple and connected.</p> <ul> <li>If not simple, we still have \\(v - e + r = 2\\).</li> <li>If not connected, but \\(n\\) connected components instead, then we have</li> </ul> \\[     v - e + r = n + 1. \\] <p>From Euler's Formula, some corollaries are derived to easily show a graph is nonplanar.</p> <p>Corollary</p> <ul> <li>If \\(G\\) is a connected planar simple graph with \\(e\\) edges and \\(v\\) vertices (\\(v \\ge 3\\)), then</li> </ul> \\[     e \\le 3v - 6. \\] <ul> <li>If \\(G\\) is a connected planar simple graph with \\(e\\) edges and \\(v\\) vertices (\\(v \\ge 3\\)), and no circuits of length \\(3\\), then</li> </ul> \\[     e \\le 2v - 4. \\] <p>NOTE: the condition simple is neccessary.</p> Proof <p>Define degree of a region is the number of edge on the boundary of the region.</p> <ul> <li>Since it's a simple graph, the degree of each region is at least \\(3\\).</li> <li>And the degree of the unbounded region is at least \\(3\\).</li> </ul> <p>Thus</p> \\[     2e = \\sum\\limits_{\\text{all region $R$}}\\text{deg}(R) \\ge 3r \\Rightarrow \\frac{2}{3}e \\ge r. \\] <p>From Euler formula, we have</p> \\[     e - v + 2 = r \\le \\frac{2}{3}e \\Rightarrow e \\le 3v - 6. \\] Example <p>Show that \\(K_5\\) and \\(K_{3, 3}\\) are nonplanar.</p> <p>Proof</p> <p>For \\(K_5\\), \\(v = 5\\), \\(e = 10\\). Since</p> \\[     10 = e &gt; 3v - 6 = 3 \\times 5 - 6 = 9. \\] <p>From the first corollary, \\(K_5\\) is nonplanar.</p> <p>For \\(K_{3,3}\\), \\(v = 6\\), \\(e = 9\\). Since the circuits of \\(K_{3,3}\\) is even length, thus at least \\(3\\). And</p> \\[     9 = e &gt; 2v - 4 = 2 \\times 6 - 4 = 8. \\] <p>From the first corollary, \\(K_{3,3}\\) is nonplanar.</p> <p>Furthermore, we have the sufficient and necessary condition of nonplanar graph. First we need some defintions.</p> <p>Definition</p> <p>\\(G = (V, E)\\). Let \\(\\{u, v\\} \\in E\\). Elementary subdivision is the process that remove the edge \\(\\{u, v\\}\\) and add new vertex \\(w\\) together with edges \\(\\{u, w\\}\\) and \\(\\{w, v\\}\\).</p> <p>Graphs \\(G_1 = (V_1, E_1)\\) and \\(G_2 = (V_2, E_2)\\) are called homeomorphic \u540c\u80da if they can be obtained from the same graph by a sequence of elementary subdivision.</p> <p>Kuratowski's Theorem</p> <p>A graph \\(G\\) is nonplanar iff \\(G\\) contains a subgraph homeomorphic to \\(K_5\\) or \\(K_{3,3}\\).</p>"},{"location":"Mathematics_Basis/DM/Chap_10/#graph-coloring","title":"Graph Coloring","text":"<p>Each map in the plane can be represented by a graph, by the following processes:</p> <ul> <li>Each region of the map is represented by a vertex.</li> <li>Edges connect two vertices if each regions represented by these vertices have a common border.</li> </ul> <p>The graph is called dual graph of the map.</p> <p>Definition</p> <p>A coloring of a simple graph is the assignment of a color to each vertex of the graph so that no two adjectent vertices are assigned the same color.</p> <p>The chromatic number of the graph is the least number of colors needed for a coloring of this graph.</p> <p>The Four Color Theorem</p> <p>The chromatic number of a planar graph is no greater than four.</p> <p>Example</p> <ul> <li>The chromatic number of \\(K_n\\) is \\(n\\).</li> <li>The chromatic number of the complete bipartite \\(K_{m, n}\\) is \\(2\\).     (it's also the sufficient and necessary condition)</li> <li>The chromatic number of \\(C_n\\) is <code>n % 2 == 0 ? 2 : 3</code>.</li> </ul> <p>The best algorithms known for finding the chromatic number of a graph have exponential worst-case time complexity.</p>"},{"location":"Mathematics_Basis/DM/Chap_11/","title":"Chapter 11 | Trees","text":"<p>Definition</p> <p>A tree \\(T\\) is a connected undirected graph with no simple circuit. A forest is an undirected graph with no simple circuit.</p> <p>Theorem</p> <p>An undirected graph is a tree iff there is a unique simple path between any two of its vertices.</p> <p>Definition</p> <p>A particular vertex of a tree is designated as the root. Once we specify the root, we can assign a direction to each edge. We direct each edge away from the root. Thus, a tree together with its root produces a directed graph called a rooted tree.</p> <p>Definition</p> <ul> <li>Leaf is the vertex that have no children.</li> <li>Internal vertex is the vertex that have children.</li> <li>Let \\(T\\) be a rooted tree.<ul> <li>level \\(l(v)\\) is the length of the simple path from root to \\(v\\).</li> <li>height \\(h = \\max\\limits_{v \\in V(T)} l(v)\\).</li> </ul> </li> <li>m-ary tree m \u5143\u6811 is a tree that every internal vertex has no more than \\(m\\) children.</li> <li>Full m-ary tree is a tree that every internal vertex has exactly \\(m\\) children.</li> <li>Ordered rooted tree is a tree that the children of each internal vertex are ordered, namely each children is unique. e.g. binary tree is often ordered, since it distinguishes left child and right child.</li> <li>A rooted m-ary tree of height \\(h\\) is balanced if all leaves are at levels \\(h\\) or \\(h - 1\\).</li> </ul> <p>NOTE In most cases, we treat a tree as an ordered rooted tree, and often only discuss binary tree in FDS.</p>"},{"location":"Mathematics_Basis/DM/Chap_11/#property","title":"Property","text":"<p>Theorem</p> <p>A tree with \\(n\\) vertices has \\(n - 1\\) edges.</p> <p>Theorem</p> <p>A full m-ary tree with \\(i\\) internal vertices contains \\(n = mi + 1\\) vertices.</p> <p>Moreover, suppose \\(l\\) is the number of leaves. With the equations</p> \\[     n = mi + 1,\\ \\ n = l + i, \\] <p>if one of \\(n\\), \\(i\\) and \\(l\\) is known, the other two quantities are determined.</p> <p>Theorem</p> <p>There are at most \\(m^h\\) leaves in m-ary tree of height \\(h\\).</p> <p>Corollary</p> <p>If an m-ary tree of height \\(h\\) has \\(l\\) leaves, then \\(h \\ge \\lceil \\log _m l \\rceil\\). If it's full and balanced, then \\(h = \\lceil \\log _m l \\rceil\\).</p>"},{"location":"Mathematics_Basis/DM/Chap_11/#application","title":"Application","text":""},{"location":"Mathematics_Basis/DM/Chap_11/#binary-search-tree-bst","title":"Binary Search Tree (BST)","text":"<p>Refer to Binary Search Tree.</p>"},{"location":"Mathematics_Basis/DM/Chap_11/#prefix-codes","title":"Prefix Codes","text":"<p>Definition</p> <p>To ensure that no bit string corresponds to more than one sequence of letters, the bit string for a letter must never occur as the first part of the string for another letter. Codes with this propertiy are called prefix codes.</p> <p>Example</p> <p>To encode \\(\\{a, b, c, d\\}\\) four characters, \\(\\{0, 1, 01, 001\\}\\) is not prefix codes, but \\(\\{0, 10, 110, 111\\}\\) is.</p> <p>A prefix code can be represented by a binary tree.</p> <p> </p> <p>Extension: Huffman Coding</p> <p>Remains</p>"},{"location":"Mathematics_Basis/DM/Chap_11/#decision-trees","title":"Decision Trees","text":"<p>Definition</p> <p>A rooted tree in which each internal vertex corresponds to a decision, with a subtree at these vertices for each possible outcome of the decision, is called a decision tree.</p>"},{"location":"Mathematics_Basis/DM/Chap_11/#tree-traversal","title":"Tree Traversal","text":"<p>Refer to Traversals.</p>"},{"location":"Mathematics_Basis/DM/Chap_11/#spanning-trees","title":"Spanning Trees","text":"<p>We can consturct spanning trees by DFS or BFS.</p> <p>Refer to MST.</p>"},{"location":"Mathematics_Basis/DM/Chap_12_13/","title":"Chapter 12 | Boolean Algrebra  Chapter 13 | Modeling Computation","text":"<p>Refer to Digital Logic Design and Computer Organization.</p>"},{"location":"Mathematics_Basis/DM/Chap_2/","title":"Chapter 2 | Basic Structures: Sets and Functions","text":""},{"location":"Mathematics_Basis/DM/Chap_2/#set-and-subset","title":"Set and Subset","text":"<p>Definition</p> <p>The objects in a set are elements. A set contains its elements.</p> <ul> <li>Ways to describe sets: List; Predicate; Venn Diagram.</li> <li>Properties of sets: Certainty; Don't care order and repetition of elements; Finite and Infinite set.</li> <li> <p>Subset</p> \\[     S \\subseteq T \\Leftrightarrow (\\forall x \\in S \\Rightarrow x \\in T). \\] </li> <li> <p>Proper subset (\u771f\u5b50\u96c6)</p> \\[     S \\subset T \\Leftrightarrow (\\forall x \\in S \\Rightarrow x \\in T) \\wedge S \\ne T. \\] </li> <li> <p>Empty set \\(\\emptyset\\) and Universal set \\(U\\)</p> \\[     \\text{For any set } A,\\ \\ A \\subseteq A,\\ \\ \\emptyset \\subseteq A \\subseteq U. \\] </li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_2/#operations","title":"Operations","text":"<ul> <li>Union \\(A \\cup B = \\{x | x \\in A \\vee x \\in B\\}\\).</li> <li>Intersection  \\(A \\cap B = \\{x | x \\in A \\wedge x \\in B\\}\\).</li> <li>Difference    \\(A -B = \\{x | x \\in A \\wedge x \\notin B\\}\\).</li> <li>Complement    \\(\\overline{A} = U - A\\).</li> <li>Symmetric Difference \\(A \\oplus B = (A - B) \\cup (B - A)\\).<ul> <li> <p>Properties</p> \\[     A \\oplus B = B \\oplus A,\\ \\ (A \\oplus B) \\oplus C = A \\oplus (B \\oplus C). \\] \\[     A \\oplus A = \\emptyset,\\ \\ A \\oplus \\emptyset = A. \\] \\[     A \\oplus B = A \\oplus C \\Rightarrow B = C. \\] </li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_2/#power-set","title":"Power Set","text":"<p>Definition</p> <p>For a set \\(S\\), the power set of \\(S\\) is the set of all subsets of the set \\(S\\), denoted by</p> \\[     P(S) = 2^S = \\{T | T \\subseteq S\\}. \\] <p>If a set has \\(n\\) elements, then its power set has \\(2^n\\) elements.</p>"},{"location":"Mathematics_Basis/DM/Chap_2/#cartesian-product","title":"Cartesian Product","text":"<p>Definition</p> <p>For two sets \\(A\\) and \\(B\\), the Cartesian product of \\(A\\) and \\(B\\) is the set of all ordered pairs \\((a, b)\\) where \\(a \\in A\\) and \\(b \\in B\\), denoted by</p> \\[     A \\times B = \\{(a, b) | a \\in A \\wedge b \\in B\\}. \\] <p>Generally for \\(n\\) sets \\(A_1, \\dots, A_n\\), </p> \\[     A_1 \\times A_2 \\times \\ \\cdots \\times A_n = \\{(a_1, a_2, \\dots a_n) | a_i \\in A_i \\text{ for } i = 1, 2, \\dots, n\\}. \\] <p>Properties</p> <ul> <li>\\(A \\times \\emptyset = \\emptyset \\times A = \\emptyset\\).</li> <li>In general, \\(A \\times B \\ne B \\times A\\).</li> <li>In general, \\((A \\times B) \\times C \\ne A \\times (B \\times C)\\), unless we identify \\(((a, b), c)= (a, (b, c))\\).</li> <li> \\[ \\begin{aligned}     A \\times (B \\cup C) = (A \\times B) \\cup (A \\times C), \\\\     A \\times (B \\cap C) = (A \\times B) \\cap (A \\times C). \\end{aligned} \\] </li> <li> <p>If \\(A \\subseteq C\\) and \\(B \\subseteq D\\), then \\(A \\times B \\subseteq C \\times D\\), but not vice versa.</p> </li> <li>\\((A \\cap B) \\times (C \\cap D) = (A \\times C) \\cap (B \\times D)\\).</li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_2/#cardinality-of-finite-and-infinite-sets","title":"Cardinality of Finite and Infinite Sets","text":"<p>Cardinality is the size of a set \\(S\\), denoted by \\(|S|\\).</p>"},{"location":"Mathematics_Basis/DM/Chap_2/#finite-sets","title":"Finite Sets","text":"<p>Theorem | Prniciple of Inclusion-exclusion \u5bb9\u65a5\u539f\u7406</p> \\[     |A \\cup B| = |A| + |B| - |A \\cap B|. \\] <p>More generally, </p> \\[     \\left|\\bigcup_{i = 1}^nA_i\\right| = \\sum_{i = 1}^n|A_i| - \\sum_{1 \\le i \\ne j \\le n} |A_i \\cap A_j| + \\cdots + (-1)^{n-1}\\left|\\bigcap_{i = 1]}^n A_i\\right|. \\]"},{"location":"Mathematics_Basis/DM/Chap_2/#inifinite-sets","title":"Inifinite Sets","text":"<p>Definition</p> <p>Two sets have the same cardinality or say are equinumerous (\u7b49\u52bf\u7684) iff there is a bijective from \\(A\\) to \\(B\\), i.e.\\(|A| = |B|\\).</p> <ul> <li>Countable (denumerable) set<ul> <li>A set that is equinumerous with \\(\\mathbb{N}\\), e.g. \\(\\mathbb{Z}\\), \\(\\mathbb{Q}\\), \\(\\mathbb{N} \\times \\mathbb{N}\\).</li> <li>\\(\\aleph_0\\) is called countable.</li> <li>Properties<ul> <li>Countable set is the smallest infinite set.</li> <li>The union of two / finite number of / a countable number of countable sets is countable.</li> </ul> </li> </ul> </li> <li>Uncountable set</li> </ul> <p>Theorem</p> <ul> <li>\\(\\left|2^A\\right| &gt; |A|\\).</li> <li>\\(|\\mathbb{R}| = \\aleph_1 &gt; \\aleph_0\\).</li> </ul> <p>Continuum Hypothesis (CH) \u8fde\u7eed\u7edf\u5047\u8bbe</p> \\[     \\exists ? \\omega,\\ \\ s.t.\\ \\aleph_0 \\lt \\omega \\lt \\aleph_1. \\]"},{"location":"Mathematics_Basis/DM/Chap_2/#functions","title":"Functions","text":"\\[     f: A \\rightarrow B \\Leftrightarrow \\forall a \\in A,\\ \\ \\exists b \\in B : f(a) = b \\] <p>\\(f\\) maps \\(A\\) to \\(B\\).</p> <ul> <li>\\(A\\) is the domain of \\(f\\), \\(\\text{Dom } f = A\\).</li> <li>\\(B\\) is the codomain of \\(f\\), \\(\\text{Codom } f = B\\).</li> <li>\\(f(a) = b,\\ \\ a \\in A,\\ \\ b \\in B\\). \\(b\\) is the image of \\(a\\), \\(a\\) is a pre-image of \\(b\\).</li> <li> <p>\\(\\text{Range}(f) = \\{b \\in B | \\exist a \\in A, f(a) = b\\}\\).</p> </li> <li> <p>One-to-one function (Injective) \u5355\u5c04</p> \\[     (\\forall a, b \\in A) \\wedge (a \\ne b) \\Rightarrow f(a) \\ne f(b). \\] </li> <li> <p>Onto function (Subjective) \u6ee1\u5c04</p> \\[     \\forall b \\in B,\\ \\ \\exist a \\in A,\\ \\ s.t.\\ f(a) = b. \\] </li> <li> <p>One-to-one Correspondence (Bijective) \u53cc\u5c04</p> <p>     one-to-one + onto </p> </li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_2/#inverse-and-composition","title":"Inverse and Composition","text":"<p>Definition | Inverse Function</p> <p>If \\(f\\) is bijective, then \\(f^{-1}:B\\rightarrow A\\) is the inverse function of\\(f\\).</p> \\[     \\forall a \\in A, b \\in B,\\ \\ (f(a) = b) \\Leftrightarrow (f^{-1}(b) = a). \\] <p>Thus we also call a one-to-one correspondence invertible.</p> <p>Definition | Composition</p> <p>If \\(g: A\\rightarrow B\\) and \\(f:B\\rightarrow C\\), then \\(f\\circ g : A\\rightarrow C\\) the composition of \\(f\\) and \\(g\\).</p> \\[     \\forall a \\in A,\\ \\ (f\\circ g)(a) = f(g(a)). \\] <p>Two Important Functions</p> <ul> <li>Floor functions \\(\\lfloor x \\rfloor\\) (or by convetion \\([x]\\), or called greatest integer function).</li> <li>Ceiling functions \\(\\lceil x \\rceil\\).</li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_2/#growth-of-function","title":"Growth of Function","text":"<p>Quote</p> <p>Similar to Algorithm Analysis in FDS.</p> <p>BIG-O NOTATION, BIG-OMEGA NOTATION and BIG-THETA NOTATION.</p> <p>Theorem</p> <p>If</p> \\[     f(x) = a_nx^n + \\cdots + a_1x + a_0, \\] <p>then \\(f(x)\\) is \\(O(x^n)\\).</p> <p>Definition</p> <p>If \\(f(x) = \\Theta(g(x))\\), then we say \\(f(x)\\) is of order \\(g(x)\\).</p>"},{"location":"Mathematics_Basis/DM/Chap_3/","title":"Chapter 3 | The Fundamentals: Algorithms","text":""},{"location":"Mathematics_Basis/DM/Chap_3/#algorithms","title":"Algorithms","text":"<p>Definition</p> <p>An algorithm is a finite set of precise instructions for performing a computation or solving problem.</p> <p>Pseudocode is instructions given in a generic language similar to a computer language.</p> <p>Properties</p> <ul> <li>Input and Output</li> <li>Definiteness</li> <li>Correctness</li> <li>Finiteness</li> <li>Effectiveness</li> <li>Generality</li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_3/#complexity","title":"Complexity","text":"<p>Definition</p> <p>Complexity is the amount of time or space needed to execute the algorithm.</p> <ul> <li>Space Complexity</li> <li>Time Complexity<ul> <li>Types: best-case time; worst-case time; average-case time.</li> </ul> </li> </ul> <ul> <li>P-Class Problem<ul> <li>P for \u300cpolynomial\u300d</li> <li>problems that can be solved by polynomial time algorithm</li> </ul> </li> <li>NP-Class Problem<ul> <li>NP for \u300cnondeterministic polynomial\u300d</li> <li>problems for which a solution can be checked in polynomial time</li> </ul> </li> <li>NP-Complete (NPC) Problem<ul> <li>If any of these problems can be solved by polynomial worst-case time algorithm, then all can be solved by polynomial worst-case time algorithms.</li> </ul> </li> </ul> <p> </p>"},{"location":"Mathematics_Basis/DM/Chap_4/","title":"Chapter 4 | Number Theory and Cryptography","text":"<p>Refer to Abstract Algebra.</p>"},{"location":"Mathematics_Basis/DM/Chap_5/","title":"Chapter 5 | Induction and Recursion","text":""},{"location":"Mathematics_Basis/DM/Chap_5/#mathematical-induction","title":"Mathematical Induction","text":"<p>The Well-Ordering Property: Every nonnegative integers has a least element.</p> <p>The First Principle of Mathematical Induction</p> <p>To prove by mathematical induction that \\(P(n)\\) is true for every positive integer\\(n\\),</p> <ol> <li>Basic Step. The proposition \\(P(1)\\) is shown to be true.</li> <li>Inductive Step. The implication \\(P(n) \\rightarrow P(n + 1)\\) is shown to be true for every positive integer \\(n\\).</li> </ol> <p>The Second Principle of Mathematical Induction</p> <ol> <li>Basic Step. The proposition \\(P(1)\\) is shown to be true.</li> <li>Inductive Step. The implication \\([P(1) \\wedge P(2) \\wedge \\cdots \\wedge P(n)] \\rightarrow P(n + 1)\\) is shown to be true for every positive integer \\(n\\).</li> </ol>"},{"location":"Mathematics_Basis/DM/Chap_5/#recursion","title":"Recursion","text":"<p>Recursively Defined Functions</p> <p>Recursive or Inductive Definition</p> <ol> <li>Specify the value of function at zero.</li> <li>Give a rule for finding its value as an integer from its values at smaller integers.</li> </ol> <p>Moreover, we can use recursion to define sets, algorithm and so on.</p> <p>Definition</p> <p>An algorithm is called recursive if it solves a problem by reducing it to an instance of the same problem with smaller input.</p>"},{"location":"Mathematics_Basis/DM/Chap_6/","title":"Chapter 6 | Counting","text":""},{"location":"Mathematics_Basis/DM/Chap_6/#basic-counting-principle","title":"Basic Counting Principle","text":"<p>Theorem | SUM RULE</p> <p>Suppose that the tasks \\(T_1, T_2, \\dots, T_k\\) can be down in \\(n_1, n_2, \\dots, n_m\\) ways respectively and no two of these tasks can be down at the same time. Then the number of ways to do one of these tasks is</p> \\[     n_1 + n_2 + \\dots + n_k. \\] <p>Or, if \\(A_1, A_2, \\dots, A_k\\) are disjoint sets, then</p> \\[     |A_1 \\cup A_2 \\cup \\dots \\cup A_k| = |A_1| + |A_2| + \\dots + |A_k|. \\] <p>Theorem | PRODUCT RULE</p> <p>Suppose that a procedure can be performed in \\(k\\) succeessive steps, step \\(i\\) can be done in \\(n_i\\) ways. Then the number of different ways that the procedure can be is the product</p> \\[     n_1 \\cdot n_2 \\cdot \\cdots \\cdot n_k. \\] <p>Or, if \\(A_1, A_2, \\dots, A_k\\) are finite sets, then</p> \\[     |A_1 \\times A_2 \\times \\cdots \\times A_k| = |A_1| \\cdot |A_2| \\cdot \\cdots \\cdot |A_k|. \\]"},{"location":"Mathematics_Basis/DM/Chap_6/#pigeonhole-principle","title":"Pigeonhole Principle","text":"<p>Pigeonhole Principle</p> <p>If \\(k + 1\\) or more objects are placed into \\(k\\) boxes then there is at least one box containing two or more of the objects.</p> <p>Generalized Pigeonhole Principle</p> <p>If \\(N\\) are placed into \\(k\\) boxes then there is at least one box containing at least \\(\\lceil N / k \\rceil\\) objects.</p> <p>We give some elegant applications of this principle.</p> <p>Example</p> Example 1Example 2Example 3Example 4Example 5 <p>Given \\(n + 1\\) positive integers \\(a_i\\) (\\(i = 1, \\dots, n + 1\\)) and \\(a_i \\le 2n\\), show that \\(\\exist i, j, \\text{ s.t. } a_i \\mid a_j\\).</p> <p>Solution.</p> <p>Suppose \\(a_i = 2^{k_i} q_i\\) for some \\(k_i\\) and odd \\(q_i\\).</p> <p>Then</p> <ul> <li>the pigeonhole is \\(\\{q_i\\}_{i = 1}^{n}\\), totally \\(n\\) elements.</li> <li>the pigeon is \\(\\{a_i\\}_{i = 1}^{n + 1}\\), totally \\(n + 1\\) elements.</li> </ul> <p>Thus \\(\\exists\\ i, j, \\text{ s.t. } q_i = q_j\\), and \\(a_i = 2^{k_i}q_i\\), \\(a_j = 2^{k_j}q_j\\). Without loss of generation, if \\(k_i &lt; k_j\\), then \\(a_i \\mid a_j\\).</p> <p>Theorem</p> <p>Every sequence of \\(n^2 + 1\\) distinct real numbers \\(\\{a_i\\}_{i = 1}^{n^2+1}\\) contains a subsequence of length \\(n + 1\\) that is either strictly increasing or strictly decreasing.</p> <p>Proof</p> <p>Define</p> <ul> <li>\\(i_k\\) by the length of the longest increasing subsequence starting at \\(a_k\\).</li> <li>\\(d_k\\) by the length of the longest decreasing subsequence starting at \\(a_k\\).</li> </ul> <p>First we prove that if \\(s \\neq t\\), then either \\(i_s \\neq i_t\\) or \\(d_s = d_t\\).</p> <p>Since \\(a_i\\) are distinct, without loss of generation, for \\(s &lt; t\\) either \\(a_s &lt; a_t\\) or \\(a_s &gt; a_t\\).</p> <ul> <li>If \\(a_s &lt; a_t\\), then \\(i_s &gt; i_t\\).</li> <li>If \\(a_s &gt; a_t\\), then \\(d_s &gt; d_t\\).</li> </ul> <p>We prove the theorem by contradiction. Suppose that there are no increasing or decreasing subsequence of length \\(n + 1\\), then we have</p> \\[     1 \\le i_k, d_k \\le n, k = 1, \\dots, n^2 + 1. \\] <p>Thus there are \\(n^2\\) possible ordered pair \\((i_k, d_k)\\).</p> <p>Then</p> <ul> <li>the pigeonhole is \\(\\{(i_k, d_k)\\}\\), totally \\(n^2\\) elements.</li> <li>the pigeon is \\(\\{a_i\\}_{i = 1}^{n^2 + 1}\\), totally \\(n^2 + 1\\) elements.</li> </ul> <p>Thus \\(\\exists\\ s, t, \\text{ s.t. } i_s = i_t, d_s = d_t\\), which leads to a contradiction.</p> <p>Let \\(x_1, \\dots x_n\\) be a sequence of integers, then there are some successive integers in the sequence s.t. their sum can be divied by \\(n\\).</p> <p>Solution.</p> <p>Let \\(A_i = \\sum\\limits_{k = 1}^{i}x_k\\).</p> <ul> <li>If \\(\\exists\\ i\\), s.t. \\(n \\mid A_i\\), then it's true.</li> <li> <p>If not, then \\(n \\nmid A_i, i = 1, 2, \\dots n\\). Then</p> <ul> <li>the pigeonhole is \\([1]_n, \\dots, [n - 1]_n\\), totally \\(n - 1\\) elements.</li> <li>the pigeon is \\(\\{A_i\\}_{i = 1}^{n}\\), totally \\(n\\) elements.</li> </ul> <p>Thus \\(\\exists\\ i &lt; j, \\text{ s.t. } A_i \\equiv A_j (\\text{ mod } n)\\). Thus \\(n \\mid (A_j - A_i)\\).</p> </li> </ul> <p>Every sequence \\(a_1, a_2, \\dots, a_N\\) (\\(N = 2^n\\)), where \\(a_i \\in \\{x_1, x_2, \\dots, x_n\\}\\), contains a successive subsequence such that their product is a perfect square.</p> <p>Solution.</p> <p>Let \\(A_i\\) = \\prod_{k = 1}^{i} a_k$, then \\(A_i = \\sum\\limits_{k = 1}^{n} x_k^{\\alpha_{ik}}\\).</p> <ul> <li>If \\(\\exists\\ i\\), s.t. \\(A_i\\) is a perfect square, then it's true.</li> <li> <p>If not, then \\(A_i\\) are not perfect square. Suppose a bijection</p> \\[     A_i \\leftrightarrow (\\alpha_{i1}, \\dots, \\alpha_{in}) \\leftrightarrow (\\alpha_{i1} (\\text{ mod } 2), \\dots, \\alpha_{in} (\\text{ mod } 2)). \\] <ul> <li>the pigeonhole is \\(\\{0, 1\\} \\times \\{0, 1\\} \\times \\cdots \\times \\{0, 1\\} - (0, 0, \\cdots, 0)\\), totally \\(2^n - 1\\) elements.</li> <li>the pigeon is \\(\\{A_i\\}_{i = 1}^{2^n}\\), totally \\(2^n\\) elements.</li> </ul> <p>Thus \\(\\exists\\ i &lt; j, \\text{ s.t. }\\) </p> \\[     \\frac{A_j}{A_i} = x_1^{2k_1} x_2^{2k_2} \\cdots x_n^{2k_n} = (x_1^{k_1} x_2^{k_2} \\cdots x_n^{k_n})^2, \\] <p>which is a perfect square.</p> </li> </ul> <p>During \\(30\\) days, a baseball team plays at least \\(1\\) game a day, but no more than \\(45\\) games in total. Show that there must be a period of some number of consecutive adays during which the team ust play exactly \\(14\\) games.</p> <p>Solution.</p> <p>Let \\(a_i\\) be the number of games played on before the ith day. Then \\(a_1 &lt; a_2 &lt; \\cdots &lt; a_{30}\\), and</p> \\[     1 \\le a_i \\le 45,\\ \\ 15 \\le a_i + 14 \\le 59. \\] <p>Then</p> <ul> <li>the pigeonhole is \\(1 \\sim 59\\), totally \\(59\\) elements.</li> <li>the pigeon is \\(\\{a_i\\}_{i = 1}^{30}, \\{a_i + 14\\}_{i = 1}^{30}\\), totally \\(60\\) elements.</li> </ul> <p>Thus \\(\\exists\\ i &lt; j, \\text{ s.t. } a_i = a_j + 14\\).</p>"},{"location":"Mathematics_Basis/DM/Chap_6/#permutations-and-combinations","title":"Permutations and Combinations","text":"<p>Definition</p> <p>Given a set of distinct object \\(X = \\{x_1, \\cdots, x_n\\}\\),</p> <ul> <li>a permutation of \\(X\\) is an order arrangement of \\(X\\).</li> <li>an r-permutation (\\(r \\le n\\)) is an ordering of a subset of r-elements is denoted by \\(P(n, r)\\). The number of r-permutations is denoted by \\(P(n, r)\\).</li> <li>an r-combination of \\(X\\) is an unordered selection of \\(r\\) elements. The number of r-combinations is denoted by \\(C(n, r)\\).</li> </ul> <p>Theorem</p> \\[     P(n, r) = n(n - 1)\\cdot(n - r + 1) = \\frac{n!}{(n - r)!}. \\] \\[     C(n, r) = \\frac{n!}{r!(n - r)!} = \\frac{P(n, r)}{r!}. \\] <p>Property</p> <ul> <li>\\(\\sum\\limits_{k = 0}^{n}C(n, k) = 2^n\\).</li> <li>Pascal's Identity \\(C(n + 1, k) = C(n, k) + C(n, k - 1)\\).</li> <li>Vandermonde's Identity \\(C(m + n, r) = \\sum\\limits_{k = 0}^rC(m, r - k)C(n, k)\\).</li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_6/#binomial-coefficients","title":"Binomial Coefficients","text":"<p>Theorem | Binomial Therorem</p> <p>If \\(a\\) and \\(b\\) are eal numbers and \\(n\\) is a positive integer, then</p> \\[     (a + b)^n = C(n, 0)a^nb^0 + C(n, 1)a^{n - 1}b^1 + \\cdots + C(n, n)a^0b^n. \\]"},{"location":"Mathematics_Basis/DM/Chap_6/#generalized-permutations-and-combinations","title":"Generalized Permutations and Combinations","text":""},{"location":"Mathematics_Basis/DM/Chap_6/#permutation-with-repetition","title":"Permutation with repetition","text":"<p>Theorem</p> <p>The number of r-permutations of a set of \\(n\\) objects with repetition allowed is \\(n^r\\).</p>"},{"location":"Mathematics_Basis/DM/Chap_6/#combination-with-repetition","title":"Combination with repetition","text":"<p>Theorem</p> <p>The umber of r-combination of a set with \\(n\\) elements with repetition allowed is \\(C(n + r - 1, r)\\).</p> <p>Example</p> Example 1Example 2 <p>How many solutions in nonnegative integers are there to the equation \\(x_1 + x_2 + x_3 + x_4 = 29\\), where \\(x_1 &gt; 0, x_2 &gt; 1, x_3 &gt; 2, x_4 \\ge 0\\) ?</p> <p>Solution.</p> <p>\\(C(4 + 23 - 1, 23) = C(26, 23) = C(26, 3) = 2600\\).</p> <p>How many ways to place \\(2t + 1\\) indistinguishable balls into \\(3\\) distinguishable boxes such that the total number of balls in arbitrary two boxes is larger than the third one?</p> <p>Solution.</p> <p>\\(C(3 + 2t + 1 - 1, 2t + 1) - C(3, 1) \\cdot C(3 + t - 1, t) = C(2t + 3, 2) - 3C(t + 2, 2)\\).</p> <p>Theorem</p> <p>The number of ways to distriute \\(n\\) distinguishable objects into \\(K\\) distinguishable boxes so that \\(n_i\\) objects are placed into box \\(i\\) equals</p> \\[     \\frac{n!}{n_1!n_2!\\cdots n_k!}. \\]"},{"location":"Mathematics_Basis/DM/Chap_6/#generating-permutations","title":"Generating Permutations","text":"<p>Definition</p> <p>The permutaion \\(a_1 a_2 \\cdots a_n\\) precedes the permutation of \\(b_1 b_2 \\cdots b_n\\), if for some \\(k\\), \\(a_1 = b_1, \\cdots, a_{k - 1} = b_{k - 1}\\) and \\(a_k &lt; b_k\\). It defines an order called lexicographic order.</p> <p>Example</p> <p>The next permutaion in lexicographic order after \\(362541\\) is \\(364125\\).</p>"},{"location":"Mathematics_Basis/DM/Chap_7/","title":"Chapter 7 | Discrete Probability","text":"<p>Refer to Probability and Statistics.</p>"},{"location":"Mathematics_Basis/DM/Chap_8/","title":"Chapter 8 | Advanced Counting Techniques","text":""},{"location":"Mathematics_Basis/DM/Chap_8/#recurrence-relations","title":"Recurrence Relations","text":"<p>Definition</p> <p>A recurrence relation for a sequence \\(\\{a_n\\}\\) is an equation that express \\(a_n\\) in terms of one or more of the previous terms of the sequence.</p> <ul> <li>A sequence is called a solution of recurrence relation if its term satisfy the recurrence relation.</li> <li>A particular solution needs some initial conditions.</li> </ul> <p>Actually, there are many recurrence relations that don't have a solution. But for some certain cases, we have some methods to obtain the solutions. Here we only focus the following form of recurrence relations:</p> <p>Definition</p> <p>A linear homogeneous recurrence relation of degree \\(k\\) with constant coeffieients is a recurrence relation of the form</p> \\[     a_n  = c_1 a_{n - 1} + c_2 a_{n - 2} + \\cdots + c_k a_{n - k}, \\] <p>where \\(c_i \\in \\mathbb{R}\\) and \\(c_k \\neq 0\\).</p> Example <ul> <li>\\(a_n = a_{n - 1} + a_{n - 2}^2\\) is not linear.</li> <li>\\(a_n = 2a_{n - 1} + 1\\) is not homogeneous.</li> <li>\\(a_n = na_{n - 1}\\) has no constant coefficients.</li> </ul> <p>Now we discuss the solution for it. First we define characteristic equation by</p> \\[     r^k - c_1 r^{k - 1} - c_2 r^{k - 2} - \\cdots - c_{k - 1} r - c_k = 0. \\] <p>and the roots \\(r_i\\) are called the characteristic roots.</p> <p>degree \\(k = 2\\)</p> <p>Theorem</p> <p>Let \\(c_1\\) and \\(c_2\\) be real numbers. Suppose that the characteristic equation</p> \\[     r^2 - c_1 r - c_2 = 0 \\] <ul> <li> <p>has two distinct roots \\(r_1\\) and \\(r_2\\). Then the solution is of the form</p> \\[     a_n = \\alpha_1 r_1^n + \\alpha_2 r_2^n,\\ \\ \\text{ where $\\alpha_1$ and $\\alpha_2$ are constants}. \\] </li> <li> <p>has only one root \\(r_0\\). Then the solution is of the form</p> \\[     a_n = \\alpha_1 r_0^n + \\alpha_2 n r_0^n,\\ \\ \\text{ where $\\alpha_1$ and $\\alpha_2$ are constants}. \\] </li> </ul> Example distinct rootsmultiple roots <p>Find the solution of \\(a_n = a_{n - 1} + 2a_{n - 2}\\) with intial conditions \\(a_0 = 2\\) and \\(a_1 = 7\\).</p> <p>Solution.</p> <p>The charateristic equation is \\(r^2 - r - 2 = 0\\) and the roots are \\(r = 2\\) and \\(r = -1\\). The general solution is</p> \\[     a_n = \\alpha_1 2^n + \\alpha_2 (-1)^n. \\] <p>Substitue initial condition, we have \\(\\alpha_1 = 3\\) and \\(\\alpha_2 = -1\\).</p> <p>Thus the solution is \\(a_n = 3 \\cdot 2^n - (-1)^n\\).</p> <p>Find the solution of \\(a_n = 6a_{n - 1} - 9a_{n - 2}\\) with intial conditions \\(a_0 = 1\\) and \\(a_1 = 6\\).</p> <p>Solution.</p> <p>The charateristic equation is \\(r^2 - 6r + 9 = 0\\) and the root is \\(r = 1\\). The general solution is</p> \\[     a_n = \\alpha_1 3^n + \\alpha_2 n 3^n. \\] <p>Substitue initial condition, we have \\(\\alpha_1 = 1\\) and \\(\\alpha_2 = 1\\).</p> <p>Thus the solution is \\(a_n = 3^n + n \\cdot 3^n\\).</p> <p>degree \\(k\\)</p> <p>Theorem</p> <p>Let \\(c_1, \\dots, c_k\\) be real numbers. Suppose that the characteristic equation</p> \\[     r^k - c_1 r^{k - 1} - c_2 r^{k - 2} - \\cdots - c_{k - 1} r - c_k = 0. \\] <ul> <li> <p>has \\(k\\) distinct roots \\(r_1, \\dots, r_k\\). Then the solution is of the form</p> \\[     a_n = \\alpha_1 r_1^n + \\alpha_2 r_2^n + \\cdots + \\alpha_k r_k^n,\\ \\ \\text{ where $\\alpha_i$ are constants}. \\] </li> <li> <p>has \\(t\\) distinct roots \\(r_1, \\dots, r_t\\) with multiplicitiese \\(m_1, \\dots, m_t\\). Then the solution is of the form</p> \\[ \\begin{aligned}     a_n &amp;= (\\alpha_{1, 0} + \\alpha_{1, 1}n + \\cdots + \\alpha_{1, m_1} n^{m_1 - 1}) r_1^n \\\\         &amp;+ (\\alpha_{2, 0} + \\alpha_{2, 1}n + \\cdots + \\alpha_{2, m_2} n^{m_2 - 1}) r_2^n \\\\         &amp;+ \\cdots \\\\         &amp;+ (\\alpha_{t, 0} + \\alpha_{t, 1}n + \\cdots + \\alpha_{t, m_t} n^{m_t - 1}) r_t^n \\\\ \\end{aligned} ,\\ \\  \\text{ where $\\alpha_{ij}$ are constants}. \\] </li> </ul> Example distinct rootsmultiple roots <p>Find the solution of \\(a_n = 6a_{n - 1} - 11a_{n - 2} + 6a_{n - 3}\\) with intial conditions \\(a_0 = 2\\), \\(a_1 = 5\\) and \\(a_2 = 15\\).</p> <p>Solution.</p> <p>The charateristic equation is \\(r^3 - 6r^2 + 11r - 6 = 0\\) and the roots are \\(r = 1\\), \\(r = 2\\) and \\(r = 3\\). The general solution is</p> \\[     a_n = \\alpha_1 \\cdot 1^n + \\alpha_2 \\cdot 2^n + \\alpha_3 \\cdot 3^n. \\] <p>Substitue initial condition, we have \\(\\alpha_1 = 1\\), \\(\\alpha_2 = -1\\) and \\(\\alpha_3 = 2\\).</p> <p>Thus the solution is \\(a_n = 1 - 2^n + 2 \\cdot 3^n\\).</p> <p>Find the solution of \\(a_n = -3a_{n - 1} - 3a_{n - 2} - a_{n - 3}\\) with intial conditions \\(a_0 = 1\\), \\(a_1 = -2\\) and \\(a_2 = -1\\).</p> <p>Solution.</p> <p>The charateristic equation is \\(r^3 + 3r^2 + 3r + 1 = (r + 1)^3 = 0\\) and the root is \\(r = -1\\) of multiplicity \\(3\\). The general solution is</p> \\[     a_n = (\\alpha_{1, 0} + \\alpha_{1, 1} n + \\alpha_{1, 2} n^2)(-1)^n. \\] <p>Substitue initial condition, we have \\(\\alpha_{1, 0} = 1\\), \\(\\alpha_{1, 1} = 3\\) and \\(\\alpha_{1, 2} = -2\\).</p> <p>Thus the solution is \\(a_n = (1 + 3n - 2n^2)(-1)^n\\).</p> <p>Moreover, we can also consider the nonhomogeneous cases.</p> <p>Definition</p> <p>A linear nonhomogeneous recurrence relation of degree \\(k\\) with constant coeffieients is a recurrence relation of the form</p> \\[     a_n  = c_1 a_{n - 1} + c_2 a_{n - 2} + \\cdots + c_k a_{n - k} + F(n), \\] <p>where \\(c_i \\in \\mathbb{R}\\) and \\(F(n) \\not\\equiv = 0\\). And</p> \\[     a_n  = c_1 a_{n - 1} + c_2 a_{n - 2} + \\cdots + c_k a_{n - k} \\] <p>is called the associated homogeneous recurrence relation.</p> <p>Theorem</p> <p>If \\(\\{a_n^{(p)}\\}\\) is particular solution of the linear nonhomogeneous recurrence relation with constant coefficients</p> \\[     a_n  = c_1 a_{n - 1} + c_2 a_{n - 2} + \\cdots + c_k a_{n - k} + F(n), \\] <p>then each solution is of the form \\(\\{a_n^{(p)} + a_n^{(h)}\\}\\), where \\(a_n^{(h)}\\) is a solution of the associated homogeneous recurrence relation</p> \\[     a_n  = c_1 a_{n - 1} + c_2 a_{n - 2} + \\cdots + c_k a_{n - k}. \\] Example <p>Find solutions of \\(a_n = 3a_{n - 1} + 2n\\) with \\(a_1 = 3\\).</p> <p>Solution.</p> <p>The associated linear homogeneous recurrence relation is \\(a_n = 3a_{n-1}\\), whose solution is \\(a_n^{(h)} = \\alpha \\cdot 3^n\\).</p> <p>It's reasonable to try \\(a_n^{(p)} = cn + d\\) as a solution. Then from \\(a_n = 3a_{n - 1} + 2n\\) we have</p> \\[     cn + d = 3(c(n - 1) + d) + 2n, \\] <p>and thus \\(c = -1\\) and \\(d = -\\cfrac{3}{2}\\).</p> <p>The general solution is</p> \\[     a_n = a_n^{(p)} + a_n^{(h)} = -n - \\frac{3}{2} + \\alpha \\cdot 3^n. \\] <p>Substitute \\(a_1 = 3\\), we have \\(\\alpha = \\cfrac{11}{6}\\). Thus</p> \\[     a_n = a_n^{(p)} + a_n^{(h)} = -n - \\frac{3}{2} + \\frac{11}{6} \\cdot 3^n. \\] <p>Moreover, for specific forms of \\(F(n)\\), we have the following theorem.</p> <p>Theorem</p> <p>If \\(F(n) = (b_t n^t + b_{t - 1} n^{t - 1} + \\cdots + b_1 n + b_0) s^n\\), where \\(b_i, s \\in \\mathbb{R}\\).</p> <ul> <li> <p>When \\(s\\) is not a characteristic root of the associated linear homogeneous recurrence relation, there is a particular solution of the form</p> \\[     a_n^{(p)} = (p_t n^t + p_{t - 1} n^{t - 1} + \\cdots + p_1 n + p_0) s^n. \\] </li> <li> <p>When \\(s\\) is a characteristic root with multiplicity \\(m\\), there is a particular solution of the form</p> \\[     a_n^{(p)} = n^m (p_t n^t + p_{t - 1} n^{t - 1} + \\cdots + p_1 n + p_0) s^n. \\] </li> </ul> <p>Example</p> <p>Consider \\(a_n = 6a_{n - 1} - 9a_{n - 2} + F(n)\\). The charateristic root is \\(r = 3\\) of multiplicity \\(2\\).</p> <ul> <li>If \\(F(n) = 3^n\\), then \\(a_n^{(p)} = n^2 p_0 3^n\\).</li> <li>If \\(F(n) = n \\cdot 3^n\\), then \\(a_n^{(p)} = n^2(p_1 n + p_0) 3^n\\).</li> <li>If \\(F(n) = n^2 2^n\\), then \\(a_n^{(p)} = (p_2 n^2 + p_1 n + p_0) 2^n\\).</li> <li>If \\(F(n) = (n^2 + 1) 3^n\\), then \\(a_n^{(p)} = n^2 (p_2 n^2 + p_1 n + p_0) 3^n\\).</li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_8/#generating-functions","title":"Generating Functions","text":"<p>Definition</p> <p>The generating function for the sequence \\(a_0, a_1, \\dots\\) is the infineite series</p> \\[     G(x) = a_0 + a_1 x + \\dots = \\sum\\limits_{k = 0}^{\\infty} a_k x^k. \\] <p>For finite sequence, then</p> \\[     G(x) = a_0 + a_1 x + \\dots + a_n x^n = \\sum\\limits_{k = 0}^{n} a_k x^k. \\] <p>Property</p> <p>Let \\(f(x) = \\sum\\limits_{k = 0}^{\\infty}a_k x^k\\) and \\(g(x) = \\sum\\limits_{k = 0}^{\\infty}b_k x^k\\).</p> <ul> <li>\\(f(x) + g(x) = \\sum\\limits_{k = 0}^{\\infty}(a_k + b_k) x^k\\)</li> <li>\\(f(x) \\cdot g(x) = \\sum\\limits_{k = 0}^{\\infty}(\\sum\\limits_{j = 0}^k a_j b_{k - j}) x^k\\)</li> <li>\\(\\alpha f(x) = \\sum\\limits_{k = 0}^{\\infty} \\alpha a_k x^k\\)</li> <li>\\(x f'(x) = \\sum\\limits_{k = 0}^{\\infty} k a_k x^k\\)</li> <li>\\(f(\\alpha x) = \\sum\\limits_{k = 0}^{\\infty} \\alpha^k a_k x^k\\)</li> </ul> <p>Definition</p> <p>For \\(u \\in \\mathbb{R}\\) and \\(k \\in \\mathbb{N}^*\\), the extended binomial coefficient is</p> \\[     \\binom{u}{k} = \\left\\{         \\begin{aligned}             &amp; \\frac{1}{k!} u(u - 1)\\cdots(u - k + 1), &amp; k &gt; 0, \\\\             &amp; 1, &amp; k = 0.         \\end{aligned}     \\right. \\] <p>In particular, for \\(u = -n\\),</p> \\[     \\binom{-n}{r} = (-1)^r \\binom{n + r - 1}{r}. \\] <p>Theorem | Extended Binomial Theorem</p> <p>If \\(x, u \\in \\mathbb{R}\\) and \\(|x| &lt; 1\\), then</p> \\[     (1 + x)^u = \\sum\\limits_{k = 0}^{\\infty}\\binom{u}{k}x^k. \\]"},{"location":"Mathematics_Basis/DM/Chap_8/#application","title":"Application","text":"<p>Solve Recurrence Relations</p> <p>Solve \\(a_n = 8a_{n - 1} + 10^{n - 1}\\) with initial condition \\(a_0 = 1\\).</p> <p>Solution.</p> <p>Let \\(G(x) = \\sum\\limits_{k = 0}^{\\infty} a_k x^k\\). Then</p> \\[ \\begin{aligned}     G(x) - a_0 &amp;= \\sum\\limits_{k = 1}^{\\infty} a_k x^k = \\sum\\limits_{k = 1}^{\\infty} (8 a_{k - 1} x^k + 10^{k - 1} x^k) \\\\                &amp;= 8 \\sum\\limits_{k = 1}^{\\infty} a_{k - 1} x^k + \\sum\\limits_{k = 1}^{\\infty} 10^{k - 1} x^k \\\\                &amp;= 8 x\\sum\\limits_{k = 1}^{\\infty} a_{k - 1} x^{k - 1} + x \\sum\\limits_{k = 1}^{\\infty} 10^{k - 1} x^{k - 1} \\\\                &amp;= 8 x\\sum\\limits_{k = 0}^{\\infty} a_{k - 1} x^k + x \\sum\\limits_{k = 0}^{\\infty} 10^k x^k \\\\                &amp;= 8 G(x) + \\frac{x}{1 - 10x}. \\end{aligned} \\] <p>Thus</p> \\[ \\begin{aligned}     G(x) &amp;= \\frac{1 - 9x}{(1 - 8x)(1 - 10x)} = \\frac{1}{2}\\left(\\frac{1}{1 - 8x} + \\frac{1}{1 - 10x}\\right) \\\\          &amp;= \\sum\\limits_{l = 0}^{\\infty} \\frac{1}{2} \\left(8^k + 10^k\\right) x^k \\end{aligned} \\] <p>Therefore</p> \\[     a_k = \\frac{1}{2} \\left(8^k + 10^k\\right). \\] <p>Combination</p> <ul> <li> <p>Find \\(C(n, k)\\).</p> \\[     G(x) = (1 + x)^n = \\sum\\limits_{k = 0}^n C(n, k) x^k. \\] </li> <li> <p>Find \\(C(n + r - 1, r)\\).</p> \\[ \\begin{aligned}     G(x) &amp;= (1 + x + x^2 + x^3 + \\cdots)^n = \\frac{1}{(1 - x)^n} = (1 + (-x))^{-n} \\\\          &amp;= \\sum\\limits_{r = 0}^{\\infty}\\binom{-n}{r}(-x)^r = \\sum\\limits_{r = 0}^{\\infty}\\binom{-n}{r}(-1)^r x^r \\\\          &amp;= \\sum\\limits_{r = 0}^{\\infty}(-1)^r C(n + r - 1, r) (-1)^r x^r \\\\          &amp;= \\sum\\limits_{r = 0}^{\\infty}C(n + r - 1, r) x^r \\end{aligned} \\] </li> </ul> Example <p>Find the number of solution of</p> \\[     x_1 + x_2 + x_3 = 17 \\] <p>where \\(x_1, x_2, x_3 \\in \\mathbb{N}\\) with \\(2 \\le x_1 \\le 5\\), \\(3 \\le x_2 \\le 6\\) and \\(4 \\le x_3 \\le 7\\).</p> <p>Solution.</p> <p>It's the same to find the coefficient of \\(x^{17}\\) in the expansion of</p> \\[     G(x) = (x^2 + x^3 + x^4 + x^5)(x^3 + x^4 + x^5 + x^6)(x^4 + x^5 + x^6 + x^7). \\] <p>Thus the answer is \\(3\\).</p> <p>Permutation</p> <p>Definition</p> <p>The exponential generating function for \\(\\{a_n\\}\\) is \\(\\sum\\limits_{n = 0}^{\\infty} \\cfrac{a_n}{n!}x^n\\).</p> <p>Example</p> <p>How many different strings with four characaters can be formed from four \\(A\\), two \\(B\\), two \\(C\\), two \\(D\\), two \\(E\\) and one \\(F\\), one \\(G\\), one \\(H\\)?</p> <p>Solution.</p> <p>It's the same to find the coefficient of \\(\\cfrac{x^{4}}{4!}\\) in the expansion of</p> \\[     G(x) = \\left(1 + \\frac{x}{1!} + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\frac{x^4}{4!}\\right)\\left(1 + \\frac{x}{1!} + \\frac{x^2}{2!}\\right)^4 \\left(1 + \\frac{x}{1!}\\right)^3. \\] <p>Thus the answer is \\(3029\\).</p> Example <p>How many different strings with length \\(r\\) made from digits \\(1, 3, 5, 7, 9\\) are there which contain even \\(3\\)s and even \\(7\\)s?</p> <p>Solution.</p> \\[     G(x) = \\left(1 + \\frac{x^2}{2!} + \\frac{x^4}{4!} + \\cdots\\right)^2 \\left(1 + \\frac{x}{1!} + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots\\right)^3 \\] <p>Notice that \\(e^x + e^{-x} = 2 \\left(1 + \\cfrac{x^2}{2!} + \\cfrac{x^4}{4!} + \\cdots \\right)\\), hence</p> \\[ \\begin{aligned}     G(x) &amp;= \\left(\\frac{e^x + e^{-x}}{2}\\right)^2 \\cdot (e^x)^3 = \\frac{1}{4}(e^{5x} + 2e^{3x} + e^{x}) \\\\          &amp;= \\frac{1}{4}\\left(\\sum\\limits_{r = 0}^{\\infty}5^r \\frac{x^r}{r!} + 2 \\sum\\limits_{r = 0}^{\\infty}3^r \\frac{x^r}{r!} + \\sum\\limits_{r = 0}^{\\infty} \\frac{x^r}{r!}\\right) \\\\          &amp;= \\sum\\limits_{r = 0}^{\\infty} \\left(\\frac{1}{4} \\cdot 5^r + \\frac{1}{2} \\cdot 3^r\\right) \\frac{x^r}{r!}. \\end{aligned} \\] <p>Thus the answer is \\(\\frac{1}{4} \\cdot 5^r + \\frac{1}{2} \\cdot 3^r\\).</p>"},{"location":"Mathematics_Basis/DM/Chap_8/#principle-of-inclusion-exclusion","title":"Principle of Inclusion-Exclusion","text":"<p>Recall principle of inclusion-exclusion</p> \\[     \\left|\\bigcup_{i = 1}^nA_i\\right| = \\sum_{i = 1}^n|A_i| - \\sum_{1 \\le i \\ne j \\le n} |A_i \\cap A_j| + \\cdots + (-1)^{n-1}\\left|\\bigcap_{i = 1]}^n A_i\\right|. \\] <p>An Alternative Form</p> <p>Suppose \\(A_i \\{x | x \\text{ has property } P_i\\}\\), and</p> \\[     \\small     N(P_{i_1} \\dots P_{i_k}) = |\\{x | x \\text{ has all the properties } P_{i_1}, \\dots, P_{i_k}\\}| = \\left|\\bigcap_{i = 1}^nA_i\\right|. \\] \\[     \\small     N(P_{i_1}' \\dots P_{i_k}') = |\\{x | x \\text{ has none of the properties } P_{i_1}, \\dots, P_{i_k}\\}| = N - \\left|\\bigcup_{i = 1]}^n A_i\\right|. \\] <p>Example</p> <p>How many solution does</p> \\[     x_1 + x_2 + x_3 = 11 \\] <p>have, where \\(x_i \\in \\mathbb{N}\\) with \\(x_1 \\le 3\\), \\(x_2 \\le 4\\) and \\(x_3 \\le 6\\)?</p> <p>Solution.</p> <p>Let \\(P_1 : x_1 &gt; 3\\), \\(P_2 : x_2 &gt; 4\\) and \\(P_3 : x_3 &gt; 6\\). The answer is</p> \\[ \\small \\begin{aligned}     N(P_1' P_2' P_3') &amp;= N - N(P_1) - N(P_2) - N(P_3) \\\\                       &amp;+ N(P_1 P_2) + N(P_1 P_3) + N(P_2 P_3) - N(P_1 P_2 P_3). \\end{aligned} \\] <p>We have</p> \\[ \\small \\begin{aligned}     &amp; N = C(3 + 11 - 1, 11) = 78, \\\\     &amp; N(P_1) = C(3 + 7 - 1, 7) = 36, N(P_2) = C(3 + 6 - 1, 6) = 28, \\\\     &amp; N(P_3) = C(3 + 4 - 1, 4) = 15, \\\\     &amp; N(P_1 P_2) = C(3 + 2 - 1, 2) = 6, N(P_1 P_3) = N(P_2 P_3) = N(P_1 P_2 P_3) = 0. \\end{aligned} \\] <p>Thus</p> \\[     N(P_1' P_2' P_3') = 6. \\] <p>Example</p> <p>How many onto functions are there from a set of \\(6\\) elements to a set of \\(3\\) elements?</p> <p>Solution.</p> <p>Suppose codomain = \\(\\{b_1, b_2, b_3\\}\\). Let \\(P_i\\) be the property that \\(b_i\\) are not in the range of the function. Then a function is onto iff it has none of properties \\(P_1\\), \\(P_2\\) or \\(P_3\\).</p> <p>The answer is</p> \\[ \\small \\begin{aligned}     N(P_1' P_2' P_3') &amp;= N - N(P_1) - N(P_2) - N(P_3) \\\\                       &amp;+ N(P_1 P_2) + N(P_1 P_3) + N(P_2 P_3) - N(P_1 P_2 P_3). \\end{aligned} \\] <p>We have</p> \\[ \\small \\begin{aligned}     &amp; N = 3^6, \\\\     &amp; N(P_i) = 2^6, \\\\     &amp; N(P_i P_j) = 1^6, \\\\     &amp; N(P_1 P_2 P_3) = 0. \\end{aligned} \\] <p>Thus</p> \\[     N(P_1' P_2' P_3') = 3^6 - C(3, 1) 2^6 + C(3, 2) 1^6 = 540. \\] <p>Theorem</p> <p>There are</p> \\[     \\small     n^m - \\binom{n}{1} (n - 1)^m + \\cdots + (-1)^{n - 1} \\binom{n}{n - 1} \\cdot 1^m = \\sum\\limits_{k = 0}^{n - 1}(-1)^k \\binom{n}{k} (n - k)^m \\] <p>onto functions from a set with \\(m\\) elements to a set with \\(n\\) elements, where \\(m, n \\in \\mathbb{N}^*\\).</p>"},{"location":"Mathematics_Basis/DM/Chap_9/","title":"Chapter 9 | Relations","text":"<p>Definition</p> <p>A binary relation \\(R\\) between \\(A\\) and \\(B\\) is a subset of the Cartesian product \\(A \\times B\\).</p> \\[     R \\subseteq A \\times B \\] <p>when \\(A = B\\), R is a relation on \\(A\\).</p> <ul> <li>The domain of \\(R\\): \\(\\text{Dom}(R) = \\{x \\in A | (x, y) \\in R \\text{ for some } y \\in B\\}\\).</li> <li>The range of \\(R\\): \\(\\text{Ran}(R) = \\{y \\in B | (x, y) \\in R \\text{ for some } x \\in A\\}\\).</li> </ul> <p>Property</p> <p>There are \\(2^{n^2}\\) relations on a set with \\(n\\) elements.</p>"},{"location":"Mathematics_Basis/DM/Chap_9/#combining-relations","title":"Combining Relations","text":"<p>Since the nature of relation is a set, thus we have union, intersection, complement and difference. Moreover, we have composite.</p> <p>Definition</p> <p>Let \\(R\\) be a relation from a set \\(A\\) to \\(B\\) and \\(S\\) is a relation from \\(B\\) to \\(C\\). The composite of \\(R\\) and \\(S\\) is the relation</p> \\[     S \\circ R = \\{(a, c) | a \\in A, c \\in C, \\exist\\ b \\in B\\ s.t.\\ (a, b) \\in R, (b, c) \\in S\\}. \\] <ul> <li>Denote \\(R^1 = R,\\ \\ R^{n + 1} = R^n \\circ R\\).</li> <li>Denote inverse \\(R^{-1} = \\{(y, x) | (x, y) \\in R\\}\\).</li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_9/#properties","title":"Properties","text":"<p>Definition</p> <p>Let \\(R\\) be a relation on a set \\(A\\). Then</p> <ul> <li>\\(R\\) is reflexive \\(\\Leftrightarrow\\) \\(\\forall\\ x \\in A, \\ \\ (x, x) \\in R\\).</li> <li>\\(R\\) is irreflexive \\(\\Leftrightarrow\\) \\(\\forall\\ x \\in A, \\ \\ (x, x) \\notin R\\).</li> </ul> <p>Remark</p> <ul> <li>\\(R\\) is reflexive iff \\(R_= \\subseteq R\\).</li> <li>There are \\(2^{n(n - 1)}\\) reflexive relations on a set with \\(n\\) elements.</li> </ul> <p>Definition</p> <p>Let \\(R\\) be a relation on a set \\(A\\). Then</p> <ul> <li>\\(R\\) is symmetric \\(\\Leftrightarrow\\) \\(\\forall\\ x, y \\in A,\\ \\ (x, y) \\in R \\Rightarrow (y, x) \\in R\\).</li> <li>\\(R\\) is antisymmetric \\(\\Leftrightarrow\\) \\(\\forall\\ x, y \\in A,\\ \\ (x, y) \\in R, (y, x) \\in R \\Rightarrow x = y\\).</li> </ul> <p>Remark</p> <ul> <li>\\(R\\) is symmetric iff \\(R^{-1} = R\\).</li> <li>\\(R\\) is antisymmetric iff \\(R \\cap R^{-1} \\subseteq R_=\\).</li> <li>Non-symmetric is not the same as antisymmetric (e.g. \\(R_=\\))</li> <li>There are \\(2^{\\frac{n(n + 1)}{2}}\\) symmetric relations on a set with \\(n\\) elements.</li> <li>There are \\(2^{n} \\cdot 3^{\\frac{n(n - 1)}{2}}\\) reflexive antisymmetric on a set with \\(n\\) elements.</li> </ul> <p>Definition</p> <p>Let \\(R\\) be a relation on a set \\(A\\), then</p> <ul> <li>\\(R\\) is transitive \\(\\Leftrightarrow\\) \\(\\forall x, y, z \\in A, ((x, y) \\in R \\wedge (y, z) \\in R) \\Rightarrow (x, z) \\in R\\).</li> </ul> <p>Remark</p> <ul> <li>\\(R\\) is transitive \\(\\Leftrightarrow\\) \\(R \\circ R \\subseteq R\\).</li> </ul> <p>Theorem</p> <p>The relation \\(R\\) on a set \\(A\\) is transitive iff \\(R^n \\subseteq R\\) for \\(n = 2, 3, \\dots\\).</p>"},{"location":"Mathematics_Basis/DM/Chap_9/#representation","title":"Representation","text":""},{"location":"Mathematics_Basis/DM/Chap_9/#matrix-form","title":"Matrix Form","text":"<p>Suppose \\(R\\) is a relation from \\(A = \\{a_1, a_2, \\cdots, a_m\\}\\) to \\(B = \\{b_1, b_2, \\cdots, b_n\\}\\). The relation \\(R\\) can be represented by matrix \\(M_R = (m_{ij})_{m \\times n}\\), where</p> \\[     m_{ij} = \\left\\{         \\begin{aligned}             &amp; 1, &amp; (a_i, b_j) \\in R \\\\             &amp; 0, &amp; (a_i, b_j) \\notin R \\\\         \\end{aligned}     \\right. \\] <p>Property</p> <ul> <li>\\(M_R^2\\) = \\(M_R \\circ M_R\\).</li> <li>\\(R\\) is reflexive \\(\\Leftrightarrow\\) All terms \\(m_{ii}\\) in the main diagnoal of \\(M_R\\) are \\(1\\).</li> <li>\\(R\\) is symmetric \\(\\Leftrightarrow\\) \\(m_{ij} = m_{ji}\\) for all \\(i\\) and \\(j\\), namely a symmetric matrix.</li> <li>\\(R\\) is trasitive \\(\\Leftrightarrow\\) whenever \\(c_{ij}\\) in \\(C = M_R^2\\) is nonzero then entry \\(m_{ij}\\) in \\(M_R\\) is also nonzero. (\\(R^2 \\subseteq R\\))</li> <li>Combining Relations</li> </ul> \\[     M_{R_1 \\cup R_2} = M_{R_1} \\vee M_{R_2},\\ \\     M_{R_1 \\cap R_2} = M_{R_1} \\wedge M_{R_2},\\ \\     M_{S \\circ R} = M_R \\times M_S. \\]"},{"location":"Mathematics_Basis/DM/Chap_9/#graph-form","title":"Graph Form","text":"<p>Each order pair is represented using an arc with its direction indicated by an arrow, thus we can use digraph to represent a relation.</p> <p>Property</p> <ul> <li>\\(R\\) is reflexive \\(\\Leftrightarrow\\) There are loops at each vertex.</li> <li>\\(R\\) is symmetric \\(\\Leftrightarrow\\) Each edge is accompanied by an inverse edge.</li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_9/#closures-of-relations","title":"Closures of Relations","text":"<p>Definition</p> <p>Let \\(R\\) be a relation on a set \\(A\\). If there is a relation \\(S\\) satisfying</p> <ul> <li>\\(S\\) with property \\(\\textbf{P}\\) (reflexive, symmetry, or transitivity).</li> <li>\\(\\forall\\ S'\\) with property \\(\\textbf{P}\\) and \\(R \\subseteq S'\\), then \\(S \\subseteq S'\\).</li> </ul> <p>Then \\(S\\) is called a closure of \\(R\\) w.r.t. \\(\\textbf{P}\\).</p> <p>Theorem</p> <p>Let \\(R\\) be a relation on a set \\(A\\).</p> <ul> <li> <p>Reflexive closure of relation \\(R\\):</p> \\[     r(R) = R \\cup \\Delta. \\] <p>where \\(\\Delta = \\{(a, a) | a \\in A\\}\\) is diagonal relation on \\(A\\).</p> </li> <li> <p>Symmetric closure of relation \\(R\\):</p> \\[     s(R) = R \\cup R^{-1}. \\] </li> <li> <p>Transitive closure of relation \\(R\\):</p> \\[     t(R) = R^*,\\ \\ \\text{ where $R^*$ is connectivity relation}. \\] </li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_9/#method-to-find-transitive-closure","title":"Method to Find Transitive Closure","text":"<p>To find the transitive closure, we gives the following definition and theorem.</p> <p>Theorem</p> <p>Let \\(R\\) be a relation on set \\(A\\). There is a path of length \\(n\\) from \\(a\\) to \\(b\\) \\(\\Leftrightarrow\\) \\((a, b) \\in R^n\\).</p> <p>Definition</p> <p>The connectivity relation \\(R^* = \\{(a, b) | \\text{there is a path from \\(a\\) to \\(b\\)}\\}\\).</p> \\[     R^* = \\bigcup\\limits_{n = 1}^{\\infty} R^n. \\] <p>If \\(R\\) is on set \\(A\\) with \\(n\\) elements, then</p> \\[     R^* = \\bigcup\\limits_{i = 1}^{n} R^i. \\] <p>Lemma</p> <p>Let \\(A\\) be a set with \\(n\\) elements, and let \\(R\\) be a relation on \\(A\\). If there is a path in \\(R\\) from \\(a\\) to \\(b\\), then there is such a path with length not exceeding \\(n\\). Moreover, when \\(a \\ne b\\), if there is a path in \\(R\\) from \\(a\\) to \\(b\\), then there is such a path with length not exceeding \\(n - 1\\).</p> <p>Theorem</p> <p>\\(M_R\\) is the 0-1 matrix of the relation \\(R\\) on a set \\(A\\) with \\(n\\) elements, Then the 0-1 matrix of the transitive closure \\(R^*\\) is </p> \\[     M_{R^*} = M_R \\vee M_R^2 \\vee \\cdots \\vee M_R^n. \\] <p>Example</p> <p>Find 0-1 matrix of the transitive closure of the relation \\(R\\) where</p> \\[     M_R = \\begin{bmatrix}         1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0     \\end{bmatrix}. \\] <p>Solution.</p> \\[     M_{R^*} = M_R \\vee M_R^2 \\vee M_R^3 =     \\begin{bmatrix}         1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0     \\end{bmatrix}     \\vee     \\begin{bmatrix}         1 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0     \\end{bmatrix}     \\vee     \\begin{bmatrix}         1 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1     \\end{bmatrix}     =     \\begin{bmatrix}         1 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1     \\end{bmatrix}. \\] <p>Computational complexity of this method is \\((n - 1)n^2(2n-1)+(n-1)n^2 = O(n^4)\\). Now we introduce an \\(O(n^3)\\) method to solve out transtive closure.</p> <p>Warshall's Algorithm</p> <p>Definition</p> <p>For a path \\(a, x_1, \\dots, x_{m-1}, b\\), the interior vertices are \\(x_1, \\cdots, x_{m - 1}\\).</p> <p>Define</p> \\[ \\begin{aligned}     W_0 &amp;= M^R, \\\\     W_k &amp;= \\left[w_{ij}^{(k)}\\right]_{n \\times n}. \\end{aligned} \\] <p>where</p> \\[ \\begin{aligned}     w_{ij}^{(k)} = \\left\\{         \\begin{aligned}             &amp; 1, &amp;&amp; \\substack{                     \\text{there is a path from $v_i$ to $v_j$ s.t all the} \\\\                     \\text{interior vertices of the path are in the set $\\tiny \\{v_1, v_2, \\cdots, v_k\\}$,}                 } \\\\             &amp; 0, &amp;&amp; {\\tiny\\text{otherwise.}}         \\end{aligned}     \\right. \\end{aligned} \\] <p>And from definition we have</p> \\[     W_n = M_{R^*}. \\] <p>Lemma</p> \\[     w_{ij}^{(k)} = w_{ij}^{(k - 1)} \\vee \\left(w_{ik}^{(k - 1)} \\wedge w_{kj}^{(k - 1)}\\right). \\] <p>Method</p> <p>Compute \\(M_{R^*}\\) by computing \\(W_0 \\rightarrow W_1 \\rightarrow \\cdots \\rightarrow W_n = M_{R^*}\\).</p> Example <p>Find transitive closure of</p> \\[     M_R = \\begin{bmatrix}         0 &amp; 0 &amp; 0 &amp; 1 \\\\         1 &amp; 0 &amp; 1 &amp; 0 \\\\         1 &amp; 0 &amp; 0 &amp; 1 \\\\         0 &amp; 0 &amp; 1 &amp; 0     \\end{bmatrix}. \\] <p>Solution.</p> W0W1 / W2W3W4 \\[     W_0 = M_R = \\begin{bmatrix}         0 &amp; 0 &amp; 0 &amp; 1 \\\\         1 &amp; 0 &amp; 1 &amp; 0 \\\\         1 &amp; 0 &amp; 0 &amp; 1 \\\\         0 &amp; 0 &amp; 1 &amp; 0     \\end{bmatrix}. \\] <p>Consider the entries of \\(1\\) in the first row (<code>14</code>) and the first column (<code>21</code>, <code>31</code>). Combine them with elimination of <code>1</code> and we get <code>24</code> and <code>34</code>, and thus set</p> \\[     w_{24}^{(1)} = w_{34}^{(1)} = 1. \\] \\[     W_1 = W_2 = \\begin{bmatrix}         0 &amp; 0 &amp; 0 &amp; 1 \\\\         1 &amp; 0 &amp; 1 &amp; 1 \\\\         1 &amp; 0 &amp; 0 &amp; 1 \\\\         0 &amp; 0 &amp; 1 &amp; 0     \\end{bmatrix}. \\] <ul> <li>Consider the entries of \\(1\\) in the second row (<code>21</code>, <code>23</code>, <code>24</code>) and the second column (<code>none</code>). Thus \\(W_2 = W_1\\).</li> <li>Consider the entries of \\(1\\) in the third row (<code>31</code>, <code>34</code>) and the third column (<code>23</code>, <code>43</code>). Combine them with elimination of <code>3</code> and we get <code>21</code>, <code>24</code>, <code>41</code> and <code>44</code>, and thus set</li> </ul> \\[     w_{21}^{(3)} = w_{24}^{(3)} = w_{41}^{(3)} = w_{44}^{(3)} = 1. \\] \\[     W_3 = \\begin{bmatrix}         0 &amp; 0 &amp; 0 &amp; 1 \\\\         1 &amp; 0 &amp; 1 &amp; 1 \\\\         1 &amp; 0 &amp; 0 &amp; 1 \\\\         1 &amp; 0 &amp; 1 &amp; 1     \\end{bmatrix}. \\] <ul> <li>Consider the entries of \\(1\\) in the fourth row (<code>41</code>, <code>43</code>, <code>44</code>) and the fourth column (<code>14</code>, <code>24</code>, <code>34</code>, <code>44</code>). Combine them with elimination of <code>4</code> and we get <code>11</code>, <code>13</code>, <code>14</code>, <code>21</code>, <code>23</code>, <code>24</code> and <code>31</code>, <code>33</code>, <code>34</code>, and thus set</li> </ul> \\[ \\begin{aligned}     w_{11}^{(4)} = w_{13}^{(4)} = w_{14}^{(4)} = 1. \\\\     w_{21}^{(4)} = w_{23}^{(4)} = w_{24}^{(4)} = 1. \\\\     w_{31}^{(4)} = w_{33}^{(4)} = w_{34}^{(4)} = 1. \\end{aligned} \\] \\[     M_{R^*} = W_4 = \\begin{bmatrix}         1 &amp; 0 &amp; 1 &amp; 1 \\\\         1 &amp; 0 &amp; 1 &amp; 1 \\\\         1 &amp; 0 &amp; 1 &amp; 1 \\\\         1 &amp; 0 &amp; 1 &amp; 1     \\end{bmatrix}. \\]"},{"location":"Mathematics_Basis/DM/Chap_9/#equivalence-relations","title":"Equivalence Relations","text":"<p>Definition</p> <p>Relation \\(R_\\sim : A \\leftrightarrow A\\) is an equivalence relation if it's reflexive, symmetric and transitive.</p> <p>Definition</p> <p>Let \\(R: A \\leftrightarrow A\\) is an equivalence relatiion. For any \\(a \\in A\\), the equivalence class of \\(a\\) is the set of the elements related to \\(a\\), denoted by</p> \\[     [a]_R = \\{x \\in A | (x, a) \\in R\\}. \\] <p>If \\(b \\in [a]_R\\), then \\(b\\) is called a representitive of this equivalence class.</p> <p>Property</p> <ul> <li>\\(\\forall a \\in A,\\ \\ a \\in [a]_R \\neq \\emptyset.\\)</li> <li>\\((a, b) \\in R \\Rightarrow [a]_R = [b]_R.\\)</li> <li>\\((a, b) \\notin R \\Rightarrow [a]_R \\cap [b]_R = \\emptyset.\\)</li> <li>\\(\\bigcup\\limits_{a \\in A}[a]_R = A.\\)</li> </ul> <p>Definition</p> <p>The set of all equivalence classes of \\(R\\) is the quotient set of \\(A\\) w.r.t. \\(R\\).</p> \\[     A / R = \\{[a]_R | a \\in A\\}. \\] <p>Remark</p> <ul> <li>If \\(A\\) is finite, then \\(A / R\\) is finite.</li> <li>If \\(A\\) has \\(n\\) elements and each \\([a]_R\\) has \\(m\\) elements, then \\(m | n\\) and \\(A / R\\) has \\(n / m\\) elements.</li> </ul> <p>Definition</p> <p>A partition \\(\\pi\\) on a set \\(S\\) is a family \\(\\{A_1, A_2, \\cdots, A_n\\}\\) of subsets of \\(S\\) and</p> <ul> <li>\\(\\bigcup\\limits_{k = 1}^{n} A_k = S.\\)</li> <li>\\(A_j \\cap A_k = \\emptyset\\) for every \\(j, k\\) with \\(j \\ne k\\).</li> </ul> <p>Theorem</p> <ul> <li>Let \\(R\\) be an equivalence relation on a set \\(S\\), then the equivalence classes of \\(R\\) form a partition of \\(X\\).</li> <li>Conversely, given a partition \\(\\{A_i | i \\in I\\}\\) of set \\(S\\), there is an equivalence relation \\(R\\) that has the set \\(A_i\\) , \\(i \\in I\\) as its equivalence classes.</li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_9/#partial-order","title":"Partial Order","text":"<p>Definition</p> <p>Relation \\(R_\\preceq: S \\leftrightarrow S\\) is a partial order \u504f\u5e8f if it's reflexive, antisymmetric and transitive.</p> <p>A set \\(S\\) together with a partial order \\(R_\\preceq\\) is called a partial order set or poset, denoted by \\((S, R_\\preceq)\\).</p> <p>Definition</p> <p>\\(a, b \\in (S, \\preceq)\\) are called comparable if either \\(a \\preceq b\\) or \\(b \\preceq a\\), otherwise incomparable.</p> <p>If every elements of \\((S, \\preceq)\\) are comaparable, then \\(S\\) is called a totally order \u5168\u5e8f or linearly order set and \\(\\preceq\\) is called a total order or linear order. A totally ordered set is also called a chain.</p>"},{"location":"Mathematics_Basis/DM/Chap_9/#hasse-diagram","title":"Hasse Diagram","text":"<p>We can represent the partial order by a graph called Hasse Diagram. To construct a Hasse diagram, follow the steps below.</p> <ol> <li>Start with the directed graph for the relation.</li> <li>Remove all loops.</li> <li>Remove the edges that must be present because of the transitivity.</li> <li>Finally arrage each edges so that its inital vertex is below its terminal vertex, and remove all arrows.</li> </ol> <p>Example</p> <p>Draw the Hasse diagram representing</p> <ul> <li>the partial order \\(\\{(a, b) | a \\mid b\\}\\) on the set \\(\\{1, 2, 3, 4, 6, 8, 12\\}\\).</li> <li>the partial order \\(\\{(A, B) | A \\subseteq B\\}\\) on power set \\(2^S\\), where \\(S = \\{a, b, c\\}\\).</li> </ul> <p> </p>"},{"location":"Mathematics_Basis/DM/Chap_9/#maximial-minimal-greatest-and-least-element","title":"Maximial, Minimal, Greatest and Least Element","text":"<p>Definition</p> <p>Let \\((A, \\preceq)\\) be a partial ordered set, \\(B \\subseteq A\\).</p> <ul> <li>\\(a\\) is a maximal element \u6781\u5927\u5143 of \\(B\\) if \\(a \\in B \\wedge \\text{ There is no } x \\in B \\text{ s.t. } a \\prec x\\).  \\(b\\) is a minimal element \u6781\u5c0f\u5143 of \\(B\\) if \\(b \\in B \\wedge \\text{ There is no } x \\in B \\text{ s.t. } x \\prec b\\).</li> <li>\\(a\\) is a the greatest element \u6700\u5927\u5143 of \\(B\\) if \\(a \\in B \\wedge \\forall\\ x \\in B, x \\preceq a\\).  \\(b\\) is a the least element \u6700\u5c0f\u5143 of \\(B\\) if \\(b \\in B \\wedge \\forall\\ x \\in B, b \\preceq x\\).</li> <li>\\(c\\) is an upper bound \u4e0a\u754c of \\(B\\) if \\(c \\in A \\wedge \\forall\\ x \\in B : x \\preceq c\\).  \\(d\\) is a lower bound \u4e0b\u754c of \\(B\\) if \\(d \\in A \\wedge \\forall\\ x \\in B : d \\preceq x\\).</li> <li>\\(c\\) is a least upper bound (lub) \u4e0a\u786e\u754c of \\(B\\) if \\(c\\) is a upper bound and \\(\\forall\\ x\\) is a upper bound, \\(c \\preceq x\\).  \\(d\\) is a greatest lower bound (glb) \u4e0b\u786e\u754c of \\(B\\) if \\(d\\) is a lower bound and \\(\\forall\\ x\\) is a lower bound, \\(x \\preceq d\\).</li> </ul> <p>Example</p> <p>For \\((A, \\subseteq)\\), where \\(A = 2^S\\), \\(S = \\{a, b, c, d\\}\\), consider the following concepts of \\(B = \\{\\emptyset, \\{a\\}, \\{c\\}, \\{a, b\\}\\}\\).</p> <ul> <li>minimal element: \\(\\emptyset\\).</li> <li>maximal element: \\(\\{c\\}, \\{a,b\\}\\).</li> <li>the greatest element: none.</li> <li>the least element: \\(\\emptyset\\).</li> <li>upper bound: \\(\\{a, b, c\\}, \\{a, b, c, d\\}\\).</li> <li>least upper bound: \\(\\{a, b, c\\}\\).</li> <li>lower bound: \\(\\emptyset\\).</li> <li>least lower bound: \\(\\emptyset\\).</li> </ul>"},{"location":"Mathematics_Basis/DM/Chap_9/#lattice","title":"Lattice","text":"<p>Definition</p> <p>A partially ordered set in which every pair of elements has both a least upper bound and a greatest lower bound is called a lattice \u683c.</p> <p>Example</p> <ul> <li>The poset \\((\\mathbb{Z}, \\mid)\\) is a lattice, since for the pair \\((a, b)\\), \\(\\text{lub}(a, b) = \\text{lcm}(a, b)\\) and \\(\\text{glb}(a, b) = \\text{gcd}(a, b)\\).</li> <li>The poset \\((\\{1, 2, 3, 4, 5\\}, \\mid)\\) is not a lattice, since \\(\\text{lub}(2, 3) = \\text{lcm}(2, 3) = 6 \\notin \\{1, 2, 3, 4, 5\\}\\).</li> </ul>"},{"location":"Mathematics_Basis/NA/Chap_1/","title":"Chapter 1 | Mathematical Preliminaries","text":""},{"location":"Mathematics_Basis/NA/Chap_1/#error","title":"Error","text":""},{"location":"Mathematics_Basis/NA/Chap_1/#truncation-error","title":"Truncation Error","text":"<p>the error involved using a truncated or finite summation.</p>"},{"location":"Mathematics_Basis/NA/Chap_1/#roundoff-error","title":"Roundoff Error","text":"<p>the error produced when performing real number calculations. It occurs because the arithmetic performed in a machine involved numbers with a finite number of digits.</p> <p>Chopping &amp; Rounding</p> <p>Given a real number \\(y = 0.d_1d_2\\dots d_kd_{k+1}\\dots \\times 10^N\\), the floating-point representation of \\(y\\) is \\(fl(y)\\):</p> \\[     fl(y) = \\left\\{         \\begin{aligned}             &amp; 0.d_1d_2\\dots d_k \\times 10^N, &amp; {\\tt Chopping} \\\\             &amp; {\\tt chop}\\left(y + 5 \\times 10^{n - (k + 1)}\\right). &amp; {\\tt Rounding}         \\end{aligned}     \\right. \\] <p>Definition 1.0</p> <p>If \\(p^*\\) is an approximation to \\(p\\), then absolute error is \\(|p - p^*|\\) and relative error is \\(\\dfrac{|p - p^*|}{|p|}\\).</p> <p>The number \\(p^*\\) is said to be approximate to \\(p\\) to \\(t\\) significant digits if \\(t\\) is the largest nonnegative integer s.t.</p> \\[     \\frac{|p - p^*|}{|p|} &lt; 5 \\times 10^{-t}. \\] Example <p>For the floating-point representation of \\(y\\), the relative error is</p> \\[     \\left|\\frac{y - fl(y)}{y}\\right|. \\] <p>For chopping representation,</p> \\[     \\left|\\frac{y - fl(y)}{y}\\right| = \\left|\\frac{0.d_{k + 1}d_{k + 2}\\dots}{0.d_1d_2\\dots}\\right| \\times 10^{-k} \\le \\frac{1}{0.1} \\times 10^{-k} = 10^{-k + 1}. \\] <p>For rounding representation,</p> \\[     \\left|\\frac{y - fl(y)}{y}\\right| \\le \\frac{0.5}{0.1} \\times 10^{-k} = 0.5 \\times 10^{-k + 1}. \\]"},{"location":"Mathematics_Basis/NA/Chap_1/#effect-of-error","title":"Effect of Error","text":"<ul> <li>Subtraction may reduce significant digits. e.g. 0.1234 - 0.1233 = 0.001.</li> <li>Division by small number of multiplication by large number magnify the abosolute error without modifying the relative error.</li> </ul>"},{"location":"Mathematics_Basis/NA/Chap_1/#some-solutions-to-reduce-error","title":"Some Solutions to Reduce Error","text":"<p>Quadratic Formula</p> <p>The roots of \\(ax^2 + bx + c = 0\\) is </p> \\[     x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}. \\] <p>Sometimes \\(b\\) is closer to \\(\\sqrt{b^2 - 4ac}\\), which may cause the subtraction to reduce significant digits. An alternate way is to modify the formula to</p> \\[     x = \\frac{-2c}{b \\pm \\sqrt{b^2 - 4ac}}. \\] <p>But it may cause the division by small number. So it's a tradeoff to use one of the two formulae above.</p> <p>Horner's Method \u79e6\u4e5d\u97f6\u7b97\u6cd5</p> \\[ \\begin{aligned}     f(x) &amp;= a_nx^n + a_{n-1}x^{n-1} + \\dots + a_1x + a_0      \\\\ &amp;= (\\dots((a_nx+a_{n-1})x+a_{n-2})x+\\dots+a_1)x+a_0 \\end{aligned} \\]"},{"location":"Mathematics_Basis/NA/Chap_1/#stable-algorithms-and-convergence","title":"Stable Algorithms and Convergence","text":"<p>Definition 1.1</p> <p>An algorithm that satisfies that small changes in the initial data produce correspondingly small changes in the final results is called stable; otherwise it is unstable.  An algorithm is called conditionally stable if it is stable only for certain choices of initial data.</p> <p>Suppose \\(E_0 &gt; 0\\) denotes an initial errors, \\(E_n\\) denotes the magnitude of an error after \\(n\\) subsequent operations, then we define</p> <ul> <li>Linear growth of errors \\(E_n \\approx C n E_0.\\)<ul> <li>unavoidable and acceptable.</li> </ul> </li> <li>Exponential growth of errors \\(E_n \\approx C^n E_0.\\)<ul> <li>unacceptable.</li> </ul> </li> </ul> Example <p>The recursive equation \\(p_n = \\dfrac{10}{3}p_{n - 1} - p_{n - 2}\\) has the solution</p> \\[     p_n = c_1 \\left(\\frac13\\right)^n + c_23^n. \\] <p>If \\(p_0 = 1, p_1 = \\dfrac13\\), then the solution is</p> \\[     p_n = \\left(\\frac13\\right)^n. \\] <p>Suppose we have five-digit rounding representation, then \\(\\hat p_0 = 1.0000\\), \\(\\hat p_1 = 0.33333\\), and the solution is </p> \\[     \\hat p_n = 1.0000 \\left(\\frac13\\right)^n - 0.12500 \\times 10^{-5} \\cdot 3^n. \\] <p>Then</p> \\[     p_n - \\hat p_n = 0.12500 \\times 10^{-5} \\cdot 3^n \\] <p>grow exponentially with \\(n\\).</p> <p>On the other hand, the recursive equation \\(p_n = 2p_{n - 1} - p_{n - 2}\\) has the solution</p> \\[     p_n = c_1 + c_2n \\] <p>If \\(p_0 = 1, p_1 = \\dfrac13\\), then the solution is</p> \\[     p_n = 1 - \\frac23 n. \\] <p>Suppose we have five-digit rounding representation, then \\(\\hat p_0 = 1.0000\\), \\(\\hat p_1 = 0.33333\\), and the solution is </p> \\[     \\hat p_n = 1.0000 - 0.66667 n. \\] <p>Then</p> \\[     p_n - \\hat p_n = \\left(0.66667 - \\frac23\\right) n \\] <p>grow linearly with \\(n\\).</p> <p>Error of floating-point number (IEEE 754 standard)</p>"},{"location":"Mathematics_Basis/NA/Chap_1/#range-of-normal-representation","title":"Range of normal representation","text":"<p>For Single-Precision</p> <ul> <li>Smallest \\(\\text{0/1\\ 00000001\\ 00\\dots00}\\)<ul> <li>\\(\\pm 1.0 \\times 2^{-126} \\approx \\pm 1.2 \\times 10^{-38}\\).</li> </ul> </li> <li>Largest \\(\\text{0/1\\ 11111110\\ 11\\dots11}\\)<ul> <li>\\(\\pm 2.0 \\times 2^{127} \\approx \\pm 3.4 \\times 10^{38}\\).</li> </ul> </li> </ul> <p>For Double-Precision</p> <ul> <li>Smallest \\(\\pm 1.0 \\times 2^{-1022} \\approx \\pm 2.2 \\times 10^{-308}\\).</li> <li>Largest \\(\\pm 2.0 \\times 2^{1023} \\approx \\pm 1.8 \\times 10^{308}\\).</li> </ul>"},{"location":"Mathematics_Basis/NA/Chap_1/#relative-precision","title":"Relative precision","text":"<ul> <li>Single-Precision: \\(2^{-23}\\)<ul> <li>Equivalent to \\(23 \\times \\log_{10}2 \\approx 6\\) decimal digits of precision\uff086 \u4f4d\u6709\u6548\u6570\u5b57\uff09.</li> </ul> </li> <li>Double-Precision: \\(2^{-52}\\)<ul> <li>Equivalent to \\(52 \\times \\log_{10}2 \\approx 16\\) decimal digits of precision\uff0816 \u4f4d\u6709\u6548\u6570\u5b57\uff09.</li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/NA/Chap_2/","title":"Chapter 2 | Solution of Equations in One Variable","text":""},{"location":"Mathematics_Basis/NA/Chap_2/#bisection-method","title":"Bisection Method","text":"<p>Theorem 2.0</p> <p>Suppose that \\(f \\in C[a, b]\\) and \\(f(a)\\cdot f(b) \\lt 0\\). The Bisection method generates a sequence \\(\\{p_n\\}\\) \\((n = 0, 1, 2,\\dots)\\) approximating a zero \\(p\\) of \\(f\\) with</p> \\[     |p_n-p| \\le \\frac{b - a}{2^n},\\ \\ \\text{ when }n \\ge 1. \\]"},{"location":"Mathematics_Basis/NA/Chap_2/#key-points-for-algorithm-implementation","title":"Key Points for Algorithm Implementation","text":"<ul> <li>\\(mid = a + (b - a)\\ /\\ 2, \\text{but not } mid = (a + b)\\ /\\ 2\\), for accuracy and not exceeding the limit of range.</li> <li>\\(sign(FA)\\cdot sign(FM)\\gt 0, \\text{but not }FA\\cdot FM \\gt 0\\), for saving time.</li> </ul>"},{"location":"Mathematics_Basis/NA/Chap_2/#pros-cons","title":"Pros &amp; Cons","text":"<p>Pros</p> <ul> <li>Simple premise, only requires a continuous \\(f\\).</li> <li>Always converges to a solution.</li> </ul> <p>Cons</p> <ul> <li>Slow to converge, and a good intermediate approximation can be inadvertently discarded.</li> <li>Cannot find multiple roots and complex roots.</li> </ul>"},{"location":"Mathematics_Basis/NA/Chap_2/#fixed-point-iteration","title":"Fixed-Point Iteration","text":"\\[     f(x) = 0 \\leftrightarrow x = g(x) \\] <p>Theorem12.1 | Fixed-Point Theorem</p> <p>Let \\(g \\in C[a,b]\\) be such that \\(g(x) \\in [a,b], \\forall x \\in [a, b]\\). Suppose \\(g'\\) exists on \\((a,b)\\) and that a constant \\(k\\) \\((0\\lt k \\lt 1)\\) exists, s.t.</p> \\[     \\forall\\ x \\in (a,b),\\ \\ |g'(x)| \\le k. \\] <p>Then \\(\\forall\\ p_0 \\in[a,b]\\), the sequence \\(\\{p_n\\}\\) defined by</p> \\[     p_n=g(p_{n-1}) \\] <p>converges to the unique fixed-point \\(p\\in [a,b]\\).</p> <p>.</p> <p>Corollary 2.2</p> \\[ \\begin{aligned}     |p_n - p| &amp;\\le k^n \\max\\{p_0 = a, b - p_0\\}, \\\\     |p_n - p| &amp;\\le \\frac{k^n}{1 - k}|p_1 - p_0|. \\end{aligned} \\] <p>\\(\\Rightarrow\\) The smaller \\(k\\) is, the faster it converges.</p>"},{"location":"Mathematics_Basis/NA/Chap_2/#newtons-method-newton-raphson-method","title":"Newton's Method (Newton-Raphson Method)","text":"<p>Newton's method is an improvement of common fixed-point iteration method above.</p> <p>Theorem 2.3</p> <p>Let \\(f\\in C^2[a,b]\\) and \\(\\exists\\ p\\in[a,b]\\), s.t. \\(f(p) = 0\\) and \\(f'(p) \\ne 0\\), then \\(\\exists\\ \\delta \\gt 0\\), \\(\\forall\\ p_0 \\in [p - \\epsilon, p + \\epsilon]\\), s.t. the sequence \\(\\{p_n\\}_{n = 1}^\\infty\\) defined by </p> \\[     p_n = p_{n-1} - \\frac{f(p_{n-1})}{f'(p_{n-1})} \\] <p>converges to \\(p\\). </p>"},{"location":"Mathematics_Basis/NA/Chap_2/#error-analysis-for-iterative-methods","title":"Error Analysis for Iterative Methods","text":"<p>Definition 2.0</p> <p>Suppose \\(\\{p_n\\}\\) is a sequence that converges to \\(p\\), and \\(\\forall n\\), \\(p_n \\ne p\\). If positive constants \\(\\alpha\\) and \\(\\lambda\\) exist with</p> \\[     \\lim_{n \\rightarrow \\infty} \\frac{|p_{n + 1} - p|}{|p_n - p|^\\alpha} = \\lambda, \\] <p>then \\(\\{p_n\\}\\) conveges to \\(p\\) of order \\(\\alpha\\), with asymptotic error constant \\(\\lambda\\).</p> <p>Specially,</p> <ul> <li>If \\(\\alpha = 1\\), the sequence is called linearly convergent.</li> <li>If \\(\\alpha = 2\\), the sequence is called quadratically convergent.</li> </ul> <p>Theorem 2.4</p> <p>The common fixed-point iteration method (\\(g'(p) \\ne 0\\)) with the premise in Fixed-Point Theorem is linearly convergent.</p> Proof \\[     \\lim_{n\\rightarrow \\infty}\\frac{|p_{n+1} - p|}{|p_n - p|} = \\lim_{n\\rightarrow \\infty}\\frac{g'(\\xi)|p_n - p|}{|p_n - p|} = |g'(p)|. \\] <p>Theorem 2.5</p> <p>The Newton's method \\((g'(p)=0)\\) is at least quadratically convergent.</p> Proof \\[ \\begin{aligned}     &amp; 0 = f(p) = f(p_n) + f'(p_n)(p - p_n) + \\frac{f''(\\xi _n)}{2!}(p - p_n)^2 \\\\     &amp; \\Rightarrow p = \\underbrace{p_n - \\frac{f(p_n)}{f\"(p_n)}}_{p_{n+1}} - \\frac{f''(\\xi _n)}{2!f'(p_n)}(p - p_n)^2 \\\\     &amp; \\Rightarrow \\frac{|p_{n+1} - p|}{|p_n - p|^2} = \\frac{f''(\\xi _n)}{2f'(p_n)}, \\\\      &amp; \\lim_{n\\rightarrow \\infty}\\frac{|p_{n+1} - p|}{|p_n - p|^2} = \\frac{f''(p_n)}{2f'(p_n)}. \\end{aligned} \\] <p>More commonly,</p> <p>Theorem 2.6</p> <p>Let \\(p\\) be a fixed point of \\(g(x)\\). If there exists some constant \\(\\alpha \\ge 2\\) such that \\(g\\in C^\\alpha [p-\\delta, p+\\delta]\\), \\(g'(p)=\\dots=g^{(\\alpha-1)}(p)=0\\), \\(g^{(\\alpha)}(p)=0\\). Then the iterations with \\(p_n = g(p_{n-1})\\), \\(n \\ge 1\\), is of order \\(\\alpha\\).</p>"},{"location":"Mathematics_Basis/NA/Chap_2/#multiple-roots-situation","title":"Multiple Roots Situation","text":"<p>Notice that from the proof above, Newton's method is quadratically convergent if \\(f'(p_n) \\ne 0\\). If \\(f'(p) = 0\\), then the equation has multiple roots at \\(p\\).</p> <p>If \\(f(x) = (x - p)^mq(x)\\) and \\(q(p)\\ne 0\\), for Newton's Method, </p> \\[     g'(p) = \\frac{f(p)f''(p)}{f'(p)^2} = \\left.\\frac{f(x)f''(x)}{f'(x)^2}\\right|_{x=p} = \\dots =1 - \\frac{1}{m} \\lt 1 \\] <p>It is still convegent, but not quadratically.</p> A way to speed it up <p>Let</p> \\[     \\mu (x) = \\frac{f(x)}{f'(x)}, \\] <p>then \\(\\mu (x)\\) has the same root as \\(f(x)\\) but no multiple roots anymore. And</p> \\[     g(x) = x - \\frac{\\mu (x)}{\\mu'(x)} = x - \\frac{f(x)f'(x)}{f'(x)^2 - f(x)f''(x)}. \\] <p>Newton's method can be used there again.</p>"},{"location":"Mathematics_Basis/NA/Chap_2/#pros-cons_1","title":"Pros &amp; Cons","text":"<p>Pros</p> <ul> <li>Quadratic convegence</li> </ul> <p>Cons</p> <ul> <li>Requires additional calculation of \\(f''(x)\\)</li> <li>The denominator consists of the difference of the two numbers both close to \\(0\\).</li> </ul>"},{"location":"Mathematics_Basis/NA/Chap_2/#accelerating-convergence","title":"Accelerating Convergence","text":""},{"location":"Mathematics_Basis/NA/Chap_2/#aitkens-delta2-method","title":"Aitken's \\(\\Delta^2\\) Method","text":"<p>Definition 2.1</p> <p>Forward Difference \\(\\Delta p_n = p_{n+1} - p_n\\). Similarly \\(\\Delta^kp_n = \\Delta(\\Delta^{k-1}p_n)\\)</p> <p>Representing Aitken's \\(\\Delta^2\\) Method by forward difference, we have</p> \\[     \\hat{p_n} = p_n - \\frac{(\\Delta p_n)^2}{\\Delta^2p_n}. \\] <p>Theorem 2.7</p> <p>Suppose that \\(\\{p_n\\}\\) is a sequence, \\(\\lim\\limits_{n\\rightarrow \\infty}p_n = p\\), \\(\\exists N\\), s.t. \\(\\forall\\ n &gt; N\\), \\((p_n - p)(p_{n+1} -p) \\gt 0\\). Then the sequence \\(\\{\\hat{p_n}\\}\\) converges to \\(p\\) faster than \\(\\{p_n\\}\\) in the sense that</p> \\[     \\lim_{n\\rightarrow \\infty}\\frac{\\hat{p_n} - p}{p_n - p} = 0. \\] <p>The algorithm to implement it is called Steffensen\u2019s Acceleration.</p>"},{"location":"Mathematics_Basis/NA/Chap_3/","title":"Chapter 3 | Interpolation and Polynomial Approximation","text":""},{"location":"Mathematics_Basis/NA/Chap_3/#lagrange-interpolation","title":"Lagrange Interpolation","text":"<p>Suppose we have function \\(y = f(x)\\) with the given points \\((x_0, y_0), (x_1, y_1), \\dots, (x_n, y_n)\\), and then construct a relatively simple approximating function \\(g(x) \\approx f(x)\\).</p> <p>Theorem 3.0</p> <p>If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers and \\(f\\) is a function with given values \\(f(x_0), \\dots, f(x_n)\\), then a unique polynomial \\(P(x)\\) of degree at most \\(n\\) exists with</p> \\[     P(x_k) = f(x_k),\\ \\ \\text{ for each } k = 0, 1, \\dots, n, \\] <p>and</p> \\[     P(x) = \\sum\\limits_{k = 0}^n f(x_k)L_{n, k}(x), \\] <p>where, for each \\(k = 0, 1, \\dots, n\\),</p> \\[     L_{n, k}(x) = \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n \\frac{(x - x_i)}{(x_k - x_i)}. \\] <p>\\(L_{n, k}(x)\\) is called the n th Lagrange interpolating polynomial.</p> Proof <p>First we prove for the structure of function \\(L_{n, k}(x)\\).</p> <p>From the definition of \\(P(x)\\), \\(L_{n, k}(x)\\) has the following properties.</p> <ul> <li>\\(L_{n, k}(x_i) = 0\\) when \\(i \\ne k\\).</li> <li>\\(L_{n, k}(x_k) = 1\\).</li> </ul> <p>To satisfy the first property, the numerator of \\(L_{n, k}(x)\\) contains the term</p> \\[     (x - x_0)(x - x_1) \\cdots (x - x_{k - 1})(x - x_{k + 1}) \\cdots (x - x_n) = \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n (x - x_i). \\] <p>To satisfy the second property, the denominator of \\(L_{n, k}(x)\\) must be equal to the numerator at \\(x = x_k\\), thus</p> \\[ \\begin{aligned}     L_{n, k}(x) &amp;= \\frac{(x - x_0)(x - x_1) \\cdots (x - x_{k - 1})(x - x_{k + 1}) \\cdots (x - x_n)}{(x_k - x_0)(x - x_1) \\cdots (x_k - x_{k - 1})(x_k - x_{k + 1}) \\cdots (x_k - x_n)}      \\\\ &amp;= \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n \\frac{x - x_i}{x_k - x_i}. \\end{aligned} \\] <p> </p> <p>For uniqueness, we prove by contradition. If not, suppose \\(P(x)\\) and \\(Q(x)\\) both satisfying the conditions, then \\(D(x) = P(x) - Q(x)\\) is a polynomial of degree \\(\\text{deg}(D(x)) \\le n\\), but \\(D(x)\\) has \\(n + 1\\) distinct roots \\(x_0, x_1, \\dots, x_n\\), which leads to a contradiction.</p> <p>Theorem 3.1</p> <p>If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers in the interval \\([a, b]\\) and \\(f \\in C^{n + 1}[a, b]\\). \\(\\forall x \\in [a, b], \\exists \\xi \\in [a, b], s.t.\\)</p> \\[     f(x) = P(x) + \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i), \\] <p>where \\(P(x)\\) is the Lagrange interpolating polynomial. And</p> \\[     R(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] <p>is the truncation error.</p> Proof <p>Since \\(R(x) = f(x) - P(x)\\) has at least \\(n + 1\\) roots, thus</p> \\[     R(x) = K(x)\\prod\\limits_{i = 0}^n (x - x_i). \\] <p>For a fixed \\(x \\ne x_k\\), define \\(g(t)\\) in \\([a, b]\\) by</p> \\[     g(t) = R(t) - K(x)\\prod\\limits_{i = 0}^n (t - x_i). \\] <p>Since \\(g(t)\\) has \\(n + 2\\) distinct roots \\(x, x_0, \\dots, x_n\\), by Generalized Rolle's Theorem, we have</p> \\[     \\exists\\ \\xi \\in [a, b],\\ s.t.\\ g^{(n + 1)}(\\xi) = 0. \\] <p>Namely,</p> \\[     f^{(n + 1)}(\\xi) - \\underbrace{P^{n + 1}(\\xi)}_{0} - K(x)(n + 1)! = 0. \\] <p>Thus </p> \\[     K(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!},\\ R(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] Example <p>Suppose a table is to be prepared for the function \\(f(x) = e^x\\) for \\(x\\) in \\([0, 1]\\). Assume that each entry of the table is accurate up to 8 decimal places and the step size is \\(h\\). What should \\(h\\) be for linear interpolation to give an absolute error of at most \\(10^{-6}\\)?</p> <p>Solution.</p> \\[ \\begin{aligned}     |f(x) - P(x)| &amp;= \\left|\\frac{f^{(2)}(\\xi)}{2!}(x - x_j)(x - x_{j + 1})\\right|      \\\\ &amp;= \\left|\\frac{e^\\xi}{2}(x - kh)(x - (k + 1)h)\\right| \\le \\frac{e}{2} \\cdot \\frac{h^2}{4} = \\frac{eh^2}{8}. \\end{aligned} \\] <p>Thus let \\(\\dfrac{eh^2}{8} \\le 10^{-6}\\), we have \\(h \\le 1.72 \\times 10^{-3}\\). To make \\(N = \\dfrac{(1 - 0)}{h}\\) an integer, we can simply choose \\(h = 0.001\\).</p> Extrapolation <p>Suppose \\(a = \\min\\limits_i \\{x_i\\}\\), \\(b = \\max\\limits_i \\{x_i\\}\\). Interpolation estimates value \\(P(x)\\), \\(x \\in [a, b]\\), while Extrapolation estimates value \\(P(x)\\), \\(x \\notin [a, b]\\).</p> <ul> <li>In genernal, interpolation is better than extrapolation.</li> </ul>"},{"location":"Mathematics_Basis/NA/Chap_3/#nevilles-method","title":"Neville's Method","text":"<p>Motivation: When we have more interpolating points, the original Lagrange interpolating method should re-calculate all \\(L_{n, k}\\), which is not efficient.</p> <p>Definition 3.0</p> <p>Let \\(f\\) be a function defined at \\(x_0, x_1, \\dots, x_n\\), and suppose that \\(m_1, \\dots, m_k\\) are \\(k\\) distinct integers with \\(0 \\le m_i \\le n\\) for each \\(i\\). The Lagrange polynomial that agrees with \\(f(x)\\) at the \\(k\\) points denoted by \\(P_{m_1, m_2, \\dots, m_k}(x)\\).</p> <p>Thus \\(P(x) = P_{0, 1, \\dots, n}(x)\\), where \\(P(x)\\) is the n th Lagrange polynomials that interpolate \\(f\\) at \\(k + 1\\) points \\(x_0, \\dots, x_k\\).</p> <p>Theorem 3.2</p> <p>Let \\(f\\) be a function defined at \\(x_0, x_1, \\dots, x_n\\), and \\(x_i \\ne x_j\\), then</p> \\[     P(x) = \\frac{(x - x_j)P_{0, 1, \\dots, j - 1, j + 1, \\dots k}(x) - (x - x_i)P_{0, 1, \\dots, i - 1, i + 1, \\dots k}(x)}{(x_i - x_j)}. \\] <p>Denote that</p> \\[     Q_{i, j} = P_{i - j, i - j + 1, \\dots, i - 1, i}, \\] <p>then from Theorem 3.2, the interpolating polynomials can be generated recursively. </p>"},{"location":"Mathematics_Basis/NA/Chap_3/#newton-interpolation","title":"Newton Interpolation","text":"<p>Differing from Langrange polynomials, we try to represent \\(P(x)\\) by the following form:</p> \\[ \\begin{aligned}     N(x) = &amp;\\ a_0 + a_1(x - x_0) + a_2(x - x_0)(x - x_1) + \\cdots     \\\\ &amp; + a_n(x - x_0)(x - x_1) \\cdots (x - x_{n - 1}). \\end{aligned} \\] <p>Definition 3.1</p> <p>\\(f[x_i] = f(x_i)\\) is the zeroth divided difference w.r.t. \\(x_i\\).</p> <p>The k th divided difference w.r.t. \\(x_i, x_{i + 1}, x_{i + k}\\) is defined recursively by</p> \\[ \\small     f[x_i, x_{i + 1}, \\dots, x_{i + k - 1}, x_{i + k}] = \\frac{f[x_{i + 1}, x_{i + 2}, \\dots, x_{i + k}] - f[x_i, x_{i + 1}, \\dots, x_{i + k - 1}]}{x_{i + k} - x_i}. \\] <p>Then we derive the Newton's interpolatory divided-difference formula</p> \\[     N(x) = f[x_0] + \\sum\\limits_{k = 1}^n f[x_0, x_1, \\dots, x_k]\\prod\\limits_{i = 0}^{k - 1}(x - x_i). \\] <p>And the divided difference can be generated as below, which is similar to Neville's Method.</p> <p>Theorem 3.3</p> <p>If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers in the interval \\([a, b]\\) and \\(f \\in C^{n + 1}[a, b]\\). \\(\\forall x \\in [a, b], \\exists \\xi \\in [a, b], s.t.\\)</p> \\[     f(x) = N(x) + f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i). \\] <p>where \\(N(x)\\) is the Newton's interpolatory divided-difference formula. And</p> \\[     R(x) = f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i). \\] <p>is the truncation error.</p> Proof <p>By definition of divided difference, we have</p> \\[ \\left\\{ \\begin{aligned}     f(x) &amp;= f[x_0] + (x - x_0)f[x, x_0], &amp; \\textbf{Eq.1} \\\\     f[x, x_0] &amp;= f[x_0, x_1] + (x - x_1)f[x, x_0, x_1], &amp; \\textbf{Eq.2} \\\\     &amp; \\ \\ \\vdots \\\\     f[x, x_0, \\dots, x_{n - 1}] &amp;= f[x_0, \\dots, x_n] + (x - x_n)f[x, x_0, \\dots, x_n]. &amp; \\textbf{Eq.n - 1} \\end{aligned} \\right. \\] <p>then compute</p> \\[ \\textbf{Eq.1} + (x - x_0) \\times \\textbf{Eq.2} + \\cdots + (x - x_0) \\cdots (x - x_{n - 1}) \\times \\textbf{Eq.n - 1}. \\] <p>i.e. </p> \\[     f(x) = N(x) + \\underbrace{f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i)}_{R(x)}. \\] <p>Note</p> <p>Since the uniqueness of n-th interpolating polynomial,</p> <ul> <li>\\(N(x) \\equiv P(x)\\).</li> <li> <p>They have the same truncation error, which is</p> \\[     f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] </li> </ul> <p>Theorem 3.4</p> <p>Suppose that \\(f \\in C^n[a, b]\\) and \\(x_0, x_1, \\dots, x_n\\) are distinct numbers in \\([a, b]\\). Then \\(\\exists\\ \\xi \\in (a, b)\\), s.t.</p> \\[     f[x_0, x_1, \\dots, x_n] = \\frac{f^{(n)}(\\xi)}{n!}. \\]"},{"location":"Mathematics_Basis/NA/Chap_3/#special-case-equal-spacing","title":"Special Case: Equal Spacing","text":"<p>Definition 3.2</p> <p>Forward Difference </p> \\[ \\begin{aligned}     \\Delta f_i &amp;= f_{i + 1} - f_i, \\\\     \\Delta^k f_i &amp;= \\Delta\\left(\\Delta^{k - 1} f_i \\right) = \\Delta^{k - 1}f_{i + 1} - \\Delta^{k - 1}f_i. \\end{aligned} \\] <p>Backward Difference </p> \\[ \\begin{aligned}     \\nabla f_i &amp;= f_i - f_{i - 1}, \\\\     \\nabla^k f_i &amp;= \\nabla\\left(\\nabla^{k - 1} f_i \\right) = \\nabla^{k - 1}f_i - \\nabla^{k - 1}f_{i - 1}. \\end{aligned} \\] Property of Forward / Backward Difference <ul> <li>Linearity \\(\\Delta (af(x) + bg(x)) = a \\Delta f + b \\Delta g\\).</li> <li> <p>If \\(\\text{deg}(f(x)) = m\\), then</p> \\[     \\text{deg}\\left(\\Delta^kf(x)\\right) = \\left\\{     \\begin{aligned}         &amp; m - k, &amp; 0 \\le k \\le m, \\\\         &amp; 0, &amp; k &gt; m.     \\end{aligned}     \\right. \\] </li> <li> <p>Decompose the recursive definition,</p> \\[ \\begin{aligned}     \\Delta^n f_k &amp;= \\sum\\limits_{j = 0}^n (-1)^j \\binom{n}{j} f_{n + k - j}, \\\\     \\nabla^n f_k &amp;= \\sum\\limits_{j = 0}^n (-1)^{n - j} \\binom{n}{j} f_{j + k - n}. \\end{aligned} \\] </li> <li> <p>Vice versa,</p> \\[     f_{n + k} = \\sum\\limits_{j = 0}^n \\binom{n}{j} \\Delta^j f_k. \\] </li> </ul> <p>Suppose \\(x_0, x_1, \\dots x_n\\) are equally spaced, namely \\(x_i = x_0 + ih\\). And let \\(x = x_0 + sh\\), then \\(x - x_i = (s - i)h\\). Thus</p> \\[ \\begin{aligned}     N(x) &amp;= f[x_0] + \\sum\\limits_{k = 1}^n s(s - 1) \\cdots (s - k + 1) h^k f[x_0, x_1, \\dots, x_k]     \\\\ &amp;= f[x_0] + \\sum\\limits_{k = 1}^n \\binom{s}{k} k! h^k f[x_0, x_1, \\dots, x_k] \\end{aligned} \\] <p>is called Newton forward divided-difference formula.</p> <p>From mathematical induction, we can derive that</p> \\[     f[x_0, x_1, \\dots, x_k] = \\frac{1}{k!h^k}\\Delta^k f(x_0). \\] <p>Thus we get the Newton Forward-Difference Formula</p> \\[     N(x) = f[x_0] + \\sum\\limits_{k = 1}^n \\binom{s}{k} \\Delta^k f(x_0). \\] <p>Inversely, \\(x = x_n + sh\\), then \\(x - x_i = (s + n - i)h\\), Thus</p> \\[ \\begin{aligned}     N(x) &amp;= f[x_n] + \\sum\\limits_{k = 1}^n s(s + 1) \\cdots (s + k - 1) h^k f[x_n, x_{n - 1}, \\dots, x_{n - k}]     \\\\ &amp;= f[x_n] + \\sum\\limits_{k = 1}^n (-1)^k \\binom{-s}{k} k! h^k f[x_n, x_{n - 1}, \\dots, x_{n - k}]. \\end{aligned} \\] <p>is called Newton backward divided-difference formula.</p> <p>From mathematical induction, we can derive that</p> \\[     f[x_n, x_{n - 1}, \\dots, x_0] = \\frac{1}{k!h^k}\\nabla^k f(x_n). \\] <p>Thus we get the Newton Backward-Difference Formula</p> \\[     N(x) = f[x_n] + \\sum\\limits_{k = 1}^n (-1)^k \\binom{-s}{k} \\nabla^k f(x_n). \\]"},{"location":"Mathematics_Basis/NA/Chap_3/#hermit-interpolation","title":"Hermit Interpolation","text":"<p>Definition 3.3</p> <p>Let \\(x_0, x_1, \\dots, x_n\\) be \\(n + 1\\) distinct numbers in \\([a, b]\\) and \\(m_i \\in \\mathbb{N}\\). Suppose that \\(f \\in C^m[a, b]\\), where \\(m = \\max \\{m_i\\}\\). The osculating polynomial approximating \\(f\\) is the polynomial \\(P(x)\\) of least degree such that</p> \\[     \\frac{d^k P(x_i)}{dx^k} = \\frac{d^k f(x_i)}{dx^k}, \\text{ for each } i = 0, 1, \\dots, n \\text{ and } k = 0, 1, \\dots, m_i. \\] <p>From definition above, we know that when \\(m_i = 0\\) for each \\(i\\), it's the n-th Lagrange polynomial. And the cases that \\(m_i = 1\\) for each \\(i\\), then it's Hermit Polynomials.</p> <p>Theorem 3.5</p> <p>If \\(f \\in C^1[a, b]\\) and \\(x_0, \\dots, x_n \\in [a, b]\\) are distinct, the unique polynomial of least degree agreeing with \\(f\\) and \\(f'\\) at \\(x_0, \\dots, x_n\\) is the Hermit Polynomial of degree at most \\(\\bf 2n + 1\\) defined by</p> \\[     H_{2n + 1}(x) = \\sum\\limits_{j = 0}^n f(x_j) H_{n ,j}(x) + \\sum\\limits_{j = 0}^n f'(x_j) \\hat H_{n, j}(x), \\] <p>where</p> \\[     H_{n, j}(x) = [1 - 2(x - x_j)L'_{n, j}(x_j)]L^2_{n, j}(x), \\] <p>and</p> \\[     \\hat H_{n, j}(x) = (x - x_j)L^2_{n, j}(x). \\] <p>Moreover, if \\(f \\in C^{2n + 2}[a, b]\\), then \\(\\exists\\ \\xi \\in (a, b)\\), s.t.</p> \\[     f(x) = H_{2n + 1}(x) + \\underbrace{\\frac{1}{(2n + 2)!}\\prod\\limits_{i = 0}^n (x - x_i)^2 f^{2n + 2}(\\xi)}_{R(x)}. \\] <p>The theorem above gives a complete description of Hermit interpolation. But in pratice, to compute \\(H_{2n + 1}(x)\\) through the formula above is tedious. To make it compute easier, we introduce a method that is similar to Newton's interpolation.</p> <p>Define the sequence \\(\\{z_k\\}_{0}^{2n + 1}\\) by</p> \\[     z_{2i} = z_{2i + 1} = x_i. \\] <p>Based on the Theorem 3.4, we redefine that </p> \\[     f[z_{2i}, z_{2i + 1}] = f'(z_{2i}) = f'(x_i). \\] <p>Then Hermite polynomial can be represented by</p> \\[     H_{2n + 1}(x) = f[z_0] + \\sum\\limits_{k = 1}^{2n + 1} f[z_0, \\dots, z_k]\\prod\\limits_{i = 0}^{k - 1}(x - z_i). \\]"},{"location":"Mathematics_Basis/NA/Chap_3/#cubic-spline-interpolation","title":"Cubic Spline Interpolation","text":"<p>Motivation: For osculating polynomial approximation, we can let \\(m_i\\) be bigger to get high-degree polynomials. It can somehow be better but higher degree tends to causes a fluctuation or say overfitting.</p> <p>An alternative approach is to divide the interval into subintervals and approximate them respectively, which is called piecewise-polynomial approximation. The most common piecewise-polynomial approximation uses cubic polynomials called cubic spline approximation.</p> <p>Definition 3.4</p> <p>Given a function \\(f\\) defined on \\([a, b]\\) and a set of nodes \\(a = x_0 &lt; x_1 &lt; \\cdots &lt; x_n = b\\), a cubic spline interpolant \\(S\\) for \\(f\\) is a function that satisfies the following conditions.</p> <ul> <li>\\(S(x)\\) is a cubic polynomial, denoted \\(S_j(x)\\), on the subinterval \\([x_j, x_j + 1]\\), for each \\(j = 0, 1, \\dots, n - 1\\);</li> <li>\\(S(x_j) = f(x_j)\\) for each \\(j = 0, 1, \\dots, n\\);</li> <li>\\(S_{j + 1}(x_{j + 1}) = S_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\);</li> <li>\\(S'_{j + 1}(x_{j + 1}) = S'_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\);</li> <li>\\(S''_{j + 1}(x_{j + 1}) = S''_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\);</li> <li>One of the following sets of boundary conditions:<ul> <li>\\(S''(x_0) = S''(x_n) = 0\\) (free or natural boundary);</li> <li>\\(S'(x_0) = f'(x_0)\\) and \\(S'(x_n) = f'(x_n)\\) (clamped boundary).</li> </ul> </li> </ul> <p>The spline of natural boundary is called natural spline. </p> <p>Theorem 3.6</p> <p>The cubic spline interpolation of either natural boundary or clamped boundary is unique.</p> <p>(Since the coefficient matrix \\(A\\) is strictly diagonally dominant.)</p> <p>Suppose interpolation function in each subinterval is</p> \\[     S_j(x) = a_j + b_j(x - x_j) + c_j(x - x_j)^2 + d_j(x - x_j)^3. \\] <p>From the conditions in the definition above, by some algebraic process, we can derive the solution with the following equations,</p> \\[ \\begin{aligned}     h_j &amp;= x_{j + 1} - x_j; \\\\     a_j &amp;= f(x_j); \\\\     b_j &amp;= \\frac{1}{h_j}(a_{j + 1}) - \\frac{h_j}{3}(2c_j + c_{j + 1}); \\\\     d_j &amp;= \\frac{1}{3h_j}{c_{j + 1} - c_j}. \\end{aligned} \\] <p>While \\(c_j\\) is given by solving the following linear system,</p> \\[     A\\mathbf{x} = \\mathbf{b}, \\] <p>where</p> \\[ \\small A =  \\begin{bmatrix}     1 &amp; 0 &amp; 0 &amp; \\cdots &amp; \\cdots &amp; 0 \\\\     h_0 &amp; 2(h_0 + h_1) &amp; h_1 &amp; \\cdots &amp; \\cdots &amp; \\vdots \\\\     0 &amp; h_1 &amp; 2(h_1 + h_2) &amp; h_2 &amp; \\ddots &amp; \\vdots \\\\     \\vdots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; 0 \\\\     \\vdots &amp; \\ddots &amp; \\ddots &amp; h_{n - 2} &amp; 2(h_{n - 2} + h_{n - 1}) &amp; h_{n - 1} \\\\     0 &amp; \\cdots &amp; \\cdots &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} , \\mathbf{x} =  \\begin{bmatrix}     c_0 \\\\ c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\] <p>For natural boundary,</p> \\[ \\small \\mathbf{b} =  \\begin{bmatrix}     0 \\\\     \\frac{3}{h_1}(a_2 - a_1) - \\frac{3}{h_0}(a_1 - a_0) \\\\     \\vdots \\\\     \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) - \\frac{3}{h_{n - 2}}\\ (a_{n - 1} - a_{n - 2}) \\\\     0 \\end{bmatrix}. \\] <p>For clamped boundary,</p> \\[ \\small \\mathbf{b} =  \\begin{bmatrix}     \\frac{3}{h_0}(a_1 - a_0) - 3f'(a) \\\\     \\frac{3}{h_1}(a_2 - a_1) - \\frac{3}{h_0}(a_1 - a_0) \\\\     \\vdots \\\\     \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) - \\frac{3}{h_{n - 2}}\\ (a_{n - 1} - a_{n - 2}) \\\\     3f'(b) - \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) \\\\ \\end{bmatrix}. \\] <p>For More Accuracy...</p> <p>If \\(f \\in C[a, b]\\) and \\(\\frac{\\max h_i}{\\min h_i} \\le C &lt; \\infty\\). Then \\(S(x)\\ \\overset{\\text{uniform}}{\\longrightarrow}\\ f(x)\\) when \\(\\max h_i \\rightarrow 0\\).</p> <p>That is the accuracy of approximation can be improved by adding more nodes without increasing the degree of the splines.</p>"},{"location":"Mathematics_Basis/NA/Chap_3/#curves","title":"Curves","text":"<p>We've discussed the interpolation of functions above, but we may encounter the case to interpolate a curve.</p>"},{"location":"Mathematics_Basis/NA/Chap_3/#straightforward-technique","title":"Straightforward Technique","text":"<p>For given points \\((x_0, y_0), (x_1, y_1), \\dots, (x_n, y_n)\\), we can construct two approximation functions with</p> \\[     x_i = x(t_i),\\ y_i = y(t_i). \\] <p>The interpolation method can be Lagrange, Hermite and Cubic spline, whatever.</p>"},{"location":"Mathematics_Basis/NA/Chap_3/#bezier-curve","title":"Bezier Curve","text":"<p>In nature, it's piecewise cubic Hermite polynomial, and the curve is called Bezier curve.</p> <p>Similarly, suppose two function \\(x(t)\\) and \\(y(t)\\) at each interval. We have the following condtions.</p> \\[     x(0) = x_0,\\ x(1) = x_1,\\ x'(0) = \\alpha_0,\\ x'(1) = \\alpha_1. \\] \\[     y(0) = y_0,\\ y(1) = y_1,\\ y'(0) = \\beta_0,\\ y'(1) = \\beta_1. \\] <p>The solution is</p> \\[ \\begin{aligned}     x(t) &amp;= [2(x_0 - x_1) + (\\alpha_0 + \\alpha_1)]t^3 + [3(x_1 - x_0) - (\\alpha_1 + 2\\alpha_0)]t^2 + \\alpha_0 t + x_0. \\\\     y(t) &amp;= [2(y_0 - y_1) + (\\beta_0 + \\beta_1)]t^3 + [3(y_1 - y_0) - (\\beta_1 + 2\\beta_0)]t^2 + \\alpha_0 t + y_0. \\end{aligned} \\]"},{"location":"Mathematics_Basis/NA/Chap_4/","title":"Chapter 4 | Numerical Differentiation and Integration","text":"<p>Before everything start, it's necessary to introduce an approach to reduce truncation error: Richardson's extrapolation.</p>"},{"location":"Mathematics_Basis/NA/Chap_4/#richardsons-extrapolation","title":"Richardson's Extrapolation \u5916\u63a8","text":"<p>Suppose for each \\(h\\), we have a formula \\(N(h)\\) to approximate an unknown value \\(M\\). And suppose the truncation error have the form</p> \\[     M - N(h) = K_1 h + K_2 h^2 + K_3 h^3 + \\cdots, \\] <p>for some unknown constants \\(K_1\\), \\(K_2\\), \\(K_3\\), \\(\\dots\\). This has \\(O(h)\\) approximation.</p> <p>First, we try to make some transformation to reduce the \\(K_1 h\\) term.</p> \\[ \\begin{aligned}     M &amp;= N(h) + K_1 h + K_2 h^2 + K_3 h^3 + \\cdots, \\\\     M &amp;= N\\left(\\frac{h}{2}\\right) + K_1 \\frac{h}{2} + K_2 \\frac{h^2}{4} + K_3 \\frac{h^3}{8} + \\cdots, \\end{aligned} \\] <p>Eliminate \\(h\\), and we have</p> \\[     \\small     M = \\left[N\\left(\\frac{h}{2}\\right) + \\left(N\\left(\\frac{h}{2}\\right) - N(h)\\right)\\right] + K_2\\left(\\frac{h^2}{2} - h^2\\right) + K_3 \\left(\\frac{h^3}{4} - h^3\\right) + \\cdots \\] <p>Define \\(N_1(h) \\equiv N(h)\\) and</p> \\[     N_2(h) = N_1 \\left(\\frac{h}{2}\\right) + \\left(N_1\\left(\\frac{h}{2}\\right) - N_1(h)\\right). \\] <p>Then we have \\(O(h^2)\\) approximation formula for \\(M\\):</p> \\[     M = N_2(h) - \\frac{K_2}{2}h^2 - \\frac{3K_3}{4}h^3 - \\cdots. \\] <p>Repeat this process by eliminating \\(h^2\\), and then we have</p> \\[     M = N_3(h) + \\frac{K_3}{8}h^3 + \\cdots, \\] <p>where</p> \\[     N_3(h) = N_2 \\left(\\frac{h}{2}\\right) + \\frac{1}{3}\\left(N_2\\left(\\frac{h}{2}\\right) - N_2(h)\\right). \\] <p>We can repeat this process recursively, and finally we get the following conclusion.</p> <p>Richardson's Extrapolation</p> <p>If \\(M\\) is in the form</p> \\[     M = N(h) + \\sum\\limits_{j = 1}^{m - 1}K_jh^j + O(h^m), \\] <p>then for each \\(j = 2, 3, \\dots, m\\), we have an \\(O(h^j)\\) approximation of the form</p> \\[     N_j(h) = N_{j - 1}\\left(\\frac{h}{2}\\right) + \\frac{1}{2^{j - 1} - 1}\\left(N_{j - 1}\\left(\\frac{h}{2}\\right) - N_{j - 1}(h)\\right). \\] <p>Also it can be used if the truncation error has the form</p> \\[     \\sum\\limits_{j = 1}^{m - 1}K_jh^{\\alpha_j} + O(h^{\\alpha_m}). \\] <p> </p>"},{"location":"Mathematics_Basis/NA/Chap_4/#numerical-differentiation","title":"Numerical Differentiation","text":""},{"location":"Mathematics_Basis/NA/Chap_4/#first-order-differentiation","title":"First Order Differentiation","text":"<p>Suppose \\(\\{x_0, x_1, \\dots, x_n\\}\\) are distinct in some interval \\(I\\) and \\(f \\in C^{n + 1}(I)\\), then we have the Lagrange interpolating polynomials,</p> \\[     f(x) = \\sum\\limits_{k = 0}^{n}f(x_k)L_k(x) + \\frac{f^{(n + 1)}(\\xi(x))}{(n + 1)!}\\prod\\limits_{i = 0}^{n}(x - x_i). \\] <p>Differentiate \\(f(x)\\) and substitute \\(x_j\\) to it,</p> \\[     f'(x_j) = \\sum\\limits_{k = 0}^{n}f(x_k)L_k'(x) + \\frac{f^{(n + 1)}(\\xi(x_j))}{(n + 1)!} \\prod\\limits_{\\substack{k = 0 \\\\ k \\ne j}}^n(x_j - x_k), \\] <p>which is called an (n + 1)-point formula to approximate \\(f'(x_j)\\).</p> <p>For convenience, we only discuss the equally spaced situation. Suppose the interval is \\([a, b]\\) divided to \\(n\\) parts, and denote </p> \\[ \\begin{aligned}     h = \\frac{b - a}{n}, \\\\     x_i = a + ih. \\end{aligned} \\] <p>When \\(n = 1\\), we simply get the two-point formula,</p> \\[     f'(x_0) = \\frac{f(x_0 + h) - f(x_0)}{h} - \\frac{h}{2}f''(\\xi), \\] <p>which is known as forward-difference formula. Inversely, by replacing \\(h\\) with \\(-h\\),</p> \\[     f'(x_0) = \\frac{f(x_0) - f(x_0 - h)}{h} + \\frac{h}{2}f''(\\xi), \\] <p>is known as backward-difference formula.</p> <p>When \\(n = 3\\), we get the three-point formulae. Due to symmetry, there are only two.</p> \\[ \\begin{aligned}     f'(x_0) &amp;= \\frac{1}{2h}[-3(f(x_0)) + 4f(x_0 + h) - f(x_0 + 2h)] + \\frac{h^2}{3}f^{(3)}(\\xi), \\\\     &amp; \\text{where some } \\xi \\in [x_0, x_0 + 2h], \\\\     f'(x_0) &amp;= \\frac{1}{2h}[f(x_0 + h) + f(x_0 - h)] - \\frac{h^2}{6}f^{(3)}(\\xi), \\\\     &amp; \\text{where some } \\xi \\in [x_0 - h, x_0 + h]. \\end{aligned} \\] <p>When \\(n = 5\\), we get the five-point formulae. The following are the useful two of them.</p> \\[ \\begin{aligned}     f'(x_0) &amp;= \\frac{1}{12h}[-25f(x_0) + 48f(x_0 + h) - 36f(x_0 + 2h) \\\\             &amp; \\ \\ \\ \\ + 16f(x_0 + 3h) - 3f(x_0 + 4h)] + \\frac{h^4}{5}f^{(5)}(\\xi), \\\\     &amp; \\text{where some } \\xi \\in [x_0, x_0 + 4h], \\\\     f'(x_0) &amp;= \\frac{1}{12h}[f(x_0 - 2h) - 8f(x_0 - h)] + 8f(x_0 + h) - f(x_0 + 2h)] \\\\             &amp; \\ \\ \\ \\ + \\frac{h^4}{30}f^{(5)}(\\xi), \\\\     &amp; \\text{where some } \\xi \\in [x_0 - 2h, x_0 + 2h]. \\end{aligned} \\] <p>Differentiation with Richardson's Extrapolation</p> <p>Consider the three-point formula,</p> \\[     f'(x_0) = \\frac{1}{2h}[f(x_0 + h) + f(x_0 - h)] - \\frac{h^2}{6}f^{(3)}(\\xi) - \\frac{h^4}{120}f^{(5)}(\\xi) - \\cdots. \\] <p>In this case, considering Richardson's extrapolation, we have</p> \\[     N_1(h) = \\frac{1}{2h}[f(x_0 + h) + f(x_0 - h)]. \\] <p>Eliminate \\(h^2\\), we have</p> \\[     f'(x_0) = N_2(h) + \\frac{h^4}{480}f^{(5)}(x_0) + \\cdots, \\] <p>where</p> \\[     N_2(h) = N_1 \\left(\\frac{h}{2}\\right) + \\frac{1}{3}\\left(N_1\\left(\\frac{h}{2}\\right) - N_1(h)\\right). \\] <p>Continuing this procedure, we have,</p> \\[     N_j(h) = N_{j - 1}\\left(\\frac{h}{2}\\right) + \\frac{1}{4^{j - 1} - 1}\\left(N_{j - 1}\\left(\\frac{h}{2}\\right) - N_{j - 1}(h)\\right). \\] Example <p>Suppose \\(x_0 = 2.0\\), \\(h = 0.2\\), \\(f(x) = xe^x\\), the exact value of \\(f'(x_0)\\) is 22.167168. While the extrapolation process is shown below.</p> <p> </p>"},{"location":"Mathematics_Basis/NA/Chap_4/#higher-differentiation","title":"Higher Differentiation","text":"<p>Take second order differentiation as an example. From Taylor polynomial about point \\(x_0\\), we have</p> \\[ \\begin{aligned}     f(x_0 + h) &amp;= f(x_0) + f'(x_0)h + \\frac{1}{2}f''(x_0)h^2 + \\frac{1}{6}f'''(x_0)h^3 + \\frac{1}{24}f^{(4)}(\\xi_1)h^4, \\\\     f(x_0 - h) &amp;= f(x_0) - f'(x_0)h + \\frac{1}{2}f''(x_0)h^2 - \\frac{1}{6}f'''(x_0)h^3 + \\frac{1}{24}f^{(4)}(\\xi_{-1})h^4, \\\\ \\end{aligned} \\] <p>where \\(x_0 - h &lt; \\xi_{-1} &lt; x_0 &lt; \\xi_1 &lt; x_0 + h\\).</p> <p>Add these equations and take some transformations, we get</p> \\[     f''(x_0) = \\frac{1}{h^2}[f(x_0 - h) - 2f(x_0) + f(x_0 + h)] - \\frac{h^2}{12}f^{(4)}(\\xi), \\] <p>where \\(x_0 - h &lt; \\xi &lt; x_0 + h\\), and \\(f^{(4)}(\\xi) = \\dfrac{1}{2}(f^{(4)}(\\xi_1) + f^{(4)}(\\xi_{-1}))\\).</p>"},{"location":"Mathematics_Basis/NA/Chap_4/#error-analysis","title":"Error Analysis","text":"<p>Now we examine the formula below,</p> \\[     f'(x_0) = \\frac{1}{2h}[f(x_0 + h) + f(x_0 - h)] - \\frac{h^2}{6}f^{(3)}(\\xi). \\] <p>Suppose that in evaluation of \\(f(x_0 + h)\\) and \\(f(x_0 - h)\\), we encounter roundoff errors \\(e(x_0 + h)\\) and \\(e(x_0 - h)\\). Then our computed values \\(\\tilde f(x_0 + h)\\) and \\(\\tilde f(x_0 - h)\\) satisfy the following formulae,</p> \\[ \\begin{aligned}     f(x_0 + h) &amp;= \\tilde f(x_0 + h) + e(x_0 + h), \\\\      f(x_0 - h) &amp;= \\tilde f(x_0 - h) + e(x_0 - h). \\end{aligned} \\] <p>Thus the total error is </p> \\[     R = f'(x_0) - \\frac{\\tilde f(x_0 + h) - \\tilde f(x_0 - h)}{2h} = \\frac{e(x_0 + h) - e(x_0 - h)}{2h} - \\frac{h^2}{6}f^{(3)}(\\xi). \\] <p>Moreover, suppose the roundoff error \\(e\\) are bounded by some number \\(\\varepsilon &gt; 0\\) and \\(f^{(3)}(x)\\) is bounded by some number \\(M &gt; 0\\), then</p> \\[     |R| \\le \\frac{\\varepsilon}{h} + \\frac{h^2}{6}M. \\] <p>Thus theoretically the best choice of \\(h\\) is \\(\\sqrt[3]{3\\varepsilon / M}\\). But in reality we cannot compute such an \\(h\\) since we know nothing about \\(f^{(3)}(x)\\).</p> <p>In all, we should aware that the step size \\(h\\) cannot be too large or too small.</p>"},{"location":"Mathematics_Basis/NA/Chap_4/#numerical-integration","title":"Numerical Integration","text":""},{"location":"Mathematics_Basis/NA/Chap_4/#numerical-quadrature","title":"Numerical Quadrature","text":"<p>Similarly to the differentiation case, suppose \\(\\{x_0, x_1, \\dots, x_n\\}\\) are distinct in some interval \\(I\\) and \\(f \\in C^{n + 1}(I)\\), then we have the Lagrange interpolating polynomials,</p> \\[     f(x) = \\sum\\limits_{k = 0}^{n}f(x_k)L_k(x) + \\frac{f^{(n + 1)}(\\xi(x))}{(n + 1)!}\\prod\\limits_{i = 0}^{n}(x - x_i). \\] <p>Integrate \\(f(x)\\) and we get</p> \\[ \\begin{aligned}     \\int_a^b f(x) dx &amp;= \\int_a^b \\sum\\limits_{k = 0}^{n}f(x_k)L_k'(x) dx + \\int_a^b \\frac{f^{(n + 1)}(\\xi(x))}{(n + 1)!} \\prod\\limits_{k = 0}^n (x - x_k)dx,      \\\\ &amp;= \\sum\\limits_{i = 0}^n A_i f(x_i) + \\underbrace{\\frac{1}{(n + 1)!} \\int_a^b \\prod_{i = 0}^n(x - x_i)f^{(n + 1)}(\\xi(x))dx}_{E(f)}, \\end{aligned} \\] <p>where</p> \\[     A_i = \\int_a^b L_i(x)dx. \\] <p>Definition 4.0</p> <p>The degree of accuracy, or say precision, of a quadrature formula is the largest positive integer \\(n\\) such that the formula is exact for \\(x^k\\), for each \\(k = 0, 1, \\dots, n\\).</p> <p>Similarly, we suppose equally spaced situation here again.</p> <p>Theorem 4.0 | (n + 1)-point closed Newton-Cotes formulae</p> <p>Suppose \\(x_0 = a\\), \\(x_n = b\\), and \\(h = (b - a) / n\\), then \\(\\exists\\ \\xi \\in (a, b)\\), s.t.</p> \\[     \\int_a^b f(x)dx = \\sum\\limits_{i = 0}^n A_i f(x_i) + \\frac{h^{n + 3}f^{(n + 2)}(\\xi)}{(n + 2)!}\\int_0^n t^2(t - 1)\\cdots(t - n)dt, \\] <p>if \\(n\\) is even and \\(f \\in C^{n + 2}[a, b]\\), and</p> \\[     \\int_a^b f(x)dx = \\sum\\limits_{i = 0}^n A_i f(x_i) + \\frac{h^{n + 2}f^{(n + 1)}(\\xi)}{(n + 1)!}\\int_0^n t(t - 1)\\cdots(t - n)dt, \\] <p>if \\(n\\) is odd and \\(f \\in C^{n + 1}[a, b]\\),</p> <p>where</p> \\[     A_i = \\int_a^b L_i(x) dx = \\int_a^b \\prod\\limits_{\\substack{j = 0 \\\\ j \\ne i}}^n \\frac{(x - x_j)}{(x_i - x_j)} dx. \\] <p> (n + 1)-point closed Newton-Cotes formulae </p> <p>\\(n = 1\\) Trapezoidal rule</p> \\[     \\int_{x_0}^{x_1}f(x)dx = \\frac{h}{2}[f(x_0) + f(x_1)] - \\frac{h^3}{12}f''(\\xi)     ,\\ \\ \\text{ where some } x_0 &lt; \\xi &lt; x_1. \\] <p>\\(n = 2\\) Simpson's rule</p> \\[     \\int_{x_0}^{x_2}f(x)dx = \\frac{h}{3}[f(x_0) + 4f(x_1) + f(x_2)] - \\frac{h^5}{90}f^{(4)}(\\xi)     ,\\ \\ \\text{ where some } x_0 &lt; \\xi &lt; x_2. \\] <p>\\(n = 3\\) Simpson's Three-Eighths rule</p> \\[     \\int_{x_0}^{x_3}f(x)dx = \\frac{3h}{8}[f(x_0) + 3f(x_1) + 3f(x_2) + f(x_3)] - \\frac{3h^5}{80}f^{(4)}(\\xi), \\\\     \\text{where some } x_0 &lt; \\xi &lt; x_3. \\] Trapezoidal rule Simpson's rule <p>Theorem 4.1 | (n + 1)-point open Newton-Cotes formulae</p> <p>Suppose \\(x_{-1} = a\\), \\(x_{n + 1} = b\\), and \\(h = (b - a) / (n + 2)\\), then \\(\\exists\\ \\xi \\in (a, b)\\), s.t.</p> \\[     \\int_a^b f(x)dx = \\sum\\limits_{i = 0}^n A_i f(x_i) + \\frac{h^{n + 3}f^{(n + 2)}(\\xi)}{(n + 2)!}\\int_{-1}^{n + 1} t^2(t - 1)\\cdots(t - n)dt, \\] <p>if \\(n\\) is even and \\(f \\in C^{n + 2}[a, b]\\), and</p> \\[     \\int_a^b f(x)dx = \\sum\\limits_{i = 0}^n A_i f(x_i) + \\frac{h^{n + 2}f^{(n + 1)}(\\xi)}{(n + 1)!}\\int_{-1}^{n + 1} t(t - 1)\\cdots(t - n)dt, \\] <p>if \\(n\\) is odd and \\(f \\in C^{n + 1}[a, b]\\),</p> <p>where</p> \\[     A_i = \\int_a^b L_i(x) dx = \\int_a^b \\prod\\limits_{\\substack{j = 0 \\\\ j \\ne i}}^n \\frac{(x - x_j)}{(x_i - x_j)} dx. \\] <p> (n + 1)-point open Newton-Cotes formulae </p> <p>\\(n = 0\\) Midpoint rule</p> \\[     \\int_{x_{-1}}^{x_1}f(x)dx = 2hf(x_0) + \\frac{h^2}{3}f''(\\xi)     \\text{where some } x_{-1} &lt; \\xi &lt; x_1. \\]"},{"location":"Mathematics_Basis/NA/Chap_4/#composite-numerical-integration","title":"Composite Numerical Integration","text":"<p>Motivation: Although the Newton-Cotes gives a better and better approximation as \\(n\\) increases, since it's based on interpolating polynomials, it also owns the oscillatory nature of high-degree polynomials.</p> <p>Similarly, we discuss a piecewise approach to numerical integration with low-order Newton-Cotes formulae. These are the techniques most often applied.</p> <p>Theorem 4.2 | Composite Trapezoidal rule</p> <p>\\(f \\in C^2[a, b]\\), \\(h = (b - a) /n\\), and \\(x_j = a + jh\\), \\(j = 0, 1, \\dots, n\\). Then \\(\\exists\\ \\mu \\in (a, b)\\), s.t.</p> \\[     \\int_a^b f(x)dx = \\frac{h}{2}\\left[f(a) + 2 \\sum\\limits_{j = 1}^{n - 1}f(x_j) + f(b)\\right] - \\frac{b - a}{12}h^2 f''(\\mu). \\] <p> </p> <p>Theorem 4.3 | Composite Simpson's rule</p> <p>\\(f \\in C^2[a, b]\\), \\(h = (b - a) /n\\), and \\(x_j = a + jh\\), \\(j = 0, 1, \\dots, n\\). Then \\(\\exists\\ \\mu \\in (a, b)\\), s.t.</p> \\[     \\small     \\int_a^b f(x)dx = \\frac{h}{3}\\left[f(a) + 2 \\sum\\limits_{j = 1}^{(n / 2) - 1}f(x_{2j}) + 4 \\sum\\limits_{j = 1}^{n / 2}f(x_{2j - 1}) + f(b)\\right] - \\frac{b - a}{180}h^4 f^{(4)}(\\mu). \\] <p> </p> <p>Theorem 4.4 | Composite Midpoint rule</p> <p>\\(f \\in C^2[a, b]\\), \\(h = (b - a) / (n + 2)\\), and \\(x_j = a + (j + 1)h\\), \\(j = -1, 0, \\dots, n + 1\\). Then \\(\\exists\\ \\mu \\in (a, b)\\), s.t.</p> \\[     \\int_a^b f(x)dx = 2h \\sum\\limits_{j = 0}^{n / 2}f(x_{2j}) + \\frac{b - a}{6} h^2 f''(\\mu). \\] <p> </p> <p>Stability</p> <p>Composite integration techniques are all stable w.r.t roundoff error.</p> Example: Composite Simpson's rule <p>Suppose \\(f(x_i)\\) is apporximated by \\(\\tilde f(x_i)\\) with</p> \\[     f(x_i) = \\tilde f(x_i) + e_i. \\] <p>Then the accumulated error is</p> \\[     e(h) = \\left|\\frac{h}{3}\\left[e_0 + 2\\sum\\limits_{j = 1}^{(n / 2) - 1}e_{2j} + 4\\sum\\limits_{j = 1}^{n / 2}e_{2j-1} + e_n\\right]\\right|. \\] <p>Suppose \\(e_i\\) are uniformly bounded by \\(\\varepsilon\\), then</p> \\[     e(h) \\le \\frac{h}{3}\\left[\\varepsilon + 2 \\left(\\frac{n}{2} - 1\\right) + 4 \\frac{n}{2} \\varepsilon + \\varepsilon \\right] = nh\\varepsilon = (b - a)\\varepsilon. \\] <p>That means even though divide an interval to more parts, the roundoff error will not increase, which is quite stable.</p>"},{"location":"Mathematics_Basis/NA/Chap_4/#romberg-integration","title":"Romberg Integration","text":"<p>Romberg integration combine the Composite Trapezoidal rule and Richardson's extrapolation to derive a more useful approximation.</p> <p>Suppose we divide the interval \\([a, b]\\) into \\(m_1 = 1\\), \\(m_2 = 2\\), \\(\\dots\\), and \\(m_n = 2^{n - 1}\\) subintervals respectively. For each division, then the step size \\(h_k\\) is \\((b - a) / m_k = (b - a) / 2^{k - 1}\\).</p> <p>Then we use \\(R_{k, 1}\\) to denote the composite trapezoidal rule,</p> \\[ \\small     R_{k, 1} = \\int_a^b f(x) dx = \\frac{h_k}{2} \\left[f(a) + f(b) + 2 \\left(\\sum\\limits_{i = 1}^{2^{k - 1} - 1} f(a + ih_k)\\right)\\right] - \\frac{(b - a)}{12}h^2_kf''(\\mu_k). \\] <p>Mathematically, we have the recursive formula,</p> \\[     R_{k, 1} = \\frac{1}{2}\\left[R_{k - 1, 1} + h_{k - 1}\\sum\\limits_{i = 1}^{2^{k - 2}}f(a+(2i-1)h_k)\\right]. \\] <p>Theorem 4.5</p> <p>the Composite Trapezoidal rule can represented by an alternative error term in the form</p> \\[     \\int_a^b f(x) dx - R_{k, 1} = \\sum\\limits_{i = 1}^{\\infty} K_i h^{2i}_k, \\] <p>where \\(K_i\\) depends only on \\(f^{(2i-1)}(a)\\) and \\(f^{(2i-1)}(b)\\).</p> <p>This nice theorem makes Richardson's extrapolation available to reduce the truncation error! Similar to Differentiation with Richardson's Extrapolation, we have the following formula.</p> <p>Romberg Integration</p> \\[     R_{k, j} = R_{k, j - 1} + \\frac{R_{k, j - 1} - R_{k - 1, j - 1}}{4^{j - 1} - 1}, \\] <p>with an \\(O(h^{2j}_k)\\) approximation.</p> <p> </p>"},{"location":"Mathematics_Basis/NA/Chap_4/#adaptive-quadrature-methods","title":"Adaptive Quadrature Methods","text":"<p>Motivation: On the premise of equal spacing, in some cases, the left half of the interval is well approximated, and maybe we only need to subdivide the right half to approximate better. Here we introduce the Adaptive quadrature methods based on the Composite Simpson's rule.</p> <p>First, we want to derive, if we apply Simpson's rule in two subinterval and add them up, how much precision does it improve compared to only applying Simpson's rule just in the whole interval.</p> <p>From Simpson's rule, we have</p> \\[     \\int_a^b f(x) dx = S(a, b) - \\frac{h^5}{90}f^{(4)}(\\mu), \\] <p>where</p> \\[     S(a, b) = \\frac{h}{3}[f(a) + 4f(a + h) + f(b)]. \\] <p>If we divide \\([a, b]\\) into two subintervals, applying Simpson's rule respectively (namely apply Composite Simpson's rule with \\(n = 4\\) and step size \\(h / 2\\)), we have</p> \\[     \\int_a^b f(x) dx = S\\left(a, \\frac{a + b}{2}\\right) + S\\left(\\frac{a + b}{2}, b\\right) - \\frac{1}{16}\\left(\\frac{h^5}{90}\\right)f^{(4)}(\\tilde \\mu). \\] <p>Moreover, assume \\(f^{(4)}(\\mu) \\approx f^{(4)}(\\tilde \\mu)\\), then we have</p> \\[     S\\left(a, \\frac{a + b}{2}\\right) + S\\left(\\frac{a + b}{2}, b\\right) - \\frac{1}{16}\\left(\\frac{h^5}{90}\\right)f^{(4)}(\\tilde \\mu) \\approx S(a, b) - \\frac{h^5}{90}f^{(4)}(\\mu), \\] <p>so</p> \\[     \\frac{h^5}{90}f^{(4)}(\\mu) \\approx \\frac{16}{15}\\left[S(a, b) - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right]. \\] <p>Then,</p> \\[ \\begin{aligned}     \\left|\\int_a^b f(x) dx - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right|     = \\left|\\frac{1}{16}\\left(\\frac{h^5}{90}\\right)f^{(4)}(\\tilde \\mu)\\right|     \\\\ \\approx \\frac{1}{15} \\left|S(a, b) - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right|. \\end{aligned} \\] <p>This result means that the subdivision approximates \\(\\int_a^b f(x) dx\\) about 15 times better than it agree with \\(S(a, b)\\). Thus suppose we have a tolerance \\(\\varepsilon\\) across the interval \\([a, b]\\). If</p> \\[     \\left|S(a, b) - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right| &lt; 15\\varepsilon, \\] <p>then we expect to have</p> \\[     \\left|\\int_a^bf(x)dx - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right| &lt; \\varepsilon, \\] <p>and the subdivision is thought to be a better approximation to \\(\\int_a^bf(x)dx\\).</p> <p>Conclusion</p> <p>Suppose we have a tolerance \\(\\varepsilon\\) on \\([a, b]\\), and we expect the tolerance is uniform. Thus at the subinterval \\([p, q] \\subseteq [a, b]\\), with \\(q - p = k(b - a)\\), we expect the tolerance as \\(k\\varepsilon\\).</p> <p>Moreover, suppose the approximation of Simpson's rule on \\([p, q]\\) is \\(S\\) while the approxiamtion of Simpson's rule on \\([p, (p + q) / 2]\\) and \\([(p + q) / 2, q]\\) are \\(S_1\\) and \\(S_2\\) respectively. </p> <p>Then the criterion to subdivide is that</p> \\[     |S_1 + S_2 - S| &lt; M \\cdot k \\varepsilon. \\] <p>where \\(M\\) is often taken as 10 but not 15, which we derive above, since it also consider the error between \\(f^{(4)}(\\mu)\\) and \\(f^{(4)}(\\tilde \\mu)\\).</p>"},{"location":"Mathematics_Basis/NA/Chap_4/#gaussian-quadrature","title":"Gaussian Quadrature","text":"<p>Instead of equal spacing in the Newton-Cotes formulae, the selection of the points \\(x_i\\) also become variables. Gaussian Quadrature is aimed to construct a formula \\(\\sum\\limits_{k = 0}^n A_kf(x_k)\\) to approximate \\(\\int_a^b w(x)f(x)dx\\) of precision degree \\(2n + 1\\) with \\(n + 1\\) points, where \\(w(x)\\) is a weight function. (Compare to the equally spaced strategy only have a precision of around \\(n\\)).</p> <p>That means, to determine \\(x_i\\) and \\(A_i\\) (totally \\(2n + 2\\) unknowns) such that the formula is accurate for \\(f(x) = 1, x, \\dots, x^{2n + 1}\\) (totally \\(2n + 2\\) equations). The selected points \\(x_i\\) are called Gaussian points.</p> <p>Problem</p> <p>Theoretically, since we have \\(2n + 2\\) unknowns and \\(2n + 2\\) eqautions, we can solve out \\(x_i\\) and \\(A_i\\). But the equations are not linear!</p> <p>Thus we give the following theorem to find Gaussian points without solving the nonlinear eqautions. Recap the definition of weight function and orthogonality of polynomials, and the construction of the set of orthogonal polynomials.</p> <p>Theorem 4.6</p> <p>\\(x_0, \\dots, x_n\\) are Gaussian point iff \\(W(x) = \\prod\\limits_{k = 0}^n (x - x_k)\\) is orthogonal to all the polynomials of degree no greater than \\(n\\) on interval \\([a,b]\\) w.r.t the weight function \\(w(x)\\).</p> Proof <p>\\(\\Rightarrow\\) </p> <p>if \\(x_0, \\dots x_n\\) are Gaussian points, then the degree of precision of the formula \\(\\int_a^b w(x)f(x)dx \\approx \\sum\\limits_{k = 0}^n A_k f(x_k)\\) is at least \\(2n + 1\\).</p> <p>Then \\(\\forall\\ P(x) \\in \\Pi_n\\),  \\(\\text{deg}(P(x)W(x)) \\le 2n + 1\\). Thus</p> \\[     \\int_a^b w(x)P(x)W(x)dx = \\sum\\limits_{k = 0}^n A_k P(x_k)\\underbrace{W(x_k)}_{0} = 0. \\] <p>\\(\\Leftarrow\\)</p> <p>\\(\\forall\\ P(x) \\in \\Pi_{2n + 1}\\), let \\(P(x) = W(x)q(x) + r(x),\\ \\ q(x), r(x) \\in \\Pi_{n}\\), then</p> \\[ \\begin{aligned}     \\int_a^b w(x)P(x) dx &amp;= \\int_a^b w(x)W(x)q(x)dx + \\int_a^b w(x)r(x)dx \\\\     &amp;= \\sum\\limits_{k = 0}^n A_k r(x_k) = \\sum\\limits_{k = 0}^n A_k P(x_k). \\end{aligned} \\] <p>Recap that the set of orthogonal polynomials \\(\\{\\varphi_0, \\dots, \\varphi_n, \\dots\\}\\) is linearly independent and \\(\\varphi_{n + 1}\\) is orthogonal to any polynomials of degree no greater than \\(n\\).</p> <p>Thus we can take \\(\\varphi_{n + 1}(x)\\) to be \\(W(x)\\) and the roots of \\(\\varphi_{n + 1}(x)\\) are the Gaussian points.</p> <p>Genernal Solution</p> <p>Problem: Assume</p> \\[     \\int_a^b w(x)f(x)dx \\approx \\sum\\limits_{i = 0}^n A_k f(x_k). \\] <p>Step.1 Construct the set of orthogonal polynomial on the interval \\([a, b]\\) by Gram-Schimidt Process from \\(\\varphi_0(x) \\equiv 1\\) to \\(\\varphi_{n}(x)\\).</p> <p>Step.2 Find the roots of \\(\\varphi_{n}(x)\\), which are the Gaussian points \\(x_0, \\dots, x_n\\).</p> <p>Step.3 Solve the linear systems of the equation given by the precision for \\(f(x) = 1, x, \\dots, x^{2n + 1}\\), and obtain \\(A_0, \\dots, A_n\\).</p> <p>Although the method above is theoretically available, but it's tedious. But we have some special solutions which have been calculated. With some transformations, we can make them to solve the general problem too.</p>"},{"location":"Mathematics_Basis/NA/Chap_4/#legendre-polynomials","title":"Legendre Polynomials","text":"<p>A typical set of orthogonal functions is Legrendre Polynomials.</p> <p>Legrendre Polynomials</p> \\[     P_k(x) = \\frac{1}{2^k k!} \\frac{d^k}{dx^k}(x^2 - 1)^k, \\] <p>or equally defined recursively by</p> \\[ \\begin{aligned}     &amp; P_0(x) = 1,\\ P_1(x) = x, \\\\     &amp; P_{k + 1}(x) = \\frac{1}{k + 1}\\left((2k + 1)x P_k(x) - k P_{k - 1}(x)\\right). \\end{aligned} \\] <p>Property</p> <ul> <li> <p>\\(\\{P_n(x)\\}\\) are orthogonal on \\([-1, 1]\\), w.r.t the weight function</p> \\[     w(x) \\equiv 1. \\] \\[     (P_j, P_k) = \\left\\{     \\begin{aligned}         &amp; 0, &amp;&amp; k \\ne l, \\\\         &amp; \\frac{2}{2k + 1}, &amp;&amp; k = l.     \\end{aligned}     \\right. \\] </li> <li> <p>\\(P_n(x)\\) is a monic polynomial of degree \\(n\\).</p> </li> <li>\\(P_n(x)\\) is symmetric w.r.t the origin.</li> <li>The \\(n\\) roots of \\(P_n(x)\\) are all on \\([-1, 1]\\).</li> </ul> <p>The first few of them are</p> \\[ \\begin{aligned}     P_0(x) &amp;= 1, \\\\     P_1(x) &amp;= x, \\\\     P_2(x) &amp;= x^2 - \\frac13, \\\\     P_3(x) &amp;= x^3 - \\frac35 x, \\\\     P_4(x) &amp;= x^4 - \\frac67 x^2 + \\frac{3}{35}. \\end{aligned} \\] <p>The following table gives the pre-calculated values.</p> <p>And from interval \\([-1, 1]\\) to \\([a, b]\\), we have a linear map</p> \\[     t = \\frac{2x - a - b}{b - a} \\Leftrightarrow x = \\frac12 [(b - a)t + a + b]. \\] <p>Thus we have</p> \\[     \\int_a^b f(x) dx = \\int_{-1}^1 f\\left(\\frac{(b - a)t + (b + a)}{2}\\right)\\frac{b - a}{2}dt. \\] <p>The formula using the roots of \\(P_{n + 1}(x)\\) is called the Gauss-Legendre quadrature formula.</p> Example <p>Approxiamte \\(\\int_1^{1.5} e^{-x^2}dx\\) (exact value to 7 decimal places is 0.1093643).</p> <p>Solution.</p> \\[     \\int_1^{1.5} e^{-x^2} = \\frac14 \\int_{-1}^1 e^{-(t + 5)^2 / 16}dt, \\] <p>For \\(n = 2\\),</p> \\[     \\int_1^{1.5} e^{-x^2} \\approx \\frac14 [e^{-(0.57735 + 5)^2 / 16} + e^{-(-57735 + 5)^2 / 16}] = 0.1094003. \\] <p>For \\(n = 3\\),</p> \\[ \\begin{aligned}     \\int_1^{1.5} e^{-x^2} &amp;\\approx \\frac14 [0.55556e^{-(0.77460 + 5)^2 / 16} + 0.88889e^{-(5)^2 / 16}      \\\\ &amp; \\ \\ \\ \\ + 0.55556e^{-(-0.77460 + 5)^2 / 16}]     \\\\ &amp; = 0.1093642. \\end{aligned} \\]"},{"location":"Mathematics_Basis/NA/Chap_4/#chebyshev-polynomials","title":"Chebyshev Polynomials","text":"<p>Also, Chebyshev polynomials are typical set of orthogonal polynomials too. We don't discuss much about it there.</p> <p>The formula using the roots of \\(T_{n + 1}(x)\\) is called the Gauss-Chebyshev quadrature formula.</p>"},{"location":"Mathematics_Basis/NA/Chap_5/","title":"Chapter 5 | Initial-Value Problems for Ordinary Differential Equations","text":"<p>Initial-Value Problem (IVP)</p> <p>Basic IVP (one variable first order)</p> <p>Approximate the solution \\(y(t)\\) to a problem of the form</p> \\[     \\frac{dy}{dt} = f(t, y), t \\in [a, b], \\] <p>subject to an initial condition</p> \\[     y(a) = \\alpha. \\] <p>Higher-Order Systems of First-Order IVP</p> <p>Moreover, adding the number of unknowns, it becomes approximating \\(y_1(t), \\dots y_m(t)\\) to a problem of the form</p> \\[ \\begin{aligned}     \\frac{dy_1}{dt} &amp;= f_1(t, y_1, y_2, \\dots, y_m), \\\\     \\frac{dy_2}{dt} &amp;= f_2(t, y_1, y_2, \\dots, y_m), \\\\     &amp; \\vdots \\\\     \\frac{dy_m}{dt} &amp;= f_m(t, y_1, y_2, \\dots, y_m), \\end{aligned} \\] <p>for \\(t \\in [a, b]\\), subject to the initial conditions</p> \\[     y_1(a) = \\alpha_1, y_2(a) = \\alpha_2, \\dots,  y_m(a) = \\alpha_m.  \\] <p>Higher-Order IVP</p> <p>Or adding the number of order, it becomes mth-order IVP of the form</p> \\[     y^{(m)} = f(t, y, y', y'', \\dots, y^{(m - 1)}), \\] <p>for \\(t \\in [a, b]\\), subject to the inital conditions</p> \\[     y(a) = \\alpha_1, y'(a) = \\alpha_2, \\dots,  y^{(m)}(a) = \\alpha_m.  \\]"},{"location":"Mathematics_Basis/NA/Chap_5/#general-idea","title":"General Idea","text":"<p>Of all the method we introduce below, we can only approximate some points, but not the whole function \\(y(t)\\). The approximation points are called mesh points. Moreover, we will only approximate the equally spaced points. Suppose we apporximate the values at \\(N\\) points on \\([a, b]\\), then the mesh points are</p> \\[     t_i = a + ih, \\] <p>where  \\(h = (b - a) / N\\) is the step size. To get the value between mesh points, we can use interpolation method. Since we know the derivative value \\(f(t, y)\\) at the mesh point, it's nice to use Hermit interpolation or cubic spline interpolation.</p>"},{"location":"Mathematics_Basis/NA/Chap_5/#availability-and-uniqueness","title":"Availability and Uniqueness","text":"<p>Before finding out the approximation, we need some conditions to guarantee its availability and uniqueness.</p> <p>Definition 5.0</p> <p>\\(f(t, y)\\) is said to satisfy a Lipschitz condition in the variable \\(y\\) on a set \\(D \\in \\mathbb{R}^2\\) if \\(\\exists\\ L &gt; 0\\), s.t.</p> \\[     |f(t, y_1) - f(t, y_2)| \\le L|y_1 - y_2|,\\ \\ \\forall\\ (t, y_1), (t, y_2) \\in D. \\] <p>\\(L\\) is called a Lipschitz constant for \\(f\\).</p> <p>Definition 5.1</p> <p>A set \\(D \\in \\mathbb{R}^2\\) is said to be convex if</p> \\[     \\forall\\ (t, y_1), (t, y_2) \\in D,\\ \\ \\forall\\ \\lambda \\in [0, 1],\\ \\ ((1 - \\lambda)t_1 + \\lambda t_2, (1 - \\lambda)y_1 + \\lambda y_2) \\in D. \\] <p> </p> <p>Theorem 5.0 | Sufficient Condition</p> <p>\\(f(t, y)\\) is defined on a convex set \\(D \\in \\mathbb{R}^2\\). If \\(\\exists\\ L &gt; 0\\) s.t.</p> \\[     \\left|\\frac{\\partial f}{\\partial y}(t, y)\\right| \\le L,\\ \\ \\forall\\ (t, y) \\in D, \\] <p>then \\(f\\) satisfies a Lipschitz condition on \\(D\\) in the variable \\(y\\) with Lipschitz constatnt \\(L\\).</p> <p>Theorem 5.1 | Unique Solution</p> <p>\\(f(t, y)\\) is continuous on \\(D = \\{(t, y) | a \\le t \\le b, -\\infty &lt; y &lt; \\infty\\}\\)(\\(D\\) is convex). If \\(f\\) satisfies a Lipschitz condition on \\(D\\) in the variable \\(y\\), then the IVP</p> \\[     y'(t) = f(t, y),\\ \\ a \\le t \\le b,\\ \\ y(a) = \\alpha, \\] <p>has a unique solution \\(y(t)\\) for \\(a \\le t \\le b\\).</p> <p>Definition 5.2</p> <p>The IVP </p> \\[     \\frac{dy}{dt} = f(t, y),\\ a \\le t \\le b,\\ y(a) = \\alpha, \\] <p>is said to be a well-posed problem if</p> <ul> <li>A unique solution \\(y(t)\\) exists;</li> <li>\\(\\forall\\ \\varepsilon &gt; 0, \\exists\\ k(\\varepsilon) &gt; 0\\), s.t. \\(\\forall |\\varepsilon_0| &lt; \\varepsilon\\), and \\(|\\delta(t)|\\) is continuous with \\(|\\delta(t)| &lt; \\varepsilon\\) on \\([a, b]\\), a unique solution \\(z(t)\\) to the IVP</li> </ul> \\[     \\frac{dz}{dt} = f(t, y) + \\delta(t),\\ \\ a \\le t \\le b,\\ \\ y(a) = \\alpha,\\ \\ \\text{ (perturbed problem)} \\] <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 exists with</p> \\[     |z(t) - y(t)| &lt; k(\\varepsilon)\\varepsilon,\\ \\ \\forall\\ t \\in [a, b]. \\] <p>Numerical methods will always be concerned with solving a perturbed problem since roundoff error is unavoidable. Thus we want the problem to be well-posed, which means the perturbation will not affect the result approxiamtion a lot.</p> <p>Theorem 5.2</p> <p>\\(f(t, y)\\) is continuous on \\(D = \\{(t, y) | a \\le t \\le b, -\\infty &lt; y &lt; \\infty\\}\\)(\\(D\\) is convex). If \\(f\\) satisfies a Lipschitz condition on \\(D\\) in the variable \\(y\\), then the IVP</p> \\[     y'(t) = f(t, y),\\ \\ a \\le t \\le b,\\ \\ y(a) = \\alpha, \\] <p>is well-posed.</p> <p>Besides, we also need to discuss the sufficient condition of mth-order system of first-order IVP.</p> <p>Similarly, we have the definition of Lipschitz condition and it's sufficient for the uniqueness of the solution.</p> <p>Definition 5.3</p> <p>\\(f(t, y_1, \\dots, y_m)\\) define on the convex set</p> \\[     D = \\{(t, y_1, \\dots, y_m | a \\le t \\le b, -\\infty &lt; u_i &lt; \\infty, \\text{ for each } i = 1, 2, \\dots, m\\} \\] <p>is said to satisfy Lipschitz condition on \\(D\\) in the variables \\(y_1, \\dots, y_m\\) if \\(\\exists\\ L &gt; 0\\), s.t</p> \\[ \\begin{aligned}     &amp; \\forall\\ (t, y_1, \\dots, y_n), (t, z_1, \\dots, z_n) \\in D, \\\\     &amp; |f(t, y_1, \\dots, y_m) - f(t, z_1, \\dots, z_m)| \\le L \\sum\\limits_{j = 1}^m |y_j - z_j|. \\end{aligned} \\] <p>Theorem 5.3</p> <p>if \\(f(t, y_1, \\dots, y_m)\\) satisfy Lipschitz condition, then the mth order systems of first-order IVP has a unique solution \\(y_1(t), \\dots, y_n(t)\\).</p>"},{"location":"Mathematics_Basis/NA/Chap_5/#eulers-method","title":"Euler's Method","text":"<p>To measure the error of Taylor methods (Euler's method is Taylor's method of order 1), we first give the definition of local truncation error, which somehow measure the truncation error at a specified step.</p> <p>Definition 5.4</p> <p>The difference method</p> \\[ \\begin{aligned}     w_0 &amp;= \\alpha, \\\\     w_{i + 1} &amp;= w_i + h\\phi(t_i, w_i). \\end{aligned} \\] <p>has the local truncation error</p> \\[     \\tau_{i + 1}(h) = \\frac{y_{i + 1} - (y_i + h\\phi(t_i, y_i))}{h} = \\frac{y_{i + 1} - y_i}{h} - \\phi(t_i, y_i). \\]"},{"location":"Mathematics_Basis/NA/Chap_5/#forward-explicit-eulers-method","title":"Forward / Explicit Euler's Method","text":"<p>We use Taylor's Theorem to derive Euler's method. Suppose \\(y(t)\\) is the unique solution of IVP, for each mesh points, we have</p> \\[     y(t_{i + 1}) = y(t_i) + hy'(t_i) + \\frac{h^2}{2}y''(\\xi_i), \\xi_i \\in [t_i, t_{i + 1}], \\] <p>and since \\(y'(t_i) = f(t_i, y(t_i))\\), we have</p> \\[     y(t_{i + 1}) = y(t_i) + hf.(t_i, y(t_i)) + \\frac{h^2}{2}y''(\\xi_i), \\xi_i \\in [t_i, t_{i + 1}]. \\] <p>If \\(w_i \\approx y(t_i)\\) and we delete the remainder term, we get the (Explicit) Euler's method.</p> <p>(Explicit) Euler's Method</p> \\[ \\begin{aligned}     w_0 &amp;= \\alpha, \\\\     w_{i + 1} &amp;= w_i + hf(t_i, w_i). \\text{(difference equations)} \\end{aligned} \\] <p>The local truncation error of Euler's Method is</p> \\[     \\tau_{i + 1}(h) = \\frac{hy'(t_i) + \\frac{h^2}{2}y''(\\xi_i)}{h} - f(t_i, y_i) = \\frac{h}{2}y''(\\xi_i) = O(h). \\] <p>Theorem 5.4</p> <p>The error bound of Euler's method is</p> \\[     |y(t_i) - w_i| \\le \\frac{hM}{2L}\\left[e^{L(t_i - a)} - 1\\right], \\] <p>where \\(M\\) is the upper bound of \\(|y''(t)|\\).</p> <p>Although it has an exponential term (which is not such an accurate error bound), the most important intuitive is that the error bound is proportional to \\(h\\).</p> <p>If we consider the roundoff error, we have the following theorem.</p> <p>Theorem 5.5</p> <p>Suppose there exists roundoff error and \\(u_i = w_i + \\delta_i\\), \\(i = 0, 1, \\dots\\), then</p> \\[     |y(t_i) - u_i| \\le \\frac{1}{L}\\left(\\frac{hM}{2} + \\frac{\\delta}{h}\\right)\\left[e^{L(t_i - a)} - 1\\right] + |\\delta_0|e^{L(t_i - a)}, \\] <p>where \\(\\delta\\) is the upper bound of \\(\\delta_i\\).</p> <p>This comes to a different case, we can't make \\(h\\) too small, and the optimal choice of \\(h\\) is</p> \\[     h = \\sqrt{\\frac{2\\delta}{M}}. \\]"},{"location":"Mathematics_Basis/NA/Chap_5/#backward-implicit-eulers-method","title":"Backward/ Implicit Euler's Method","text":"<p>Inversely, if we use Taylor's Theorem in the following way,</p> \\[     y(t_i) = y(t_{i + 1}) - hy'(t_{i + 1}) + \\frac{h^2}{2}y''(\\xi_{i + 1}), \\xi_i \\in [t_i, t_{i + 1}]. \\] <p>Thus we get the Implicit Euler's Method,</p> <p>Implicit Euler's Method</p> \\[ \\begin{aligned}     w_0 &amp;= \\alpha, \\\\     w_{i + 1} &amp;= w_i + hf(t_{i + 1}, w_{i + 1}). \\end{aligned} \\] <p>Since \\(w_{i + 1}\\) is on the both side, we may use some methods in Chapter 2 to solve out the equation.</p> <p>The local truncation error of implicit Euler's method is</p> \\[ \\begin{aligned}     \\tau_{i + 1}(h) &amp;= \\frac{hy'(t_i) + \\frac{h^2}{2}y''(\\xi_i)}{h} - f(t_{i + 1}, y_{i + 1})      \\\\ &amp;= y'(t_i) - y'(t_{i + 1}) + \\frac{h}{2}y''(\\xi_i)     \\\\ &amp;= y'(t_i) - y'(t_i) - hy''(\\xi'_i) + \\frac{h}{2}y''(\\xi_i)     \\\\ &amp;= -\\frac{h}{2}y''(\\eta_i) = O(h). \\end{aligned} \\]"},{"location":"Mathematics_Basis/NA/Chap_5/#trapezoidal-method","title":"Trapezoidal Method","text":"<p>Notice the local truncation errors of two Euler's method are</p> \\[     \\tau_{i + 1}(h) = \\frac{h}{2}y''(\\xi'),\\ \\      \\tau_{i + 1}(h) = -\\frac{h}{2}y''(\\eta'). \\] <p>If we combine these two, we then obtain a method of \\(O(h^2)\\) local truncation error, which is called Trapezoidal Method.</p> <p>Trapezoidal Method</p> \\[ \\begin{aligned}     w_0 &amp;= \\alpha, \\\\     w_{i + 1} &amp;= w_i + \\frac{h}{2}[f(t_i, w_i) + f(t_{i + 1}, w_{i + 1})]. \\end{aligned} \\]"},{"location":"Mathematics_Basis/NA/Chap_5/#higher-order-taylor-method","title":"Higher-Order Taylor Method","text":"<p>In Euler's methods, we only employ first order Taylor polynomials. If we expand it more to nth Taylor polynomials, we have</p> \\[ \\begin{aligned}     y(t_{i + 1}) &amp;= y(t_i) + hy'(t_i) + \\frac{h^2}{2}y''(t_i) + \\cdots + \\frac{h^n}{n!}y^{(n)}(t_i) + \\frac{h^{n + 1}}{(n + 1)!} y^{(n + 1)}(\\xi_i),     \\\\ &amp; \\text{ where } \\xi_i \\in [t_i, t_{i + 1}], \\end{aligned} \\] <p>and since \\(y^{(k)}(t) = f^{(k - 1)}(t, y(t))\\), we have</p> \\[ \\begin{aligned}     y(t_{i + 1}) &amp;= y(t_i) + hf(t_i, y(t_i)) + \\frac{h^2}{2}f'(t_i, y(t_i)) + \\cdots + \\frac{h^n}{n!}f^{(n - 1)}(t_i, y(t_i))     \\\\ &amp; \\ \\ \\ \\ + \\frac{h^{n + 1}}{(n + 1)!} f^{(n)}(\\xi_i, y(\\xi_i)). \\end{aligned} \\] <p>Taylor Method of order n</p> \\[ \\begin{aligned}     w_0 &amp;= \\alpha, \\\\     w_{i + 1} &amp;= w_i + hT^{(n)}(t_i, w_i), \\end{aligned} \\] <p>where</p> \\[     T^{(n)}(t_i, w_i) = f(t_i, w_i) + \\frac{h}{2}f'(t_i, w_i) + \\cdots + \\frac{h^{n - 1}}{n!}f^{(n - 1)}(t_i, w_i). \\] <p>Theorem 5.6</p> <p>The local truncation error of Taylor method of order \\(n\\) is \\(O(h^n)\\).</p>"},{"location":"Mathematics_Basis/NA/Chap_5/#runge-kutta-method","title":"Runge-Kutta Method","text":"<p>Although Taylor method gives more accuracy as \\(n\\) increase, but it's not an easy job to calculate nth derivative of \\(f\\). Here we introduce a new method called Runge-Kutta Method, which has low local truncation error and doesn't need to compute derivatives. It's based on Taylor's Theorem in two variables.</p> Theorem 5.7 | Taylor's Theorem in two variables (Recap) <p>\\(f(x, y)\\) has continous \\(n + 1\\) partial derivatives on the neigbourhood of \\(A(x_0, y_0)\\), denoted by \\(U(A)\\), then \\(\\forall\\ (x, y) \\in U(A)\\), denote \\(\\Delta x = x - x_0\\), \\(\\Delta y = y - y_0\\), \\(\\exists\\ \\theta \\in (0, 1)\\), s.t.</p> \\[     f(x, y) = P_n(x, y) + R_n(x, y), \\] <p>where</p> \\[ \\small \\begin{aligned}     P_n(x, y) &amp;= f(x_0, y_0) + \\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)f(x_0, y_0) + \\frac{1}{2!}\\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)^2f(x_0, y_0) +      \\\\ &amp; \\ \\ \\ \\ \\ \\cdots + \\frac{1}{n!}\\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)^nf(x_0, y_0), \\end{aligned} \\] \\[ \\small     R_n(x, y) = \\frac{1}{(n + 1)!}\\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)^{n + 1}f(x_0 + \\theta \\Delta x, y_0 + \\theta \\Delta y). \\] <p>and</p> \\[ \\small     \\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)^{k}f(x, y) = \\sum\\limits_{i = 0}^k \\binom{k}{i} (\\Delta x)^i (\\Delta y)^{k - i} \\frac{\\partial^k f}{\\partial x^i \\partial y^{k - i}}(x, y). \\] Derivation of Midpoint Method <p>To equate</p> \\[     T^{(2)}(t, y) = f(t, y) + \\frac{h}{2}f'(t, y) = f(t, y) + \\frac{h}{2}\\frac{\\partial f}{\\partial t}(t,y) + \\frac{h}{2}\\frac{\\partial f}{\\partial y}(t,y)\\cdot f(t, y), \\] <p>with</p> \\[ \\begin{aligned}     a_1f(t + \\alpha_1, y + \\beta_1) &amp;= a_1f(t, y) + a_1 \\alpha_1 \\frac{\\partial f}{\\partial t}(t, y)      \\\\ &amp; \\ \\ \\ \\ + a_1 \\beta_1 \\frac{\\partial f}{\\partial y}(t, y) + a_1 \\cdot R_1(t + \\alpha_1, y + \\beta_1), \\end{aligned} \\] <p>except for the last residual term \\(R_1\\), we have</p> \\[     a_1 = 1,\\ \\ \\alpha_1 = \\frac{h}{2},\\ \\ beta_1 = \\frac{h}{2}f(t, y). \\] <p>Thus</p> \\[     T^{(2)}(t, y) = f\\left(t + \\frac{h}{2}, y + \\frac{h}{2}f(t, y)\\right) - R_1. \\] <p>Since the term \\(R_1\\) is only concerned with the 2<sup>nd</sup>-order partial derivatives of \\(f\\), if they are bounded, then \\(R_1\\) is \\(O(h^2)\\), the order of the local truncation error. Replace \\(T^{(2)}(t,y)\\) by the formula above in the Taylor method of order 2, we have the Midpoint Method.</p> <p>Midpoint Method</p> \\[ \\begin{aligned}     w_0 &amp;= \\alpha, \\\\     w_{i + 1} &amp;= w_i + hf\\left(t_i + \\frac{h}{2}, w_i + \\frac{h}{2}f(t_i, w_i)\\right). \\end{aligned} \\] Derivation of Modified Euler Method and Heun's Method <p>Similarly, approximate</p> \\[     T^{(3)}(t, y) = f(t, y) + \\frac{h}{2}f'(t, y) + \\frac{h^2}{6}f''(t,y), \\] <p>with</p> \\[     a_1f(t,y) + a_2f(t + \\alpha_2, y + \\delta_2f(t,y)). \\] <p>But it doesn't have sufficient equation to determine the 4 unknowns \\(a_1, a_2, \\alpha_2, \\delta_2\\) above. Instead we have</p> \\[     a_1 + a_2 = 1, \\     \\alpha_2 = \\delta_2\\ \\overset{\\Delta}{=\\!=}\\ ph. \\] <p>Thus a number of \\(O(h^2)\\) methods can be derived, and they are generally in the form</p> \\[ \\begin{aligned}     w_{i + 1} &amp;= w_i + h(a_1 K_1 + a_2 K_2) \\\\     K_1 &amp;= f(t_i, w_i), \\\\     K_2 &amp;= f(t_i + ph, y + ph K_1). \\end{aligned} \\] <p>The most important are the following two.</p> <p>Modified Euler Method</p> \\[ \\begin{aligned}     w_0 &amp;= \\alpha, \\\\     w_{i + 1} &amp;= w_i + \\frac{h}{2}[f(t_i, w_i) + f(t_i + h, w_i + hf(t_i, w_i))]. \\end{aligned} \\] <p>Heun's Method</p> \\[ \\begin{aligned}     w_0 &amp;= \\alpha, \\\\     w_{i + 1} &amp;= w_i + \\frac{h}{4}\\left[f(t_i, w_i) + 3f\\left(t_i + \\frac23 h, w_i + \\frac23 hf(t_i, w_i)\\right)\\right]. \\end{aligned} \\] <p>In a similar manner, approximate \\(T^{(n)}(t, y)\\) with</p> \\[     \\lambda_1 K_1 + \\lambda_2 K_2 + \\cdots + \\lambda_m K_m, \\] <p>where</p> \\[ \\begin{aligned}     K_1 &amp;= f(t, y), \\\\     K_2 &amp;= f(t + \\alpha_2 h, y + \\beta_{21} h K_1), \\\\     &amp; \\ \\ \\vdots \\\\     K_m &amp;= f(t + \\alpha_m h, y + \\beta_{m1} h K_1 + \\beta_{m, m - 1} hK_{m - 1}), \\end{aligned} \\] <p>with different \\(m\\), we can derive quite a lot of Runge-Kutta methods.</p> <p>The most common Runge-Kutta method is given below.</p> <p>Runge-Kutta Order Four</p> \\[ \\begin{aligned}     w_0 &amp;= \\alpha, \\\\     w_{i + 1} &amp;= w_i + \\frac{h}{6}(K_1 + 2K_2 + 2K_3 + K_4), \\\\     K_1 &amp;= f(t_i, w_i), \\\\     K_2 &amp;= f\\left(t_i + \\frac{h}{2}, w_i + \\frac{h}{2}K_1\\right), \\\\     K_3 &amp;= f\\left(t_i + \\frac{h}{2}, w_i + \\frac{h}{2}K_2\\right), \\\\     K_4 &amp;= f\\left(t_i + h, w_i + h K_3\\right). \\end{aligned} \\] <p>The local trancation error of Runge-Kutta are given below, where \\(n\\) is the number of evaluations per step.</p> <p>Property</p> <p>Compared to Taylor's method, evaluate \\(f(t, y)\\) the same times, Runge-Kutta gives the lowest error.</p>"},{"location":"Mathematics_Basis/NA/Chap_5/#multistep-method","title":"Multistep Method","text":"<p>In the previous section, the difference equation only relates \\(w_{i + 1}\\) with \\(w_{i}\\). Multistep Method discuss the situation of relating more term of previous predicted \\(w\\).</p> <p>Definition 5.5</p> <p>An m-step multistep method for solving the IVP</p> \\[     y'(t) = f(t, y),\\ \\ a \\le t \\le b,\\ \\ y(a) = \\alpha, \\] <p>has a difference equation</p> \\[ \\begin{aligned}     w_{i + 1} &amp;= a_{m - 1}w_i + a_{m - 2}w_{i - 1} + \\cdots a_0 w_{i + 1 - m} + h[b_mf(t_{i + 1}, w_{i + 1})     \\\\ &amp; \\ \\ \\ \\ + b_{m - 1}f(t_i, w_i) + \\cdots + b_0 f(t_{i + 1 - m}, w_{i + 1 - m})], \\end{aligned} \\] <p>with the starting values</p> \\[     w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ \\dots,\\ \\ w_{m - 1} = \\alpha_{m - 1}. \\] <p>If \\(b_m = 0\\), it's called explicit, or open. Otherwise, it's called implicit, or closed.</p> <p>Similarly, we first define local truncation error for multistep method to measure the error at a specified step.</p> <p>Definition 5.6</p> <p>For a m-step multistep method, the local truncation error at \\((i + 1)\\)th step is</p> \\[ \\begin{aligned}     \\tau_{i + 1}(h) &amp;= \\frac{y(t_{i + 1}) - a_{m - 1}y(t_i) - \\cdots - a_0 y(t_{i + 1 - m})}{h}     \\\\ &amp; \\ \\ \\ \\ - b_{m - 1}f(t_i, y(t_{i + 1})) + \\cdots + b_0 f(t_{i + 1 - m}, y(t_{i + 1 - m}))]. \\end{aligned} \\]"},{"location":"Mathematics_Basis/NA/Chap_5/#adams-method","title":"Adams Method","text":"<p>In this section, we only consider the case of \\(a_{m - 1} = 1\\) and \\(a_i = 0\\), \\(i \\ne m - 1\\).</p> <p>To derive the Adams method, we start from a simple formula.</p> \\[     y(t_{i + 1}) - y(t_i) = \\int_{t_i}^{t_{i + 1}}y'(t)dt = \\int_{t_i}^{t_{i + 1}}f(t, y(t))dt. \\] <p>i.e.</p> \\[     y(t_{i + 1}) = y(t_i) + \\int_{t_i}^{t_{i + 1}}f(t, y(t))dt. \\] <p>We cannot integrate \\(f(t, y(t))\\) without knowing \\(y(t)\\). Instead we use an interpolating polynomial \\(P(t)\\) by points \\((t_0, w_0),\\ \\dots,\\ (t_i, w_i)\\) to approximate \\(f(t, y)\\). Thus we approximate \\(y(t_{i + 1})\\) by</p> \\[     y(t_{i + 1}) \\approx w_i + \\int_{t_i}^{t_{i + 1}} P(t) dt. \\] <p>For convenience, we use Newton backward-difference formula to represent \\(P(t)\\).</p>"},{"location":"Mathematics_Basis/NA/Chap_5/#adams-bashforth-m-step-explicit-method","title":"Adams-Bashforth m-Step Explicit Method","text":"<p>To derive an Adam-Bashforth explicit m-step technique, we form the interpolating polynomial \\(P_{m - 1}(t)\\) by </p> \\[     (t_i, f(t_i, y(t_i))),\\ \\ \\dots,\\ \\ (t_{i + 1 - m}, f(t_{i + 1 - m}, y(t_{i + 1 - m}))). \\] <p>Then suppose \\(t = t_i + sh\\), with \\(dt = hds\\), we have</p> \\[ \\begin{aligned}     \\int_{t_i}^{t_{i + 1}} f(t, y(t)) dt &amp;= \\int_{t_i}^{t_{i + 1}} \\sum\\limits_{k = 0}^{m - 1}(-1)^k \\binom{-s}{k}\\nabla^k f(t_i, y(t_i))dt     \\\\ &amp; \\ \\ \\ \\ + \\int_{t_i}^{t_{i + 1}} \\frac{f^{(m)}(\\xi_i, y(\\xi_i))}{m!}\\prod\\limits_{k = 0}^{m - 1}(t - t_{i - k})dt     \\\\ \\left(\\text{Note that } dt = hds\\right)     &amp;= h\\sum\\limits_{k = 0}^{m - 1}\\nabla^k (-1)^k \\int_0^1 \\binom{-s}{k}ds     \\\\ &amp; \\ \\ \\ \\ + h^{m + 1}\\int_0^1 \\binom{-s}{m}f^{(m)}(\\xi_i, y(\\xi_i))ds.     \\\\ \\left(\\substack{\\text{Weighted Mean Value} \\\\ \\text{Theorem for Integral}}\\right)     &amp;= h\\sum\\limits_{k = 0}^{m - 1}\\nabla^k (-1)^k \\int_0^1 \\binom{-s}{k}ds     \\\\ &amp; \\ \\ \\ \\ + h^{m + 1}f^{(m)}(\\mu_i, y(\\mu_i))\\int_0^1 \\binom{-s}{m}ds. \\end{aligned} \\] <p>Since for a given \\(k\\), we have</p> \\(k\\) \\(0\\) \\(1\\) \\(2\\) \\(3\\) \\(4\\) \\((-1)^k\\int_0^1 \\binom{-s}{k}ds\\) \\(1\\) \\(\\frac12\\) \\(\\frac{5}{12}\\) \\(\\frac38\\) \\(\\frac{251}{720}\\) <p>Thus</p> \\[ \\begin{aligned}     y(t_{i + 1}) &amp;= y(t_i) + h\\left[f(t_i, y(t_i)) + \\frac12 \\nabla f(t_i, y(t_i)) + \\frac{5}{12} \\nabla^2 f(t_i, y(t_i)) + \\cdots\\right]     \\\\ &amp; \\ \\ \\ \\ + h^{m + 1}f^{(m)}(\\mu_i, y(\\mu_i))\\int_0^1 \\binom{-s}{m}ds. \\end{aligned} \\] <p>Adams-Bashforth m-Step Explicit Method</p> \\[     w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ \\dots,\\ \\ w_{m - 1} = \\alpha_{m - 1}, \\] \\[     w_{i + 1} = w_i + h\\sum\\limits_{k = 0}^{m - 1}\\nabla^k f(t_i, y(t_i)) (-1)^k \\int_0^1 \\binom{-s}{k}ds, \\] <p>with local truncation error</p> \\[     \\tau_{i + 1}(h) = h^{m}y^{(m + 1)}(\\mu_i)(-1)^m\\int_0^1 \\binom{-s}{m}ds. \\] <p>Adams-Bashforth Two-Step Explicit Method</p> \\[     w_0 = \\alpha,\\ \\ w_1 = \\alpha_1, \\] \\[     w_{i + 1} = w_i + \\frac{h}{2}[3f(t_i, w_i) - f(t_{i - 1}, w_{i - 1})], \\] <p>with local truncation error</p> \\[     \\tau_{i + 1}(h) = \\frac{5}{12}y'''(\\mu_i)h^2. \\] <p>Adams-Bashforth Three-Step Explicit Method</p> \\[     w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2, \\] \\[     w_{i + 1} = w_i + \\frac{h}{12}[23f(t_i, w_i) - 16f(t_{i - 1}, w_{i - 1}) + 5f(t_{i - 2}, w_{i - 2})], \\] <p>with local truncation error</p> \\[     \\tau_{i + 1}(h) = \\frac{3}{8}y^{(4)}(\\mu_i)h^3. \\] <p>Adams-Bashforth Four-Step Explicit Method</p> \\[     w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2,\\ \\ w_3 = \\alpha_3, \\] \\[ \\begin{aligned}     w_{i + 1} = w_i + \\frac{h}{24}&amp;[55f(t_i, w_i) - 59f(t_{i - 1}, w_{i - 1})     \\\\ &amp; + 37f(t_{i - 2}, w_{i - 2}) - 9f(t_{i - 3}, w_{i - 3})], \\end{aligned} \\] <p>with local truncation error</p> \\[     \\tau_{i + 1}(h) = \\frac{251}{720}y^{(5)}(\\mu_i)h^4. \\]"},{"location":"Mathematics_Basis/NA/Chap_5/#adams-moulton-m-step-implicit-method","title":"Adams-Moulton m-Step Implicit Method","text":"<p>For implicit case, we add \\((t_{i + 1}, f(t_{i + 1}, y(t_{i + 1})))\\) as one more interpolating node to construct the interpolating polynomials. Let \\(t = t_{i + 1} + sh\\) and there is \\(m\\) points. Similarly we have the conclusions below.</p> <p>Adams-Moulton m-Step Implicit Method</p> \\[     w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ \\dots,\\ \\ w_{m - 1} = \\alpha_{m - 1}, \\] \\[     w_{i + 1} = w_i + h\\sum\\limits_{k = 0}^{m}\\nabla^k f(t_{i + 1}, y(t_{i + 1})) (-1)^k \\int_{-1}^0 \\binom{-s}{k}ds, \\] <p>with local truncation error</p> \\[     \\tau_{i + 1}(h) = h^{m + 1}y^{(m + 2)}(\\mu_i)(-1)^{m + 1}\\int_{-1}^0 \\binom{-s}{m + 1}ds. \\] <p>Adams-Moulton Two-Step Implicit Method</p> \\[     w_0 = \\alpha,\\ \\ w_1 = \\alpha_1, \\] \\[     w_{i + 1} = w_i + \\frac{h}{12}[5f(t_{i + 1}, w_{i + 1}) + 8f(t_i, w_i) - f(t_{i - 1}, w_{i - 1})], \\] <p>with local truncation error</p> \\[     \\tau_{i + 1}(h) = -\\frac{1}{24}y^{(4)}(\\mu_i)h^3. \\] <p>Adams-Moulton Three-Step Implicit Method</p> \\[     w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2, \\] \\[     w_{i + 1} = w_i + \\frac{h}{24}[9f(t_{i + 1}, w_{i + 1}) + 19f(t_i, w_i) - 5f(t_{i - 1}, w_{i - 1}) + f(t_{i - 2}, w_{i - 2})], \\] <p>with local truncation error</p> \\[     \\tau_{i + 1}(h) = -\\frac{19}{720}y^{(5)}(\\mu_i)h^4. \\] <p>Adams-Moulton Four-Step Implicit Method</p> \\[     w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2,\\ \\ w_3 = \\alpha_3, \\] \\[ \\begin{aligned}     w_{i + 1} = w_i + \\frac{h}{720}&amp;[251f(t_{i + 1}, w_{i + 1}) + 646f(t_i, w_i) - 264f(t_{i - 1}, w_{i - 1})     \\\\ &amp; + 106f(t_{i - 2}, w_{i - 2}) - 19f(t_{i - 3}, w_{i - 3})], \\end{aligned} \\] <p>with local truncation error</p> \\[     \\tau_{i + 1}(h) = -\\frac{3}{160}y^{(6)}(\\mu_i)h^5. \\] <p>Note</p> <p>Comparing an m-step Adams-Bashforth explicit method and (m - 1)-step Adams-Moulton implicit method, they both involve m evaluation of \\(f\\) per step, and both have the local truncation error of the term \\(ky^{(m + 1)}(\\mu_i)h^m\\). And the latter implicit method has the smaller coefficient \\(k\\). This leads to greater stability and smaller roundoff errors for implicit methods.</p>"},{"location":"Mathematics_Basis/NA/Chap_5/#predictor-corrector-method","title":"Predictor-Corrector Method","text":"<p>Implicit method has some advantages but it's hard to calculate. We could use some iterative methods introduced in Chapter 2 to solve it but it complicates the process considerably.</p> <p>In practice, since explicit method is easy to calculate, we combine the explicit and implicit method to predictor-corrector method.</p> <p>Predictor-Corrector Method</p> <p>Step.1 Compute first \\(m\\) initial values by Runge-Kutta method.</p> <p>Step.2 Predict by Adams-Bashforth explicit method.</p> <p>Step.3 Correct by Adams-Moulton implicit method.</p> <p>NOTE: All the formulae used in the three steps have the same order of truncation error.</p> <p>Example</p> <p>Take \\(m = 4\\) which is the most common case as exmaple.</p> <p>Step.1</p> <p>From initial value \\(w_0 = \\alpha\\), we use Runge-Kutta method of order four to derive \\(w_1\\), \\(w_2\\) and \\(w_3\\). Set \\(i = 3\\).</p> <p>Step.2</p> <p>Use four-step Adams-Bashforth explicit method, we have</p> \\[ \\small     w_{i + 1}^{(0)} = w_i + \\frac{h}{24}[55f(t_i, w_i) - 59f(t_{i - 1}, w_{i - 1}) + 37f(t_{i - 2}, w_{i - 2}) - 9f(t_{i - 3}, w_{i - 3})], \\] <p>Step.3</p> <p>Use three-step Adams-Moulton </p> \\[ \\small     w_{i + 1}^{(1)} = w_i + \\frac{h}{24}[9f(t_{i + 1}, w_{i + 1}^{(1)}) + 19f(t_i, w_i) - 5f(t_{i - 1}, w_{i - 1}) + f(t_{i - 2}, w_{i - 2})], \\] <p>Then we have two options</p> <ul> <li>\\(i = i + 1\\), and go back to Step. 2, to approximate the next point.</li> <li>Or, for higher accuracy, we can repeat Step.3 iteratively by </li> </ul> \\[ \\small     w_{i + 1}^{(k + 1)} = w_i + \\frac{h}{24}[9f(t_{i + 1}, w_{i + 1}^{(k)}) + 19f(t_i, w_i) - 5f(t_{i - 1}, w_{i - 1}) + f(t_{i - 2}, w_{i - 2})]. \\]"},{"location":"Mathematics_Basis/NA/Chap_5/#other-methods","title":"Other Methods","text":"<p>If we derive the multistep method by Taylor expansion, we can have more methods. We take an example to show it.</p> <p>Suppose we want to derive a difference equation in the form</p> \\[     w_{i + 1} = a_0 w_{i - 1} + h(b_2 f(t_{i + 1}, w_{i + 1}) + b_1 f(t_i, w_i) + b_0 f(t_{i - 1}, w_{i - 1})). \\] <p>We use \\(y_i\\) and \\(f_i\\) to denote \\(y(t_i)\\) and \\(f(t_i, y_i)\\) respectively.</p> <p>Expand \\(y_{i - 1}, f_{i + 1}, f_{i}, f_{i - 1}\\), we have</p> \\[ \\begin{aligned}     y_{i - 1} &amp;= y_i - hy_i' + \\frac12 h^2 y_i'' - \\frac16h^3y_i''' + O(h^4), \\\\     f_{i + 1} &amp;= y_i' + hy_i'' + \\frac12 h^2 y_i''' + O(h^3), \\\\     f_i &amp;= y_i', \\\\     f_{i - 1} &amp;= y_i' - hy_i'' + \\frac12 h^2 y_i''' + O(h^3), \\\\ \\end{aligned} \\] <p>and note that</p> \\[     y_{i + 1} = y_i + hy_i' + \\frac12 h^2 y_i'' + \\frac16 h^3 y_i'''+ O(h^4). \\] <p>Equate them with</p> \\[     y_{i + 1} = a_0 y_{i - 1} + h(b_2 f_{i + 1} + b_1 f_i + b_0 f_{i - 1}). \\] <p>We can solve out</p> \\[     a_0 = 1,\\ \\ b_2 = \\frac13,\\ \\ b_1 = \\frac43,\\ \\ b_0 = \\frac13. \\] <p>Thus</p> \\[     w_{i + 1} = w_{i - 1} + \\frac{h}{3}(f(t_{i + 1}, w_{i + 1}) + 4f(t_i, w_i) + f(t_{i - 1}, w_{i - 1})). \\] <p>Simpson Implicit Method</p> \\[     w_0 = \\alpha,\\ \\ w_1 = \\alpha_1, \\\\     w_{i + 1} = w_{i - 1} + \\frac{h}{3}(f(t_{i + 1}, w_{i + 1}) + 4f(t_i, w_i) + f(t_{i - 1}, w_{i - 1})), \\] <p>with local truncation error</p> \\[     \\tau_{i + 1}(h) = \\frac{h^4}{90}y^{(5)}(\\xi_i). \\] <p>Note that it's an implicit method, and the most commonly used corresponding predictor is</p> <p>Milne's Method</p> \\[     w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2 \\\\     w_{i + 1} = w_{i - 3} + \\frac{4h}{3}(2f(t_i, w_i) + - f(t_{i - 1}, w_{i - 1}) + 2f(t_{i - 2}, w_{i - 2})), \\] <p>with local truncation error</p> \\[     \\tau_{i + 1}(h) = \\frac{14}{90}h^4 y^{(5)}(\\xi_i). \\] <p>which can also derive by Taylor expansion in the similar manner.</p>"},{"location":"Mathematics_Basis/NA/Chap_5/#higher-order-systems-of-first-order-ivp","title":"Higher-Order Systems of First-Order IVP","text":"<p>Actually it just vectorize the method that we've mentioned above. Take Runge-Kutta Order Four as example.</p> <p>Runge-Kutta Method for Systems of Differential Equations</p> \\[ \\begin{aligned}     w_{1, 0} &amp;= \\alpha_1, w_{2, 0} = \\alpha_2, \\dots, w_{m, 0} = \\alpha_m. \\\\     w_{i, j + 1} &amp;= w_{i, j} + \\frac{h}{6}(K_{1, i} + 2K_{2, i} + 2K_{3, i} + K_{4, i}), \\\\     K_{1, i} &amp;= f_i(t_j, w_{1, j}, w_{2, j}, \\dots, w_{m, j}), \\\\     K_{2, i} &amp;= f_i\\left(t_j + \\frac{h}{2}, w_{1, j} + \\frac{h}{2}K_{1, 1}, w_{2, j} + \\frac{h}{2}K_{1, 2}, \\dots, w_{m, j} + \\frac{h}{2}K_{1, m}\\right), \\\\     K_{3, i} &amp;= f_i\\left(t_j + \\frac{h}{2}, w_{1, j} + \\frac{h}{2}K_{2, 1}, w_{2, j} + \\frac{h}{2}K_{2, 2}, \\dots, w_{m, j} + \\frac{h}{2}K_{2, m}\\right), \\\\     K_{4, i} &amp;= f_i\\left(t_j + h, w_{1, j} + h K_{3, 1}, w_{2, j} + h K_{3, 2}, \\dots, w_{m, j} + h K_{3. m} \\right). \\end{aligned} \\]"},{"location":"Mathematics_Basis/NA/Chap_5/#higher-order-ivp","title":"Higher-Order IVP","text":"<p>We can deal with higher-order IVP the same as higher-order systems of first-order IVP. The only transformation we need to do is to let \\(u_1(t) = y(t)\\), \\(u_2(t) = y'(t)\\), \\(\\dots\\), and \\(u_m(t) = y^{(m - 1)}(t)\\).</p> <p>This produces the first-order system</p> \\[ \\begin{aligned}     \\frac{du_1}{dt} &amp;= \\frac{dy}{dt} = u_2, \\\\     \\frac{du_2}{dt} &amp;= \\frac{dy'}{dt} = u_3, \\\\     &amp; \\vdots \\\\     \\frac{du_{m - 1}}{dt} &amp;= \\frac{dy^{(m - 2)}}{dt} = u_m, \\\\     \\frac{du_m}{dt} &amp;= \\frac{dy^{(m - 1)}}{dt} = y^{(m)} = f(t, y', y'', \\dots, y^{(m - 1)}) = f(t, u_1, \\dots, u_m), \\end{aligned} \\] <p>with initial conditions</p> \\[     u_1(a) = y(a) = \\alpha_1,\\ \\ u_2(a) = y'(a) = \\alpha_2,\\ \\ \\dots,\\ \\ u_m(a) = y^{(m - 1)}(a) = \\alpha_m.  \\]"},{"location":"Mathematics_Basis/NA/Chap_5/#stability","title":"Stability","text":"<p>Definition 5.7</p> <p>A one-step difference-equation method with local truncation error \\(\\tau_i(h)\\) is consistent with the differential equation it approximates if</p> \\[     \\lim\\limits_{h \\rightarrow 0} \\max\\limits_{1 \\le i \\le N}|\\tau_i(h)| = 0. \\] <p>For m-step multistep methods, it's also required that</p> \\[     \\lim\\limits_{h \\rightarrow 0} |\\alpha_i - y_i| = 0,\\ \\ i = 1, 2, \\dots, m - 1, \\] <p>since at most cases, these \\(\\alpha_i\\) are derived by one-step methods.</p> <p>Definition 5.8</p> <p>A one-step / multistep difference-equation method is convergent w.r.t the differential equation it approximates if</p> \\[     \\lim\\limits_{h \\rightarrow 0} \\max\\limits_{1 \\le i \\le N}|w_i - y(t_i)| = 0. \\] <p>Definition 5.9</p> <p>A stable method is one whose results depent continously on the initial data, in the sense that small changes or perturbations in the intial conditions produce correspondingly small changes in the subsequent approximations.</p>"},{"location":"Mathematics_Basis/NA/Chap_5/#one-step-method","title":"One Step Method","text":"<p>It's relatively natural to think the stability is somewhat analogous to the discussion of well-posed, and thus it's not surprising Lipschitz condition appears. The following theorem gives the relation among consistency, convergence and stability.</p> <p>Theorem 5.8</p> <p>Suppose the IVP is approximated by a one-step method in the form</p> \\[ \\begin{aligned}     w_0 &amp;= \\alpha, \\\\     w_{i + 1} &amp;= w_i + h \\phi(t_i, w_i, h). \\end{aligned} \\] <p>If \\(\\exists\\ h_0 &gt; 0\\) and $ \\phi(t, w, h)$ is continuous and satisfies a Lipschitz condition in the varaible \\(w\\) with Lipschitz constant \\(L\\) on the set</p> \\[     D = \\{(t, w, h) | a \\le t \\le b, -\\infty &lt; w &lt; \\infty, 0 \\le h \\le h_0\\}. \\] <p>Then</p> <ul> <li>The method is stable.</li> <li>The difference method is convergent iff it's consistent, which is equivalent to</li> </ul> \\[     \\phi(t, y, 0) = f(t, y),\\ \\ t \\in [a, b]. \\] <ul> <li>(Relation between Global Error and Local Truncation Error) If \\(\\exists \\tau(h)\\) s.t. \\(|\\tau_i(h)| \\le \\tau(h)\\) whenever \\(0 \\le h \\le h_0\\), then</li> </ul> \\[     |y(t_i) - w_i| \\le \\frac{\\tau(h)}{L}e^{L(t_i - a)}. \\]"},{"location":"Mathematics_Basis/NA/Chap_5/#multistep-method_1","title":"Multistep Method","text":"Theorem 5.9 | Relation between Global Error and Local Truncation Error <p>Suppose the IVP is approximated by a predictor-corrector method with an m-step Adams-Bashforth predictor equation</p> \\[     w_{i + 1} = w_i + h[b_{m - 1}f(t_i, w_i) + \\cdots + b_0f(t_{i + 1 - m}, w_{i + 1 - m})], \\] <p>with local truncation error \\(\\tau_{i + 1}(h)\\), and an (m - 1)-step Adams-Moulton corrector eqation</p> \\[     w_{i + 1} = w_i + h[\\tilde b_{m - 1}f(t_i, w_i) + \\cdots + \\tilde  b_0f(t_{i + 1 - m}, w_{i + 1 - m})], \\] <p>with local truncation error \\(\\tilde \\tau_{i + 1}(h)\\). In addition, suppose \\(f(t, y)\\) and \\(f_y(t, y)\\) are continous on \\(D = \\{(t, y) | a \\le t \\le b, -\\infty &lt; y &lt; \\infty\\}\\) and \\(f_y\\) is bounded. Then the local truncation error \\(\\sigma_{i + 1}(h)\\) of the predictor-corrector method is</p> \\[ \\small     \\sigma_{i + 1}(h) = \\tilde \\tau_{i + 1}(h) + \\tau_{i + 1}(h) \\tilde b_{m - 1} \\frac{\\partial f}{\\partial y}(t_{i + 1}, \\theta_{i + 1}),\\ \\ \\text{ for some } \\theta_{i + 1} \\in (0, h\\tau_{i + 1}(h)). \\] <p>Moreover, \\(\\exists\\ k_1, k_2\\), s.t.</p> \\[     |w_i - y(t_i)| \\le \\left[\\max\\limits_{0 \\le j \\le m - 1}|w_j - y(t_j)| + k_1\\sigma(h) \\right] e^{k_2(t_i - a)}, \\] <p>where \\(\\sigma(h) = \\max\\limits_{m \\le j \\le N}|\\sigma_j(h)|\\).</p> <p>For the difference equation of multistep method, we first introduce characteristc polynomial associated with the method, given by</p> \\[     P(\\lambda) = \\lambda^m - a_{m - 1}\\lambda^{m - 1} - a_{m - 2}\\lambda^{m - 2} - \\cdots - a_1 \\lambda - a_0. \\] <p>Definition 5.10</p> <p>Suppose the roots of the characteristic equation</p> \\[     P(\\lambda) = \\lambda^m - a_{m - 1}\\lambda^{m - 1} - a_{m - 2}\\lambda^{m - 2} - \\cdots - a_1 \\lambda - a_0 = 0 \\] <p>associated with the multistep difference method</p> \\[     w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ \\dots,\\ \\ w_{m - 1} = \\alpha_{m - 1}, \\\\     w_{i + 1} = a_{m - 1}w_i + a_{m - 2}w_{i - 1} + \\cdots a_0 w_{i + 1 - m} + hF(t_i, h, w_{i + 1}, w_i \\dots, w_{i + 1 - m}). \\] <p>If \\(|\\lambda_i| \\le 1\\), for each \\(i = 1, 2, \\dots, m\\) and all roots with modulus value \\(1\\) are simple roots, then the difference method is said to satisfy the root condition.</p> <ul> <li>Methods that satisfy the root condition and have \\(\\lambda = 1\\) as the only root of the characteristic equation of magnitude one are called strongly stable.</li> <li>Methods that satisfy the root condition and have more than one disctinct root with magnitude one are called weakly stable.</li> <li>Methods that do not satisfy the root condition are called unstable.</li> </ul> <p>Theorem 5.10</p> <p>A multistep method of the form</p> \\[     w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ \\dots,\\ \\ w_{m - 1} = \\alpha_{m - 1}, \\\\     w_{i + 1} = a_{m - 1}w_i + a_{m - 2}w_{i - 1} + \\cdots a_0 w_{i + 1 - m} + hF(t_i, h, w_{i + 1}, w_i \\dots, w_{i + 1 - m}). \\] <p>is stable iff it satisfies the root condition. Moreover, if the difference method is consistent with the differential equation, then the method is stable iff it is convergent.</p>"},{"location":"Mathematics_Basis/NA/Chap_5/#stiff-equations","title":"Stiff Equations","text":"<p>However, a special type of solution is not easy to deal with, when the magnitude of derivative increases but the solution does not. They are call stiff equations.</p> <p>Stiff differential eqaution are characterized as those whose exact solution has a term of the form \\(e^{-ct}\\), where \\(c\\) is a large positive constant. This is called the transient solution. Actaully there is another important part of solution called steady-state solution. </p> <p>Let's first define test equation to examine what happen with the stiff cases.</p> <p>Definition 5.11</p> <p>A test equation is the IVP</p> \\[     y' = \\lambda y,\\ \\ y(0) = \\alpha,\\ \\ \\text{ where } \\text{Re}(\\lambda) &lt; 0. \\] <p>The solution is obviously \\(y(t) = \\alpha e^{\\lambda t}\\), which is the transient solution. At this case, the steady-state solution is zero, thus the approximation characteristics of a method are easy to determine.</p> Example of Euler's Method <p>Take Euler's method as an example, and denote \\(H = h\\lambda\\). We have</p> \\[     w_0 = \\alpha, \\\\     w_{i + 1} = w_i + h(\\lambda w_i) = (1 + h\\lambda)w_i = (1 + H)w_i, \\] <p>so</p> \\[     w_{i + 1} = (1 + H)^{i + 1}w_0 = (1 + H)^{i + 1}\\alpha. \\] <p>The absolute error is</p> \\[     |y(t_i) - w_i| = |(e^H)^i - (1 + H)^i||\\alpha|. \\] <p>When \\(H &lt; 0\\), \\((e^H)^i\\) decays to zero as \\(i\\) increases. Thus we want \\((1 + H)^i\\) to decay too, which implies that</p> \\[     |1 + H| &lt; 1. \\] <p>From another perspective, consider the roundoff error of the initial value \\(\\alpha\\) by,</p> \\[     w_0 = \\alpha + \\delta_0. \\] <p>At the ith step the roundoff error is</p> \\[     \\delta_i = (1 + H)^i \\delta_0. \\] <p>To control the error, we also want \\(|1 + H| &lt; 1\\).</p> <p>In general, when applying to the test equation, we have the difference equation of the form</p> \\[     w_{i + 1} = Q(H)w_i,\\ \\ H = h \\lambda. \\] <p>To make \\(Q(H)\\) approximate \\(e^H\\), we want at least \\(|Q(H)| &lt; 1\\) to make it decay to 0. The inequality delimit a region in a complex plane.</p> <p>For a multistep method, the difference equation is in the form</p> \\[     (1 - H b_m)w_{i + 1} - (a_{m - 1} + H b_{m - 1})w_i - \\cdots - (a_0 + H b_0)w_{i + 1 - m} = 0. \\] <p>We can define a characteristic polynomial</p> \\[     Q(z, H) = (1 - H b_m)z^m - (a_{m - 1} + H b_{m - 1})z^{m - 1} - \\cdots - (a_0 + H b_0). \\] <p>Suppose \\(\\beta_i\\) are the distinct roots of \\(Q(z, H)\\), then it can be proved that \\(\\exists\\ c_i\\), s.t.</p> \\[     w_{i} = \\sum\\limits_{k = 1}^m c_k\\beta_k^i. \\] <p>Again, to make \\(w_i\\) approximate \\((e^H)^i\\), we want at least all \\(|\\beta_k| &lt; 1\\) to make it decay to \\(0\\). It also delimit a region in a complex plane.</p> <p>Definition 5.12</p> <p>The Region R of abosolute stability for a one-step method is</p> \\[     R = \\{H \\in \\mathbb{C} | |Q(H)| &lt; 1\\}, \\] <p>and for a multistep method,</p> \\[     R = \\{H \\in \\mathbb{C} | |\\beta_k| &lt; 1,\\ \\ \\text{ for all zero points of } Q(z, H)\\}. \\] <ul> <li>A numerical method is said to be A-stable if its region \\(R\\) of abosolute stability contains left half-plane, which means \\(\\text{Re}(\\lambda) &lt; 0\\), stable with stiff equation.</li> <li>Method A is said to be more stable than method B if the region of absolute stability of A is larger than that of B.</li> </ul> <p>The region of Euler's method is like</p> <p>Thus it's only stable for some cases of stiff equation. When the negative \\(\\lambda\\) becomes smaller and get out of the region, it becomes not stable.</p> <p>Similarly, for Runge-Kutta Order Four method (explicit), applying test equation, we have</p> \\[     w_{i + 1} = \\left(1 + H + \\frac{H^2}{2} + \\frac{H^3}{6} + \\frac{H^4}{24}\\right)w_i. \\] <p>And the region is like</p> <p>Let's consider some implicit method. Say Euler's implicit method, we have</p> \\[     w_{i + 1} = \\frac{1}{1 - H} w_i, \\] <p>whose region is</p> <p>Thus it's A-stable.</p> <p>Also, implicit Trapezoidal method and 2<sup>nd</sup>-order Runge-Kutta implict method are A-stable, both with the difference equation</p> \\[     w_{i + 1} = \\frac{2 + H}{2 - H}w_i, \\] <p>whose region is just right covers the left half-plane.</p> <p>Thus an important intuitive is that implicit method is more stable than explicit method in the stiff discussion.</p>"},{"location":"Mathematics_Basis/NA/Chap_6/","title":"Chapter 6 | Direct Methods for Solving Linear Systems","text":""},{"location":"Mathematics_Basis/NA/Chap_6/#linear-systems-of-equations","title":"Linear Systems of Equations","text":""},{"location":"Mathematics_Basis/NA/Chap_6/#gaussian-elimination-with-backward-substitution","title":"Gaussian Elimination with Backward Substitution","text":"\\[     A\\mathbf{x} = \\mathbf{b} \\] <p>Let \\(A^{(1)} = A, \\mathbf{b}^{(1)} = \\mathbf{b}\\).</p>"},{"location":"Mathematics_Basis/NA/Chap_6/#elimination","title":"Elimination","text":"<p>For Step \\(k\\ (1\\le k \\le n-1)\\), if \\(a^{(k)}_{kk}\\ne0\\) (pivot element), compute \\(m_{ik} = \\dfrac{a^{(k)}_{ik}}{a^{(k)}_{kk}}\\) and</p> \\[ \\left\\{ \\begin{aligned}     a_{ij}^{(k+1)} = a_{ij}^{(k)} - m_{ik}a_{kj}^{(k)} \\\\     b_i^{(k+1)} = b_i^{(k)} - m_{ik}b_k^{(k)} \\end{aligned} ,\\ \\ \\text{ where } i, j = k+1, \\dots, n. \\right. \\] <p>After \\(n-1\\) steps,</p> \\[ \\begin{bmatrix}     a^{(1)}_{11} &amp; a^{(1)}_{12} &amp; \\cdots &amp; a^{(1)}_{1n} \\\\     &amp; a^{(2)}_{22} &amp; \\cdots &amp; a^{(2)}_{2n} \\\\     &amp; &amp; \\cdots &amp; \\vdots \\\\     &amp; &amp; &amp; a^{(n)}_{nn} \\\\ \\end{bmatrix} \\begin{bmatrix}     x_1 \\\\     x_2 \\\\     \\vdots \\\\     x_n \\end{bmatrix} = \\begin{bmatrix}     b^{(1)}_1 \\\\     b^{(2)}_2 \\\\     \\vdots \\\\ b^{(n)}_n \\end{bmatrix}. \\]"},{"location":"Mathematics_Basis/NA/Chap_6/#backward-substitution","title":"Backward Substitution","text":"<p>Then,</p> \\[ x_n = \\frac{b_n^{(n)}}{a^{(n)}_{nn}},\\ \\ x_i = \\frac{1}{a_{ii}^{(i)}}\\left(b^{(i)}_i - \\sum_{j = i + 1}^n a^{(i)}_{ij} x_j\\right),\\ \\ \\text{ where }i = n-1, \\dots, 1. \\]"},{"location":"Mathematics_Basis/NA/Chap_6/#complexity","title":"Complexity","text":"<p>Recap that we have</p> \\[     \\sum\\limits_{i = 1}^n 1 = n,\\ \\     \\sum\\limits_{i = 1}^n i = \\frac{n(n+1)}{2},\\ \\     \\sum\\limits_{i = 1}^n i^2 = \\frac{n(n+1)(2n+1)}{6}.\\ \\ \\] <p>For Elimination,</p> <p>Multiplications/divisions</p> \\[     \\sum\\limits_{k=1}^{n-1}((n - k) + (n - k)(n - k + 2)) = \\frac{n^3}{3} + \\frac{n^2}{2} - \\frac{5}{6}n. \\] <p>Addtion/subtraction</p> \\[     \\sum\\limits_{k = 1}^{n-1}(n - k)(n - k + 1) = \\frac{n^3}{3} - \\frac{n}{3}. \\] <p>For Backward-substitution,</p> <p>Multiplications/divisions</p> \\[     1 + \\sum\\limits_{k=1}^{n-1}(n - k + 1) = \\frac{n^2}{2} + \\frac{n}{2}. \\] <p>Addtion/subtraction</p> \\[     \\sum\\limits_{i = 1}^{n - 1}((n - i + 1) + 1) = \\frac{n^2}{2} - \\frac{n}{2}. \\] <p>In total,</p> <p>Multiplications/divisions</p> \\[     \\frac{n^3}{3} + n^2 - \\frac{n}{3}. \\] <p>Addtion/subtraction</p> \\[ \\frac{n^3}{3} + \\frac{n^2}{2}-\\frac{5n}{6}. \\]"},{"location":"Mathematics_Basis/NA/Chap_6/#pivoting-strategies","title":"Pivoting Strategies","text":"<p>Motivation: For Gaussian Elimination with Backward Substituion, if the pivot element \\(a_{kk}^{(k)}\\) is small compared to \\(a_{ik}^{(k)}\\), then \\(m_{ik}\\) is large with high roundoff error. Thus we need some transformation to improve the accuracy.</p>"},{"location":"Mathematics_Basis/NA/Chap_6/#partial-pivoting-aka-maximal-column-pivoting","title":"Partial Pivoting (a.k.a Maximal Column Pivoting)","text":"<p>Determine the smallest \\(p \\ge k\\) such that</p> \\[     \\left|a_{pk}^{(k)}\\right| = \\max_{k \\le i \\le n} \\left|a_{ik}^{(k)}\\right|, \\] <p>and interchange row \\(p\\) and row \\(k\\) .</p> <p>Requires \\(O(N^2)\\) additional comparisons.</p>"},{"location":"Mathematics_Basis/NA/Chap_6/#scaled-partial-pivoting-aka-scaled-column-pivoting","title":"Scaled Partial Pivoting (a.k.a Scaled-Column Pivoting)","text":"<p>Determine the smallest \\(p \\ge k\\) such that</p> \\[     \\frac{\\left|a_{pk}^{(k)}\\right|}{s_p} = \\max\\limits_{k \\le i \\le n} \\frac{\\left|a_{ik}^{(k)}\\right|}{s_i}, \\] <p>and interchange row \\(p\\) and row \\(k\\), where \\(s_i = \\max\\limits_{1 \\le j \\le n} \\left|a_{ij}\\right|\\).</p> <p>(Simply put, place the element in the pivot position that is largest relative to the entries in its  row.)</p> <p>Requires \\(O(N^2)\\) additional comparisons and \\(O(N^2)\\) divisions.</p>"},{"location":"Mathematics_Basis/NA/Chap_6/#complete-pivoting-aka-maximal-pivoting","title":"Complete Pivoting (a.k.a Maximal Pivoting)","text":"<p>Search all the entries \\(a_{ij}\\) for \\(i,j = k, \\dots,n\\) to find the entry with the largest magnitude. Both row and column interchanges are performed to bring this entry to the pivot position.</p> <p>Requires \\(O\\left(\\dfrac{1}{3}N^3\\right)\\) additional comparisons.</p>"},{"location":"Mathematics_Basis/NA/Chap_6/#lu-factorization","title":"LU Factorization","text":"<p>Considering the matrix form of Gaussian Elimination, for total \\(n-1\\) steps, we have</p> \\[ L_{n-1}L_{n-2}\\dots L_1[A\\ \\textbf{b}] =  \\begin{bmatrix}     a^{(1)}_{11} &amp; a^{(1)}_{12} &amp; \\cdots &amp; a^{(1)}_{1n} &amp; b_1^{(1)} \\\\     &amp; a^{(2)}_{22} &amp; \\cdots &amp; a^{(2)}_{2n} &amp; b_2^{(2)} \\\\     &amp; &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\     &amp; &amp; &amp; a^{(n)}_{nn} &amp; b_n^{(n)} \\\\ \\end{bmatrix}, \\] <p>where</p> \\[ L_k = \\begin{bmatrix}     1 &amp; 0 &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; 0 \\\\     0 &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\vdots \\\\     \\vdots &amp; \\ddots &amp; 1 &amp; \\ddots &amp; \\ddots &amp; \\vdots\\\\     \\vdots &amp; \\ddots &amp; -m_{k+1, k} &amp; \\ddots &amp; \\ddots &amp; \\vdots\\\\     \\vdots &amp; \\ddots&amp; \\vdots &amp; \\ddots &amp; \\ddots &amp; 0 \\\\     0 &amp; \\cdots &amp; -m_{n, k} &amp; \\cdots &amp; \\cdots &amp; 1 \\end{bmatrix}. \\] <p>It's simple to compute that</p> \\[ L_k^{-1} = \\begin{bmatrix}     1 &amp; 0 &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; 0 \\\\     0 &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\vdots \\\\     \\vdots &amp; \\ddots &amp; 1 &amp; \\ddots &amp; \\ddots &amp; \\vdots\\\\     \\vdots &amp; \\ddots &amp; m_{k+1, k} &amp; \\ddots &amp; \\ddots &amp; \\vdots\\\\     \\vdots &amp; \\ddots&amp; \\vdots &amp; \\ddots &amp; \\ddots &amp; 0 \\\\     0 &amp; \\cdots &amp; m_{n, k} &amp; \\cdots &amp; \\cdots &amp; 1 \\end{bmatrix}. \\] <p>Thus we let</p> \\[ L_1^{-1}L_2^{-1}\\dots L_{n-1}^{-1} =  \\begin{bmatrix}     1 &amp; 0 &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; 0 \\\\     m_{2,1} &amp; 1 &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\vdots \\\\     \\vdots &amp; \\ddots &amp; 1 &amp; \\ddots &amp; \\ddots &amp; \\vdots\\\\     \\vdots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\vdots\\\\     \\vdots &amp; \\ddots &amp;\\ddots&amp; \\ddots &amp; 1 &amp; 0\\\\     m_{n,1} &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; m_{n, n-1} &amp; 1 \\end{bmatrix} = L, \\] <p>and</p> \\[ U = \\begin{bmatrix}     a^{(1)}_{11} &amp; a^{(1)}_{12} &amp; \\cdots &amp; a^{(1)}_{1n} \\\\     0 &amp; a^{(2)}_{22} &amp; \\cdots &amp; a^{(2)}_{2n} \\\\     \\vdots &amp; \\ddots &amp; \\ddots &amp; \\vdots \\\\     0 &amp; \\cdots &amp; 0 &amp; a^{(n)}_{nn} \\\\ \\end{bmatrix}. \\] <p>Then we get</p> \\[     A = LU. \\] <p>Theorem 6.0</p> <p>If Gaussian elimination can be performed on the linear system \\(A\\mathbf{x} = \\mathbf{b}\\) without row interchanges, then the matrix \\(A\\) can be factored into the product of a lower-triangular matrix \\(L\\) and an upper-triangular matrix \\(U\\) .</p> <p>If \\(L\\) has to be unitary, then the factorization is unique.</p>"},{"location":"Mathematics_Basis/NA/Chap_6/#special-types-of-matrices","title":"Special Types of Matrices","text":""},{"location":"Mathematics_Basis/NA/Chap_6/#strictly-diagonally-dominant-matrix","title":"Strictly Diagonally Dominant Matrix \u4e25\u683c\u4e3b\u5bf9\u89d2\u5360\u4f18\u77e9\u9635","text":"<p>Definition 6.0</p> <p>The \\(n \\times n\\) matrix \\(A\\) is said to be strictly diagnoally dominant when</p> \\[     |a_{ii}| \\gt \\sum\\limits_{\\substack{j=1 \\\\ j\\ne i}}^{n}|a_{ij}|,\\text{ for each } i = 1, \\dots, n. \\] <p>Theorem 6.1</p> <p>A strictly diagonally dominant matrix is nonsingular. And Gaussian elimination can be performed without row or column interchanges, and computations will be stable with respect to the growth of roundoff errors.\uff08\u6ee1\u79e9\u3001\u65e0\u9700\u4ea4\u6362\u884c\u5217\u3001\u8bef\u5dee\u7a33\u5b9a\uff09</p>"},{"location":"Mathematics_Basis/NA/Chap_6/#positive-definite-matrix","title":"Positive Definite Matrix \u6b63\u5b9a\u77e9\u9635","text":"Definition 6.1 (Recap) <p>A matrix \\(A\\) is positive definite if it's symmetric and if \\(\\forall\\ (\\mathbf{0} \\ne) \\mathbf{x} \\in \\mathbb{R}^n\\), \\(\\mathbf{x}^tA\\mathbf{x} &gt; 0\\).</p> Theorem 6.2 <p>If \\(A\\) is an \\(n \\times n\\) positive definite matrix, then</p> <ul> <li>\\(A\\) is nonsingular;</li> <li>\\(a_{ii} &gt; 0\\), for each \\(i = 1, 2, \\dots, n\\);</li> <li>\\(\\max\\limits_{1 \\le k, j \\le n}|a_{kj}| \\le \\max\\limits_{1 \\le i \\le n}|a_{ii}|\\);</li> <li>\\((a_{ij})^2 &lt; a_{ii}a_{jj}\\), for each \\(i \\ne j\\).</li> </ul>"},{"location":"Mathematics_Basis/NA/Chap_6/#choleskis-method-ldlt-factorization","title":"Choleski's Method (LDLt factorization)","text":"<p>Further decompose \\(U\\) to \\(D\\tilde U\\).</p> <p>\\(A\\) is symmetric \\(\\Rightarrow\\) \\(L = \\tilde U^t\\).</p> <p>Thus</p> \\[     A = LU = LD\\tilde U = LDL^t.\\ \\ (\u4e0b\u4e09\u89d2\u77e9\u9635\u4e58\u5bf9\u89d2\u77e9\u9635\u518d\u4e58\u5176\u8f6c\u7f6e) \\] <p>Let</p> \\[ D^{1/2} =  \\begin{bmatrix}     \\sqrt{u_{11}} &amp; &amp; &amp; \\\\     &amp; \\sqrt{u_{22}} &amp; &amp; \\\\     &amp; &amp; \\ddots &amp; \\\\     &amp; &amp; &amp; \\sqrt{u_{nn}} \\end{bmatrix}, \\] <p>and \\(\\widetilde{L} = LD^{1/2}\\).</p> <p>Then</p> \\[     A = \\tilde L \\tilde L^t.\\ \\ (\u4e0b\u4e09\u89d2\u77e9\u9635\u4e58\u5176\u8f6c\u7f6e) \\]"},{"location":"Mathematics_Basis/NA/Chap_6/#tridiagonal-linear-system","title":"Tridiagonal Linear System \u4e09\u5bf9\u89d2\u77e9\u9635","text":"<p>Definition 6.2</p> <p>An \\(n \\times n\\) matrix \\(A\\) is called a band matrix if \\(\\exists\\ p, q\\) \\((1 &lt; p, q &lt; n)\\), s.t. whenever \\(i + p \\le j\\) or \\(j + q \\le i\\), \\(a_{ij} = 0\\). And \\(w = p + q - 1\\) is called the bandwidth.</p> <p>Specially, if \\(p = q = 2\\), then \\(A\\) is called tridiagonal, with the following form,</p> \\[ \\begin{bmatrix}     b_1 &amp; c_1 &amp; &amp; &amp; \\\\     a_2 &amp; b_2 &amp; c_2 &amp; &amp; \\\\     &amp; \\ddots &amp; \\ddots &amp; \\ddots \\\\     &amp; &amp; a_{n-1} &amp; b_{n-1} &amp; c_{n-1} \\\\     &amp; &amp; &amp; a_n &amp; b_n \\\\ \\end{bmatrix} \\]"},{"location":"Mathematics_Basis/NA/Chap_6/#crout-factorization","title":"Crout Factorization","text":"\\[ A = LU =  \\begin{bmatrix}     l_{11} \\\\     l_{21} &amp; l_{22} \\\\     &amp; \\ddots &amp; \\ddots \\\\     &amp; &amp; \\ddots &amp; \\ddots \\\\     &amp; &amp; &amp; l_{n, n- 1} &amp; l_{n, n} \\end{bmatrix} \\begin{bmatrix}     1 &amp; u_{12}\\\\     &amp; 1 &amp; u_{23} \\\\     &amp; &amp; \\ddots &amp; \\ddots \\\\     &amp; &amp; &amp; \\ddots &amp; u_{n-1,n}\\\\     &amp; &amp; &amp; &amp; 1  \\end{bmatrix}. \\] <p>the time complexity is \\(O(N)\\).</p>"},{"location":"Mathematics_Basis/NA/Chap_7/","title":"Chapter 7 | Iterative Techniques in Matrix Algebra","text":""},{"location":"Mathematics_Basis/NA/Chap_7/#norms-of-vectors-and-matrices","title":"Norms of Vectors and Matrices","text":""},{"location":"Mathematics_Basis/NA/Chap_7/#vector-norms","title":"Vector Norms","text":"<p>Definition 7.0</p> <p>A vector norm on \\(\\mathbb{R}^n\\) is a function \\(||\\cdot||\\), from \\(\\mathbb{R}^n\\) into \\(\\mathbb{R}\\) with the following properties.</p> \\[ \\begin{aligned}     \\forall \\mathbf{x, y} \\in \\mathbb{R}^n, \\alpha \\in \\mathbb{R}, &amp;     (1)\\ ||\\mathbf{x}|| \\ge 0;\\ ||\\mathbf{x}|| = 0 \\Leftrightarrow \\mathbf{x} = \\mathbf{0};     \\\\ &amp;     (2)\\ ||\\alpha \\mathbf{x}|| = |\\alpha| \\cdot ||\\mathbf{x}||;     \\\\ &amp;     (3)\\ ||\\mathbf{x} + \\mathbf{y}|| \\le ||\\mathbf{x}|| + ||\\mathbf{y}||.     \\\\ \\end{aligned} \\] <p>Commonly used examples</p> <ul> <li>L1 Norm: \\(||\\mathbf{x}||_1 = \\sum\\limits_{i = 1}^n|x_i|\\).</li> <li>L2 Norm / Euclidean Norm: \\(||\\mathbf{x}||_2 = \\sqrt{\\sum\\limits_{i = 1}^n|x_i|^2}\\).</li> <li>p-Norm: \\(||\\mathbf{x}||_p = \\left(\\sum\\limits_{i = 1}^n|x_i|^p\\right)^{1/p}\\).</li> <li>Infinity Norm: \\(||\\mathbf{x}||_\\infty = \\max\\limits_{1 \\le i \\le n} |x_i|\\).</li> </ul>"},{"location":"Mathematics_Basis/NA/Chap_7/#convergence-of-vector","title":"Convergence of Vector","text":"<p>Definition 7.1</p> <p>Similarly with a scalar, a sequence of vectors \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) in \\(\\mathbb{R}^n\\) is said to converge to \\(\\mathbf{x}\\) with respect to norm \\(||\\cdot||\\), if</p> \\[     \\forall\\ \\varepsilon \\gt 0,\\ \\ \\exists\\ N \\in \\mathbb{N},\\ \\ \\text{ s.t. } \\forall\\ k \\gt N,\\ \\ ||\\mathbf{x}^{(k)} - \\mathbf{x}|| \\lt \\varepsilon. \\] <p>Theorem 7.0</p> <p>\\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) in \\(\\mathbb{R}^n\\) converges to \\(\\mathbf{x}\\) with respect to norm \\(||\\cdot||_\\infty\\) iff</p> \\[     \\forall\\ i,\\ \\ \\lim\\limits_{k \\rightarrow \\infty}x_i^{(k)} = x_i. \\]"},{"location":"Mathematics_Basis/NA/Chap_7/#equivalence","title":"Equivalence","text":"<p>Definition 7.2</p> <p>\\(||\\mathbf{x}||_A\\) and \\(||\\mathbf{x}||_B\\) are equivalent, if</p> \\[     \\exists\\ C_1, C_2,\\ \\ \\text{ s.t. } C_1||\\mathbf{x}||_B \\le ||\\mathbf{x}||_A \\le C_2||\\mathbf{x}||_B. \\] <p>Theorem 7.1</p> <p>All the vector norms on \\(\\mathbb{R}^n\\) are equivalent.</p>"},{"location":"Mathematics_Basis/NA/Chap_7/#matrix-norms","title":"Matrix Norms","text":"<p>Definition 7.3</p> <p>A matrix norm on \\(\\mathbb{R}^n\\) is a function \\(||\\cdot||\\), from \\(M_n(\\mathbb{R})\\) matrices into \\(\\mathbb{R}\\) with the following properties.</p> \\[ \\begin{aligned}     \\forall A, B \\in M_n(\\mathbb{R}), \\alpha \\in \\mathbb{R}, &amp;     (1)\\ ||A|| \\ge 0;\\ ||A|| = 0 \\Leftrightarrow A = \\mathbf{O};     \\\\ &amp;     (2)\\ ||\\alpha A|| = |\\alpha| \\cdot ||A||;     \\\\ &amp;     (3)\\ ||A + B|| \\le ||A|| + ||B||;     \\\\ &amp;     (4)\\ ||AB|| \\le ||A|| \\cdot ||B||. \\end{aligned} \\] <p>Commonly used examples</p> <ul> <li>Frobenius Norm: \\(||A||_F = \\sqrt{\\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^n|a_{ij}|^2}\\).</li> <li>Natural Norm: \\(||A||_p = \\max\\limits_{\\mathbf{x}\\ne \\mathbf{0}} \\dfrac{||A\\mathbf{x}||_p}{||\\mathbf{x}||_p} =  \\max\\limits_{||\\mathbf{x}||_p = 1} ||A\\mathbf{x}||_p\\), where \\(||\\cdot||_p\\) is the vector norm.<ul> <li>\\(||A||_\\infty = \\max\\limits_{1\\le i \\le n}\\sum\\limits_{j=1}^n|a_{ij}|\\).</li> <li>\\(||A||_1= \\max\\limits_{1\\le j \\le n}\\sum\\limits_{i=1}^n|a_{ij}|\\).</li> <li>(Spectral Norm) \\(||A||_2= \\sqrt{\\lambda_{max}(A^TA)}\\).</li> </ul> </li> </ul> <p>Corollary 7.2</p> <p>For any vector \\(\\mathbf{x} \\ne 0\\), matrix \\(A\\), and any natural norm \\(||\\cdot||\\), we have</p> \\[     ||A\\mathbf{x}|| \\le ||A|| \\cdot ||\\mathbf{x}||. \\]"},{"location":"Mathematics_Basis/NA/Chap_7/#eigenvalues-and-eigenvectors","title":"Eigenvalues and Eigenvectors","text":"Definition 7.4 (Recap) <p>If \\(A\\) is a square matrix, the characteristic polynomial of \\(A\\) is defined by</p> \\[     p(\\lambda) = \\text{det}(A - \\lambda I). \\] <p>The roots of \\(p\\) are eigenvalues. If \\(\\lambda\\) is an eigenvalue and \\(\\mathbf{x} \\ne 0\\) satisfies \\((A - \\lambda I)\\mathbf{x} = \\mathbf{0}\\), then \\(\\mathbf{x}\\) is an eigenvector.</p>"},{"location":"Mathematics_Basis/NA/Chap_7/#spectral-radius","title":"Spectral Radius","text":"<p>Definition 7.5</p> <p>The spectral radius of a matrix \\(A\\) is defined by</p> \\[     \\rho(A) = \\max|\\lambda|,\\ \\ \\text{ where $\\lambda$ is an eigenvalue of $A$}. \\] <p>(Recap that for complex \\(\\lambda = \\alpha + \\beta i\\), \\(|\\lambda| = \\sqrt{\\alpha^2 + \\beta^2}\\).)</p> <p>Theorem 7.3</p> <p>\\(\\forall\\ A \\in M_n(\\mathbb{R})\\),</p> <ul> <li>\\(||A||_2 = \\sqrt{\\rho(A^tA)}\\).</li> <li>\\(\\rho(A) \\le ||A||\\), for any natural norm \\(||\\cdot||\\).</li> </ul> Proof <p>A proof for the second property. Suppose \\(\\lambda\\) is an eigenvalue of \\(A\\) with eigenvector \\(\\mathbf{x}\\) and \\(||\\mathbf{x}|| = 1\\),</p> \\[     |\\lambda| = |\\lambda| \\cdot ||\\mathbf{x}|| = ||\\lambda \\mathbf{x}|| \\le ||A|| \\cdot ||\\mathbf{x}|| = ||A||. \\] <p>Thus,</p> \\[     \\rho(A) = \\max|\\lambda| \\le ||A||. \\]"},{"location":"Mathematics_Basis/NA/Chap_7/#convergence-of-matrix","title":"Convergence of Matrix","text":"<p>Definition 7.6</p> <p>\\(A \\in M_n(\\mathbb{R}))\\) is convergent if</p> \\[     \\lim_{k \\rightarrow \\infty}\\left(A^k\\right)_{ij} = 0,\\ \\     \\text{ for each } i = 1, 2, \\dots, n \\text{ and } j = 1, 2, \\dots, n. \\] <p>Theorem 7.4</p> <p>The following statements are equivalent.</p> <ul> <li>\\(A\\) is a convergent matrix.</li> <li>\\(\\lim\\limits_{n \\rightarrow \\infty} ||A^n|| = 0\\), for some natural norm.</li> <li>\\(\\lim\\limits_{n \\rightarrow \\infty} ||A^n|| = 0\\), for all natural norms.</li> <li>\\(\\rho(A) &lt; 1\\).</li> <li>\\(\\forall\\ \\mathbf{x}\\), \\(\\lim\\limits_{n \\rightarrow \\infty} ||A^n \\mathbf{x}|| = \\mathbf{0}\\). </li> </ul>"},{"location":"Mathematics_Basis/NA/Chap_7/#iterative-techniques-for-solving-linear-systems","title":"Iterative Techniques for Solving Linear Systems","text":"\\[     A\\mathbf{x} = \\mathbf{b} \\Leftrightarrow  (D - L - U)\\mathbf{x} = \\mathbf{b} \\Leftrightarrow D\\mathbf{x} = (L + U)\\mathbf{x} + \\mathbf{b} \\\\ \\] <p>Thus,</p> \\[     \\mathbf{x} = D^{-1}(L + U)\\mathbf{x} + D^{-1}\\mathbf{b}. \\]"},{"location":"Mathematics_Basis/NA/Chap_7/#jacobi-iterative-method","title":"Jacobi Iterative Method","text":"<p>Let \\(T_j = D^{-1}(L+U)\\) and \\(\\mathbf{c}_\\mathbf{j} = D^{-1}\\mathbf{b}\\), then</p> \\[     \\mathbf{x}^{(k)} = T_j\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\mathbf{j}. \\]"},{"location":"Mathematics_Basis/NA/Chap_7/#gauss-seidel-iterative-method","title":"Gauss-Seidel Iterative Method","text":"\\[     \\small     \\mathbf{x}^{(k)} = D^{-1}(L\\mathbf{x}^{(k)} + U\\mathbf{x}^{(k - 1)}) + D^{-1}\\mathbf{b} \\Leftrightarrow \\mathbf{x}^{(k)} = (D - L)^{-1}U\\mathbf{x}^{(k - 1)} + (D - L)^{-1}\\mathbf{b} \\] <p>Let \\(T_g = (D - L)^{-1}U\\) and \\(\\mathbf{c}_\\mathbf{g} = (D - L)^{-1}\\mathbf{b}\\), then</p> \\[     \\mathbf{x}^{(k)} = T_g\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\mathbf{g}. \\]"},{"location":"Mathematics_Basis/NA/Chap_7/#convergence","title":"Convergence","text":"<p>Consider the following formula</p> \\[     \\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c}, \\] <p>where \\(\\mathbf{x}^{(0)}\\) is arbitrary.</p> <p>Lemma 7.5</p> <p>If \\(\\rho(T) \\lt 1\\) , then \\((I - T)^{-1}\\) exists and </p> \\[     (I - T)^{-1} = \\sum\\limits_{j = 0}^\\infty T^j. \\] Proof <p>Suppose \\(\\lambda\\) is an eigenvalue of \\(T\\) with eigenvector \\(\\mathbf{x}\\), then since \\(T\\mathbf{x} = \\lambda \\mathbf{x} \\Leftrightarrow (I - T)\\mathbf{x} = (1 - \\lambda)\\mathbf{x}\\), thus \\(1 - \\lambda\\) is an eigenvalue of \\(I - T\\). Since \\(|\\lambda| \\le \\rho(T) &lt; 1\\), thus \\(\\lambda = 1\\) is not an eigenvalue of \\(T\\) and \\(0\\) is not an eigenvalue of \\(I - T\\). Hence, \\((I - T)^{-1}\\) exists.</p> <p>Let \\(S_m = I + T + T^2 + \\cdots + T^m\\), then</p> \\[     (I - T)S_m = (1 + T + \\cdots + T^m) - (T + T^2 + \\cdots + T^{m + 1}) = I - T^{m + 1}. \\] <p>Since \\(T\\) is convergent, thus</p> \\[     \\lim\\limits_{m \\rightarrow \\infty} (I - T)S_m = \\lim\\limits_{m \\rightarrow \\infty}(I - T^{m + 1}) = I. \\] <p>Thus, \\((I - T)^{-1} = \\lim\\limits_{m \\rightarrow \\infty}S_m = \\sum\\limits_{j = 0}^\\infty T^j\\).</p> <p>Theorem 7.6</p> <p>\\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) defined by \\(\\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c}\\) converges to the unique solution of</p> \\[     \\mathbf{x} = T\\mathbf{x} + \\mathbf{c} \\] <p>iff</p> \\[     \\rho(T) \\lt 1. \\] Proof <p>\\(\\Rightarrow\\):</p> <p>Define error \\(\\mathbf{e}^{(k)} = \\mathbf{x} - \\mathbf{x}^{(k)}\\), then</p> \\[     \\mathbf{e}^{(k)} = (T\\mathbf{x} + c) - (T\\mathbf{x}^{(k - 1)} + c) = T(\\mathbf{x} - \\mathbf{x}^{(k - 1)})T\\mathbf{e}^{(k - 1)}     \\Rightarrow \\mathbf{e}^{(k)} = T^k \\mathbf{e}^{(0)}. \\] <p>Since it converges, thus</p> \\[     \\lim\\limits_{k \\rightarrow \\infty} \\mathbf{e}^{(k)} = 0     \\Rightarrow      \\forall\\ \\mathbf{e}^{(0)},\\ \\ \\lim\\limits_{k \\rightarrow \\infty} T^k \\mathbf{e}^{(0)} = 0 \\] \\[     \\Leftrightarrow     \\rho(T) &lt; 1. \\] <p>\\(\\Leftarrow\\):</p> \\[     \\mathbf{x}^{(k)} = T^{k}\\mathbf{x}^{(0)} + (T^{k - 1} + \\cdots + T + I) \\mathbf{c}. \\] <p>Since \\(\\rho(T) &lt; 1\\), \\(T\\) is convergent and</p> \\[     \\lim\\limits_{k \\rightarrow \\infty} T^k \\mathbf{x}^{(0)} = \\mathbf{0}. \\] <p>From Lemma 7.5, we have</p> \\[     \\lim\\limits_{k \\rightarrow \\infty} \\mathbf{x}^{(k)} = \\lim\\limits_{k \\rightarrow \\infty} T^k\\mathbf{x}^{(0)} + \\left(\\sum\\limits_{j = 0}^\\infty T^j \\right)\\mathbf{c} = \\mathbf{0} + (I - T)^{-1}\\mathbf{c} = (I - T)^{-1}\\mathbf{c}. \\] <p>Thus \\(\\{\\mathbf{x}^{(k)}\\}\\) converges to \\(\\mathbf{x} \\equiv (I - T)^{-1} \\Leftrightarrow \\mathbf{x} = T\\mathbf{x} + c\\). </p> <p>Corollary 7.7</p> <p>If \\(||T||\\lt 1\\) for any matrix norm and \\(\\mathbf{c}\\) is a given vector, then \\(\\forall\\ \\mathbf{x}^{(0)}\\in \\mathbb{R}^n\\) and \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) defined by \\(\\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c}\\) converges to \\(\\mathbf{x}\\), and the following error bounds hold</p> <ul> <li>\\(||\\mathbf{x} - \\mathbf{x}^{(k)}|| \\le ||T||^k||\\mathbf{x}^{(0)} - \\mathbf{x}||\\).</li> <li>\\(||\\mathbf{x} - \\mathbf{x}^{(k)}|| \\le \\dfrac{||T||^k}{1 - ||T||}||\\mathbf{x}^{(1)} - \\mathbf{x}||\\).</li> </ul> <p>Theorem 7.8</p> <p>Suppose \\(A\\) is strictly diagonally dominant, then \\(\\forall\\ \\mathbf{x}^{(0)}\\), both Jacobi and Gauss-Seidel methods give \\(\\{\\mathbf{x}^{(k)}\\}_{k=0}^\\infty\\) that converge to the unique solution of \\(A\\mathbf{x} = \\mathbf{b}\\).</p>"},{"location":"Mathematics_Basis/NA/Chap_7/#relaxation-methods","title":"Relaxation Methods","text":"<p>Definition 7.7</p> <p>Suppose \\(\\mathbf{\\tilde x}\\) is an approximation to the solution of \\(A\\mathbf{x} = \\mathbf{b}\\), then the residual vector for \\(\\mathbf{\\tilde x}\\) w.r.t this linear system is</p> \\[     \\mathbf{r} = \\mathbf{b} - A\\mathbf{\\tilde x}. \\] <p>Further examine Gauss-Seidel method.</p> \\[ x_i^{(k)} = x_i^{(k - 1)} + \\frac{r_i^{(k)}}{a_{ii}},\\ \\ \\text{ where } r_i^{(k)} = b_i - \\sum_{j \\lt i} a_{ij}x_j^{(k)} - \\sum_{j \\ge i} a_{ij}x_j^{(k - 1)}. \\] <p>Let \\(x_i^{(k)} = x_i^{(k - 1)} + \\omega\\dfrac{r_i^{(k)}}{a_{ii}}\\), by modifying the value of \\(\\omega\\), we can somehow get faster convergence.</p> <ul> <li>\\(0 \\lt \\omega \\lt 1\\) Under-Relaxation Method</li> <li>\\(\\omega = 1\\) Gauss-Seidel Method</li> <li>\\(\\omega \\gt 1\\) Successive Over-Relaxation Method (SOR)</li> </ul> <p>In matrix form,</p> \\[     \\mathbf{x}^{(k)} = (D - \\omega L)^{-1}[(1 - \\omega)D + \\omega U]\\mathbf{x}^{(k - 1)} + (D - \\omega L)^{-1}\\mathbf{b}. \\] <p>Let \\(T_\\omega = (D - \\omega L)^{-1}[(1 - \\omega)D + \\omega U]\\) and \\(\\mathbf{c}_\\omega = (D - \\omega L)^{-1}\\mathbf{b}\\), then</p> \\[     \\mathbf{x}^{(k)} = T_\\omega\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\omega. \\] <p>Theorem 7.9 (Kahan)</p> <p>If \\(a_{ii} \\ne 0\\), then \\(\\rho(T_\\omega)\\ge |\\omega -1 |\\), which implies that SOR method can converge only if</p> \\[     0 \\lt \\omega \\lt 2. \\] Proof <p>Recap that upper and lower triangular determinant are equal to the product of the entries at its diagnoal.</p> <p>Since \\(D\\) is diagonal, \\(L\\) and \\(U\\) are lower and upper triangular matrix, thus</p> \\[ \\begin{aligned}     \\text{det}(T_\\omega) &amp;= \\text{det}((D - \\omega L)^{-1}) \\cdot \\text{det}((1 - \\omega)D + \\omega U)      \\\\ &amp;= \\text{det}(D^{-1})(1-\\omega)^n\\text{det}(D) = (1 - \\omega)^n. \\end{aligned} \\] <p>On the other hand, recap that</p> \\[     \\text{det}(T_\\omega) = \\prod\\limits_{i = 1}^n \\lambda_i,\\ \\ \\text{ where $\\lambda_i$ are eigenvalues of $T$}. \\] <p>Thus</p> \\[     \\rho(T_\\omega) = \\max\\limits_{1 \\le i \\le n} |\\lambda_i| \\ge |\\omega - 1|. \\] <p>Theorem 7.10 (Ostrowski-Reich)</p> <p>If \\(A\\) is positive definite and \\(0 \\lt \\omega \\lt 2\\), the SOR method converges for any choice of initial approximation vector \\(\\mathbf{x}^{(0)}\\).</p> <p>Theorem 7.11</p> <p>If \\(A\\) is positive definite and tridiagonal, then \\(\\rho(T_g) = \\rho^2(T_j)\\lt1\\), and the optimal choice of \\(\\omega\\) for the SOR method is</p> \\[     \\omega = \\frac{2}{1 + \\sqrt{1 - \\rho(T_j)^2}}. \\] <p>With this choice of \\(\\omega\\), we have \\(\\rho(T_\\omega) = \\omega - 1\\).</p>"},{"location":"Mathematics_Basis/NA/Chap_7/#error-bounds-and-iterative-refinement","title":"Error Bounds and Iterative Refinement","text":"<p>Definition 7.8</p> <p>The conditional number of the nonsigular matrix \\(A\\) relative to a norm \\(||\\cdot||\\) is</p> \\[     K(A) = ||A|| \\cdot ||A^{-1}||. \\] <p>A matrix \\(A\\) is well-conditioned if \\(K(A)\\) is close to \\(1\\), and is ill-conditioned when \\(K(A)\\) is significantly greater than \\(1\\).</p> <p>Proposition</p> <ul> <li>If \\(A\\) is symmetric, then \\(K(A)_2 = \\dfrac{\\max|\\lambda|}{\\min|\\lambda|}\\).</li> <li>\\(K(A)_2 = 1\\) if \\(A\\) is orthogonal.</li> <li>\\(\\forall\\ \\text{ orthogonal matrix } R\\), \\(K(RA)_2 = K(AR)_2 = K(A)_2\\).</li> <li>\\(\\forall\\ \\text{ natural norm } ||\\cdot||_p\\), \\(K(A)_p \\ge 1\\).</li> <li>\\(K(\\alpha A) = K(A)\\).</li> </ul> <p>Theorem 7.12</p> <p>For any natural norm \\(||\\cdot||\\),</p> \\[     ||\\mathbf{x} - \\mathbf{\\tilde x}|| \\le ||\\mathbf{r}|| \\cdot ||A^{-1}||, \\] <p>and if \\(\\mathbf{x} \\ne \\mathbf{0}\\) and \\(\\mathbf{b} \\ne \\mathbf{0}\\),</p> \\[     \\frac{||\\mathbf{x} - \\mathbf{\\tilde x}||}{||\\mathbf{x}||} \\le ||A||\\cdot||A^{-1}|| \\frac{||\\mathbf{r}||}{||\\mathbf{b}||} = K(A)\\frac{||\\mathbf{r}||}{||\\mathbf{b}||}. \\] <p>Iterative Refinement</p> <p>Step.1 Solve \\(A\\mathbf{x} = \\mathbf{b}\\) and get an approximation solution \\(\\mathbf{x}_{0}\\). Let \\(i = 1\\).</p> <p>Step.2 Let \\(\\mathbf{r} = \\mathbf{b} - A\\mathbf{x}_{i - 1}\\).</p> <p>Step.3 Solve \\(A\\mathbf{d} = \\mathbf{r}\\) and get the solution \\(\\mathbf{d}\\).</p> <p>Step.4 The better approximation is \\(\\mathbf{x}_{i} = \\mathbf{x}_{i - 1} + \\mathbf{d}.\\)</p> <p>Step.5 Judge whether it's precise enough. </p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 If not, let \\(i = i + 1\\) and then repeat from Step.2.</p> <p>In reality, \\(A\\) and \\(\\mathbf{b}\\) may be perturbed by an amount \\(\\delta A\\) and \\(\\delta \\mathbf{b}\\).</p> <p>For \\(A(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b} + \\delta\\mathbf{b}\\),</p> \\[     \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le ||A|| \\cdot ||A^{-1}|| \\cdot \\frac{||\\delta \\mathbf{b}||}{||\\mathbf{b}||}. \\] <p>For \\((A + \\delta A)(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b}\\),</p> \\[     \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le \\frac{||A^{-1}|| \\cdot ||\\delta A||}{1 - ||A^{-1}|| \\cdot ||\\delta A||} =      \\frac{||A|| \\cdot ||A^{-1}|| \\cdot \\frac{||\\delta A||}{||A||}}{1 - ||A|| \\cdot ||A^{-1}|| \\cdot\\frac{||\\delta A||}{||A||}}. \\] <p>Theorem 7.13</p> <p>If \\(A\\) is nonsingular and </p> \\[     ||\\delta A|| \\lt \\frac{1}{||A^{-1}||}, \\] <p>then \\((A + \\delta A)(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b} + \\delta\\mathbf{b}\\) with the error estimate</p> \\[     \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le \\frac{K(A)}{1 - K(A)\\frac{||\\delta A||}{||A||}}\\left(\\frac{||\\delta A||}{||A||} + \\frac{||\\delta\\mathbf{b}||}{||\\mathbf{b}||}\\right). \\]"},{"location":"Mathematics_Basis/NA/Chap_8/","title":"Chapter 8 | Approximation Theory","text":"<p>Abstract</p> <p>In this course, we only discuss the minimax and least square parts, and only make \\(P(x)\\) a polynomial function.</p>"},{"location":"Mathematics_Basis/NA/Chap_8/#target","title":"Target","text":"<p>Given \\(x_1, \\dots, x_m\\) and \\(y_1, \\dots, y_m\\) sampled from a funciton \\(y = f(x)\\), or the continuous function \\(f(x)\\), \\(x \\in [a, b]\\) itself, find a simpler function \\(P(x) \\approx f(x)\\).</p>"},{"location":"Mathematics_Basis/NA/Chap_8/#measurement-of-error","title":"Measurement of Error","text":"<ul> <li>(Minimax) minimize<ul> <li>(discrete) \\(E_\\infty = \\max\\limits_{1 \\le i \\le m}|P(x_i) - y_i|\\).</li> <li>(continuous) \\(E_\\infty = \\max\\limits_{a \\le x \\le b}|P(x) - f(x)|\\).</li> </ul> </li> <li>(Absolute Deviation) minimize<ul> <li>(discrete) \\(E_1 = \\sum\\limits_{i = 1}^{m}|P(x_i) - y_i|\\).</li> <li>(continuous) \\(E_1 = \\int\\nolimits_{a}^{b}|P(x) - f(x)|dx\\).</li> </ul> </li> <li>(Least Squares Method) minimize<ul> <li>(discrete) \\(E_2 = \\sum\\limits_{i = 1}^{m}|P(x_i) - y_i|^2\\).</li> <li>(continuous) \\(E_2 = \\int\\nolimits_{a}^{b}|P(x) - f(x)|^2dx\\).</li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/NA/Chap_8/#general-least-squares-approximation","title":"General Least Squares Approximation","text":"<p>Definition 8.0</p> <p>\\(w\\) is called a weight function if</p> <ul> <li> <p>(discrete)</p> \\[     \\forall\\ i \\in \\mathbb{N},\\ \\ w_i &gt; 0. \\] </li> <li> <p>(continuous)     \\(w\\) is an integrable function and on the interval \\(I\\),</p> \\[     \\forall\\ x \\in I,\\ \\ w(x) \\ge 0, \\] \\[     \\forall\\ I' \\subseteq I,\\ \\ w(x) \\not\\equiv 0. \\] </li> </ul> <p>Considering the weight function, the least square method can be more general as below.</p> <ul> <li>(discrete) \\(E_2 = \\sum\\limits_{i = 1}^{m}w_i|P(x_i) - y_i|^2\\).</li> <li>(continuous) \\(E_2 = \\int\\nolimits_{a}^{b}w(x)|P(x) - f(x)|^2dx\\).</li> </ul>"},{"location":"Mathematics_Basis/NA/Chap_8/#discrete-least-squares-approximation","title":"Discrete Least Squares Approximation","text":""},{"location":"Mathematics_Basis/NA/Chap_8/#target_1","title":"Target","text":"<p>Approximate a set of data \\(\\{(x_i, y_i) | i = 1, 2, \\dots, m\\}\\), with an algebraic polynomial</p> \\[     P_n(x) = a_nx^n + a_{n - 1}x^{n - 1} + \\cdots + a_1x + a_0, \\] <p>of degree \\(n &lt; m - 1\\) (in most case \\(n \\ll m\\)), with the least squares measurement, w.r.t. and weight function \\(w_i \\equiv 1\\).</p>"},{"location":"Mathematics_Basis/NA/Chap_8/#solution","title":"Solution","text":"\\[ \\begin{aligned}     E_2 &amp;= \\sum\\limits_{i = 1}^m (y_i - P_n(x_i))^2 \\\\         &amp;= \\sum\\limits_{i = 1}^m y_i^2 - 2\\sum\\limits_{i = 1}^m P_n(x_i)y_i + \\sum\\limits_{i = 1}^m P_n^2(x_i) \\\\         &amp;= \\sum\\limits_{i = 1}^m y_i^2 - 2\\sum\\limits_{i = 1}^m \\left(\\sum\\limits_{j = 0}^n a_jx_i^j\\right)y_i + \\sum\\limits_{i = 1}^m \\left(\\sum\\limits_{j = 0}^n a_jx_i^j \\right)^2 \\\\         &amp;= \\sum\\limits_{i = 1}^m y_i^2 - 2\\sum\\limits_{j = 0}^n a_j \\left( \\sum\\limits_{i = 1}^m y_i x_i^j \\right) + \\sum\\limits_{j = 0}^n \\sum\\limits_{k = 0}^n a_ja_k \\left(\\sum\\limits_{i = 1}^m x_i^{j + k} \\right). \\end{aligned} \\] <p>The necessary condition to minimize \\(E_2\\) is</p> \\[     0 = \\frac{\\partial E_2}{\\partial a_j} = -2 \\sum\\limits_{i = 1}^m y_i x_i^j + 2 \\sum\\limits_{k = 0}^n a_k \\sum\\limits_{i = 1}^m x_i^{j + k}. \\] <p>Then we get the \\(n + 1\\) normal equations with \\(n + 1\\) unknown \\(a_j\\),</p> \\[    \\sum\\limits_{k = 0}^n a_k \\sum\\limits_{i = 1}^m x_i^{j + k} = \\sum\\limits_{i = 1}^m y_i x_i^j. \\] <p>Let \\(b_k = \\sum\\limits_{i = 1}^m x_i^k\\) and \\(c_k = \\sum\\limits_{i = 1}^m y_i x_i^k\\), we can represent the normal equations by</p> \\[ \\begin{bmatrix}     b_{0 + 0} &amp; \\cdots &amp; b_{0 + n} \\\\     \\vdots &amp; \\ddots &amp; \\vdots \\\\     b_{n + 0} &amp; \\cdots &amp; b_{n + n} \\end{bmatrix} \\begin{bmatrix}     a_0 \\\\ \\vdots \\\\ a_n \\end{bmatrix} = \\begin{bmatrix}     c_0 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\] <p>Theorem 8.0</p> <p>Normal equations have a unique solution if \\(x_i\\) are distinct.</p> Proof <p>Suppose \\(X\\) is an \\(n + 1 \\times m\\) Vandermonde Matrix, which is</p> \\[     X = \\begin{bmatrix}     1 &amp; x_1 &amp; x_1^2 &amp; \\cdots &amp; x_1^n \\\\     1 &amp; x_2 &amp; x_2^2 &amp; \\cdots &amp; x_2^n \\\\     \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\     1 &amp; x_m &amp; x_m^2 &amp; \\cdots &amp; x_m^n \\\\     \\end{bmatrix}, \\] <p>and let \\(\\mathbf{y} = (y_1, y_1, \\cdots, y_m)^T\\).</p> <p>Then the normal equations can be represented by (notice the dimension of matrices and vectors),</p> \\[     X X^T \\mathbf{a} = X \\mathbf{y}. \\] <p>Since \\(x_i\\) are distinct, \\(X\\) is a column full rank matrix, namely</p> \\[     \\text{rank}(X) = n + 1. \\] <p>Since \\(x_i \\in \\mathbb{R}\\), thus</p> \\[     \\text{rank}(X X^T) = \\text{rank}(X) = n + 1. \\] <p>Hence the normal equations have a unique solution.</p>"},{"location":"Mathematics_Basis/NA/Chap_8/#logarithmic-linear-least-squares","title":"Logarithmic Linear Least Squares","text":"<p>To approximate by the function of the form</p> \\[     y = be^{ax}, \\] <p>or</p> \\[     y = bx^a, \\] <p>we can consider the logarithm of the equation by</p> \\[     \\ln y = \\ln b + ax, \\] <p>and</p> \\[     \\ln y = \\ln b + a \\ln x. \\] <p>Note</p> <p>It's a simple algrebric transformation. But we should point out that it minimize the logarithmic linear least squares, but not linear least squares.</p> <p>Just consider the arguments to minimize the following two errors,</p> \\[     E = \\sum\\limits_{i = 1}^m (y_i - be^{ax_i})^2, \\] <p>and</p> \\[     E' = \\sum\\limits_{i = 1}^m (\\ln y - (\\ln b + ax))^2, \\] <p>they are slightly different actually.</p>"},{"location":"Mathematics_Basis/NA/Chap_8/#continuous-least-squares-approximation","title":"Continuous Least Squares Approximation","text":"<p>Now we consider the continuous function instead of discrete points.</p>"},{"location":"Mathematics_Basis/NA/Chap_8/#target_2","title":"Target","text":"<p>Approxiate function \\(f \\in C[a, b]\\), with an algebraic polynomial</p> \\[     P_n(x) = a_nx^n + a_{n - 1}x^{n - 1} + \\cdots + a_1x + a_0, \\] <p>with the least squares measurement, w.r.t the weight function \\(w(x) \\equiv 1\\).</p>"},{"location":"Mathematics_Basis/NA/Chap_8/#solution_1","title":"Solution","text":"<p>Problem</p> <p>Similarly to the discrete situation, we can derive the normal equations by making</p> \\[     0 = \\frac{\\partial E}{\\partial a_j}, \\] <p>and we get the normal equations</p> \\[     \\sum\\limits_{k = 0}^n a_k \\int_{a}^{b} x^{j + k} dx = \\int_{a}^{b} x^j f(x)dx. \\] <p>However, notice that</p> \\[     \\int_{a}^{b} x^{j + k} dx = \\frac{b^{j + k + 1} - a^{j + k + 1}}{j + k + 1}. \\] <p>Thus the coefficient of the linear system is a Hilbert matrix, which has a large conditional number. In actual numerical calculation, this gives a large roundoff error.</p> <p>Another disadvantage is that we can not easily get \\(P_{n + 1}(x)\\) from \\(P_{n}(x)\\), similarly with the discussion of Lagrange interpolation.</p> <p>Hence we introduce a different solution based on the concept of orthogonal polynomials.</p> <p>Definition 8.1</p> <p>The set of function \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is linearly independent on \\([a, b]\\) if, whenever</p> \\[     \\forall\\ x \\in [a, b], c_0\\varphi_0(x) + c_1\\varphi_1(x) + \\cdots + c_n\\varphi_n(x) = 0 \\] <p>we have \\(c_0 = c_1 = \\cdots = c_n = 0\\). Otherwise it's linearly dependent.</p> <p>Theorem 8.1</p> <p>If \\(\\text{deg}(\\varphi_j(x)) = j\\), then \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is linearly independent on any interval \\([a, b]\\).</p> <p>Theorem 8.2</p> <p>\\(\\Pi_n\\) is the linear space spanned by \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\), where \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is linear independent, and \\(\\Pi_n\\) is the set of all polynomials of degree at most n.</p> <p>Definition 8.2</p> <p>For the linear independent set \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\), \\(\\forall P(x) \\in \\Pi_n\\), \\(P(x) = \\sum\\limits_{j = 0}^n \\alpha_j \\varphi_j(x)\\) is called a generalized polynomial.</p> <p>Definition 8.3</p> <p>Inner product w.r.t the weight function \\(w\\) is defined and denoted by</p> <ul> <li>(discrete) \\((f, g) = \\sum\\limits_{i = 1}^{m} w_i f(x_i)g(x_i)\\)</li> <li>(continuous) \\((f, g) = \\int\\nolimits_{a}^{b} w(x) f(x) g(x) dx\\).</li> </ul> <p>\\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is an orthogonal set of functions for the interval \\([a,b]\\) w.r.t the weight function \\(w\\) if</p> \\[     (\\varphi_j, \\varphi_k) = \\left\\{     \\begin{aligned}         &amp; 0, &amp;&amp; j \\ne k, \\\\         &amp; \\alpha_k &gt; 0, &amp;&amp; j = k.     \\end{aligned}     \\right. \\] <p>In addition, if \\(\\alpha_k = 1\\), then the set is orthonormal \uff08\u5355\u4f4d\u6b63\u4ea4\uff09.</p> Motivation of Orthogonality <p>Considering \\(w(x)\\), then the normal equations can be represented by</p> \\[     \\int_a^b w(x)f(x)\\varphi_j(x)dx = \\sum\\limits_{k = 0}^n a_k \\int_a^b w(x)\\varphi_k(x)\\varphi_j(x)dx. \\] <p>If we define the orthogonal set of functions as above, the equations reduce to</p> \\[     \\int_a^b w(x)f(x)\\varphi_j(x)dx = a_j \\alpha_j. \\] <p>Theorem 8.3</p> <p>\\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is an orthogonal set of functions on the interval \\([a, b]\\) w.r.t. the weight function \\(w\\), then the least squares approximation to \\(f\\) on \\([a, b]\\) w.r.t. \\(w\\) is</p> \\[     P(x) = \\sum\\limits_{k = 0}^n a_k \\varphi_k(x), \\] <p>where, for each \\(k = 0, 1, \\dots, n\\),</p> \\[     a_k = \\frac{\\int_a^b w(x) \\varphi_k(x)f(x)dx}{\\int_a^b w(x) \\varphi_k^2(x)dx} = \\frac{(\\varphi_k, f)}{(\\varphi_{k}, \\varphi_{k})} = \\frac{(\\varphi_k, f)}{\\alpha_k}. \\] <p>Base on the Gram-Schmidt Process, we have the following theorem to construct the orthogonal polynomials on \\([a, b]\\) w.r.t a weight function \\(w\\).</p> <p>Theorem 8.4</p> <p>The set of polynomial functions \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) defined in the following way is orthogonal on \\([a, b]\\) w.r.t. the weight function \\(w\\).</p> \\[ \\begin{aligned}     &amp; \\varphi_0(x) \\equiv 1,\\ \\varphi_1(x) = x - B_1, \\\\     &amp; \\varphi_k(x) = (x - B_k) \\varphi_{k - 1}(x) - C_k \\varphi_{k - 2}(x), k \\ge 2. \\\\ \\end{aligned} \\] <p>where</p> \\[     B_k = \\frac{(x\\varphi_{k - 1}, \\varphi_{k - 1})}{\\varphi_{k - 1}, \\varphi_{k - 1}},\\ \\     C_k = \\frac{(x\\varphi_{k - 1}, \\varphi_{k - 2})}{\\varphi_{k - 2}, \\varphi_{k - 2}}. \\]"},{"location":"Mathematics_Basis/NA/Chap_8/#minimax-approximation","title":"Minimax Approximation","text":"<p>The minimax approximation to minimize \\(||P - f||_\\infty||\\) has the following properties.</p> <p>Proposition</p> <ul> <li>If \\(f \\in C[a, b]\\) and \\(f\\) is not a polynomial of degree \\(n\\), then there exists a unique polynomial \\(P(x)\\) s.t. \\(||P - f||\\infty\\) is minimized.</li> <li>\\(P(x)\\) exists and must have both \\(+\\) and \\(-\\) deviation points.</li> <li> <p>(Chebyshev Theorem) The \\(n\\) degree \\(P(x)\\) minimizes \\(||P - f||_\\infty\\) \\(\\Leftrightarrow\\) \\(P(x)\\) has at least \\(n + 2\\) alternating \\(+\\) and \\(-\\) deviation points w.r.t. \\(f\\). That is, there is a set of points \\(a \\le t_1 &lt; \\dots &lt; t_{n + 2} \\le b\\) (called the Chebyshev alternating sequence) s.t.</p> \\[     P(t_k) - f(t_k) = (-1)^k||P - f||_\\infty. \\] </li> </ul> <p> </p> <p>Here we introduce Chebyshev polynomials to deal with \\(E_\\infty\\) error, and by the way, we can use it to economize the power series.</p>"},{"location":"Mathematics_Basis/NA/Chap_8/#chebyshev-polynomials","title":"Chebyshev Polynomials","text":"<p>Chebyshev polynomials are defined concisely by</p> \\[     T_n(x) = \\cos(n \\arccos x), \\] <p>or equally defined recursively by</p> \\[ \\begin{aligned}     &amp; T_0(x) = 1,\\ T_1(x) = x, \\\\     &amp; T_{n + 1}(x) = 2x T_{n}(x) - T_{n - 1}(x). \\end{aligned} \\] <p>Property</p> <ul> <li> <p>\\(\\{T_n(x)\\}\\) are orthogonal on \\((-1, 1)\\) w.r.t the weight function</p> \\[     w(x) = \\frac{1}{\\sqrt{1 - x^2}}. \\] \\[     (T_n, T_m) = \\int_{-1}^1 \\frac{T_n(x)T_m(x)}{\\sqrt{1 - x^2}}dx = \\left\\{     \\begin{aligned}         &amp; 0, &amp;&amp; n \\ne m, \\\\         &amp; \\pi, &amp;&amp; n = m = 0, \\\\         &amp; \\frac{\\pi}{2}, &amp;&amp; n = m \\ne 0.     \\end{aligned}     \\right. \\] </li> <li> <p>\\(T_n(x)\\) is a polynomial of degree \\(n\\) with the leading coefficient \\(2^{n - 1}\\).</p> </li> <li> <p>\\(T_n(x)\\) has \\(n\\) zero points at</p> \\[     \\bar{x}_k = \\cos \\left(\\frac{2k-1}{2n}\\pi\\right),\\ k = 1, 2, \\dots, n. \\] </li> <li> <p>\\(T_n(x)\\) has extrema at</p> \\[     \\bar{x}'_k = \\cos \\frac{k\\pi}n \\text{, with }     T(\\bar{x}'_k) = (-1)^k,\\ k = 1, 2, \\dots, n. \\] </li> </ul>"},{"location":"Mathematics_Basis/NA/Chap_8/#monic-chebyshev-polynomials","title":"Monic Chebyshev Polynomials","text":"<p>Monic chebyshev polynomials are defined by</p> \\[     \\tilde T_0(x) = 1,\\ \\tilde T_n(x) = \\frac{1}{2^{n - 1}} T_n(x). \\] <p>The following is an important theorem for the position of Chebyshev polynomials.</p> <p>Theorem 8.5</p> \\[     \\frac{1}{2^{n - 1}} = \\max\\limits_{x \\in [-1, 1]}|\\tilde T_n(x)| \\le \\max\\limits_{x \\in [-1, 1]}|P_n(x)|,\\ \\forall\\ P_n(x) \\in \\tilde \\Pi_n, \\] <p>where \\(\\tilde \\Pi_n\\) denotes the set of all monic polynomials of degree n.</p> <p>From theorem 8.5, we can answer where to place interpolating points to minimize the error in Lagrange interpolation. Recap that</p> \\[     R(x) = f(x) - P(x) = \\frac{f^{n + 1}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^{n}(x - x_i). \\] <p>To minimize \\(R(x)\\), since there is no control over \\(\\xi\\), so we only need to minimize</p> \\[     |w_n(x)| = \\left|\\prod\\limits_{i = 0}^{n}(x - x_i)\\right|. \\] <p>Since \\(w_n(x)\\) is a monic polynomial of degree \\((n + 1)\\), we can obtain minimum when \\(w_n(x) = \\tilde T_{n + 1}(x)\\).</p> <p>To make it equal, we can simply make their zero points equal, namely</p> \\[     x_k = \\bar{x}_{k + 1} = \\cos \\frac{2k + 1}{2(n + 1)}\\pi. \\] <p>Corollary 8.6</p> \\[     \\max\\limits_{x \\in [-1, 1]} |f(x) - P(x)| \\le \\frac{\\max\\limits_{x \\in [-1, 1]}\\left|f^{(n + 1)}(x)\\right|}{2^n (n + 1)!}. \\]"},{"location":"Mathematics_Basis/NA/Chap_8/#solution-of-minimax-approximation","title":"Solution of Minimax Approximation","text":"<p>Now we come back to the minimax approximation. To minimize</p> \\[     E_\\infty = \\max\\limits_{a \\le x \\le b}|P(x) - f(x)|, \\] <p>with a polynomial of degree \\(n\\) on the interval \\([a, b]\\), we need the following steps.</p> <p>Step.1 Find the roots of \\(T_{n + 1}(t)\\) on the interval \\([-1, 1]\\), denoted by \\(t_0, \\dots, t_n\\).</p> <p>Step.2 Extend it to the interval \\([a, b]\\) by</p> \\[     x_i = \\frac12[(b - a)t_i + a + b]. \\] <p>Step.3 Substitue \\(x_i\\) into \\(f(x)\\) to get \\(y_i\\).</p> <p>Step.4 Compute the Langrange polynomial \\(P(x)\\) of the interpolating points \\((x_i, y_i)\\).</p>"},{"location":"Mathematics_Basis/NA/Chap_8/#ecomomization-of-power-series","title":"Ecomomization of Power Series","text":"<p>Chebyshev polynomials can also be used to reduce the degree of an approximating polynomials with a minimal loss of accuracy.</p> <p>Consider approximating</p> \\[     P_n(x) = a_nx^n + a_{n - 1}x^{n - 1} + \\cdots + a_1x + a_0 \\] <p>on [-1, 1].</p> <p>Then the target is to minimize</p> \\[     \\max_{x\\in[-1,1]}|P_n(x) - P_{n - 1}(x)|. \\]"},{"location":"Mathematics_Basis/NA/Chap_8/#solution_2","title":"Solution","text":"<p>Since \\(\\frac{1}{a_n}(P_n(x) - P_{n - 1}(x))\\) is monic, thus</p> \\[     \\max_{x\\in[-1,1]}\\left|\\frac{1}{a_n}((P_n(x) - P_{n - 1}(x))\\right| \\ge \\frac{1}{2^{n - 1}}. \\] <p>Equality occurs when</p> \\[     \\frac{1}{a_n}((P_n(x) - P_{n - 1}(x)) = \\tilde T_n(x). \\] <p>Thus we can choose</p> \\[     P_{n - 1}(x) = P_n(x) - a_n \\tilde T_n(x) \\] <p>with the minimum error value of</p> \\[     \\max_{x\\in[-1,1]}|P_n(x) - P_{n - 1}(x)| = |a_n| \\max_{x\\in[-1,1]}\\left|\\frac{1}{a_n}((P_n(x) - P_{n - 1}(x))\\right| = \\frac{|a_n|}{2^{n - 1}}. \\]"},{"location":"Mathematics_Basis/NA/Chap_9/","title":"Chapter 9 | Approximating Eigenvalues","text":""},{"location":"Mathematics_Basis/NA/Chap_9/#power-method","title":"Power Method","text":"<p>The Power Method is an iterative technique used to determine the dominant eigenvalue of a matrix (the eigenvalue with the largest magnitude).</p> <p>Suppose \\(A \\in M_n(\\mathbb{R})\\) with eigenvalues satisfying \\(|\\lambda_1| \\gt |\\lambda_2| \\ge \\dots \\ge |\\lambda_n|\\) and their corresponding linearly independent eigenvectors \\(\\mathbf{v}_\\mathbf{1}, \\dots, \\mathbf{v}_\\mathbf{n}\\).</p> <p>Original Method</p> <p>Step.1 Start from any \\(\\mathbf{x}^{(0)} \\ne \\mathbf{0}\\) and \\((\\mathbf{x}^{(0)}, \\mathbf{v}_\\mathbf{1}) \\ne 0\\), and suppose \\(\\mathbf{x}^{(0)} = \\sum\\limits_{j = 1}^n\\beta_j\\mathbf{v}_\\mathbf{j}\\), \\(\\beta_1 \\ne 0.\\)</p> <p>Step.2 \\(\\mathbf{x}^{(k)} = A \\mathbf{x}^{(k - 1)} = \\sum\\limits_{j = 1}^n\\beta_j\\lambda_j^k\\mathbf{v}_\\mathbf{j} = \\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}\\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\lambda_1^k\\beta_1\\mathbf{v}_\\mathbf{1}\\).</p> <p>Step.3 \\(A\\mathbf{x}^{(k)} \\approx \\lambda_1^k\\beta_1A\\mathbf{v}_\\mathbf{1} \\approx \\lambda_1\\mathbf{x}^{(k)}\\), thus</p> \\[     \\lambda_1 = \\frac{\\mathbf{x}^{(k + 1)}_i}{\\mathbf{x}^{(k)}_i}, \\mathbf{v}_\\mathbf{1} = \\frac{\\mathbf{x}^{(k)}}{\\lambda_1^k\\beta_1}. \\] <p>Motivation for Normalization: In the original method, if \\(|\\lambda| &gt; 1\\) then \\(\\mathbf{x}^{(k)}\\) diverges. If \\(|\\lambda| &lt; 1\\) then \\(\\mathbf{x}^{(k)}\\) converges to \\(0\\). Both cases are not suitable for actual computing. Thus we need normalization to make sure \\(||\\mathbf{x}||_\\infty = 1\\) at each step to guarantee the stability.</p> <p>Normalization</p> <p>Let \\(\\mathbf{u}^{(k-1)} = \\dfrac{\\mathbf{x}^{(k-1)}}{||\\mathbf{x}^{(k-1)}||_\\infty},  \\mathbf{x}^{(k)} = A\\mathbf{u}^{(k - 1)}\\),</p> <p>then</p> \\[ \\begin{aligned}     &amp; \\mathbf{u}^{(k)} =     \\frac{\\mathbf{x}^{(k)}}{||\\mathbf{x}^{(k)}||_\\infty} =     \\frac{\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}}{\\left|\\left|\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}\\right|\\right|_\\infty}     \\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty},     \\\\     &amp; \\mathbf{x}^{(k)} =     \\frac{A^k\\mathbf{x}_\\mathbf{0}}{||A^{k - 1}\\mathbf{x}_\\mathbf{0}||} =     \\frac{\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}}{\\left|\\left|\\lambda_1^{k - 1} \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^{k - 1}\\mathbf{v}_\\mathbf{j}\\right|\\right|_\\infty} \\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\      \\lambda_1 \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty}=\\lambda_1 \\mathbf{u}^{(k)}. \\end{aligned} \\] <p>Thus </p> \\[ \\begin{aligned}     &amp; \\lambda_1 = \\frac{\\mathbf{x}_i^{(k)}}{\\mathbf{u}_i^{(k)}},\\ \\ \\text{ or more directly }, \\lambda_1 = \\mathbf{x}^{(k)}_{p_k},\\ \\ \\text{ where } \\mathbf{u}^{(k)}_{p_k} = 1 = ||\\mathbf{u}^{(k)}||_\\infty, \\\\     &amp; \\hat{\\mathbf{v}_\\mathbf{1}} = \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty}= \\mathbf{u}^{(k)}. \\end{aligned} \\] <p>Remark</p> <ul> <li>For multiple eigenvalues \\(\\lambda_1 = \\lambda_2 = \\dots = \\lambda_r\\),</li> </ul> \\[     \\mathbf{x}^{(k)} = \\lambda_1^k\\left(     \\sum_{j = 1}^r\\beta_j\\mathbf{v}_\\mathbf{j} +     \\sum_{j = r + 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}      \\right)\\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\      \\lambda_1^k\\sum_{j = 1}^r\\beta_j\\mathbf{v}_\\mathbf{j} \\] <ul> <li>The method fails to converge if \\(\\lambda_1 = -\\lambda_2\\).</li> <li>Aitken's \\(\\Delta^2\\) procedure can be used to speed up the convergence.</li> </ul>"},{"location":"Mathematics_Basis/NA/Chap_9/#rate-of-convergence","title":"Rate of Convergence","text":"<p>Examine \\(\\textbf{x}^{(k)} = \\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\dfrac{\\lambda_j}{\\lambda_1}\\right)^k\\textbf{v}_\\textbf{j}\\). We find that \\(\\left(\\dfrac{\\lambda_j}{\\lambda_1}\\right)^k\\) determines the rate of convergence, especially \\(\\left|\\dfrac{\\lambda_2}{\\lambda_1}\\right|^k\\).</p> <p>Thus, suppose</p> \\[     \\mu^{(k)} = \\mathbf{x}^{(k)}_{p_k},\\ \\ \\text{ where } \\mathbf{u}^{(k)}_{p_k} = 1 = ||\\mathbf{u}^{(k)}||_\\infty, \\\\ \\] <p>Then there is a constant \\(L\\), s.t. for some large \\(m\\),</p> \\[     |\\mu^{(m)} - \\lambda_1| \\approx L \\left|\\frac{\\lambda_2}{\\lambda_1}\\right|^m, \\] <p>which implies that</p> \\[     \\lim\\limits_{m \\rightarrow \\infty} \\frac{|\\mu^{(m + 1)} - \\lambda_1|}{|\\mu^{(m)} - \\lambda_1|} \\approx \\left|\\frac{\\lambda_2}{\\lambda_1}\\right| &lt; 1. \\] <p>Hence the seqence \\(\\{\\mu^{(m)}\\}\\) converges linearly to \\(\\lambda_1\\) and Aitken's \\(\\Delta^2\\) procedure can be applied.</p> <p>Aitken's \\(\\Delta^2\\) procedure</p> <p>Initially set \\(\\mu_0 = 0\\), \\(\\mu_1 = 0\\).</p> <p>For each step \\(k\\),</p> <ul> <li>set \\(\\mu = \\mathbf{x}^{(k)}_p\\), where \\(\\mathbf{u}^{(k - 1)}_p = 1\\).</li> <li>set the predicted eigenvalue to \\(\\lambda_1\\) be</li> </ul> \\[     \\hat\\mu = \\mu_0 - \\frac{(\\mu_1-\\mu_0)^2}{\\mu - 2\\mu_1 + \\mu_0}. \\] <ul> <li>set \\(\\mu_0 = \\mu_1\\), \\(\\mu_1 = \\mu\\).</li> </ul> <p>Target: Make \\(\\left|\\dfrac{\\lambda_2}{\\lambda_1}\\right|\\) as small as possible.</p> Example <p>Let \\(p = \\dfrac12 (\\lambda_2 + \\lambda_n)\\), and \\(B = A - pI\\), then \\(B\\) has the eigenvalues</p> \\[     \\lambda_1 - p, \\lambda_2 - p, \\dots, \\lambda_n - p. \\] <p>And since</p> \\[     \\left|\\frac{\\lambda_2 - p}{\\lambda_1 - p}\\right| &lt; \\left|\\frac{\\lambda_2}{\\lambda_1}\\right|, \\] <p>the iteration for finding the eigenvalues of \\(B\\) converges much faster than that of \\(A\\).</p> <p>But how to find \\(p\\) ...?</p>"},{"location":"Mathematics_Basis/NA/Chap_9/#inverse-power-method","title":"Inverse Power Method","text":"<p>If \\(A\\) has  eigenvalues satisfying \\(|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\gt |\\lambda_n|\\), then \\(A^{-1}\\) has</p> \\[     \\left|\\frac{1}{\\lambda_n}\\right| \\gt \\left|\\frac{1}{\\lambda_{n-1}}\\right| \\ge \\cdots \\ge \\left|\\frac{1}{\\lambda_1}\\right|. \\] <p>The dominant eigenvalue of \\(A^{-1}\\) is the eigenvalue with the smallest magnitude of \\(A\\).</p> <p>A way to find eigenvalue \\(\\lambda\\) closest to a given value \\(q\\)</p> <p>Use power method for matrix \\((A - qI)^{-1}\\), which has the eigenvalues</p> \\[     \\left|\\frac{1}{\\lambda_1 - q}\\right| \\gt \\left|\\frac{1}{\\lambda_2 - q}\\right| \\ge \\cdots \\ge \\left|\\frac{1}{\\lambda_n - q}\\right|. \\] <p>where \\(\\lambda_1\\) is the closest to \\(q\\).</p> <p>Also, inverse power method can sometimes accelerate to solve \\(\\lambda_1\\) of \\(A\\). By initialize \\(q\\) with</p> \\[     q =\\frac{\\mathbf{x}^{(0)t}A\\mathbf{x}^{(0)}}{\\mathbf{x}^{(0)t}\\mathbf{x}^{(0)}}. \\] <p>This choice is considered by the property of eigenvalue \\(\\lambda\\) if \\(\\mathbf{x}\\) is an eigenvector of \\(A\\).</p> \\[     \\lambda =\\frac{\\mathbf{x}^{t}A\\mathbf{x}}{\\mathbf{x}^{t}\\mathbf{x}} = \\frac{\\mathbf{x}^{t}A\\mathbf{x}}{||\\mathbf{x}||^2_2}. \\]"},{"location":"Mathematics_Basis/NA/Chap_9/#wielandt-deflation","title":"Wielandt Deflation","text":"<p>Our target is now to solve the eigenvalue of the second largest magnitude associated with its eigenvector. We introduce deflation techniques here, which forming a new matrix \\(B\\) with eigenvalue \\(0, \\lambda_2, \\dots, \\lambda_n\\), where \\(\\lambda_1\\) is replaced by \\(0\\).</p> <p>Theorem 9.0</p> <p>Suppose \\(\\lambda_1, \\dots, \\lambda_n\\) are eigenvalues of \\(A\\) associated with eigenvectors \\(\\mathbf{v}^{(1)}, \\dots, \\mathbf{v}^{(n)}\\), and \\(\\lambda_1\\) has multiplicity \\(1\\). Let \\(\\mathbf{x}\\) be a vector with \\(\\mathbf{x}^{t}\\mathbf{v}^{(1)} = 1\\). Then the matrix</p> \\[     B = A - \\lambda_1\\mathbf{v}^{(1)}\\mathbf{x}^t \\] <p>has eigenvalues \\(0, \\lambda_2, \\dots, \\lambda_n\\) with associated eigenvectors \\(\\mathbf{v}^{(1)}, \\mathbf{w}^{(2)}, \\dots, \\mathbf{w}^{(n)}\\), where \\(\\mathbf{v}^{(i)}\\) and \\(\\mathbf{w}^{(i)}\\) are related by the equation</p> \\[     \\mathbf{v}^{(i)} = (\\lambda_i - \\lambda_1) \\mathbf{w}^{(i)} + \\lambda_1(\\mathbf{x}^t\\mathbf{w}^{(i)})\\mathbf{v}^{(1)}. \\] <p>With the Theorem 9.0, the only thing we need to do is to choose vector \\(\\mathbf{x}\\). Wielandt deflation chooses \\(\\mathbf{x}\\) by the following formula.</p> \\[     \\mathbf{x} = \\frac{1}{\\lambda_1 v_i^{(1)}}(a_{i1}, \\dots, a_{in})^t, \\] <p>where \\(v_i^{(1)}\\) is a nonzero coordinate of the eigenvector \\(\\mathbf{v}^{(1)}\\) and \\(a_{i1}, \\dots a_{in}\\) are the entries of i-th row of \\(A\\). It's easy to verify that</p> \\[     \\mathbf{x}^t\\mathbf{v}^{(1)} = \\frac{1}{\\lambda_1 v_i^{(1)}}(\\lambda_1 v_i^{(1)}) = 1. \\] <p>Moreover, by this definition, the i-th row of \\(B\\) consists entirely of zero entries. This means that the i-th coordinate of eigenvector \\(\\mathbf{w}\\) is \\(0\\). Consequently i-th column of \\(B\\) makes no contribution for calculating eigenvector. Thus we can replace \\(B\\) by an \\((n - 1)\\times(n - 1)\\) matrix \\(B'\\), which deletes the i-th row and i-th column and has eigenvalues \\(\\lambda_2, \\dots, \\lambda_n\\).</p> <p>Wielandt Deflation</p> <p>Step.1 Find \\(\\lambda_1\\), the eigenvalue of the largest magnitude, and its associated eigenvector \\(\\mathbf{v}^{(1)}\\) by Power Method.</p> <p>Step.2 Choose \\(\\mathbf{x}\\) and Construct \\(B = A - \\lambda_1 \\mathbf{v}^{(1)} \\mathbf{x}^{(t)}\\).</p> <p>Step.3 Delete i-th row and i-th column, which gives \\(B'\\).</p> <p>Step.4 Find \\(\\lambda_2\\) and its associated eigenvector \\(\\mathbf{w}^{(2)}\\) of \\(B'\\) by Power Method.</p> <p>Step.5 Use the formula of Theorem 9.0 to get the associated eigenvector \\(\\mathbf{v}^{(2)}\\) of \\(A\\).</p> \\[     \\mathbf{v}^{(2)} = (\\lambda_2 - \\lambda_1) \\mathbf{w}^{(2)} + \\lambda_1(\\mathbf{x}^t\\mathbf{w}^{(2)})\\mathbf{v}^{(1)}. \\] <p>Example</p> <p>Find the eigenvalue of the second largest magnitude and its associated eigenvector of</p> \\[     A = \\begin{bmatrix}         -4 &amp; 14 &amp; 0 \\\\         -5 &amp; 13 &amp; 0 \\\\         -1 &amp; 0 &amp; 2     \\end{bmatrix}. \\] <p>Solution.</p> <p>Step.1 Find \\(\\lambda_1 = 6\\) and its associated eigenvector \\(\\mathbf{v}^{(1)} = \\left(1, \\dfrac{5}{7}, -\\dfrac14\\right)^t\\) by Power Method of \\(A\\).</p> <p>Step.2 Choose \\(i = 1\\), then</p> \\[     \\mathbf{x} = \\frac{1}{6 \\times 1}(-4, 14, 0)^t = \\left(-\\frac23, \\frac83, 0\\right)^t, \\] \\[     B = A - \\lambda_1\\mathbf{v}^{(1)}\\mathbf{x}^t = \\begin{bmatrix}         0 &amp; 0 &amp; 0 \\\\         -\\frac{15}{7} &amp; 3 &amp; 0 \\\\         -2 &amp; \\frac72 &amp; 2     \\end{bmatrix}. \\] <p>Step.3 Delete the i-th row and i-th column,</p> \\[     B' = \\begin{bmatrix}         3 &amp; 0 \\\\         \\frac72 &amp; 2     \\end{bmatrix}. \\] <p>Step.4 Find \\(\\lambda_2 = 3\\) and its associated eigenvector \\(\\mathbf{w}^{(2)'} = \\left(\\dfrac{2}{7}, 1\\right)^t\\) by Power Method of \\(B'\\).</p> <p>Step.5 Add the dimension of \\(\\mathbf{w}^{(2)'}\\) and \\(\\mathbf{w}^{(2)} = \\left(0, \\dfrac{2}{7}, 1\\right)^t\\).</p> \\[ \\begin{aligned}     \\mathbf{v}^{(2)} &amp;= (\\lambda_2 - \\lambda_1) \\mathbf{w}^{(2)} + \\lambda_1(\\mathbf{x}^t\\mathbf{w}^{(2)})\\mathbf{v}^{(1)}     \\\\ &amp;= (3 - 6)\\left(0, \\dfrac{2}{7}, 1\\right)^t + 6\\left[\\left(-\\frac23, \\frac83, 0\\right)\\left(0, \\dfrac{2}{7}, 1\\right)^t\\right]\\left(1, \\dfrac{5}{7}, -\\dfrac14\\right)^t     \\\\ &amp;= (4, 2, -4)^t \\overset{\\text{Normalized by } ||\\cdot||_\\infty}{=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=} (1, 0.5, -1)^t. \\end{aligned} \\]"},{"location":"Mathematics_Basis/NA/class_notes/","title":"Class Notes","text":"<p>Lecturer        \u8bb8\u5a01\u5a01</p>"},{"location":"Mathematics_Basis/NA/class_notes/#textbook","title":"Textbook","text":""},{"location":"Mathematics_Basis/NA/class_notes/#lecture-grade-100","title":"Lecture Grade (100 %)","text":"<p>Laboratory Projects (36%)</p> <ul> <li>8 labs.</li> <li>Release at a time.</li> </ul> <p>Classroom Quiz  (4%)</p> <ul> <li>Twice, 2% for each test</li> <li>Can also answer questions in class (1%  for each)</li> </ul> <p>Research Topics (15%)</p> <ul> <li>Done in groups</li> <li>18 topics to choose from</li> <li>In class presentations (5~10 minutes)</li> </ul> <p>Homework (5%)</p> <p>Final exam (40%)</p>"},{"location":"Mathematics_Basis/PS/Review/","title":"\u6982\u7387\u8bba\u4e0e\u6570\u7406\u7edf\u8ba1","text":"<p>Warning</p> <p>It's not a complete note, but a simple review of concepts and most of the concerned formulae.</p> <p>Since this course is taught and examined in Chinese, I write in Chinese there.</p>"},{"location":"Mathematics_Basis/PS/Review/#1","title":"1 \u6982\u7387\u8bba\u7684\u57fa\u672c\u6982\u5ff5","text":""},{"location":"Mathematics_Basis/PS/Review/#11","title":"1.1 \u6837\u672c\u7a7a\u95f4\u4e0e\u968f\u673a\u4e8b\u4ef6","text":"<ul> <li>\u968f\u673a\u8bd5\u9a8c\uff0c\u6837\u672c\u7a7a\u95f4\uff0c\u6837\u672c\u70b9\uff0c\u968f\u673a\u4e8b\u4ef6\uff0c\u57fa\u672c\u4e8b\u4ef6\uff0c\u5fc5\u7136\u4e8b\u4ef6\uff0c\u4e0d\u53ef\u80fd\u4e8b\u4ef6</li> <li>\u4e8b\u4ef6\u7684\u5173\u7cfb<ul> <li>\u5305\u542b\u3001\u76f8\u7b49\u3001\u548c\u4e8b\u4ef6\u3001\u79ef\u4e8b\u4ef6\u3001\u9006\u4e8b\u4ef6\u3001\u5dee\u4e8b\u4ef6</li> <li>\u4e92\u4e0d\u76f8\u5bb9/\u4e92\u65a5</li> </ul> </li> <li>\u4ea4\u6362\u5f8b\u3001\u7ed3\u5408\u5f8b\u3001\u5206\u914d\u5f8b\u3001\u5fb7\u6469\u6839\u5f8b</li> <li>\u5e76\u8054\u7cfb\u7edf\u3001\u4e32\u8054\u7cfb\u7edf</li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#12","title":"1.2 \u9891\u7387\u548c\u6982\u7387","text":"<ul> <li>\u9891\u7387\uff0c\u6982\u7387</li> <li> <p>\u5b9a\u4e49</p> <ul> <li>\u975e\u8d1f\u6027 \\(P(A) \\ge 0\\)</li> <li>\u89c4\u8303\u6027 \\(P(S) = 1\\)</li> <li> <p>\u53ef\u5217\u53ef\u52a0\u6027</p> \\[     P\\left(\\bigcup_{j = 1}^{\\infty} A_j \\right) = \\sum\\limits_{j = 1}^{\\infty} P(A_j). \\] </li> </ul> </li> <li> <p>\u6027\u8d28</p> <ul> <li> <p>\u6709\u9650\u53ef\u52a0\u6027</p> \\[     P\\left(\\bigcup_{j = 1}^{n} A_j \\right) = \\sum\\limits_{j = 1}^{n} P(A_j). \\] </li> <li> <p>\\(P(A) = 1 - P(\\overline{A})\\)</p> </li> <li>\u5f53 \\(B \\subset A\\), \\(P(A - B) = P(A) - P(B)\\), \\(P(A) \\ge P(B)\\).</li> <li>\u5bb9\u65a5\u539f\u7406 \\(P(A \\cup B) = P(A) + P(B) - P(AB)\\).</li> <li>\\(P(B | A) = \\dfrac{P(AB)}{P(A)}\\).</li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#13","title":"1.3 \u7b49\u53ef\u80fd\u6982\u578b","text":""},{"location":"Mathematics_Basis/PS/Review/#14","title":"1.4 \u6761\u4ef6\u6982\u7387","text":"<ul> <li>\u6761\u4ef6\u6982\u7387</li> <li>\u5168\u6982\u7387\u516c\u5f0f         - \\(P(A) = \\sum\\limits_{j = 1}^{n} P(B_j) P(A|B_j)\\)         - \\(P(A|C) = \\sum\\limits_{j = 1}^{n} P(AB_j | C) = \\sum\\limits_{j = 1}^{n} P(B_j | C) \\cdot P(A| B_jC)\\)</li> <li> <p>\u8d1d\u53f6\u65af\u516c\u5f0f Bayes Formula</p> \\[     P(B_i | A) = \\frac{P(B_i)P(A|B_i)}{\\sum\\limits_{j = 1}^{n}P(B_j)P(A|B_j)} \\] <ul> <li>\u5148\u9a8c\u6982\u7387\uff0c\u540e\u9a8c\u6982\u7387</li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#15","title":"1.5 \u4e8b\u4ef6\u7684\u72ec\u7acb\u6027\u4e0e\u72ec\u7acb\u8bd5\u9a8c","text":"<ul> <li>\u76f8\u4e92\u72ec\u7acb\uff0c\u4e24\u4e24\u72ec\u7acb\uff0c\u76f8\u4e92\u72ec\u7acb\u6bd4\u4e24\u4e24\u72ec\u7acb\u5f3a</li> <li>\u6761\u4ef6\u72ec\u7acb</li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#2","title":"2 \u968f\u673a\u53d8\u91cf\u53ca\u5176\u6982\u7387\u5206\u5e03","text":""},{"location":"Mathematics_Basis/PS/Review/#21","title":"2.1 \u968f\u673a\u53d8\u91cf","text":"<ul> <li>\u968f\u673a\u53d8\u91cf</li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#22","title":"2.2 \u79bb\u6563\u578b\u968f\u673a\u53d8\u91cf","text":"<ul> <li>\u79bb\u6563\u578b\u968f\u673a\u53d8\u91cf\uff0c\u79bb\u6563\u578b\u5206\u5e03</li> <li>\u6982\u7387\u5206\u5e03\u5f8b</li> <li>\u5e38\u89c1\u5206\u5e03<ul> <li>0 - 1 \u5206\u5e03 / \u4e24\u70b9\u5206\u5e03 \\(0-1(p)\\) \u4f2f\u52aa\u5229\u8bd5\u9a8c<ul> <li>\\(P(X = k) = p^k(1 - p)^{1 - k}\\)</li> <li>\\(E(X) = p\\)</li> </ul> </li> <li>\u4e8c\u9879\u5206\u5e03 \\(B(n,p)\\) n \u91cd\u4f2f\u52aa\u5229\u8bd5\u9a8c<ul> <li>\\(P(X = k)=C^k_n p^k(1 - p)^{n - k}\\)</li> <li>\\(E(X) = np\\)</li> <li>\\(D(X) = np(1 - p)\\)</li> </ul> </li> <li>\u6cca\u677e\u5206\u5e03 \\(P(\\lambda)\\)<ul> <li>\\(P(X = k)=\\dfrac{e^{-\\lambda} \\lambda^k}{k!}\\)</li> <li>\u62df\u5408\u4e8c\u9879\u5206\u5e03\uff0c\\(\\lambda=np\\)</li> <li>\\(E(X) = \\lambda\\)</li> <li>\\(D(X) = \\lambda\\)</li> </ul> </li> <li>\u51e0\u4f55\u5206\u5e03 \\(\\text{Geom}(p)\\)<ul> <li>\\(P(X = k) = p(1 - p)^{k - 1}\\)</li> <li>\u65e0\u8bb0\u5fc6\u6027</li> <li>\\(E(X) = \\dfrac{1}{p}\\)</li> <li>\\(D(X) = \\dfrac{1 - p}{p^2}\\)</li> </ul> </li> <li>\u8d85\u51e0\u4f55\u5206\u5e03 \\(H(n, a, N)\\)<ul> <li>\\(P(X = k) = \\dfrac{C_a^k C_b^{n-k}}{C_N^n},\\ \\ a + b = N\\)</li> <li>\\(E(X) = n \\dfrac{a}{N}\\)</li> <li>\\(D(X) = n \\dfrac{a(N - a)(N - n)}{N^2(N - 1)}\\)</li> </ul> </li> <li>\u5df4\u65af\u5361\u5206\u5e03 \\(NB(r, p)\\)<ul> <li>\\(P(X = k) = C_{k - 1}^{r - 1} p^r (1 - p)^{k - r}\\)</li> </ul> </li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#23","title":"2.3 \u968f\u673a\u53d8\u91cf\u7684\u6982\u7387\u5206\u5e03\u51fd\u6570","text":"<ul> <li>\u5206\u5e03\u51fd\u6570 \\(F(x) = P(X \\le x)\\)</li> <li>\u6027\u8d28\uff1a\u5355\u8c03\u4e0d\u51cf\uff1b\u65e0\u9650\u5904\u6781\u9650\uff1b\u53f3\u8fde\u7eed \\(F(x) = F(x + 0)\\) \uff08\u5de6\u95ed\u53f3\u5f00\uff09</li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#24","title":"2.4 \u8fde\u7eed\u578b\u968f\u673a\u53d8\u91cf","text":"<ul> <li> <p>\u8fde\u7eed\u578b\u968f\u673a\u53d8\u91cf\uff0c\u6982\u7387\u5bc6\u5ea6\u51fd\u6570(p.d.f.)\uff0c\u652f\u6491</p> </li> <li> <p>\\(F(x) = \\int_{-\\infty}^x f(t)dt\\).</p> </li> <li> <p>\u5e38\u89c1\u5206\u5e03</p> <ul> <li>\u5747\u5300\u5206\u5e03 \\(U(a,b)\\)<ul> <li>\\(f(x)=\\left\\{         \\begin{array}{l}             \\dfrac{1}{b-a}, &amp; x \\in (a, b), \\\\             0, &amp; \\text{else}.          \\end{array}     \\right.\\)</li> <li>\\(F(x)=\\left\\{         \\begin{array}{l}             0, &amp; x &lt; a, \\\\             \\dfrac{x - a}{b - a}, &amp; x \\in (a, b), \\\\             1, &amp; \\text{else}.         \\end{array}     \\right.\\)</li> <li>\\(E(X) = \\dfrac{a + b}{2}\\)</li> <li>\\(D(X) = \\dfrac{(b - a)^2}{12}\\)</li> </ul> </li> <li>\u6b63\u6001\u5206\u5e03 \\(N(\\mu, \\sigma^2)\\)<ul> <li>\\(f(x) = \\dfrac{1}{\\sqrt{2 \\pi}\\sigma} e^{-(x - \\mu)^2 / (2 \\sigma^2)}\\)</li> <li>\\(F(x) = \\int_{-\\infty}^{x} \\dfrac{1}{\\sqrt{2 \\pi} \\sigma} e^{-(t - \\mu)^2 / (2 \\sigma^2)} dt\\)</li> <li>\\(E(X) = \\mu\\)</li> <li>\\(D(X) = \\sigma^2\\)</li> </ul> </li> <li>\u6307\u6570\u5206\u5e03 \\(E(\\lambda)\\)<ul> <li>\\(f(x)=\\left\\{         \\begin{array}{l}             \\lambda e^{-\\lambda x}, &amp; x &gt; 0, \\\\             0, &amp; \\text{else}.         \\end{array}     \\right.\\)</li> <li>\\(F(x)=\\left\\{         \\begin{array}{l}             1 - e^{-\\lambda x}, &amp; x &gt; 0, \\\\             1, &amp; \\text{else}.         \\end{array}     \\right.\\)</li> <li>\u65e0\u8bb0\u5fc6\u6027 \\(P(X &gt; t_0 + t | X &gt; t_0) = P(X &gt; t)\\)</li> <li>\\(E(X) = \\dfrac{1}{\\lambda}\\)</li> <li>\\(D(X) = \\dfrac{1}{\\lambda^2}\\)</li> </ul> </li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#25","title":"2.5 \u968f\u673a\u53d8\u91cf\u51fd\u6570\u7684\u5206\u5e03","text":"<ul> <li>\u53ef\u52a0\u6027<ul> <li>\\(X \\sim B(n, p),\\ \\ Y \\sim B(m, p) \\Rightarrow X + Y \\sim B(m + n, p)\\)</li> <li>\\(X \\sim P(\\lambda_1),\\ \\ Y \\sim P(\\lambda_2) \\Rightarrow X + Y \\sim P(\\lambda_1 + \\lambda_2)\\)</li> <li>\\(X\\sim N(\\mu_1,\\sigma_1^2),Y\\sim N(\\mu_2,\\sigma_2^2) \\Rightarrow X\\pm Y\\sim N(\\mu_1 \\pm \\mu_2,\\sigma_1^2+\\sigma_2^2)\\)</li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#3","title":"3 \u591a\u5143\u968f\u673a\u53d8\u91cf\u53ca\u5176\u5206\u5e03","text":""},{"location":"Mathematics_Basis/PS/Review/#31","title":"3.1 \u4e8c\u5143\u79bb\u6563\u578b\u968f\u673a\u53d8\u91cf","text":"<ul> <li>\u8054\u5408\u6982\u7387\u5206\u5e03\u5f8b / \u8054\u5408\u5206\u5e03\u5f8b\uff0c\u8fb9\u9645\u5206\u5e03\u5f8b\uff0c\u6761\u4ef6\u5206\u5e03\u5f8b</li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#32","title":"3.2 \u4e8c\u5143\u968f\u673a\u53d8\u91cf\u7684\u5206\u5e03\u51fd\u6570","text":"<ul> <li>\u8054\u5408\u6982\u7387\u5206\u5e03\u51fd\u6570 / \u8054\u5408\u5206\u5e03\u51fd\u6570\uff0c\u8fb9\u9645\u5206\u5e03\u51fd\u6570\uff0c\u6761\u4ef6\u5206\u5e03\u51fd\u6570</li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#33","title":"3.3 \u4e8c\u5143\u8fde\u7eed\u578b\u968f\u673a\u53d8\u91cf","text":"<ul> <li>\u8054\u5408\u6982\u7387\u5bc6\u5ea6\u51fd\u6570 / \u8054\u5408\u5bc6\u5ea6\u51fd\u6570\uff0c\u8fb9\u9645\u5bc6\u5ea6\u51fd\u6570\uff0c\u6761\u4ef6\u5bc6\u5ea6\u51fd\u6570</li> <li>\\(F(x,y) = P(X\\le x, Y\\le y)\\)</li> <li>\\(F_{X|Y}(x|y) = \\dfrac{P(X \\le x, Y = y)}{P(Y = y)} = \\lim\\limits_{\\Delta y \\rightarrow 0^+}\\dfrac{P(X \\le x, y &lt; Y \\le y + \\Delta y)}{P(y &lt; Y \\le y + \\Delta y)}\\)</li> <li>\\(f_X(x) = \\int_{-\\infty}^{\\infty}f(x,y)dy\\)</li> <li>\\(f_{X|Y}(x|y) = \\dfrac{f(x, y)}{f_Y(y)},\\ \\ f(x, y) = f_Y(y)f_{X|Y}(x|y)\\)</li> <li>\\(F_{X|Y}(x|y) = P(X \\le x | Y = y) = \\int_{-\\infty}^x f_{X|Y}(x|y)dx\\)</li> <li>\u5e38\u89c1\u5206\u5e03<ul> <li>\u5747\u5300\u5206\u5e03</li> <li>\u6b63\u6001\u5206\u5e03 \\(X, Y \\sim N(\\mu_1, \\mu_2; \\sigma_1^2, \\sigma_2^2; \\rho)\\)<ul> <li>\\(X \\sim N(\\mu_1, \\sigma_1^2),\\ \\ Y \\sim N(\\mu_2, \\sigma_2^2)\\)</li> </ul> </li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#34","title":"3.4 \u968f\u673a\u53d8\u91cf\u7684\u72ec\u7acb\u6027","text":"<ul> <li>\\(X,Y\\) \u72ec\u7acb\u65f6\uff0c\\(P(X\\le x,Y\\le y) = P(X\\le x) \\cdot P(Y\\le y)\\), \u5373 \\(F(x, y) = F_X(x) F_Y(y)\\)</li> <li>\u5bf9\u4e8e\u8fde\u7eed\u578b\u968f\u673a\u53d8\u91cf\uff0c\u5145\u8981\u6761\u4ef6\u662f \\(f(x, y) = m(x)n(y)\\)</li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#35","title":"3.5 \u4e8c\u5143\u968f\u673a\u53d8\u91cf\u51fd\u6570\u7684\u5206\u5e03","text":"<ul> <li>\\(Z = X + Y\\)<ul> <li>\\(P(Z = z) = P(X + Y = z) = \\sum\\limits_{i = 1}^{+\\infty} P(X = x_i, Y = z - x_i)\\)</li> <li>\\(f_Z(z) = \\int_{-\\infty}^{+\\infty} f(x, z - x) dx \\overset{\u72ec\u7acb}{=\\!=\\!=} \\int_{-\\infty}^{+\\infty} f_X(x) f_Y(z - x)dx\\)</li> </ul> </li> <li>\\(M = \\max(X_1, X_2, \\dots, X_N),\\ \\ N = \\min(X_1, X_2, \\dots, X_N)\\)<ul> <li>\u76f8\u4e92\u72ec\u7acb: \\(F_{M}(z) = \\prod\\limits_{i = 1}^{n} F_i(t),\\ \\ F_{N} = 1 - \\prod\\limits_{i = 1}^{n} [1 - F_i(t)]\\)</li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#4","title":"4 \u968f\u673a\u53d8\u91cf\u7684\u6570\u5b57\u7279\u5f81","text":""},{"location":"Mathematics_Basis/PS/Review/#41","title":"4.1 \u6570\u5b66\u671f\u671b","text":"<ul> <li>\u5b9a\u4e49\uff0c\u5b58\u5728\u6027\uff0c\u6027\u8d28</li> <li>\\(E(X) = \\sum\\limits_{k = 1}^{+\\infty} x_k p_k = \\int_{-\\infty}^{\\infty}xf(x)dx\\)</li> <li>\\(E(Y) = E(g(X)) = \\sum\\limits_{k = 1}^{+\\infty} g(x_k) p_k = \\int_{-\\infty}^{+\\infty}g(x)f(x)dx\\)</li> <li>\\(E(Z) = E(h(X, Z)) = \\sum\\limits_{i = 1}^{+\\infty} \\sum\\limits_{j = 1}^{+\\infty} h(x_i, y_j) p_{ij} = \\int_{-\\infty}^{+\\infty}\\int_{-\\infty}^{+\\infty}h(x, y)f(x,y)dxdy\\)</li> <li>\u6027\u8d28<ul> <li>\\(E(C) = C\\)</li> <li>\\(E(CX) = C \\cdot E(X)\\)</li> <li>\\(E(X + Y) = E(X) + E(Y)\\)</li> <li>\u76f8\u4e92\u72ec\u7acb\uff0c\\(E(XY) = E(X)E(Y)\\)</li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#42","title":"4.2 \u65b9\u5dee\u3001\u53d8\u5f02\u7cfb\u6570","text":"<ul> <li>\u65b9\u5dee\uff0c\u6807\u51c6\u5dee</li> <li>\\(\\text{Var}(X), D(X), V(X)\\)</li> <li>\\(\\text{Var}(X) = E[(X -E(X))^2]\\)</li> <li>\\(\\text{Var}(X) = E(X^2)-(E(X))^2\\)</li> <li>\u6027\u8d28<ul> <li>\\(\\text{Var}(C) = 0\\)</li> <li>\\(\\text{Var}(CX) = C^2 \\cdot \\text{Var}(X)\\)</li> <li>\\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X, Y)\\)</li> <li>\\(\\text{Var}(X) = 0 \\Leftrightarrow P(X = C) = 1 \\wedge C = E(X)\\)</li> </ul> </li> <li>\u6807\u51c6\u5316\u968f\u673a\u53d8\u91cf<ul> <li>\\(X^*=\\dfrac{X - E(X)}{\\sqrt{\\text{Var}(X)}}\\)</li> </ul> </li> <li>\u53d8\u5f02\u7cfb\u6570<ul> <li>\\(C_v(X) = \\dfrac{\\sqrt{\\text{Var}(X)}}{E(X)}\\)</li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#43","title":"4.3 \u534f\u65b9\u5dee\u4e0e\u76f8\u5173\u7cfb\u6570","text":"<ul> <li>\u534f\u65b9\u5dee<ul> <li>\\(\\text{Cov}(X, Y) = E[(X - E(X))(Y - E(Y))]\\)</li> <li>\\(\\text{Cov}(X, Y) = E(XY)-E(X)E(Y)\\)</li> </ul> </li> <li>\u6027\u8d28<ul> <li>\\(\\text{Cov}(X, Y) = \\text{Cov}(Y, X)\\)</li> <li>\\(\\text{Cov}(X, X) = \\text{Var}(X)\\)</li> <li>\\(\\text{Cov}(aX, bY) = ab \\cdot \\text{Cov}(X, Y)\\)</li> <li>\\(\\text{Cov}(X_1 + X_2) = \\text{Cov}(X_1, Y) + \\text{Cov}(X_2, Y)\\)</li> </ul> </li> <li>\u76f8\u5173\u7cfb\u6570<ul> <li>\\(\\rho_{XY}=\\dfrac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)}\\sqrt{\\text{Var}(Y)}} = \\text{Cov}(X^*, Y^*) \\in [-1,1]\\)</li> <li>\u5927\u4e8e\u4e00\u6b63\u76f8\u5173\uff0c\u5c0f\u4e8e\u4e00\u8d1f\u76f8\u5173\uff0c\u96f6\u5219\u4e0d\u76f8\u5173</li> <li>\u4e0d\u76f8\u5173\u7b49\u4ef7\u6761\u4ef6<ul> <li>\\(\\text{Cov}(X, Y) = 0\\)</li> <li>\\(E(XY) = E(X)E(Y)\\)</li> <li>\\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\)</li> <li>\u76f8\u4e92\u72ec\u7acb\u662f\u4e0d\u76f8\u5173\u7684\u5145\u5206\u6761\u4ef6</li> </ul> </li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#44","title":"4.4 \u5176\u4ed6\u6570\u5b57\u7279\u5f81","text":"<ul> <li>\u77e9<ul> <li>\\(k\\) \u9636\u539f\u70b9\u77e9 \\(E(X^k)\\)</li> <li>\\(k\\) \u9636\u4e2d\u5fc3\u77e9 \\(E([X - E(X)]^k)\\)</li> <li>\\(k + l\\) \u9636\u6df7\u5408\u539f\u70b9\u77e9 \\(E(X^k Y^l)\\)</li> <li>\\(k + l\\) \u9636\u6df7\u5408\u4e2d\u5fc3\u77e9 \\(E([X - E(X)]^k [Y - E(Y)]^l)\\)</li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#45","title":"4.5 \u591a\u5143\u968f\u673a\u53d8\u91cf\u7684\u6570\u5b57\u7279\u5f81","text":"<ul> <li>\u591a\u5143\u968f\u673a\u53d8\u91cf\u7684\u6570\u5b57\u7279\u5f81<ul> <li>\u5bf9\u4e8e \\(\\vec{X}=(X_1,X_2,\\cdots,X_n)^T,X_i\\sim N(\\mu_i,\\sigma_i)\\)\uff0c\\(\\vec{X}\\sim N(\\vec{a},\\vec{B})\\)</li> <li>\u5f53 \\(n=2,\\ \\ \\vec{B}=\\begin{bmatrix} \\sigma_1^2&amp;\\sigma_1\\sigma_2\\rho\\\\ \\sigma_1\\sigma_2\\rho&amp;\\sigma_2^2 \\end{bmatrix}\\)</li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#5","title":"5 \u5927\u6570\u5b9a\u5f8b\u53ca\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406","text":""},{"location":"Mathematics_Basis/PS/Review/#51","title":"5.1 \u5927\u6570\u5b9a\u5f8b","text":"<ul> <li> <p>\u4f9d\u6982\u7387\u6536\u655b</p> <ul> <li>\\(\\forall\\ \\varepsilon \\gt 0,\\ \\ \\lim\\limits_{x \\rightarrow +\\infty}P(|Y_n-c|\\ge\\varepsilon)=0\\)</li> <li>\\(Y_n\\stackrel{P}\\longrightarrow c,\\ \\ n\\rightarrow +\\infty\\)</li> <li> <p>\\(n\\rightarrow +\\infty,\\ \\ X_n\\stackrel{P}\\longrightarrow a,\\ \\ Y_n\\stackrel{P}\\longrightarrow b\\), \\(g(x, y)\\) \u5728 \\((a, b)\\) \u8fde\u7eed\uff0c</p> \\[     g(X_n, Y_n)\\stackrel{P}\\longrightarrow g(a, b),\\ \\ n\\rightarrow +\\infty \\] </li> </ul> </li> <li> <p>\u9a6c\u5c14\u53ef\u592b\u4e0d\u7b49\u5f0f</p> \\[     P(|Y| \\ge \\varepsilon) \\le \\dfrac{E(|Y|^k)}{\\varepsilon^k} \\] </li> <li> <p>\u5207\u6bd4\u96ea\u592b\u4e0d\u7b49\u5f0f</p> \\[     P(|X-\\mu| \\ge \\varepsilon) \\le \\dfrac{\\sigma^2}{\\varepsilon^2} \\] </li> <li> <p>\u5927\u6570\u5b9a\u5f8b</p> <ul> <li> <p>\u8bbe \\(\\{Y_i\\}\\) \u4e3a\u4e00\u968f\u673a\u53d8\u91cf\u5e8f\u5217\uff0c\u5b58\u5728\u5e38\u6570\u5e8f\u5217 \\(\\{c_n\\}\\) \u4f7f\u5f97\uff0c</p> \\[     \\lim\\limits_{n\\rightarrow +\\infty} P \\left(\\left|\\frac{1}{n}\\sum\\limits_{i=1}^nY_i-c_n\\right|\\ge\\varepsilon\\right)=0 \\] </li> <li> <p>\\(n\\rightarrow+\\infty,\\ \\ \\dfrac{1}{n}\\sum\\limits_{i=1}^nY_i-c_n\\stackrel{P}\\longrightarrow0\\)</p> </li> <li>\u7279\u522b\u5730\uff0c\u5f53 \\(c_n=c\\)\uff0c\u53ef\u5199\u4e3a \\(n\\rightarrow+\\infty,\\ \\ \\dfrac{1}{n}\\sum\\limits_{i=1}^nY_i\\stackrel{P}\\longrightarrow c\\)</li> </ul> </li> <li> <p>\u4f2f\u52aa\u5229\u5927\u6570\u5b9a\u5f8b</p> \\[     \\lim_{n\\rightarrow+\\infty} P \\left(\\left|\\dfrac{n_A}{n}-p\\right|\\ge\\varepsilon\\right)=0 \\] </li> <li> <p>\u8f9b\u94a6\u5927\u6570\u5b9a\u5f8b\uff08\u72ec\u7acb\u540c\u5206\u5e03\uff09</p> \\[     \\lim_{n\\rightarrow+\\infty} P \\left(\\left|\\frac{1}{n}\\sum_{i=1}^nX_i-\\mu\\right|\\ge\\varepsilon\\right)=0 \\] <ul> <li>\u63a8\u8bba</li> </ul> \\[     \\lim_{n\\rightarrow+\\infty} P \\left(\\left|\\frac{1}{n}\\sum_{i=1}^n h(X_i) - E(h(X_1)) \\right| \\ge \\varepsilon\\right) = 0 \\] </li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#52","title":"5.2 \u4e2d\u5fc3\u6781\u9650\u5b9a\u7406","text":"<p>\u72ec\u7acb\u540c\u5206\u5e03</p> <ul> <li> <p>\u6797\u5fb7\u4f2f\u683c-\u83b1\u7ef4\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406</p> \\[     \\lim\\limits_{n\\rightarrow+\\infty}P\\left(\\dfrac{\\sum\\limits_{i=1}^nX_i-E(\\sum\\limits_{i=1}^nX_i)}{\\sqrt{\\text{Var}\\left(\\sum\\limits_{i=1}^nX_i\\right)}}\\le x\\right)     = \\lim\\limits_{n\\rightarrow+\\infty} P \\left(\\dfrac{\\sum\\limits_{i=1}^nX_i-n\\mu}{\\sigma\\sqrt{n}}\\le x \\right) =\\Phi(x) \\] </li> <li> <p>\u68e3\u83ab\u5f17-\u62c9\u666e\u62c9\u65af\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406</p> \\[     \\lim\\limits_{n\\rightarrow+\\infty} P \\left(\\dfrac{n_A-np}{\\sqrt{np(1-p)}}\\le x \\right) = \\Phi(x) \\] </li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#6","title":"6 \u7edf\u8ba1\u91cf\u4e0e\u62bd\u6837\u5206\u5e03","text":""},{"location":"Mathematics_Basis/PS/Review/#61","title":"6.1 \u968f\u673a\u6837\u672c\u4e0e\u7edf\u8ba1\u91cf","text":"<ul> <li> <p>\u603b\u4f53\u3001\u4e2a\u4f53\u3001\u6709\u9650\u603b\u4f53\u3001\u65e0\u9650\u603b\u4f53</p> </li> <li> <p>\u62bd\u6837\u3001\u6837\u672c\uff08\u4ee3\u8868\u6027\u3001\u72ec\u7acb\u6027\uff09\u3001\u6837\u672c\u5bb9\u91cf\u3001\u968f\u673a\u6837\u672c</p> </li> <li> <p>\u7edf\u8ba1\u91cf\uff1a\u6837\u672c\u5747\u503c\uff0c\u6837\u672c\u65b9\u5dee\uff0c\u6837\u672c \\(k\\) \u9636\u539f\u70b9\u77e9\uff0c\u6837\u672c \\(k\\) \u9636\u4e2d\u5fc3\u77e9</p> </li> <li> <p>\u6837\u672c\u5747\u503c</p> \\[     \\overline{X} = \\frac{1}{n} \\sum\\limits_{i = 1}^{n} X_i \\] </li> <li> <p>\u6837\u672c\u65b9\u5dee</p> \\[     S^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\overline{X})=\\frac{1}{n-1}\\left(\\sum_{i=1}^nX_i^2-n\\overline{X}^2\\right) \\] </li> <li> <p>\u6837\u672c \\(k\\) \u9636\uff08\u539f\u70b9\uff09\u77e9</p> \\[     A_k=\\frac{1}{n}\\sum_{i=1}^nX_i^k \\] </li> <li> <p>\u6837\u672c \\(k\\) \u9636\u4e2d\u5fc3\u77e9</p> \\[     B_k=\\frac{1}{n}\\sum_{i=1}^n(X_i-\\overline{X})^k \\] </li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#62-chi2-t-f","title":"6.2 \\(\\chi^2\\) \u5206\u5e03\uff0c\\(t\\) \u5206\u5e03\uff0c\\(F\\) \u5206\u5e03","text":"<ul> <li>\\(\\chi^2\\) \u5206\u5e03<ul> <li>\\(\\chi^2(n) = \\sum\\limits_{i = 1}^{n} X_i^2,\\ \\ X_i \\sim N(0, 1)\\)</li> <li>\\(X, Y\\) \u76f8\u4e92\u72ec\u7acb, \\(X \\sim \\chi^2(m),\\ \\ Y \\sim \\chi^2(n) \\Rightarrow X + Y \\sim \\chi^2(m + n)\\)</li> <li>\\(E(\\chi^2(n)) = n\\)</li> <li>\\(D(\\chi^2(n)) = 2n\\)</li> <li>\\(n &gt; 40\\), \\(\\chi^2_\\alpha(n) \\approx \\dfrac{1}{2}(z_\\alpha + \\sqrt{2n - 1})^2\\)</li> </ul> </li> <li>\\(t\\) \u5206\u5e03<ul> <li>\\(t(n) = \\dfrac{X}{\\sqrt{Y / n}},\\ \\ X \\sim N(0, 1),\\ \\ Y \\sim \\chi^2(n)\\)</li> <li>\\(n \\rightarrow +\\infty,\\ \\ t(n) \\rightarrow N(0, 1)\\)</li> <li>\\(E(T) = 0,\\ \\ n \\ge 2\\)</li> <li>\\(D(T) = \\dfrac{n}{n - 1},\\ \\ n \\ge 3\\)</li> <li>\\(n &gt; 45,\\ \\ t_\\alpha (n) \\approx z_\\alpha\\)</li> </ul> </li> <li>\\(F\\) \u5206\u5e03<ul> <li>\\(F(n_1, n_2) = \\dfrac{U / n_1}{V / n_2}\\)</li> <li>\\(F \\sim F(n_1, n_2) \\Rightarrow \\dfrac{1}{F} \\sim F(n_2, n_1)\\)</li> <li>\\(F_{1 - \\alpha}(n_1, n_2) = \\dfrac{1}{F_\\alpha(n_2, n_1)}\\)</li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#63","title":"6.3 \u6b63\u6001\u603b\u4f53\u4e0b\u62bd\u6837\u5206\u5e03","text":"<ul> <li>\\(E(\\overline{X}) = \\mu, \\ \\ \\text{Var}(\\overline{X}) = \\dfrac{\\sigma^2}{n}\\)</li> <li>\\(E(S^2) = \\sigma^2,\\ \\ E(B_2) = \\dfrac{n - 1}{n} \\sigma^2\\)</li> <li>\u6b63\u6001\u603b\u4f53\u4e0b<ul> <li>\\(\\overline{X}\\sim N(\\mu,\\dfrac{\\sigma^2}{n})\\)</li> <li>\\(\\text{Var}(S^2) = \\dfrac{2\\sigma^4}{n - 1}\\)</li> <li>\\(\\dfrac{\\sum\\limits_{i = 1}^{n} (X_i - \\overline{X})^2}{\\sigma^2} = \\dfrac{(n-    1) S^2}{\\sigma^2}\\sim\\chi^2(n-1)\\)</li> <li>\\(\\overline{X}\\) \u4e0e \\(S^2\\) \u76f8\u4e92\u72ec\u7acb</li> <li>\\(\\dfrac{\\overline{X} - \\mu}{S / \\sqrt{n}} \\sim t(n - 1)\\)</li> <li>\\(\\dfrac{S_1^2 / \\sigma_1^2}{S_2^2 / \\sigma_2^2} \\sim F(n_1 - 1, n_2 - 1)\\)</li> <li>\\(\\dfrac{(\\overline{X} - \\overline{Y}) - (\\mu_1 - \\mu_2)}{\\sqrt{\\dfrac{\\sigma_1^2}{n_1^2} + \\dfrac{\\sigma_2^2}{n_2}}} \\sim N(0, 1)\\)</li> <li>\\(\\sigma_1^2 = \\sigma_2^2 = \\sigma,\\ \\ \\dfrac{(\\overline{X} - \\overline{Y}) - (\\mu_1 - \\mu_2)}{S_w\\sqrt{\\dfrac{1}{n_1^2} + \\dfrac{1}{n_2}}} \\sim t(n_1 + n_2 - 2),\\ \\ S_w^2 = \\dfrac{(n_1 - 1)S_1^2 + (n_2 - 1) S_2^2}{n_1 + n_2 - 2}\\)</li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#7","title":"7 \u53c2\u6570\u4f30\u8ba1","text":""},{"location":"Mathematics_Basis/PS/Review/#_2","title":"\u70b9\u4f30\u8ba1","text":"<ul> <li>\u77e9\u6cd5<ul> <li>\u7528\u77e9\u8868\u793a\u7edf\u8ba1\u91cf\uff0c\u518d\u7528 \\(A_k \\rightarrow \\mu_k,\\ \\ B_k \\rightarrow \\nu_k\\) \u66ff\u6362</li> </ul> </li> <li>\u6781\u5927\u4f3c\u7136\u6cd5<ul> <li>\\(L(\\theta) = \\prod\\limits_{i = 1}^n f(x_i; \\theta)\\)</li> <li>\\(L(\\hat \\theta) = \\max\\limits_{\\theta \\in \\Theta} L(\\theta). \\left(\\left.\\dfrac{dL(\\theta)}{d\\theta}\\right|_{\\theta = \\hat{\\theta}} = 0\\right)\\)</li> <li>\u4e3a\u4e86\u8ba1\u7b97\u65b9\u4fbf\uff0c\u53d6\u5bf9\u6570 \\(l(\\theta)=\\ln L(\\theta)\\).</li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#_3","title":"\u8bc4\u4ef7\u51c6\u5219","text":"<ul> <li>\u65e0\u504f\u6027\u51c6\u5219<ul> <li>\u65e0\u504f\u4f30\u8ba1 \\(E(\\hat{\\theta})=\\theta\\).</li> <li>\u6e10\u8fd1\u65e0\u504f\u4f30\u8ba1 \\(\\lim\\limits_{n\\rightarrow+\\infty}E(\\hat{\\theta})=\\theta\\).</li> </ul> </li> <li>\u6709\u6548\u6027\u51c6\u5219<ul> <li>\u65b9\u5dee \\(\\text{Var}(\\theta)\\)</li> <li>\u65b9\u5dee\u8d8a\u5c0f\u8d8a\u6709\u6548.</li> </ul> </li> <li>\u5747\u65b9\u8bef\u5dee\u51c6\u5219<ul> <li>\u5747\u65b9\u8bef\u5dee \\(\\text{Mse}(\\hat{\\theta})=E[(\\hat{\\theta}-\\theta)^2]\\).</li> <li>\u5747\u65b9\u8bef\u5dee\u8d8a\u5c0f\u8d8a\u6709\u6548.</li> </ul> </li> <li>\u76f8\u5408\u6027\u51c6\u5219<ul> <li>\\(\\lim\\limits_{n\\rightarrow+\\infty} P (|\\hat{\\theta}_n-\\theta|&lt;\\varepsilon)=1,\\ \\ \\hat{\\theta}_n\\stackrel{P}\\longrightarrow\\theta\\) \u5219\u79f0\u4e3a\u76f8\u5408\u4f30\u8ba1\u91cf.</li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#_4","title":"\u533a\u95f4\u4f30\u8ba1","text":"<ul> <li>\u6b63\u6001\u603b\u4f53</li> </ul> <ul> <li> <p>\u975e\u6b63\u6001\u603b\u4f53</p> <ul> <li> <p>0 - 1 \u5206\u5e03</p> \\[     P\\left(-z_{\\alpha/2}&lt;\\frac{n\\overline{X}-np}{\\sqrt{np(1-p)}}&lt;z_{\\alpha/2}\\right)\\approx 1-\\alpha \\] </li> <li> <p>\u5176\u4ed6\u5206\u5e03\u5747\u503c\u4e3a \\(\\mu\\)</p> \\[     \\frac{\\sum\\limits_{i=1}^nX_i-n\\mu}{\\sqrt{n}\\sigma}\\sim N(0,1) \\] <p>\\(\\left(\\overline{X}\\pm\\dfrac{\\sigma}{\\sqrt{n}}z_{\\alpha/2}\\right)\\)\uff0c\\(\\sigma^2\\) \u672a\u77e5\u65f6\u4f7f\u7528 \\(S^2\\) \u66ff\u4ee3.</p> </li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/PS/Review/#8","title":"8 \u5047\u8bbe\u68c0\u9a8c","text":"<ul> <li>\u539f\u5047\u8bbe\uff0c\u5907\u62e9\u5047\u8bbe</li> <li>\u68c0\u9a8c\uff0c\u63a5\u6536\u57df\uff0c\u62d2\u7edd\u57df\uff0c\u68c0\u9a8c\u7edf\u8ba1\u91cf</li> <li>\u7b2c\u4e00\u7c7b\u9519\u8bef\uff08\u5f03\u771f\uff09\uff0c\u7b2c\u4e8c\u7c7b\u9519\u8bef\uff08\u53d6\u4f2a\uff09</li> <li>\u663e\u8457\u6027\u6c34\u5e73</li> </ul>"},{"location":"Mathematics_Basis/TC/Lec_1/","title":"Lecture 1","text":""},{"location":"Mathematics_Basis/TC/Lec_1/#introduction","title":"Introduction","text":""},{"location":"Mathematics_Basis/TC/Lec_1/#1automata-theory","title":"1.Automata theory","text":"<ul> <li>finite automata</li> <li>pushdown automata</li> <li>Turing machines<ul> <li>Church-Turing Thesis</li> </ul> </li> </ul>"},{"location":"Mathematics_Basis/TC/Lec_1/#2-compuatability-theory","title":"2. Compuatability Theory","text":""},{"location":"Mathematics_Basis/TC/Lec_1/#3-complexity-theory-hardness-and-complexity-class","title":"3. Complexity theory:  hardness and complexity class.","text":""},{"location":"Mathematics_Basis/TC/Lec_1/#problem","title":"Problem","text":"<p>Definition</p> <p>Optimization Problem</p> <p>e.g. Given \\(G: (V, E, W)\\), what is the minimum spanning tree?</p> <p>Search Problem</p> <p>e.g. Given \\(G\\) and an integer \\(k\\), find a spanning tree whose weight is at most \\(k\\) or tells such a tree not exist.</p> <p>Decision Problem</p> <p>e.g. Given \\(G\\) and \\(k\\), is there a spanning tree with weight at most \\(k\\)?</p> <p>Counting Problem</p> <p>e.g. Given \\(G\\) and \\(k\\), what is the number of spanning trees with weight at most \\(k\\)?</p> <p>Since decision problem is the easiest among these four, and it's relatively easy to answer (just yes or no), the problem we discuss is limited to decision problem. If decision problem is hard, so as others.</p> <p>If we specify \\(G\\) and \\(k\\) of the decision problem above, we have an instance of the problem. If the answer of the instance is yes, then we call it yes-instance, otherwise no-instance.</p> <p>For computer, we need to encode the problem to make computer understand. So the decision problem above is encoded to</p> <p>Given a string \\(W\\), is \\(w \\in \\{\\text{encode}&lt;G, k&gt;: &lt;G, k&gt; \\text{is a yes-instance}\\}\\)?</p> <p>So we abstract the problem to a set, which we call a language.</p> <p>Definition | Alphabet and String</p> <p>An alphabet \\(\\Sigma\\) is a finite set of symbols. e.g. \\(\\Sigma = \\{0 , 1\\}\\), \\(\\Sigma = \\emptyset\\).</p> <p>A string is a finite sequence of symbols from some \\(\\Sigma\\).</p> <ul> <li>The length of a string \\(|w|\\) is the number of symbols in \\(w\\).</li> <li>Specially, we use \\(e\\) to denote an empty string with \\(|e| = 0\\).</li> </ul> <p>\\(\\Sigma^i\\) is the set of all strings of length \\(i\\) over \\(\\Sigma\\).</p> <ul> <li> <p>e.g. for \\(\\Sigma = \\{0, 1\\}\\), \\(\\Sigma^0 = \\{e\\}\\), \\(\\Sigma^1 = \\{0, 1\\}\\), \\(\\Sigma^2 = \\{00, 01, 10, 11\\}\\).</p> </li> <li> <p>Specially, \\(\\Sigma^* = \\bigcup\\limits_{i \\ge 0} \\Sigma^i\\), \\(\\Sigma^+ = \\bigcup\\limits_{i \\ge 1} \\Sigma^i\\).</p> </li> </ul> <p>Definition | Operation of String</p> <ul> <li> <p>concatnation</p> <ul> <li>\\(u = a_1\\dots a_n, v = b_1\\dots b_m\\), \\(w = uv = a_1\\dots a_nb_1\\dots b_m\\).</li> </ul> </li> <li> <p>exponentiation</p> <ul> <li>\\(w^i = \\underbrace{w \\cdots w}_{i\\  \\text{times}}\\).</li> </ul> </li> <li> <p>reversal</p> <ul> <li>\\(w = a_1\\dots a_n\\), \\(w^R = a_n\\dots a_1\\).</li> </ul> </li> </ul> <p>Definition | Language</p> <p>A language over \\(\\Sigma\\) is a subset \\(L \\subseteq \\Sigma^*\\).</p> <p>Thesis</p> <p>Decision problems is equivalent to languages. </p>"},{"location":"Mathematics_Basis/TC/Lec_1/#finite-automata","title":"Finite Automata","text":"state diagram   state diagram  <p>The circle with an arrow pointing to it is initial state and the double circle is final state. Initial state is unique while final state can be several.</p> <p>Definition</p> <p>A deterministic finite automata (DFA) is \\(M = (K, \\Sigma, \\delta, s, F)\\), where</p> <ul> <li>\\(K\\) is a finite set of states.</li> <li>\\(\\Sigma\\) is the input alphabet.</li> <li>\\(s \\in k\\) is initial state.</li> <li>\\(F \\subseteq k\\) is the set of final states.</li> <li>\\(\\delta\\) is a transition function. It describes in the current state, when read a symbol, what will the next state be.</li> </ul> \\[     \\delta: K \\times \\Sigma \\rightarrow K. \\] <p>Definition</p> <p>A configuration is an element of \\(K \\times \\Sigma^*\\) (current state and unread input). It pairs the current state the machine is in with the remaining input that it still has to process.</p> <p>Yields represents the transition from one configuration to another.</p> <ul> <li> <p>Yields in one step is a transition to the next state over 1 symbol of input. This is indicated by the yields or right tack symbol: \\(\\vdash\\). For a specific automata \\(M\\), we denote it as \\(\\vdash_M\\).</p> <ul> <li>\\((q, w) \\vdash_M (q', w')\\) if \\(w = aw'\\) for some \\(a \\in \\Sigma\\) and \\(\\delta(q, a) = q'\\).</li> </ul> </li> <li> <p>For abbreviating the transition across multiple steps, we use \\(\\vdash_M^*\\) (Yields).</p> <ul> <li>\\((q, w) \\vdash_M^* (q', w')\\) if \\((q, w) = (q', w')\\) or \\((q, w) \\vdash_M \\cdots \\vdash_M (q', w')\\).</li> </ul> </li> </ul> Example <p>For the automata in upper part of the state diagram above, if the configuration is \\((q_0, 1001)\\). We can describe the yields as</p> \\[     (q_0, 1001) \\vdash_M (q_0, 001) \\vdash_M (q_1, 01) \\vdash_M (q_2, 1) \\vdash_M (q_2, e) \\] <p>Definition</p> <p>\\(M\\) accepts \\(w \\in \\Sigma^*\\) if \\((s, w) \\vdash_M^* (q, e)\\) for some \\(q \\in F\\).</p> <p>For \\(L(M) = \\{ w \\in \\Sigma^*: M \\text{ accepts } w \\}\\), we say \\(M\\) accepts \\(L(M)\\).</p> <p>Definition</p> <p>A language is regular\uff08\u6b63\u5219\u7684\uff09 if it is accepted by some FA (finite automata).</p> <p>Definition</p> <p>Regular Operations</p> <p>If \\(A\\) and \\(B\\) are languages,</p> <ul> <li>Union \\(A \\cup B = \\{w | w \\in A \\text{ or } w \\in B\\}\\).</li> <li>Concatnation \\(A \\cdot B = \\{ab | a \\in A \\text{ and } b \\in B\\}\\).</li> <li>Kleene Star \\(A^* = \\{w_1w_2\\cdots w_k| w_i \\in A \\text{ and } k \\ge 0\\}\\).</li> </ul> <p>Theorem</p> <p>If \\(A\\) and \\(B\\) are regular, so is \\(A \\cup B\\).</p> Proof <p>\\(\\exists M_A = (K_A, \\Sigma, \\delta_A, s_A, F_A)\\) accepts \\(A\\), and \\(\\exists M_B = (K_B, \\Sigma, \\delta_B, s_B, F_B)\\) accepts \\(A\\).</p> <p>We construct \\(\\exists M_\\cup = (K_\\cup, \\Sigma, \\delta_\\cup, s_\\cup, F_\\cup)\\) where</p> <ul> <li>\\(K_\\cup = K_A \\times K_B\\).</li> <li>\\(s_\\cup = (s_A, s_B)\\).</li> <li>\\(F_\\cup = \\{(q_A, q_B) \\in K_A \\times K_B | q_A \\in F_A \\text{ or } q_B \\in F_B\\}\\).</li> <li> <p>\\(\\delta_\\cup\\): for any \\(q_A \\in K_A\\), \\(q_B \\in K_B\\), \\(a \\in \\Sigma\\),</p> \\[     \\delta((q_A, q_B), a) = (\\delta_A(q_A, a), \\delta_B(q_B, a)) \\] </li> </ul> <p>Then \\(M_\\cup\\) accepts \\(A \\cup B\\).</p> <p>Theorem</p> <p>If \\(A\\) and \\(B\\) are regular, so is \\(A \\cap B\\). If \\(A\\) is regular, so is \\(\\overline{A}\\).</p>"},{"location":"Mathematics_Basis/TC/Lec_10/","title":"Lecture 10","text":""},{"location":"Mathematics_Basis/TC/Lec_10/#numerical-functions","title":"Numerical Functions","text":"<p>Definition</p> <p>A Turing machine \\(M\\) computes \\(f: \\mathbb{N}^K \\rightarrow \\mathbb{N}\\) if \\(\\forall n_1, \\dots, n_k \\in \\mathbb{N}\\),</p> \\[     M(\\text{bin}(n_1), \\text{bin}(n_2), \\dots, \\text{bin}(n_k)) = \\text{bin}(f(n_1, \\dots, n_k)), \\] <p>where \\(\\text{bin}(x)\\) is the binary encoding of \\(x\\).</p> <p>And we say \\(f\\) is computable.</p>"},{"location":"Mathematics_Basis/TC/Lec_10/#primitive-recursive-function","title":"Primitive Recursive Function","text":""},{"location":"Mathematics_Basis/TC/Lec_10/#basic-functions","title":"Basic Functions","text":"<ol> <li>zero function</li> </ol> \\[ \\text{zero}(n_1, \\dots, n_k) = 0. \\] <ol> <li>identity function</li> </ol> \\[ \\text{id}_{k, j}(n_1, \\dots, n_k) = n_j. \\] <ol> <li>successor function</li> </ol> \\[ \\text{succ}(n) = n + 1. \\] <p>Theorem</p> <p>Basic functions are computable.</p>"},{"location":"Mathematics_Basis/TC/Lec_10/#composition","title":"Composition","text":"<p>If we have a computable function \\(g: \\mathbb{N}^k \\rightarrow \\mathbb{N}\\) and several computable functions \\(h_1, \\dots, h_k: \\mathbb{N}^l \\rightarrow \\mathbb{N}\\). The way to contruct the following function \\(f: \\mathbb{N}^l \\rightarrow \\mathbb{N}\\) is called composition.</p> \\[     f(n_1, \\dots, n_l) = g(h_1(n_1, \\dots, n_l), \\dots, h_k(n_1, \\dots, n_l)). \\]"},{"location":"Mathematics_Basis/TC/Lec_10/#recursive-definition","title":"Recursive Definition","text":"<p>If we have two computable functions \\(g: \\mathbb{N}^k \\rightarrow \\mathbb{N}\\) and \\(h: \\mathbb{N}^{k + 2} \\rightarrow \\mathbb{N}\\). The way to contruct the following function \\(f: \\mathbb{N}^{k + 1} \\rightarrow \\mathbb{N}\\) is called recursive definition.</p> <ul> <li>\\(f(n_1, \\dots, n_k, 0) = g(n_1, \\dots, n_k)\\).</li> <li>\\(\\forall t \\in \\mathbb{N}\\), \\(f(n_1, \\dots, n_k, t + 1) = h(n_1, \\dots, n_k, t, f(n_1, \\dots, n_k, t))\\).</li> </ul> <p>Example</p> <p>A typical recursive function is factorial: \\(f(0) = 1\\) and \\(f(n + 1) = (n + 1)f(n)\\). In this case, we have</p> <ul> <li>\\(k = 0\\).</li> <li>\\(g(n) = 1\\).</li> <li>\\(h(n, f(n)) = (n + 1)f(n)\\).</li> </ul> <p>Definition</p> <p>Primitive recursive functions are the basic functions and those obtained from the basic functions by applying composition and recursive definition a finite number of times.</p> <p>Theorem</p> <p>Primitive recursive functions are computable.</p> <p>Definition</p> <p>If the output of the primitive recursive function is either 0 or 1, then it's called a predicate.</p> <p>Theorem</p> <p>If \\(g\\), \\(h\\) are primitive recursive and \\(p\\) is a predicate, then</p> \\[     f(n_1, \\dots, n_k) = \\left\\{\\begin{aligned}         g(n_1, \\dots, n_k), &amp; \\text{ if } p(n_1, \\dots,n _k), \\\\         h(n_1, \\dots, n_k), &amp; \\text{ otherwise }.     \\end{aligned}\\right. \\] <p>is also primitive recursive.</p> <p>Here we give some examples of primitive recursive functions.</p> <p>Example</p> <ol> <li> <p>\\(\\text{plus2}(n) = n + 2\\)</p> \\[     \\text{succ}(\\text{succ}(n)) \\] </li> <li> <p>\\(\\text{plus}(m, n) = m + n\\)</p> \\[ \\left\\{ \\begin{aligned}     \\text{plus}(m, 0) &amp;= m = \\text{id}_{1, 1}(m), \\\\     \\text{plus}(m, n + 1) &amp;= \\text{plus}(m, n) + 1 = \\text{succ}(\\text{plus(m, n)}). \\end{aligned} \\right. \\] <p>But it's not strict enough actually. The following is the a stricter version. Since it's trivial, we needn't actually expand it commonly.</p> \\[ \\left\\{ \\begin{aligned}     \\text{plus}(m, 0) &amp;= m = \\text{id}_{1, 1}(m), \\\\     \\text{plus}(m, n + 1) &amp;= \\text{succ}(\\text{plus(m, n)}) = h(m, n, \\text{plus}(m, n)). \\end{aligned} \\right. \\] <p>where</p> \\[     h(m, n, \\text{plus}(m, n)) = \\text{succ}(\\text{id}_{3,3}(m, n, \\text{plus}(m, n))). \\] </li> <li> <p>\\(\\text{mult}(m, n) = m \\cdot n\\)</p> \\[ \\left\\{ \\begin{aligned}     \\text{mult}(m, 0) &amp;= 0 = \\text{zero}(m), \\\\     \\text{mult}(m, n + 1) &amp;= \\text{mult}(m, n) + m = \\text{plus}(\\text{mult(m, n)}, m). \\end{aligned} \\right. \\] </li> <li> <p>\\(\\text{exp}(m, n) = m^n\\)</p> \\[ \\left\\{ \\begin{aligned}     \\text{exp}(m, 0) &amp;= 1 = \\text{succ}(\\text{zero}(m)), \\\\     \\text{exp}(m, n + 1) &amp;= \\text{exp}(m, n) \\cdot m = \\text{mult}(\\text{exp(m, n)}, m). \\end{aligned} \\right. \\] </li> <li> <p>constant function: \\(f(n_1, \\dots, n_k) = c\\)</p> \\[     \\underbrace{\\text{succ}(\\dots(\\text{succ}}_{c \\text{ times}}(\\text{zero}(n_1, \\dots, n_k)) \\dots ) \\] </li> <li> <p>positive / sign function: \\(\\text{positive}(n) = \\text{sgn}(n) = \\left\\{\\begin{aligned}     1, n &gt; 0, \\\\ 0, n = 0. \\end{aligned}\\right.\\)</p> \\[ \\left\\{ \\begin{aligned}     \\text{sgn}(0) &amp;= 0, \\\\     \\text{sgn}(n + 1) &amp;= 1. \\end{aligned} \\right. \\] </li> <li> <p>predecessor function: \\(\\text{pred}(n) = \\left\\{\\begin{aligned}     &amp;n - 1, &amp;n &gt; 0, \\\\ &amp;0, &amp;n = 0. \\end{aligned}\\right.\\)</p> \\[ \\left\\{ \\begin{aligned}     \\text{pred}(0) &amp;= 0, \\\\     \\text{pred}(n + 1) &amp;= n = \\text{id}_{2, 1}(n, \\text{pred}(n)) = n. \\end{aligned} \\right. \\] </li> <li> <p>non-negative substraction \\(m \\sim n = \\max(m - n, 0)\\)</p> \\[ \\left\\{ \\begin{aligned}     m \\sim 0 &amp;= m, \\\\     m \\sim (n + 1) &amp;= (m \\sim n) \\sim 1 = \\text{pred}(m \\sim n). \\end{aligned} \\right. \\] </li> <li> <p>\\(\\text{iszero}(n) = \\left\\{\\begin{aligned}     &amp;1, &amp;n = 0, \\\\ &amp;0, &amp;n &gt; 0. \\end{aligned}\\right.\\)</p> \\[     1 \\sim \\text{positive}(n) \\] </li> <li> <p>\\(\\text{geq}(m, n) = \\left\\{\\begin{aligned}     &amp;1, &amp;m \\ge n, \\\\ &amp;0, &amp;m &lt; n. \\end{aligned}\\right.\\)</p> \\[     \\text{iszero}(n \\sim m) \\] </li> <li> <p>\\(\\text{eq}(m, n) = \\left\\{\\begin{aligned}     &amp;1, &amp;m = n, \\\\ &amp;0, &amp;m \\neq n. \\end{aligned}\\right.\\)</p> \\[     \\text{geq}(m, n) \\cdot \\text{geq}(n, m) \\] </li> <li> <p>\\(\\text{rem}(m, n) = m\\ \\%\\ n\\).</p> \\[ \\left\\{ \\begin{aligned}     \\text{rem}(0, n) &amp;= 0, \\\\     \\text{rem}(m, n) &amp;= \\left\\{\\begin{aligned}         &amp; 0, &amp;&amp; \\text{if $m + 1$ is divisible by $n$}, \\\\         &amp; \\text{rem}(m, n), &amp;&amp; \\text{otherwise}.     \\end{aligned}\\right. \\end{aligned} \\right. \\] <p>where \\(m + 1 \\text{ is divisible by } n \\Leftrightarrow \\text{eq}(\\text{rem}(m, n), \\text{pred}(n))\\).</p> </li> <li> <p>\\(\\text{div}(m, n) = \\lfloor m / n \\rfloor\\). (\\(n \\neq 0\\))</p> \\[ \\left\\{ \\begin{aligned}     \\text{div}(0, n) &amp;= 0, \\\\     \\text{div}(m, n) &amp;= \\left\\{\\begin{aligned}         &amp; \\text{div}(m, n) + 1, &amp;&amp; \\text{if $m + 1$ is divisible by $n$}, \\\\         &amp; \\text{div}(m, n), &amp;&amp; \\text{otherwise}.     \\end{aligned}\\right. \\end{aligned} \\right. \\] </li> <li> <p>\\(\\text{digit}(m, n, p) = a_{m - 1}\\).</p> \\[     \\text{div}(\\text{rem}(n, p^m), p^{m - 1}) \\] </li> <li> <p>\\(\\text{sum}_f(m, n) = \\sum\\limits_{k = 0}^{m} f(m, k)\\), where \\(f\\) is primitive recursive.</p> \\[ \\left\\{ \\begin{aligned}     \\text{sum}_f(0, n) &amp;= 0, \\\\     \\text{sum}_f(m, n + 1) &amp;= \\text{sum}_f(m, n) + f(m, \\text{succ}(n)). \\end{aligned} \\right. \\] <p>Wrong Consideration</p> \\[     \\text{sum}_f(m, n) = \\underbrace{f(m, 0) + f(m, 1) + \\dots + f(m, n)}_{n \\text{times}} \\] <p>Notice that the theorem that sum of two primitive recursive functions is primitive recursive can not induce that sum of \\(n\\) primitive recursive function is primitive recursive, when \\(n\\) is a variable and is an input of the function.</p> </li> <li> <p>\\(\\text{mult}_f(m, n) = \\prod\\limits_{k = 0}^{m} f(m, k)\\), where \\(f\\) is primitive recursive.</p> \\[ \\left\\{ \\begin{aligned}     \\text{mult}_f(0, n) &amp;= 0, \\\\     \\text{mult}_f(m, n + 1) &amp;= \\text{mult}_f(m, n) \\cdot f(m, \\text{succ}(n)). \\end{aligned} \\right. \\] </li> <li> <p>\\(g_p(n) = \\left\\{\\begin{aligned}     &amp;1, &amp;&amp; \\text{if } \\exists n' \\le n, p(n') = 1 , \\\\ &amp;0, &amp;&amp;\\text{otherwise}. \\end{aligned}\\right.\\), where \\(p\\) is a predicate.</p> \\[     g_p(n) = p(0) \\vee p(1) \\vee \\dots \\vee p(n) = \\text{positive} \\left(\\sum_{n'=0}^n p(n')\\right) = \\text{positive}(\\text{sum}_p(n)) \\] </li> </ol> <p>Note</p> <p>If \\(p\\) and \\(q\\) are predicates, then</p> \\[ \\begin{aligned}     p \\wedge q &amp;= p \\cdot q \\\\     p \\vee q &amp;= 1 \\sim \\text{iszero}(p + q) \\end{aligned} \\]"},{"location":"Mathematics_Basis/TC/Lec_10/#computability","title":"Computability","text":"<p>Suppose we define</p> \\[ \\begin{aligned} \\text{PR} &amp;= \\{\\langle f\\rangle | f \\text{ is a primitive recursive numerical function}\\}, \\\\ \\text{C} &amp;= \\{\\langle f\\rangle | f \\text{ is a computable numerical function}\\}. \\end{aligned} \\] <p>Lemma</p> <p>\\(\\text{PR}\\) is decidable.</p> <p>Lemma</p> <p>\\(\\text{C}\\) is undecidable.</p> <p>Proof</p> <p>Assume that \\(C\\) is decidable. Then \\(\\text{C} = \\{\\langle f\\rangle | f \\text{ is a unary computable numerical function}\\} \\subseteq C\\) is also decidable.</p> <p>Then \\(C'\\) is lexicographically Turing enumerable. Thus we can enumerate \\(C'\\) by</p> \\[     g_1, g_2, g_3, \\dots \\] <p>Now we construct a TM \\(M\\) = on input \\(n\\),</p> <ol> <li>enumerate \\(C'\\) to get \\(g_n\\).</li> <li>compute \\(g_n(n)\\).</li> <li>output \\(g_n(n) + 1\\).</li> </ol> <p>Then \\(g^*(n) = g_n(n) + 1\\) is unary computable numerical function and thus \\(g^* \\in C'\\).</p> <p>However, \\(\\forall n \\in \\mathbb{N}\\), \\(g^* \\neq g_n\\), which leads to a contradiction.</p> <p>So we can conclude that</p> \\[     \\text{PR} \\subsetneq \\text{C} \\] <p>That's not good since all computable function can't build up by that simple functions!</p> <p>But now we consider add an additional operation of the primitive recursive function: minimalization.</p>"},{"location":"Mathematics_Basis/TC/Lec_10/#minimalization","title":"Minimalization","text":"<p>Definition</p> <p>Suppose \\(g: \\mathbb{N}^{k + 1} \\rightarrow \\mathbb{N}\\), and we define</p> \\[     f(n_1, \\dots, n_k) = \\left\\{     \\begin{aligned}         &amp; \\text{minimum $n_{k + 1}$ such that $g(n_1, \\dots, n_k, n_{k + 1}) = 1$}, &amp;&amp; \\text{if such that $n_{k + 1}$ exists}. \\\\         &amp; 0, &amp;&amp; \\text{if such that $n_{k + 1}$ does not exist}.     \\end{aligned}         \\right. \\] <p>Then we call \\(f\\) is a minimalization of \\(g\\), denoted by</p> \\[     \\mu_m [g(n_1, \\dots, n_k, m) = 1]. \\] <p>Definition</p> <p>A function \\(g\\) is minimalizable if \\(g\\) is computable and \\(\\forall n_1, \\dots, n_k\\), \\(\\exists m \\ge 0\\), such that \\(g(n_1, \\dots, n_k, m) = 1\\).</p> <p>Example</p> \\[     \\log(m, n) = \\lceil \\log_{m + 2} (n + 1) \\rceil = \\mu_p[\\text{geq}((m + 2)^p, n + 1) = 1]. \\] <p>This function is minimalizable.</p> <p>Theorem</p> <p>If \\(g: \\mathbb{N}^{k + 1} \\rightarrow \\mathbb{N}\\) is minimalizable, then \\(\\mu_m[g(n_1, \\dots, n_k, m) = 1]\\) is computable.</p> <p>Theorem</p> <p>The problem that</p> <p>Given a function \\(g\\), is \\(g\\) minimalizable?</p> <p>is undecidable.</p>"},{"location":"Mathematics_Basis/TC/Lec_2/","title":"Lecture 2","text":"<p>Definition</p> <p>A non-deterministic finite automata (NFA) is \\(M = (K, \\Sigma, \\Delta, s, F)\\), where</p> <ul> <li>\\(K\\) is a finite set of states.</li> <li>\\(\\Sigma\\) is the input alphabet.</li> <li>\\(s \\in k\\) is initial state.</li> <li>\\(F \\subseteq k\\) is the set of final states.</li> <li> <p>\\(\\Delta\\) is a transition relation. </p> \\[     \\Delta \\subseteq (K \\times (\\Sigma \\cup \\{e\\})) \\times K \\] </li> </ul> <p>Configurations and yields are defined similarly to DFA.</p> <p>\\(M\\) accepts \\(w \\in \\Sigma^*\\) if \\((s, w) \\vdash_M^* (q, e)\\) for some \\(q \\in F\\).</p> <p>For \\(L(M) = \\{ w \\in \\Sigma^*: M \\text{ accepts } w \\}\\), we say \\(M\\) accepts \\(L(M)\\).</p> <p>The difference between DFA and NFA is that NFA allows</p> <ul> <li>more than one choice for next state.</li> <li>e-transition.</li> </ul>  state diagram of NFA  <p>Insight | A Magic</p> <p>We assume that NFA always makes the right guess.</p> <p>Example</p> <p>Construct an FA that accepts</p> \\[     L = \\{w \\in \\{a, b\\}^* | \\text{ the second symbol from the end of $w$ is $b$.}\\} \\] <p> </p> <p>Theorem</p> <p>DFA is equvilent to NFA.</p> <ul> <li>\\(\\forall \\text{ DFA } M', \\exist \\text{ NFA } M,\\ s.t.\\ L(M) = L(M').\\)</li> <li>\\(\\forall \\text{ NFA } M, \\exist \\text{ DFA } M',\\ s.t.\\ L(M) = L(M').\\)</li> </ul> <p>Proof</p> <p>The first claim is trivial. For the second claim,</p> <p>Idea DFA \\(M'\\) simulates \"tree-like computation\" of NFA \\(M\\).</p> <p>\\(\\forall \\text{ NFA } M' = (K, \\Sigma, \\Delta, s, F)\\), we construct a DFA \\(M' = (K', \\Sigma, \\delta, s', F')\\), where</p> <ul> <li>\\(K' = 2^K = \\{Q | Q \\subseteq K\\}\\).</li> <li>\\(F' = \\{Q \\subseteq K | Q \\cap F \\neq \\emptyset\\}\\).</li> <li>\\(s' = E(s)\\), where \\(E(q) = \\{p \\in K | (q, e) \\vdash_M^* (p, e) \\}\\), \\(\\forall q \\in K\\).</li> <li>\\(\\forall Q \\in K', a \\in \\Sigma,\\ \\ \\delta(Q, a) = \\bigcup\\limits_{q \\in Q} \\bigcup\\limits_{p : (q, a, p) \\in \\Delta} E(p)\\).</li> </ul> <p>Then we claim that \\(\\forall p, q \\in K, w \\in \\Sigma^*, (p, w) \\vdash_M^* (q, e) \\text{ iff } (E(p), w) \\vdash_{M'}^* (Q, e), \\text{ where } q \\in Q\\). (We can prove it by induction on \\(|w|\\))</p> <p>Thus, DFA \\(M'\\) accepts \\(w\\) \\(\\Leftrightarrow\\) NFA \\(M\\) accepts \\(w\\).</p> <p>Example</p> <p>NFA</p> <p> </p> <p>DFA</p> <p> </p> <p>Since the equivalence of NFA and DFA, so a language is regular if it's accepted by some NFA. Now we can prove the following theorem by NFA.</p> <p>Theorem</p> <p>If \\(A\\) and \\(B\\) are regular, so is \\(A \\cdot B\\) (or \\(A \\circ B\\)).</p> <p>Proof</p> <p> </p> <p>Since \\(A\\) and \\(B\\) are regular, thus there exist NFAs \\(M_A = (K_A, \\Sigma, \\Delta_A, s_A, F_A)\\) and \\(M_B = (K_B, \\Sigma, \\Delta_B, s_B, F_B)\\) which accepts \\(A\\) and \\(B\\) respectively.</p> <p>We construct an NFA \\(M_\\circ = (K_\\circ, \\Sigma, \\Delta_\\circ, s_\\circ, F_\\circ)\\), where</p> <ul> <li>\\(K_\\circ = K_A \\cup K_B\\).</li> <li>\\(s_\\circ = s_A\\).</li> <li>\\(F_\\circ = F_B\\).</li> <li>\\(\\Delta_\\circ = \\Delta_A \\cup \\Delta_B \\cup \\{(q, e, s_B) | q \\in F_A\\}\\).</li> </ul> <p>Theorem</p> <p>If \\(A\\) is regular, so is \\(A^*\\).</p> <p> </p>"},{"location":"Mathematics_Basis/TC/Lec_3/","title":"Lecture 3","text":""},{"location":"Mathematics_Basis/TC/Lec_3/#regular-expression","title":"Regular Expression","text":"<p>Definition</p> <p>Suppose an alphabet \\(\\Sigma\\),</p> <p>Example</p> Language Regular Expression \\(\\{e\\}\\) \\(\\phi^*\\) \\(\\{w \\in (a \\cup b)^* \\| w \\text{ starts with } a \\text{ and end with } b\\}\\) \\(a(a \\cup b)^* b\\) \\(\\{w \\in (a \\cup b)^* \\| w \\text{ has at least two occurences of } a\\}\\) \\((a \\cup b)^*a(a \\cup b)^* a(a \\cup b)^*\\) <p>Theorem</p> <p>A language \\(B\\) is regular iff there is some regular expression \\(R\\) with \\(L(R) = B\\).</p> <p>From NFA to regular expression. For any NFA \\(M\\), there exists a regular expression \\(R\\), such that \\(L(R) = L(M)\\).</p> <p>We first simplify a specific NFA \\(M\\) with the following steps to find its corresponding regular expression.</p> <ol> <li> <p>Simplify \\(M\\) so that</p> <ul> <li>no arc enters the initial state.</li> <li>only one final state, and no arc leaves it.</li> </ul> <p>For the following NFA \\(M\\),</p> <p> </p> <p>we simplify it to</p> <p> </p> </li> <li> <p>Eliminate States</p> <p>There are only three connection situations of states.</p> <p>  ---&gt;  </p> <p>  ---&gt;  </p> <p>  ---&gt;  </p> </li> </ol> Example <p>For the following instance of NFA,</p> <p> </p> <p>we can finally simplify it to</p> <p> </p> <p>Based on the mechanism above, we can simply give the following more abstract proof.</p> <p>Proof</p> <p>Now we back to the proof of the equivenlance of regular expression and NFA.</p> <p>(1) Regular expression \\(\\Rightarrow\\) NFA is trivial by definition.</p> <p>(2) We now proof NFA \\(\\Rightarrow\\) regular expression.</p> <p>Suppose an NFA \\(M = (K, \\Sigma, \\Delta, s, F)\\) satisfying</p> <ol> <li>\\(K = \\{q_1, \\dots, q_n\\}\\), \\(s = q_{n - 1}\\), \\(F = \\{q_n\\}\\).</li> <li>\\(\\forall p \\in K, a \\in \\Sigma,\\ \\ (p, a, q_{n - 1}) \\notin \\Delta\\).</li> <li>\\(\\forall p \\in K, a \\in \\Sigma,\\ \\ (q_n, a, p) \\notin \\Delta\\).</li> </ol> <p>For any NFA that not satisfying the properties above, we can use the mechanism above to make it satisfy.</p> <p>Then we define a subproblem and use the idea of DP (Dynamic Programming) to prove.</p> <p>\\(\\forall i, j, k \\in [1, n] (i, j, k \\in \\mathbb{N})\\), we define the regular expression \\(R_{ij}^{k}\\) satisfying</p> \\[     L(R_{ij}^{k}) = \\{w \\in \\Sigma^* | w \\text{ drives } M \\text{ from } q_i \\text{ to } q_j \\text{ with no intermediate state having index} &gt; k\\}. \\] <p>Then our goal is to get \\(R_{(n - 1)n}^{n - 2}\\).</p> <p>The base case (\\(k = 0\\)) is \\(\\forall i, j \\in [1, n] (i, j \\in \\mathbb{N})\\),</p> <ul> <li>if \\(i = j\\), \\(R_{ij}^0 = \\phi^* \\cup a_1 \\cup \\cdots \\cup a_m\\), where \\((q_i, a, q_j) \\in \\Delta\\).</li> <li>if \\(i \\neq j\\), \\(R_{ij}^0 = a_1 \\cup \\cdots \\cup a_m\\), where \\((q_i, a, q_j) \\in \\Delta\\).</li> </ul> <p>Then is the recurrence case.</p> \\[     R_{ij}^{k} = R_{ij}^{k - 1} \\cup R_{ik}^{k - 1}(R_{kk}^{k - 1})^* R_{kj}^{k - 1} \\] <p> </p> <p>Pumping Theorem</p> <p>Let \\(L\\) be a regular language. There exists an integer \\(p \\ge 1\\) such that \\(w \\in L\\) with \\(|w| \\ge p\\) can be divided into 3 parts \\(w = xyz\\), where</p> <ol> <li>for each integer \\(i \\ge 0\\), \\(xy^iz \\in L\\).</li> <li>\\(|y| &gt; 0\\).</li> <li>\\(|xy| \\le p\\).</li> </ol> <p>We call the value \\(p\\) pumping length.</p> <p>NOTE This theorem is a necceasry condition of regular language but not sufficient.</p> <p>Proof</p> <p>If \\(L\\) is finite, just let \\(p = \\max\\limits_{w \\in L} |w| + 1\\). It's trivial.</p> <p>If \\(L\\) is infinite, then there exists a DFA \\(M\\) accepting \\(L\\). Let \\(p\\) be the number of the states of \\(M\\), and \\(w = a_1\\cdots a_n\\) in \\(L\\) with \\(|w| \\ge p\\).</p> <p> </p> <p>Then \\(\\exists 0 \\le i &lt; j \\le p\\), such that \\(q_i = q_j\\).</p> <p> </p> <p>So \\(x = a_1 \\cdots a_i\\), \\(y = a_{i + 1} \\cdots a_j\\) and \\(z = a_{j + 1} \\cdots a_n\\). And</p> <ol> <li>\\(xy^iz \\in L\\) for any \\(i \\in \\mathbb{N}\\).</li> <li>\\(|y| = j - i &gt; 0\\).</li> <li>\\(|xy| = j &lt; p\\).</li> </ol> <p>Example</p> <p>Show that \\(B = \\{0^n1^n | n \\in \\mathbb{N}^*\\}\\) is not regular.</p> <p>Proof</p> <p>Assume \\(B\\) is regular. Let \\(p\\) be the pumping length given by the pumping theorem. Then \\(\\forall w \\in B, |w| \\ge p\\), or particularly, we let \\(w\\) be</p> \\[     w = 0^p 1^p \\] <p>Then \\(w\\) can be written as \\(w = xyz\\), such that</p> <ol> <li>\\(xy^iz \\in L\\) for any \\(i \\in \\mathbb{N}\\).</li> <li>\\(|y| &gt; 0\\).</li> <li>\\(|xy| \\le p\\).</li> </ol> <p>From 2. and 3., we can infer that \\(y = 0^k\\) for some integer \\(1 \\le k \\le p\\).</p> <p>However, then \\(xy^0z = 0^{p - k}1^p \\notin L\\), which leads to a contradition with 1.</p> <p>Example</p> <p>Show that \\(B = \\{w \\in \\{0, 1\\}^* | w \\text{ has equal number of } 0 \\text{ and } 1\\}\\) is not regular.</p> <p>Proof</p> <p>Simply proof by reduction. Suppose \\(B\\) is regular, then \\(B \\cap \\{0^n1^m | n, m \\in \\mathbb{N}^*\\}\\) must also be regular, which is \\(\\{0^n1^n | n \\in \\mathbb{N}^*\\}\\). But we've proved it not regular. So \\(B\\) is not regular.</p>"},{"location":"Mathematics_Basis/TC/Lec_3/#atomic","title":"Atomic","text":"<ul> <li>For a regular expression \\(\\phi\\), its language is \\(L(\\phi) = \\phi\\).</li> <li>For a regular expression \\(a \\in \\Sigma\\), its language is \\(L(a) = \\{a\\}\\).</li> </ul>"},{"location":"Mathematics_Basis/TC/Lec_3/#composite","title":"Composite","text":"<ul> <li>For a regular expression \\(L(R_1 \\cup R_2)\\), its language is \\(L(R_1 \\cup R_2) = L(R_1) \\cup L(R_2)\\).</li> <li>For a regular expression \\(L(R_1 \\circ R_2)\\), its language is \\(L(R_1 \\circ R_2) = L(R_1) \\circ L(R_2)\\).</li> <li>For a regular expression \\(L(R^*)\\), its language is \\(L(R^*) = (L(R))^*\\).</li> </ul> <p>Precedence \\(^* &gt; \\circ &gt; \\cup\\).</p>"},{"location":"Mathematics_Basis/TC/Lec_4/","title":"Lecture 4","text":""},{"location":"Mathematics_Basis/TC/Lec_4/#context-free-grammar-and-context-free-language","title":"Context-Free Grammar and Context-Free Language","text":"<p>Definition</p> <p>A context-free grammar (CFG) \\(G = (V, \\Sigma, S, R)\\), where</p> <ul> <li>\\(V\\) is a finite set of symbols.</li> <li>\\(\\Sigma \\subseteq V\\) is the set of terminals.<ul> <li>\\(V - \\Sigma\\) is the set of non-terminals.</li> </ul> </li> <li>\\(S \\in V - \\Sigma\\) is the start symbol.</li> <li>\\(R \\subseteq (V - \\Sigma) \\times V^*\\) is a finite set of rules.<ul> <li>\\((A, w)\\) means \\(A \\rightarrow w\\).</li> </ul> </li> </ul> <p>Definition</p> <ul> <li> <p>\\(\\forall x, y, u \\in U^*, A \\in U - \\Sigma\\), if \\((A, u) \\in R\\), then we have \\(xAy \\Rightarrow_G xuy\\) and we say it derive in one step.</p> </li> <li> <p>\\(\\forall w, y, u \\in U^*, A \\in U - \\Sigma\\), if \\(w = u\\) or \\(w \\Rightarrow_G \\cdots \\Rightarrow_G u\\), then we have \\(w \\Rightarrow^*_G u\\) and we say it a derivation from \\(w\\) to \\(u\\) of length \\(n\\).</p> </li> </ul> <p>Definition</p> <p>\\(G\\) generates \\(w \\in \\Sigma^*\\) if \\(S \\Rightarrow^*_G w\\).</p> <p>For \\(L(G) = \\{w \\in \\Sigma^* | G \\text{ generates } w\\}\\)., we say \\(G\\) generates \\(L(G)\\).</p> <p>Definition</p> <p>We do a leftmost derivation of a string from a grammar by</p> <ol> <li>Begin with a string consisting only of the start symbol.</li> <li>Pick the leftmost variable occurrence in the current string.</li> <li>Pick any production that has that variable on the left of the \\(\\rightarrow\\). .</li> <li>Replace the chosen occurrence of that variable by the right-hand side of the chosen production.</li> <li>Repeat steps 2-4 until the string contains only terminals.</li> </ol> <p>Similarly, we have rightmost derivation.</p> <p>Sometimes, leftmost derivation is the same as rightmost derivation.</p> <p>Example</p> <p>For grammar \\(S \\rightarrow SS\\), \\(S \\rightarrow (S)\\), \\(S \\rightarrow e\\), to get \\(()()\\) from \\(S\\),</p> <ul> <li>(leftmost) \\(S \\rightarrow SS \\rightarrow (S)S \\rightarrow ()S \\rightarrow ()(S) \\rightarrow ()()\\)</li> <li>(rightmost) \\(S \\rightarrow SS \\rightarrow S(S) \\rightarrow S() \\rightarrow (S)() \\rightarrow ()()\\)</li> </ul> <p>But sometimes not.</p> <p>Example</p> <p>For grammar \\(E \\rightarrow E + E\\), \\(E \\rightarrow E \\times E\\), \\(E \\rightarrow (E)\\), \\(E \\rightarrow a\\) to get \\(a + a \\times a\\) from \\(S\\),</p> <p>We call it an ambiguous expression. To find out the difference, we first introduce parse tree.</p>"},{"location":"Mathematics_Basis/TC/Lec_4/#parse-tree","title":"Parse Tree","text":"<p>Definition</p> <p>Parse trees are a way of recording derivations that focus less upon the order in which derivation steps were applied and more on which productions were employed.</p> <p>A parse tree for a string \\(s\\) in a languages described by a CFG is a tree in which</p> <ol> <li>The root of the tree is labeled with \\(S\\).</li> <li>Each internal node is labeled with a variable.</li> <li>Each leaf node is labeled with a terminal.</li> <li>The leaf nodes, read from left to right, provide the string \\(s\\).</li> <li>Each internal node labeled with a variable \\(A\\) has children labeled with the symbols of \\(\\alpha\\) (reading left to right through the children) only if \\(A \\rightarrow \\alpha\\) is a production in the grammar.</li> </ol> <p>Example</p> <p>For the two cases above, their corresponding parse trees are</p> <pre><code>graph TD;\n    N1((\"S\"))\n    N2((\"S\"))\n    N3((\"S\"))\n    N4((\"(\"))\n    N5((\"S\"))\n    N6((\")\"))\n    N7((\"(\"))\n    N8((\"S\"))\n    N9((\")\"))\n    N10((\"e\"))\n    N11((\"e\"))\n    N1 === N2\n    N1 === N3\n    N2 === N4\n    N2 === N5\n    N2 === N6\n    N3 === N7\n    N3 === N8\n    N3 === N9\n    N5 === N10\n    N8 === N11</code></pre> First Probability (multiply then add)Second Probability (add then muliply) <pre><code>graph TD;\n    N1((\"E\"))\n    N2((\"E\"))\n    N3((\"+\"))\n    N4((\"E\"))\n    N5((\"a\"))\n    N6((\"E\"))\n    N7((\"\u00d7\"))\n    N8((\"E\"))\n    N9((\"a\"))\n    N10((\"a\"))\n    N1 === N2\n    N1 === N3\n    N1 === N4\n    N2 === N5\n    N4 === N6\n    N4 === N7\n    N4 === N8\n    N6 === N9\n    N8 === N10</code></pre> <pre><code>graph TD;\n    N1((\"E\"))\n    N2((\"E\"))\n    N3((\"\u00d7\"))\n    N4((\"E\"))\n    N5((\"E\"))\n    N6((\"+\"))\n    N7((\"E\"))\n    N8((\"a\"))\n    N9((\"a\"))\n    N10((\"a\"))\n    N1 === N2\n    N1 === N3\n    N1 === N4\n    N2 === N5\n    N2 === N6\n    N2 === N7\n    N4 === N8\n    N5 === N9\n    N7 === N10</code></pre> <p>For the latter case, to eliminate ambiguity, we can define another grammar,</p> \\[ \\begin{aligned} E &amp;\\rightarrow E + T \\\\ E &amp;\\rightarrow T \\\\ T &amp;\\rightarrow T \\times F \\\\ T &amp;\\rightarrow F \\\\ F &amp;\\rightarrow (E) \\\\ F &amp;\\rightarrow a \\\\ \\end{aligned} \\] <p>Then we can build the unique parse tree of \\(a + a \\times a\\).</p> <pre><code>graph TD;\n    N1((\"E\"))\n    N2((\"E\"))\n    N3((\"+\"))\n    N4((\"T\"))\n    N5((\"T\"))\n    N6((\"T\"))\n    N7((\"\u00d7\"))\n    N8((\"F\"))\n    N9((\"F\"))\n    N10((\"F\"))\n    N11((\"a\"))\n    N12((\"a\"))\n    N13((\"a\"))\n\n    N1 === N2\n    N1 === N3\n    N1 === N4\n    N2 === N5\n    N4 === N6\n    N4 === N7\n    N4 === N8\n    N5 === N9\n    N6 === N10\n    N8 === N11\n    N9 === N12\n    N10 === N13</code></pre> <p>Definition</p> <p>A CFG is in Chomsky normal form (CNF) if every rule of it is of one of the following forms.</p> <ul> <li>\\(S \\rightarrow e\\).</li> <li>\\(A \\rightarrow BC\\), where \\(B, C \\in V - \\Sigma - \\{S\\}\\).</li> <li>\\(A \\rightarrow a\\), where \\(a \\in \\Sigma\\).</li> </ul> <p>Property</p> <p>Suppose \\(G\\) is a CFG in CNF, if \\(G\\) generates a string of length \\(n \\ge 1\\), then the length of derivation is exactly \\(2n-1\\).</p> <p>Theorem</p> <p>Every CFG has an equivalent CFG in CNF.</p>"},{"location":"Mathematics_Basis/TC/Lec_4/#pushdown-automata-pda","title":"Pushdown Automata (PDA)","text":"<p>Definition</p> <p>PDA = NFA + stack</p> <p>A PDA is a 6-tuple \\(P = (K, \\Gamma, \\Sigma, \\Delta, S, F)\\),</p> <ul> <li>\\(K\\) is a finite set of states.</li> <li>\\(\\Gamma\\) is the stack alphabet.</li> <li>\\(\\Sigma\\) is the input alphabet.</li> <li>\\(S \\in k\\) is initial state.</li> <li>\\(F \\subseteq k\\) is the set of final states.</li> <li> <p>\\(\\Delta\\) is a transition relation. It describes in the current state and a string at the top of the stack, when read a symbol, what will the next state be, and then pop the top string, push another string onto the stack.</p> \\[     \\Delta \\subseteq (K \\times (\\Sigma \\cup \\{e\\}) \\times \\Gamma^*) \\times (K \\times \\Gamma^*) \\] </li> </ul> <p>Definition</p> <ul> <li> <p>Yields in one step</p> <ul> <li>\\((p, x, \\alpha) \\vdash_P (q, y, \\beta)\\) if \\(\\exists ((p, a, \\gamma), (q, \\eta))\\), such that \\(x = ay\\), \\(\\alpha = \\gamma\\tau\\) and \\(\\beta = \\gamma\\tau\\) for some \\(\\tau \\in \\Gamma^*\\).</li> </ul> </li> <li> <p>Yields</p> <ul> <li>\\((p, x, \\alpha) \\vdash_P^* (q, y, \\beta)\\) if \\((p, x, \\alpha) = (q, y, \\beta)\\) or \\((p, x, a) \\vdash_P \\cdots \\vdash_P (q, y, \\beta)\\).</li> </ul> </li> </ul> <p>Definition</p> <p>\\(P\\) accepts \\(w \\in \\Sigma\\) if \\((s, w, e) \\vdash_P^* (q, e, e)\\) for some \\(q \\in F\\).</p> <p>For \\(L(P) = \\{ w \\in \\Sigma^*: P \\text{ accepts } w \\}\\), we say \\(P\\) accepts \\(L(P)\\).</p> <p>Example</p> <p>Show that \\(\\{w \\in \\{0, 1\\}^* | \\text{ the number of 0 equals the number of 1}\\}\\) can be accepted by a PDA.</p> <p>Proof</p> <p>Construct a PDA \\(P = (K, \\Gamma, \\Sigma, \\Delta, S, F)\\), where</p> <ul> <li>\\(K = \\{s, q, f\\}\\).</li> <li>\\(\\Gamma = \\{0, 1, \\$\\}\\), where \\(\\$\\) means the bottom of the stack.</li> <li>\\(\\Sigma = \\{0, 1\\}\\).</li> <li>\\(S = s\\).</li> <li>\\(F = \\{f\\}\\).</li> <li> <p>transition relation</p> \\[ \\begin{aligned}     \\Delta = \\{ &amp; ((s, e, e), (q, \\$)), \\\\                 &amp; ((q, 0, 0), (q, 00)), \\\\                 &amp; ((q, 0, \\$), (q, 0\\$)),  \\\\                 &amp; ((q, 0, 1), (q, e)),  \\\\                 &amp; ((q, 1, 0), (q, 00)), \\\\                 &amp; ((q, 1, \\$), (q, 0\\$)),  \\\\                 &amp; ((q, 1, 1), (q, e)),  \\\\                 &amp; ((q, e, \\$), (f, e))              \\} \\end{aligned} \\] </li> </ul> <p>I think \\(\\$ \\in \\Gamma\\) here is to make \\(q\\) not the final state.</p> <p>Considering that PDA is NFA, we can construct a better PDA, with</p> <ul> <li>\\(K = \\{s\\}\\).</li> <li>\\(\\Gamma = \\{0, 1\\}\\).</li> <li>\\(\\Sigma = \\{0, 1\\}\\).</li> <li>\\(S = s\\).</li> <li>\\(F = \\{s\\}\\).</li> <li> <p>transition relation</p> \\[ \\begin{aligned}     \\Delta = \\{ &amp; ((s, 0, 1), (s, e)), \\\\                 &amp; ((s, 0, e), (s, 0)), \\\\                 &amp; ((s, 1, 0), (s, e)),  \\\\                 &amp; ((s, 1, e), (s, 1))              \\} \\end{aligned} \\] </li> </ul>"},{"location":"Mathematics_Basis/TC/Lec_5/","title":"Lecture 5","text":""},{"location":"Mathematics_Basis/TC/Lec_5/#equivalence-of-cfg-and-pda","title":"Equivalence of CFG and PDA","text":"<p>Theorem</p> <p>CFG and PDA are equivalent.</p> <ul> <li>\\(\\forall \\text{ PDA } M, \\exist \\text{ CFG } G,\\ s.t.\\ L(M) = L(G).\\)</li> <li>\\(\\forall \\text{ CFG } G, \\exist \\text{ PDA } M,\\ s.t.\\ L(G) = L(M).\\)</li> </ul> <p>Proof</p> <p>CFG \\(\\Rightarrow\\) PDA.</p> <p>\\(\\forall G = (V, \\Sigma, S, R)\\), we construct a PDA \\(M = (K, \\Sigma, \\Gamma, \\Delta, s, F)\\), where</p> <ul> <li>\\(\\Gamma = V\\),</li> <li>\\(K = \\{s, f\\}\\),</li> <li>\\(F = \\{f\\}\\),</li> <li> <p>transition relation</p> \\[ \\begin{aligned}     \\Delta = \\{         &amp; ((s, e, e), (f, S)), \\\\         &amp; ((f, e, A), (f, u)) \\text{ for each } (A, u) \\in R, \\\\         &amp; ((f, a, a), (f, e)) \\text{ for each } a \\in \\Sigma         \\} \\end{aligned} \\] </li> </ul> <p>PDA \\(\\Rightarrow\\) CFG.</p> <p>First we define simple PDA.</p> <p>A PDA \\(M = (K, \\Sigma, \\Gamma, \\Delta, s, F)\\) is simple if</p> <ol> <li>\\(|F| = 1\\),</li> <li>For each transition \\(((p, a, \\alpha), (q, \\beta)) \\in \\Delta\\),<ul> <li>either \\(\\alpha = e\\) and \\(|\\beta| = 1\\),</li> <li>or \\(|\\alpha| = 1\\) and \\(\\beta = e\\).</li> </ul> </li> </ol> <p>For any PDA \\(M\\), we first transform it to a corresponding simple PDA \\(M' = (K, \\Sigma, \\Gamma, \\Delta', s, F')\\),</p> <ol> <li>If \\(|F| &gt; 1\\), we create a new state \\(f'\\). For each \\(q \\in F\\), we add a new transition \\(((q, e, e), (f', e))\\) and set \\(F' = \\{f'\\}\\).</li> <li> <ul> <li>2.1 \\(|\\alpha| \\ge 1\\) and \\(|\\beta| \\ge 1\\).</li> <li>2.2 \\(|\\alpha| \\ge 1\\) and \\(\\beta = e\\).</li> <li>2.3 \\(\\beta = e\\) and \\(|\\beta| \\ge 1\\).</li> <li>2.4 \\(\\alpha = e\\) and \\(\\beta = e\\).</li> </ul> <p>For each transition \\(\\delta \\in \\Delta\\), if it's not satifsying the condition of simple PDA, then it only be of the following four cases.</p> <p>For 2.1 \\(((p, a, \\alpha), (q, \\beta))\\), create a new state \\(r\\) and replace it with \\(((p, a, \\alpha), (r, e))\\) and \\(((r, e, e), (q, \\beta))\\), which comes to 2.2 and 2.3.</p> <p>For 2.2 \\(((p, a, \\alpha), (q, e))\\), supposing \\(\\alpha = c_1 c_2 \\cdots c_k\\), create new states \\(r_1, \\dots, r_k\\) and replace it with \\(((p, a, c_1), (r_1, e)), ((r_1, e, c_2), (r_2, e)), \\dots ((r_{k - 1}, e, c_k), (q, e))\\).</p> <p>For 2.3, similar to 2.2.</p> <p>For 2.4 \\(((p, a, e), (q, e))\\), create a new state \\(r\\), pick some \\(c \\in \\Gamma\\), and replace it with \\(((p, a, e), (r, c))\\) and \\(((r, e, c), (q, e))\\).</p> </li> </ol> <p>Now that we have a simple PDA \\(M' = (K, \\Sigma, \\Gamma, \\Delta', s, F')\\) from the PDA \\(M\\), we construct a CFG \\(G = (V, \\Sigma, S, R)\\).</p> <p>We define the non-terminal \\(V - \\Sigma = \\{A_{pq} | \\text{ for } p, q \\in k\\}\\). And we claim that</p> \\[     A_{pq} \\Rightarrow w \\in \\Sigma^* \\Leftrightarrow w \\in \\{ u \\in \\Sigma^* | (p, u, e) \\vdash_P^* (q, e, e)\\}. \\] <p>Then \\(S = A_{sf}\\). And we construct the rules \\(R\\) by</p> <ol> <li>\\(\\forall p \\in K\\), \\(A_{pp} \\rightarrow e\\).</li> <li>\\(\\forall p, q \\in K\\),<ul> <li>\\(A_{pq} \\rightarrow A_{pr}A_{rq}, \\forall r \\in K\\).</li> <li>\\(A_{pq} \\rightarrow a A_{p'q} b, \\forall ((p, a, e), (p', \\alpha)) \\in \\Delta, ((q', b, \\alpha), (q, e)) \\in \\Delta\\) for some \\(\\alpha \\in \\Gamma\\).</li> </ul> </li> </ol> <p>Then for the claim before,</p> <ul> <li>\\(\\Rightarrow\\) : by induction on length of derivation from \\(A_{pq}\\) to \\(w\\).</li> <li>\\(\\Leftarrow\\) : by induction on the number of steps of the computation.</li> </ul> <p>Since the equivalence of PDA and CFG, we can use PDA to define context-free language. Since NFA is a subset of PDA, so regular language is a subset of context-free language.</p> <p>Theorem</p> <p>Every regular language is context-free.</p>"},{"location":"Mathematics_Basis/TC/Lec_5/#closure-property","title":"Closure Property","text":"<p>Similar to regular language, we have context-free language properties closure.</p> <p>Theorem</p> <p>If \\(A\\) and \\(B\\) are context-free, so is \\(A \\cup B\\), \\(A \\circ B\\) and \\(A^*\\). But NOT \\(A \\cap B\\) and \\(\\overline{A}\\).</p> <p>Proof</p> <p>For CFG \\(G_A = (V_A, \\Sigma, S_A, R_A)\\) and \\(G_B = (V_B, \\Sigma, S_B, R_B)\\), then</p> <ul> <li>\\(G_{A \\cup B} = (V_A \\cup V_B, \\Sigma, S_{A \\cup B}, R_A \\cup R_B \\cup \\{S \\rightarrow S_A | S_B\\})\\).</li> <li>\\(G_{A \\circ B} = (V_A \\cup V_B, \\Sigma, S_{A \\cup B}, R_A \\cup R_B \\cup \\{S \\rightarrow S_AS_B\\})\\).</li> <li>\\(G_{A^*} = (V_A \\cup V_B, \\Sigma, S_{A \\cup B}, R_A \\cup R_B \\cup \\{S \\rightarrow e\\} \\cup \\{S \\rightarrow SS_A\\})\\).</li> </ul> <p>One counter example of \\(A \\cap B\\) and \\(\\overline{A}\\) is \\(A = \\{a^i b^j c^k | i = j\\}\\) and \\(B = \\{a^i b^j c^k | j = k\\}\\). Both \\(A\\) and \\(B\\) are context-free, but \\(A \\cap B = \\{a^nb^nc^n | n \\ge 0\\}\\) is not context-free (we will prove it later after giving pumping theorem for CFL).</p> <p>Since \\(A \\cap B = \\overline{\\overline{A} \\cup \\overline{B}}\\), we can show that \\(\\overline{A}\\) is not context-free.</p>"},{"location":"Mathematics_Basis/TC/Lec_5/#pumping-theorem","title":"Pumping Theorem","text":"<p>Pumping Theorem for CFL</p> <p>Let \\(L\\) be a context-free language. There exists an integer \\(p \\ge 1\\) such that any \\(w \\in L\\) with \\(|w| \\ge p\\) can be divided into \\(w = uvxyz\\), where</p> <ol> <li>for each integer \\(i \\ge 0\\), \\(uv^ixy^iz \\in L\\).</li> <li>\\(|v| + |y| &gt; 0\\).</li> <li>\\(|vxy| \\le p\\).</li> </ol> <p>We call the value \\(p\\) pumping length.</p> <p>NOTE This theorem is a necceasry condition of regular language but not sufficient.</p> <p>Proof</p> <p>Since \\(L\\) is context-free, there exists a CFG \\(G = (V, \\Sigma, S, R)\\) with \\(L(G) = L\\). Let \\(b = \\max\\{|u| : (A, u) \\in R\\}\\), namely the largest fanout (the number of children) of nodes in the parse tree.</p> <p>If a tree with fanout less than or equal \\(b\\) has \\(n\\) leaves, then its height \\(h \\ge \\log_b n\\).</p> <p>Let \\(p = b^{|V - \\Sigma| - 1}\\), for any \\(w \\in L\\) with \\(|w| \\ge p\\). Let \\(T\\) be the parse tree of \\(w\\) with smallest number of nodes, and the height of \\(T\\) is \\(h \\ge log_b p = |V - \\Sigma| + 1\\).</p> <p>Since the length of the longest path from the root of the parse tree to its leaves is \\(|V - \\Sigma| + 1\\), with the number of nodes \\(|V - \\Sigma| + 2\\). Namely the number of non-terminals along the path is \\(|V - \\Sigma| + 1\\). Thus there must be some non-terminal \\(Q\\) that appears twice along the path.</p> <p> </p> <p>From the graph of parse tree, we can derive</p> <ol> <li>for each integer \\(i \\ge 0\\), \\(uv^ixy^iz \\in L\\).</li> <li>if \\(v = y = e\\), then there exists a parse tree smaller than \\(T\\), which leads to a contradiction. Thus \\(|v| + |y| &gt; 0\\).</li> <li>for the subtree with root \\(Q\\), its height \\(h' \\le |V - \\Sigma| + 1\\), thus \\(|vxy| = \\# leaves \\le b^{|V - \\Sigma| + 1} = p\\).</li> </ol> <p>Example</p> <p>Show that \\(L = \\{a^nb^nc^n | n \\ge 0\\}\\) is not context-free.</p> <p>Proof</p> <p>Assume \\(L\\) is context-free. Let \\(p\\) be the pumping length given by the pumping theorem. Then \\(\\forall w \\in L, |w| \\ge p\\), or particularly, we let \\(w\\) be</p> \\[     w = a^p b^p c^p. \\] <p>Then \\(w\\) can be written as \\(w = uvxyz\\), such that</p> <ol> <li>\\(uv^ixy^iz \\in L\\) for any \\(i \\in \\mathbb{N}\\).</li> <li>\\(|v| + |y| &gt; 0\\).</li> <li>\\(|vxy| \\le p\\).</li> </ol> <p>From 2. and 3., we can infer there will be three cases.</p> <ul> <li>\\(|vxy| = b^p\\).</li> <li>\\(|vxy| = a^{p - k}b^k\\) for some  for some integer \\(1 \\le k \\le p\\).</li> <li>\\(|vxy| = b^{p - k}c^k\\) for some  for some integer \\(1 \\le k \\le p\\).</li> </ul> <p>However, in all these three cases, \\(uv^0xy^0z \\notin L\\), which leads to a contradition with 1.</p>"},{"location":"Mathematics_Basis/TC/Lec_6/","title":"Lecture 6","text":"<p>FA and PDA, as we mentioned in previous lectures, can only accept a few of the languages. Now we are going to give a stronger automata, the Turing machine.</p>"},{"location":"Mathematics_Basis/TC/Lec_6/#turing-machine","title":"Turing Machine","text":"<p>Definition</p> <p>A deterministic Turing machine (DTM) is a 5-tuple \\(M = (K, \\Sigma, \\delta, s, H)\\), where</p> <ul> <li>\\(K\\) is a finite set of states.</li> <li>\\(\\Sigma\\) is the tape alphabet, containing the left end symbol \\(\\triangleright\\) the blank \\(\\sqcup\\).</li> <li>\\(s \\in K\\) is the initial state.</li> <li>\\(H \\subseteq K\\) is a set of halting state.</li> <li> <p>\\(\\delta\\) is a transition function. It describes in the current state, when read a symbol, what will the next state be and the head action be (move or write).</p> \\[     \\delta: (K - H) \\times \\Sigma \\rightarrow K \\times (\\{\\leftarrow, \\rightarrow\\} \\cup (\\Sigma - \\{\\triangleright\\})). \\] <ul> <li> <p>Also, there is a limitation of \\(\\delta\\).</p> \\[     \\forall q \\in K - H, \\exists p \\in K,\\ \\ s.t.\\  \\delta(q, \\triangleright) = (p, \\rightarrow) \\] </li> </ul> </li> </ul> <p>Definition</p> <p>A configuration of a Turing machine is a member of the set</p> \\[     K \\times \\triangleright (\\Sigma - \\{\\triangleright\\})^* \\times (\\{e\\} \\cup (\\Sigma - \\{\\triangleright\\})^* (\\Sigma - \\{\\triangleright, \\sqcup\\})) \\] <p>Specially, \\((q, \\triangleright w \\underline{a} u)\\) is a halting configuration if \\(q \\in H\\).</p> <p>Definition</p> <ul> <li>Yields in one step: \\((q_1, \\triangleright w_1 \\underline{a_1} u_1) \\vdash_M (q_2, \\triangleright w_2 \\underline{a_2} u_2)\\) if either of the following cases.<ul> <li>Writing: \\(\\delta(q_1, a_1) = (q_2, a_2), w_2 = w_1, u_2 = u_1\\).</li> <li>Moving Left: \\(\\delta(q_1, a_1) = (q_2, \\leftarrow), w_1 = w_2 q_2, u_2 = a_1 u_1\\) (Specially, \\(u_2 = e\\) if \\(a_1 = \\sqcup, u_1 = e\\)).</li> <li>Moving Right: \\(\\delta(q_1, a_1) = (q_2, \\rightarrow), w_2 = w_1 a_1, u_1 = a_2 u_2\\) (Specially, \\(u_1 = e\\) if \\(a_2 = \\sqcup, u_2 = e\\)).</li> </ul> </li> <li>Yields in \\(N\\) steps: \\((q_1, \\triangleright w_1 \\underline{a_1} u_1) \\vdash_M^N (q_2, \\triangleright w_2 \\underline{a_2} u_2)\\) if</li> </ul> \\[ (q_1, \\triangleright w_1 \\underline{a_1} u_1) \\underbrace{\\vdash_M \\cdots \\vdash_M}_{N \\text{ steps}} (q_2, \\triangleright w_2 \\underline{a_2} u_2). \\] <ul> <li> <p>Yields: \\((q_1, \\triangleright w_1 \\underline{a_1} u_1) \\vdash_M^* (q_2, \\triangleright w_2 \\underline{a_2} u_2)\\) if \\((q_1, \\triangleright w_1 \\underline{a_1} u_1) \\vdash_M \\cdots \\vdash_M (q_2, \\triangleright w_2 \\underline{a_2} u_2)\\)</p> <ul> <li>either \\((q_1, \\triangleright w_1 \\underline{a_1} u_1) = (q_2, \\triangleright w_2 \\underline{a_2} u_2)\\).</li> <li>or \\((q_1, \\triangleright w_1 \\underline{a_1} u_1) \\vdash_M \\cdots \\vdash_M (q_2, \\triangleright w_2 \\underline{a_2} u_2)\\) in some \\(k\\)(\\(k \\in \\mathbb{N}^*\\)) steps.</li> </ul> </li> </ul> <p>Now let's fix \\(\\Sigma\\), we give some simple Turing Machine examples.</p> <p>Example</p> <p>Symbol writing machine</p> <p>\\(M_a = (\\{s, h\\}, \\Sigma, \\delta, s, \\{h\\})\\) (\\(a \\in \\Sigma - \\{\\triangleright\\}\\)), where</p> <ul> <li>\\(\\forall b \\in \\Sigma - \\{\\triangleright\\}\\), \\(\\delta(s, b) = (h, a)\\).</li> <li>\\(\\delta(s, \\triangleright) = \\delta(s, \\rightarrow)\\).</li> </ul> <p>Heading moving machine</p> <p>\\(M_\\leftarrow = (\\{s, h\\}, \\Sigma, \\delta, s, \\{h\\})\\), where</p> <ul> <li>\\(\\forall b \\in \\Sigma - \\{\\triangleright\\}\\), \\(\\delta(s, b) = (h, \\leftarrow)\\).</li> <li>\\(\\delta(s, \\triangleright) = \\delta(s, \\rightarrow)\\).</li> </ul> <p>\\(M_\\rightarrow = (\\{s, h\\}, \\Sigma, \\delta, s, \\{h\\})\\), where</p> <ul> <li>\\(\\forall b \\in \\Sigma\\), \\(\\delta(s, b) = (h, \\rightarrow)\\).</li> </ul> <p>We call \\(M_a\\), \\(M_\\leftarrow\\) and \\(M_\\rightarrow\\) basic machines, sometimes we abbreviate them to \\(a\\), \\(L\\), \\(R\\).</p> <p>For simplicity, we introduce the following symbols.</p> <p>  ---&gt;  </p> <p>  ---&gt;  </p> <p>And some abbreviations.</p> <ul> <li>\\(R_\\sqcup\\) find the next blank symbol on the right. </li> <li>\\(L_\\sqcup\\) find the next blank symbol on the left.</li> <li>\\(R_{\\overline{\\sqcup}}\\) find the next non-blank symbol on the right.</li> <li>\\(L_{\\overline{\\sqcup}}\\) find the next non-blank symbol on the left.</li> </ul> <p> </p> <p>Example</p> <p>Left-shifting Machine \\(S_\\leftarrow\\)</p> \\[     \\forall w \\in (\\Sigma - \\{\\triangleright, \\sqcup\\})^*, \\exists h \\in H,\\ \\ s.t.\\ \\ (s, \\triangleright\\sqcup\\sqcup w \\sqcup) \\vdash_M^*(h, \\triangleright\\sqcup w\\sqcup). \\] <p> </p>"},{"location":"Mathematics_Basis/TC/Lec_6/#language-recognization","title":"Language Recognization","text":"<p>Definition</p> <p>input alphabet: \\(\\Sigma_0 \\subseteq (\\Sigma - \\{\\triangleright, \\sqcup\\}\\).</p> <p>input configuration: \\((s, \\triangleright \\underline{\\sqcup} w)\\).</p> <p>For \\(L(M) = \\{ w \\in \\Sigma_0^*: (s, \\triangleright \\underline{\\sqcup} w) \\vdash_M^* (h, \\dots), h \\in H \\}\\), we say \\(M\\) semidecides \\(L(M)\\) and \\(L(M)\\) is recursively enumerable / recognizable.</p> <p>Suppose a Turing machine \\(M = (K, \\Sigma_0, \\Sigma, \\delta, s, \\{y, n\\})\\), we say \\(M\\) decides a language \\(L \\subseteq \\Sigma_0\\) if</p> <ul> <li>\\(\\forall w \\in L, (s, \\triangleright \\underline{\\sqcup} w) \\vdash_M^* (y, \\dots)\\), we say \\(M\\) accepts \\(w\\).</li> <li>\\(\\forall w \\in \\Sigma_0^* - L, (s, \\triangleright \\underline{\\sqcup} w) \\vdash_M^* (n, \\dots)\\), we say \\(M\\) rejects \\(w\\).</li> </ul> <p>and \\(L\\) is recursive / decidable.</p> <p>Theorem</p> <p>If \\(L\\) is recursive, then \\(L\\) must be recursively enumerable.</p>"},{"location":"Mathematics_Basis/TC/Lec_6/#function-computation","title":"Function Computation","text":"<p>Definition</p> <p>\\(\\forall w \\in \\Sigma_0^*\\), if \\((s, \\triangleright \\underline{\\sqcup} w) \\vdash_M^* (h, \\triangleright \\underline{\\sqcup} y)\\) for some \\(h \\in H\\) and some \\(y \\in \\Sigma_0^*\\), we say \\(y\\) is the output of \\(M\\) on \\(w\\), denoted by \\(y = M(w)\\).</p> <p>Let \\(f : \\Sigma_0^* \\rightarrow \\Sigma_0^*\\), we say \\(M\\) computes \\(f\\) if</p> \\[     \\forall w \\in \\Sigma_0^*,\\ \\ M(w) = f(w). \\] <p>and \\(f\\) is recursive / computable.</p> <p>Example</p> <p>Show that \\(\\{a^nb^nc^n | n \\ge 0\\}\\) is recursive.</p>"},{"location":"Mathematics_Basis/TC/Lec_7/","title":"Lecture 7","text":""},{"location":"Mathematics_Basis/TC/Lec_7/#variant-of-turing-machine","title":"Variant of Turing Machine","text":"<p>There are some variants of the original Turing machine (TM), but they can be proved to be equivalent to TM.</p>"},{"location":"Mathematics_Basis/TC/Lec_7/#multitape-turing-machine","title":"Multitape Turing Machine","text":"<p>Definition</p> <p>A \\(k\\)-tape Turing machine is a 5-tuple \\(M = (K, \\Sigma, \\delta, s, H)\\), where the only difference from TM is</p> \\[     \\delta: (K - H) \\times \\Sigma^k \\rightarrow K \\times (\\{\\leftarrow, \\rightarrow\\} \\cup (\\Sigma - \\{\\triangleright\\})^k). \\] <p>Tip</p> <p>The idea to prove the equivalence between multitape TM and TM is to treat each element of the single tape of TM as a set of the elements on the corresponding position on the \\(k\\)-tapes.</p>"},{"location":"Mathematics_Basis/TC/Lec_7/#two-way-infinite-tape-turing-machine","title":"Two-way Infinite Tape Turing Machine","text":"<p>Definition</p> <p>A two-way infinite tape Turing machine is a 5-tuple \\(M = (K, \\Sigma, \\delta, s, H)\\), where the only difference from TM is</p> <ul> <li>\\(\\Sigma\\) no longer contains the left end symbol \\(\\triangleright\\).</li> </ul> <p>Tip</p> <p>The idea to prove the equivalence is to treat the two-way infinite tape TM as a 2-tape Turing machine.</p>"},{"location":"Mathematics_Basis/TC/Lec_7/#multi-head-turing-machine","title":"Multi Head Turing Machine","text":"<p>Just multiple heads to move.</p>"},{"location":"Mathematics_Basis/TC/Lec_7/#two-dimensional-tape-turing-machine","title":"Two-dimensional Tape Turing Machine","text":"<p>Build a mapping of integer to a 2-tuple of integer.</p>"},{"location":"Mathematics_Basis/TC/Lec_7/#random-access-turing-machine","title":"Random Access Turing Machine","text":"<p>The step of movement of the head can be multiple.</p> <p>Well, all of the variants above are trivial. The most important variant is the following one, non-deterministic TM.</p>"},{"location":"Mathematics_Basis/TC/Lec_7/#non-deterministic-turing-machine","title":"Non-deterministic Turing Machine","text":"<p>Definition</p> <p>A non-deterministic Turing machine (NTM) is a 5-tuple \\(M = (K, \\Sigma, \\Delta, s, H)\\), where</p> <ul> <li>\\(K\\) is a finite set of states.</li> <li>\\(\\Sigma\\) is the tape alphabet, containing the left end symbol \\(\\triangleright\\) the blank \\(\\sqcup\\).</li> <li>\\(s \\in K\\) is the initial state.</li> <li>\\(H \\subseteq K\\) is a set of halting state.</li> <li> <p>\\(\\Delta\\) is a transition relation. It describes in the current state, when read a symbol, what will the next state be and the head action be (move or write).</p> \\[     \\delta: (K - H) \\times \\Sigma \\rightarrow K \\times (\\{\\leftarrow, \\rightarrow\\} \\cup (\\Sigma - \\{\\triangleright\\})). \\] <ul> <li> <p>Also, there is a limitation of \\(\\delta\\).</p> \\[     \\forall q \\in K - H, \\exists p \\in K,\\ \\ s.t.\\  \\delta(q, \\triangleright) = (p, \\rightarrow) \\] </li> </ul> </li> </ul> <p>Definition</p> <p>The definition of configuration and yields are the same as DTM.</p> <p>Definition</p> <p>An NTM \\(M\\) with input alphabet \\(\\Sigma_0\\) semidecides \\(L \\subseteq \\Sigma_0^*\\) if \\(\\forall w \\in \\Sigma_0^*\\), \\(w \\in L\\) iff \\((s, \\triangleright \\underline{\\sqcup} w) \\vdash_M^* (h, \\dots)\\) for some \\(h \\in H\\).</p> <p>Definition</p> <p>An NTM \\(M\\) with input alphabet \\(\\Sigma_0\\) decides \\(L \\subseteq \\Sigma_0^*\\) if</p> <ul> <li>\\(\\exists N \\in \\mathbb{N},\\ \\forall w \\in \\Sigma_0^*, \\text{ there is no configuration } C,\\ \\ s.t.\\ (s, \\triangleright \\underline{\\sqcup} w) \\vdash_M^N C\\).</li> <li>\\(w \\in L\\) iff \\((s, \\triangleright \\underline{\\sqcup} w) \\vdash_M^* (y, \\dots)\\).</li> </ul> Tip <p>You can similarly imagine the yields of NTM as a tree, these conditions are to say that the height of the tree is finite and some branch either accepts or rejects \\(w\\).</p> <p>Theorem</p> <p>Every NTM can be simulated by a DTM.</p> Proof (sketch) <p>an NTM semidecides \\(L\\) \u2192 a DTM semidecides \\(L\\).</p> <p>Since an NTM yields as a tree, we can use a DTM to simulate it by BFS. Specifically, we use a 3-tape DTM.</p> <ul> <li>The first one stores the input \\(w\\) and do nothing.</li> <li>The second one simulates the NTM.</li> <li>The third one enumerates the choice of BFS.</li> </ul>"},{"location":"Mathematics_Basis/TC/Lec_7/#church-turing-thesis","title":"Church-Turing Thesis","text":"<p>Church-Turing Thesis</p> <p>Algorithms are equivalent to Turing machines. Since algorithms solve decision problems and Turing machines decide language. We can also say decision problems are equivalent to language.</p>"},{"location":"Mathematics_Basis/TC/Lec_7/#descriptions-of-tm","title":"Descriptions of TM","text":"<ol> <li>Formal definition \\(M = (K, \\Sigma, \\delta, s, H)\\).</li> <li>Implement-level description. Diagram.</li> <li>High-level description. Pseudo code.</li> </ol>"},{"location":"Mathematics_Basis/TC/Lec_7/#pseudo-code","title":"Pseudo Code","text":"<p>The input of pseudo code is actually the encoding of the computation object. We need to first discuss about encoding. Here are some facts.</p> <ul> <li>Any finite set can be encoded.</li> <li>Any finite collection of finite sets can be encoded.</li> </ul> <p>For any compuatation object \\(O\\), we use \\(\\text{``}O\\text{''}\\) or \\(\\langle O \\rangle\\) to denote its encoding.</p> <p>Thus FA, PDA and TM can be encoded, since they are finite collections of finite sets.</p> <p>Example</p> <p>Make a TM that decides \\(L = \\{\\langle G \\rangle : G \\text{ is a connected graph}\\}\\).</p> <p>\\(M\\) = on input \\(\\langle G \\rangle\\).</p> <ol> <li>(Default, can be omitted) if the input is illegal, then reject, else decode \\(\\langle G \\rangle\\).</li> <li>select a node of \\(G\\) and mark it.</li> <li>repeat the following until no new node is marked.</li> <li>\u00a0\u00a0\u00a0\u00a0 for each marked node</li> <li>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 mark all of its neighbor.</li> <li>if all the nodes are marked,</li> <li>\u00a0\u00a0\u00a0\u00a0 accept,</li> <li>else,</li> <li>\u00a0\u00a0\u00a0\u00a0 reject.</li> </ol>"},{"location":"Mathematics_Basis/TC/Lec_7/#decidable-problems-recursive-languages","title":"Decidable Problems (Recursive Languages)","text":"<p>Example</p> <p>R1 \\(A_\\text{DFA} = \\{\\langle D, w \\rangle : D \\text{ is a DFA  that accepts } w\\}\\).</p> <p>\\(M_{R1}\\) = on input \\(\\langle D, w \\rangle\\)</p> <ol> <li>run \\(D\\) on \\(w\\).</li> <li>if \\(D\\) accepts \\(w\\).</li> <li>\u00a0\u00a0\u00a0\u00a0 accept \\(\\langle D, w \\rangle\\),</li> <li>else,</li> <li>\u00a0\u00a0\u00a0\u00a0 reject \\(\\langle D, w \\rangle\\).</li> </ol> <p>R2</p> \\[     A_\\text{NFA} = \\{\\langle N, w \\rangle : N \\text{ is a NFA that accepts } w\\} \\] <p>\\(M_{R2}\\) = on input \\(\\langle N, w \\rangle\\)</p> <ol> <li>build a DFA \\(D\\) equivalent to \\(N\\).</li> <li>run \\(M_{R1}\\) on \\(\\langle D, w \\rangle\\).</li> <li>output the result of \\(M_{R1}\\).</li> </ol> <p>R3 </p> \\[     A_\\text{REX} = \\{\\langle R, w \\rangle : R \\text{ is a regular expression with } w \\in L(R)\\} \\] <p>\\(M_{R2}\\) = on input \\(\\langle R, w \\rangle\\)</p> <ol> <li>build an NFA \\(N\\) equivalent to \\(R\\).</li> <li>run \\(M_{R2}\\) on \\(\\langle N, w \\rangle\\).</li> <li>output the result of \\(M_{R1}\\).</li> </ol> <p>R4 \\(E_\\text{DFA} = \\{\\langle D \\rangle : D \\text{ is a DFA with } L(D) = \\emptyset\\}\\).</p> <p>\\(M_{R4}\\) = on input \\(\\langle D \\rangle\\)</p> <ol> <li>if \\(D\\) has no final state,</li> <li>\u00a0\u00a0\u00a0\u00a0 accept,</li> <li>else,</li> <li>\u00a0\u00a0\u00a0\u00a0 run BFS on the diagram, starting with the inital state.</li> <li>if there exists a path from \\(s\\) to final state,</li> <li>\u00a0\u00a0\u00a0\u00a0 reject,</li> <li>else,</li> <li>\u00a0\u00a0\u00a0\u00a0 accept.</li> </ol> <p>R5 \\(EQ_\\text{DFA} = \\{\\langle D_1, D_2 \\rangle : D_1, D_2 \\text{ are DFAs with } L(D_1) = L(D_2)\\}\\).</p> <p>\\(M_{R5}\\) = on input \\(\\langle R, w \\rangle\\)</p> <ol> <li>construct \\(D_3\\) such that \\(L(D_3) = L(D_1) \\oplus L(D_2)\\) (symmetric difference).</li> <li>run \\(M_{R4}\\) on \\(\\langle D_3 \\rangle\\).</li> <li>output the result of \\(M_{R4}\\).</li> </ol>"},{"location":"Mathematics_Basis/TC/Lec_7/#reduction","title":"Reduction","text":"<p>In the examples above, we use the idea of reduction (\u5f52\u7ea6). Take R1 and R2 as example. The equivalence of DFA and NFA guarantees that</p> \\[ \\langle N, w \\rangle \\in A_{\\text{NFA}} \\text{ iff } \\langle D, w \\rangle \\in A_{\\text{DFA}}. \\] <p>We will further discuss reduction in the next lecture.</p>"},{"location":"Mathematics_Basis/TC/Lec_8/","title":"Lecture 8","text":"<p>Continue examples of Decidable Problems from the last lecture.</p> <p>Example</p> <p>C1 \\(A_\\text{CFG} = \\{\\langle G, w \\rangle : G \\text{ is a CFG that generates } w\\}\\).</p> <p>\\(M_{C1}\\) = on input \\(\\langle G, w \\rangle\\)</p> <ol> <li>build an CFG \\(G'\\) in CNF equivalent to \\(G\\).</li> <li> <p>enumerate all the derivations of length \\(2|w| - 1\\).</p> <p>Recap the property of CFG in CNF that if \\(G\\) generates a string of length \\(n \\ge 1\\), then the length of derivation is exactly \\(2n-1\\).</p> </li> <li> <p>if any of them generates \\(w\\),</p> </li> <li>\u00a0\u00a0\u00a0\u00a0 accept \\(\\langle G, w \\rangle\\),</li> <li>else,</li> <li>\u00a0\u00a0\u00a0\u00a0 reject \\(\\langle G, w \\rangle\\).</li> </ol> <p>C2 \\(A_\\text{PDA} = \\{\\langle P, w \\rangle : P \\text{ is a PDA that accepts } w\\}\\).</p> <p>\\(M_{C2}\\) = on input \\(\\langle P, w \\rangle\\)</p> <ol> <li>build an CFG \\(G\\) equivalent to \\(P\\).</li> <li>run \\(M_{C1}\\) on \\(\\langle G, w \\rangle\\).</li> <li>output the result of \\(M_{C1}\\).</li> </ol> <p>C3 \\(E_\\text{CFG} = \\{\\langle G \\rangle : G \\text{ is a CFG with } L(G) = \\emptyset\\}\\).</p> <p>\\(M_{C3}\\) = on input \\(\\langle G \\rangle\\)</p> <ol> <li>mark all terminals and \\(e\\).</li> <li>repeat the following until no new symbol is marked.</li> <li>\u00a0\u00a0\u00a0\u00a0 for each rule that the right side is all marked symbols</li> <li>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 mark the left side symbols</li> <li>if start symbol is marked,</li> <li>\u00a0\u00a0\u00a0\u00a0 reject,</li> <li>else,</li> <li>\u00a0\u00a0\u00a0\u00a0 accept.</li> </ol> <p>C4 \\(E_\\text{PDA} = \\{\\langle P \\rangle : P \\text{ is a PDA with } L(P) = \\emptyset\\}\\).</p>"},{"location":"Mathematics_Basis/TC/Lec_8/#reduction","title":"Reduction","text":"<p>Now we give the formal definition of reduction.</p> <p>Definition</p> <p>Let \\(A\\) and \\(B\\) be two languages over \\(\\Sigma_A\\) and \\(\\Sigma_B\\) respectively. A reduction from \\(A\\) to \\(B\\) is a recursive / computable function \\(f: \\Sigma_A^* \\rightarrow \\Sigma_B^*\\) such that for any \\(x \\in \\Sigma_A^*\\)</p> \\[     x \\in A \\text{ iff } f(x) \\in B. \\] <p>Theorem</p> <p>Suppose that there exists a reduction \\(f\\) from \\(A\\) and \\(B\\). If \\(B\\) is recursive, A is recursive, denoted by \\(A \\le B\\). (If A is not recursive, B is not recursive).</p> <p>Proof</p> <p>Since B is recursive, then there exists \\(M_B\\) decides \\(B\\).</p> <p>\\(M_A\\) = on input \\(x\\),</p> <ol> <li>compute \\(f(x)\\).</li> <li>run \\(M_B\\) on \\(f(x)\\).</li> <li>output the result of \\(M_B\\).</li> </ol>"},{"location":"Mathematics_Basis/TC/Lec_8/#language-category","title":"Language Category","text":"<p>Now we know that enumerable recursive languages contains recusive languages. But we care about two questions:</p> <ol> <li>is there any language that is not enumerable?</li> <li>is there any language that is not recursive but enumerable recursive?</li> </ol> <p>Before we dive in, let's recap something about equinumerosity.</p> <p>Quote</p> <p>Equinumerosity in DM.</p> <p>Two sets have the same cardinality or say are equinumerous (\u7b49\u52bf\u7684) iff there is a bijective from \\(A\\) to \\(B\\), i.e.\\(|A| = |B|\\).</p> <p>A set is countable if it is equinumerous with \\(\\mathbb{N}\\).</p> <p>Lemma</p> <p>A is countable iff there exists an injection \\(f : A \\rightarrow \\mathbb{N}\\).</p> Proof <p>\\(\\Rightarrow\\) it's trivial.</p> <p>\\(\\Leftarrow\\) if A is finite, it's trivial. If not, sort \\(A\\) in increasing order of \\(f(a)\\) and let \\(g(a)\\) be the rank of \\(a\\).</p> <p>Corollary</p> <p>Any subset of a countable set is countable.</p> <p>Theorem</p> <p>Let \\(\\Sigma\\) be an alphabet, then \\(\\Sigma^*\\) is countable.</p> <p>Since a Turing machine can be encoded in to a string, we have the following corollary</p> <p>Corollary</p> <p>\\(\\mathcal{M} = \\{M : M \\text{ is a TM}\\}\\) is countable.</p> <p>Theorem</p> <p>Let \\(\\Sigma\\) be some non-empty alphabet, and \\(\\mathcal{L}\\) be the set of all languages over \\(\\Sigma\\). Then \\(\\mathcal{L}\\) is uncountable.</p> <p>Proof</p> <p>Assume that \\(\\mathcal{L}\\) is countable, then the langauges in \\(\\mathcal{L}\\) can be labelled as</p> \\[     L_1, L_2, L_3, \\dots \\] <p>Since \\(\\Sigma^*\\) is countable, then the string in \\(\\Sigma^*\\) can be labelled as</p> \\[     s_1, s_2, s_3, \\dots \\] <p>We construct a set \\(D = \\{s_i | s_i \\notin L_i\\}\\). Then for any \\(i \\in \\mathbb{N}^*\\), \\(s_i \\in D\\) iff \\(s_i \\notin L_i\\), and thus \\(D \\neq L_i\\). Therefore \\(D \\notin \\mathcal{L}\\).</p> <p>But \\(D \\in \\Sigma^*\\), and thus \\(D\\) is a language, which leads to a contradiction. So the assumption is false and \\(\\mathcal{L}\\) is uncountable.</p> <p>Since \\(\\mathcal{L}\\) is uncountable while \\(\\mathcal{M}\\) is countable, then we have the following fact.</p> <p>Fact</p> <p>There exists some language that is not recursively enumerable.</p> <p>Now we define</p> \\[ H = \\{\\langle M, w \\rangle : M \\text{ is a TM that halts on } w\\}. \\] <p>Theorem</p> <p>\\(H\\) is recursively enumerable.</p> <p>Proof</p> <p>\\(U\\) = on input \\(\\langle M, w \\rangle\\)</p> <ol> <li>run \\(M\\) on \\(w\\).</li> </ol> <p>Then \\(U\\) halts on \\(\\langle M, w \\rangle\\) \\(\\Leftrightarrow\\) \\(M\\) halts on \\(w\\) \\(\\Leftrightarrow\\) \\(\\langle M, w \\rangle \\in H\\).</p> <p>We also call \\(U\\) universal TM. You can think \\(U\\) is a computer, since it run an algorithm \\(M\\) on some input \\(w\\). Both of the algorithm and input are programmable.</p> <p>Theorem</p> <p>\\(H\\) is not recursive.</p> <p>Proof</p> <p>Let \\(H_d = \\{\\langle M \\rangle : M \\text{ is a TM that does NOT halt on } \\langle M \\rangle\\}\\)</p> <p>Lemma 1</p> <p>If \\(H\\) is recursive, then \\(H_d\\) is recursive.</p> <p>Proof</p> <p>Suppose \\(H\\) is recursive, then there exists a TM \\(M_H\\) that decides \\(H\\).</p> <p>\\(M_d\\) = on input \\(\\langle M \\rangle\\)</p> <ol> <li>run \\(M_H\\) on \\(\\langle M, M \\rangle\\).</li> <li>if \\(M_H\\) accepts \\(\\langle M, M \\rangle\\),</li> <li>\u00a0\u00a0\u00a0\u00a0 reject \\(\\langle M \\rangle\\),</li> <li>else,</li> <li>\u00a0\u00a0\u00a0\u00a0 accept \\(\\langle M \\rangle\\).</li> </ol> <p>Thus \\(M_d\\) decides \\(H_d\\) and \\(H_d\\) is recursive.</p> <p>Lemma 2</p> <p>\\(H_d\\) is NOT recursively enumerable.</p> <p>Proof</p> <p>Assume \\(H_d\\) is recursively enumerable, then there exists a TM \\(D\\) that semidecides \\(H_d\\).</p> \\[     D \\text{ on input } \\langle M \\rangle     \\left\\{     \\begin{aligned}         &amp; \\text{halt}, &amp;&amp; \\langle M \\rangle \\in H_d, \\\\         &amp; \\text{NOT halt}, &amp;&amp; \\langle M \\rangle \\notin H_d.     \\end{aligned}     \\right. \\] <p>Since \\(D\\) is a Turing machine, substitute \\(M\\) with \\(D\\), we have</p> \\[     D \\text{ on input } \\langle D \\rangle     \\left\\{     \\begin{aligned}         &amp; \\text{halt}, &amp;&amp; \\langle D \\rangle \\in H_d, \\\\         &amp; \\text{NOT halt}, &amp;&amp; \\langle D \\rangle \\notin H_d.     \\end{aligned}     \\right. \\] <p>It's a contradiction. So \\(H_d\\) is NOT recursively enumerable.</p> <p>Combining Lemma 1 and Lemma 2, \\(H\\) is not recursive.</p> <p>Now we get two key examples for the questions.</p> <ol> <li>\\(H_d\\) is not recursively enumerable.</li> <li>\\(H\\) is recursively enumerable but not recursive.</li> </ol>"},{"location":"Mathematics_Basis/TC/Lec_8/#examples-of-reduction","title":"Examples of Reduction","text":"<p>Now that we have a powerful example \\(H\\), which is recursively enumerable but not recursive. So to prove some language that is also recursively enumerable but not recursive, we can use reduction: just reduct \\(H\\) to that langauge. Here we give some examples.</p> <p>Example</p> <p>Show that \\(L_1 = \\{\\langle M \\rangle : M \\text{ is a TM that halts on } e\\}\\) is not recursive.</p> <p>Proof</p> <p>To use the reduction theorem, we need to construct some \\(M'\\), such that</p> \\[     \\langle M, w \\rangle \\in H \\text{ iff } \\langle M' \\rangle \\in L_1 \\] <p>\\(M'\\) = on input \\(u\\),</p> <ol> <li>run \\(M\\) on \\(w\\).</li> </ol> <p>Well, that's all.</p> <p>Example</p> <p>Show that \\(L_2 = \\{\\langle M \\rangle : M \\text{ is a TM that halts on some input} \\}\\) is not recursive.</p> <p>Show that \\(L_3 = \\{\\langle M \\rangle : M \\text{ is a TM that halts on every input} \\}\\) is not recursive.</p> <p>Proof</p> <p>Notice the proof in \\(L_1\\), we construct \\(M'\\). And actually we have</p> <p>\\(M'\\) halts on \\(e\\) \\(\\Leftrightarrow\\) \\(M'\\) halts on some input \\(\\Leftrightarrow\\) \\(M'\\) halts on every input \\(\\Leftrightarrow\\) \\(M\\) halts on \\(w\\).</p> <p>So \\(L_2\\) and \\(L_3\\) is proved.</p> <p>Example</p> <p>Show that \\(L_4 = \\{\\langle M_1, M_2 \\rangle : M_1 \\text{ and } M_2 \\text{ are two TMs with } L(M_1) = L(M_2) \\}\\) is not recursive.</p> <p>Proof</p> \\[     \\langle M \\rangle \\in L_3 \\text{ iff } \\langle M_1, M_2 \\rangle \\in L_4 \\] <p>namely</p> \\[     M \\text{ halts on every input} \\text{ iff } L(M_1) = L(M_2) \\] <p>\\(M\\) halts on every input, which means \\(L(M) = \\Sigma^*\\). So we construct</p> <p>\\(M_1 = M\\)</p> <p>\\(M_2\\) = on input \\(u\\)</p> <ol> <li>halt.</li> </ol>"},{"location":"Mathematics_Basis/TC/Lec_9/","title":"Lecture 9","text":"<p>Example</p> <p>Show that \\(R_{TM} = \\{\\langle M \\rangle : M \\text{ is a TM with } L(M) \\text{ being regular}\\}\\) is not recursive.</p> <p>Proof</p> <p>Suppose \\(M_1\\) decides on \\(\\overline{R_{TM}}\\), then we construct \\(M_2\\) runs on \\(H\\).</p> <p>\\(M_2\\) = on input \\(\\langle M, w \\rangle\\),</p> <ol> <li> <p>construct a TM \\(M'\\) = on input \\(x\\)</p> <ol> <li>run \\(M\\) on \\(w\\)</li> <li>run \\(U\\) on \\(x\\)</li> </ol> \\[     L(M') = \\left\\{     \\begin{aligned}         &amp; \\phi, &amp;&amp; \\text{if $M$ does not halt on $w$}, \\\\         &amp; L(U) = H, &amp;&amp; \\text{if $M$ halts on $w$}.     \\end{aligned}     \\right. \\] <p>NOTE that \\(\\phi\\) is regular, but \\(L(U) = H\\) is not regular (because it's even not recursice).</p> </li> <li> <p>run \\(M_1\\) on \\(\\langle M' \\rangle\\).</p> </li> <li>return the result of \\(M_1\\).</li> </ol> <p>Since \\(M_2\\) can not decide \\(H\\), thus \\(M_1\\) can not decide \\(\\overline{R_{TM}}\\). Therefore \\(\\overline{R_{TM}}\\) is not recursive and so is \\(R_{TM}\\).</p> <p>Example</p> <p>Show that \\(CF_{TM} = \\{\\langle M \\rangle : M \\text{ is a TM with } L(M) \\text{ being context-free}\\}\\) is not recursive.</p> <p>Proof</p> <p>The same reduction pattern of \\(R_{TM}\\). Since \\(\\phi\\) is context-free and \\(H\\) not.</p> <p>Example</p> <p>Show that \\(REC_{TM} = \\{\\langle M \\rangle : M \\text{ is a TM with } L(M) \\text{ being recursive}\\}\\) is not recursive.</p> <p>From the examples above, we can conclude some pattern of not recursive langauges.</p>"},{"location":"Mathematics_Basis/TC/Lec_9/#rices-theorem","title":"Rice's Theorem","text":"<p>Theorem</p> <p>Suppose \\(\\mathcal{L}(P)\\) is a non-empty proper subset of all recursively enumarable languages, then the following language is NOT recursive.</p> \\[     R(P) = \\{\\langle M \\rangle : M \\text{ is a TM with } L(M) \\in \\mathcal{L}(P)\\}. \\] <p>Proof</p> <p>Case 1. \\(\\phi \\notin \\mathcal{L}(P)\\)</p> <p>There exists \\(A \\in \\mathcal{L}(P)\\), so there exists a TM \\(M_A\\) that semidecides \\(A\\). Suppose \\(M_R\\) decides \\(R(P)\\), then we construct \\(M_H\\) on \\(H\\).</p> <p>\\(M_H\\) = on input \\(\\langle M, w \\rangle\\),</p> <ol> <li> <p>construct a TM \\(M^*\\) = on input \\(x\\)</p> <ol> <li>run \\(M\\) on \\(w\\)</li> <li>run \\(M_A\\) on \\(x\\)</li> </ol> \\[     L(M^*) = \\left\\{     \\begin{aligned}         &amp; \\phi, &amp;&amp; \\text{if $M$ does not halt on $w$}, \\\\         &amp; L(M_A) = A, &amp;&amp; \\text{if $M$ halts on $w$}.     \\end{aligned}     \\right. \\] </li> <li> <p>run \\(M_R\\) on \\(\\langle M^* \\rangle\\).</p> </li> <li>return the result of \\(M_R\\).</li> </ol> <p>Case 2. \\(\\phi \\in \\mathcal{L}(P)\\)</p> <p>Then \\(\\phi \\notin \\overline{\\mathcal{L}(P)}\\), we can reduce \\(H\\) to \\(\\overline{R(P)}\\) like in case 1. And thus $\\(R(P)\\) is not recursive.</p>"},{"location":"Mathematics_Basis/TC/Lec_9/#summary","title":"Summary","text":"<p>To prove a language \\(A\\) recursive, we can prove</p> <ul> <li>By definition: construct a Turing machine.</li> <li>By reduction: \\(A \\le \\text{known recursive language}\\).</li> </ul> <p>To prove a language \\(A\\) not recursive, we can prove</p> <ul> <li>By reduction: \\(\\text{known recursive language} \\le A\\).<ul> <li>The mostly used language is \\(H\\).</li> </ul> </li> </ul> <p>To prove a language \\(A\\) recursively enumerable, we can prove</p> <ul> <li>By definition: construct a Turing machine.</li> <li>By reduction: \\(A \\le \\text{known recursively enumerable language}\\).</li> </ul> <p>To prove a language \\(A\\) not recursively enumerable, we can prove</p> <ul> <li>By reduction: \\(\\text{known non-recursively enumerable language} \\le A\\).</li> <li>By the following theorem.</li> </ul> <p>Theorem</p> <p>If \\(A\\) and \\(\\overline{A}\\) are recursively enumerable, then \\(A\\) is recursive.</p> Proof <p>Since \\(A\\) and \\(\\overline{A}\\) are recursively enumerable, then there exists \\(M_1\\) and \\(M_2\\) that semidecides \\(A\\) and \\(\\overline{A}\\) respectively.</p> <p>Then we construct a \\(M_3\\) that decides \\(A\\).</p> <p>\\(M_3\\) = on input \\(x\\)</p> <ol> <li>run \\(M_1\\) and \\(M_2\\) on \\(x\\) in parallel.</li> <li>if \\(M_1\\) halts on \\(x\\),</li> <li>\u00a0\u00a0\u00a0\u00a0 accept \\(x\\).</li> <li>if \\(M_2\\) halts on \\(x\\),</li> <li>\u00a0\u00a0\u00a0\u00a0 reject \\(x\\).</li> </ol> <p>Example</p> <p>Since \\(H\\) is recursively enumerable and not recursive, then \\(\\overline{H}\\) is not recursively enumerable.</p> <p>Example</p> <p>Show that \\(A = \\{\\langle M \\rangle : M \\text{ is a TM that halts on some input}\\}\\) is recursively enumerable.</p> <p>Proof</p> <p>\\(M_A\\) = on input \\(\\langle M \\rangle\\).</p> <ol> <li>for \\(i\\) = \\(1, 2, 3, \\dots\\)</li> <li>\u00a0\u00a0\u00a0\u00a0 for \\(s\\) = \\(s_1, s_2, \\dots, s_i\\) (\\(s_i \\in L\\))</li> <li>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0run \\(M\\) on \\(s\\) for \\(i\\) steps,</li> <li>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if \\(M\\) halts on \\(s\\) within \\(i\\) steps,</li> <li>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0halt.</li> </ol>"},{"location":"Mathematics_Basis/TC/Lec_9/#closure-property","title":"Closure Property","text":"<p>Theorem</p> <p>If \\(A\\) and \\(B\\) are recursive langauges, then so as</p> <ul> <li>\\(A \\cup B\\).</li> <li>\\(A \\cap B\\).</li> <li>\\(\\overline{A}\\).</li> <li>\\(A \\circ B\\).</li> <li>\\(A^*\\).</li> </ul> <p>If \\(A\\) and \\(B\\) are recursively enumerable langauges, then so as</p> <ul> <li>\\(A \\cup B\\).</li> <li>\\(A \\cap B\\).</li> <li>\\(A \\circ B\\).</li> <li>\\(A^*\\).</li> </ul> <p>NOTE not so as \\(\\overline{A}\\), a counter example is \\(H\\).</p>"},{"location":"Mathematics_Basis/TC/Lec_9/#enumerator","title":"Enumerator","text":"<p>There are some extra properties of Turing machine.</p> <p>Definition</p> <p>A Turing machine \\(M\\) enumerates a language \\(L\\) if for some state \\(q\\),</p> \\[     L = \\{w | (s, \\triangleright \\underline{\\sqcup}) \\vdash_M^* (q, \\triangleright \\underline{\\sqcup}w) \\}. \\] <p>And \\(L\\) is Turing enumerable.</p> <p>\\(M\\) has no input.</p> <p>Theorem</p> <p>A language \\(L\\) is Turing enumerable iff it's recursively enumerable.</p> <p>Proof</p> <p>If \\(L\\) is finite, it's trivial. Assume \\(L\\) is infinite,</p> <p>\\(\\Rightarrow\\). There exists a TM \\(M\\) that enumerates \\(L\\), we construct \\(M'\\).</p> <p>\\(M'\\) = on input \\(x\\)</p> <ol> <li>run \\(M\\) to enumerate \\(L\\).</li> <li>every time \\(M\\) outputs a string \\(w\\),</li> <li>\u00a0\u00a0\u00a0\u00a0 if \\(w\\) equals to \\(x\\),</li> <li>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 halt.</li> </ol> <p>Then \\(M'\\) semidecides \\(L\\) and \\(L\\) is recursively enumerable.</p> <p>\\(\\Leftarrow\\). There exists a TM \\(M\\) that semidecides \\(L\\), we construct \\(M'\\).</p> <p>\\(M'\\) = </p> <ol> <li>for \\(i\\) = \\(1, 2, 3, \\dots\\)</li> <li>\u00a0\u00a0\u00a0\u00a0 for \\(s\\) = \\(s_1, s_2, \\dots, s_i\\) (\\(s_i \\in L\\))</li> <li>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 run \\(M\\) on \\(s\\) for \\(i\\) steps.</li> <li>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 if \\(M\\) halts on \\(s\\) within \\(i\\) steps.</li> <li>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 output \\(s\\).</li> </ol> <p>Here the \"some state \\(q\\)\" of \\(M'\\) is the halting state.</p> <p>Definition</p> <p>Let \\(M\\) be a TM that enumerates \\(L\\), then \\(M\\) is lexicographically enumerates \\(L\\) if whenever</p> \\[     (q, \\triangleright \\underline{\\sqcup}) w \\vdash_M^+ (q, \\triangleright \\underline{\\sqcup} w') \\] <p>we have \\(w'\\) after \\(w\\) in lexicographical order.</p> <p>Theorem</p> <p>\\(L\\) is lexicographically enumerable iff it's recursive.</p> <p>Proof</p> <p>\\(\\Leftarrow\\). There exists \\(M\\) decides \\(L\\), we construct \\(M'\\).</p> <p>\\(M'\\) on input \\(x\\)</p> <ol> <li>enumerate all strings in lexicographical order.</li> <li>for each string \\(s\\),</li> <li>\u00a0\u00a0\u00a0\u00a0 run \\(M\\) on \\(s\\),</li> <li>\u00a0\u00a0\u00a0\u00a0 if \\(M\\) accepts \\(s\\),</li> <li>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 output \\(s\\).</li> </ol> <p>\\(\\Rightarrow\\). There exists \\(M\\) lexicographically enumerate \\(L\\), we construct \\(M'\\).</p> <p>\\(M'\\) = on input \\(x\\)</p> <ol> <li>run \\(M\\) to enumerate \\(L\\).</li> <li>every time \\(M\\) outputs a string \\(w\\),</li> <li>\u00a0\u00a0\u00a0\u00a0 if \\(w\\) equals to \\(x\\),</li> <li>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 accept \\(x\\).</li> <li>reject \\(x\\).</li> </ol>"},{"location":"Pot-pourri/QISE/Review/","title":"Fundamentals of Quantum Information","text":"<p>Quantum information science and engineering (QISE).</p>"},{"location":"Pot-pourri/QISE/Review/#lecture-1-information-entropy-and-computational-complexity","title":"Lecture 1 | Information, Entropy and Computational Complexity","text":""},{"location":"Pot-pourri/QISE/Review/#information-entropy","title":"Information Entropy","text":"<p>Shannon Entropy</p> <p>If \\(X\\) is a random variable over a sete of events \\(x\\) such that event \\(x\\) occurs with probability \\(p_x\\), then the Shannon entropy of event \\(x\\) is \\(-\\log_2(p_x)\\).</p> <p>If we consider a set of possible events whose probabilities of occurrence are \\(p_1\\), \\(p_2\\), \\(\\dots\\), \\(p_n\\), then the average information is</p> \\[     H = \\langle I(p_i) \\rangle \\sim - \\sum\\limits_{i = 1}^{n} p_i \\log_2 p_i. \\] <p>Suppose a set of data to be compressed by a random variable \\(X\\), for an arbitrary sequence \\(X_1\\), \\(X_2\\), \\(\\dots\\), \\(X_n\\), the information of it is</p> \\[ -\\log_2 P(X_1, \\dots, X_n) = -nH(X) \\] <p>where \\(H(X)\\) is the average information of one element. Thus</p> \\[ P(X_1, \\dots, X_n) = 2^{-nH(X)}, \\] <p>which shows that there are at most \\(2^{nH(X)}\\) sequences (suppose each sequence has the same probability), and hence **it only requires \\(nH(X)\\) bits to encode them.</p>"},{"location":"Pot-pourri/QISE/Review/#computational-complexity","title":"Computational Complexity","text":"<ul> <li>P, NP, BPP, BQP</li> </ul> <p>In some cases, quantum algorithm can do better than classical algorithm at the computational complexity. A example is Peter Shor algorithm, which solving it in \\(O(n^3)\\), faster than classical \\(2^{O(n^{1/3})}\\) of an \\(n\\)-bit integer.</p>"},{"location":"Pot-pourri/QISE/Review/#lecture-2-diracs-bracket-and-polarization","title":"Lecture 2 | Dirac's Bracket and Polarization","text":"Introduction <p>Classical systems are represented by real numbers, or maybe real vector spaces. In some cases like discussing RC circuits, we use complex numbers to simplify the procedure. However, quantum mechanics, on the other hand, is founded intrinsically on complex vector spaces.</p>"},{"location":"Pot-pourri/QISE/Review/#diracs-bracket-notation","title":"Dirac's Bracket Notation","text":"<ul> <li>A complex vector space is composed of elements \\(\\ket{a}\\) called kets. They are column vectors in our usual notation. It has a dual space whose elements are bras \\(\\bra{a}\\).<ul> <li>If \\(\\ket{a} = (\\alpha_0, \\dots, \\alpha_n)\\), where \\(\\alpha_i \\in \\mathbb{C}\\), then \\(\\bra{a} = (\\alpha_0^*, \\dots^*, \\alpha_n^*)^T\\). where \\(\\alpha^*\\) is the conjugate of \\(\\alpha\\).</li> </ul> </li> <li>Inner product \\(\\braket{a|b}\\) generalizes the dot product of usual vectors. Note that \\(\\braket{a|b} = \\braket{b|a}^*\\).</li> <li> <p>Choosing an orthonormal basis of \\(\\ket{i}\\), we can expand an arbitrary \\(\\ket{a}\\) as</p> <p>$$     \\ket{a} = \\sum\\limits_{i} \\ket{i}\\braket{i|a}. $$ - The matrix</p> <p>$$     P_i = \\ket{i}\\bra{i} $$   is called a projection operator (or projector) in the direction \\(\\ket{i}\\).</p> </li> </ul> <p>Polarization of Light</p> <p>Review the polarization of light, we can represent linear polarization along \\(x\\) and \\(y\\) axis by</p> \\[     \\ket{h} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix},     \\ket{v} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}. \\] <p>A photon in any polarization can be represented by a state \\(\\ket{p}\\)</p> \\[     \\ket{p} = I\\ket{p} = \\ket{h}\\braket{h|p} + \\ket{v}\\braket{v|p} = \\mu \\ket{h} + \\nu \\ket{v}, \\] <p>where \\(\\mu, \\nu \\in \\mathbb{C}\\) are probability amplitudes and \\(\\braket{p|p} = \\mu^2 + \\nu^2 = 1\\).</p> <p>When measured by polarizers, probability of polarization alog \\(x\\) and \\(y\\) axis are</p> \\[     P_h = \\mu^2 = \\braket{p|h}\\braket{h|p} = \\braket{p|P_h|p},     P_v = \\mu^2 = \\braket{p|v}\\braket{v|p} = \\braket{p|P_v|p}. \\]"},{"location":"Pot-pourri/QISE/Review/#lecture-3-spins-qubits-and-linear-operators","title":"Lecture 3 | Spins, Qubits and Linear Operators","text":"<p>Stern-Gerlach Experiment</p> <p>In the Stern-Gerlach experiment, a narrow beam of silver atoms passes through an electromagnet field and then lands on a glass detector plate. When the electromagnet was turned on, Stern and Gerlach found that the silver atoms formed two distinct spots on the glass plate. This experiment verifies the quantization of angular momentum.</p>"},{"location":"Pot-pourri/QISE/Review/#qubit","title":"Qubit","text":"<p>Like the polarization example in Lecture 2, the isolated quantum spin in Stern-Gerlach experiment is another realization of two-level system which we call quantum bits, or qubit.</p> <p>A general state of a qubit is</p> \\[ \\ket{\\psi} = \\alpha\\ket{0} + \\beta\\ket{1} = \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix},\\ \\ \\alpha, \\beta \\in \\mathbb{C} \\] <p>normalized by</p> \\[ \\braket{\\psi|\\psi} = \\alpha^2 + \\beta^2 = 1. \\] <p>We can parameterize it by</p> \\[ \\ket{\\psi} = \\cos\\frac{\\theta}{2}\\ket{0} + e^{i\\phi}\\sin\\frac{\\theta}{2}\\ket{1} \\] <p>and represent it by a point on the Bloch sphere.</p>"},{"location":"Pot-pourri/QISE/Review/#linear-operator","title":"Linear Operator","text":"<p>Linear Operator</p> <p>A map \\(M: \\mathbb{C}^n \\rightarrow \\mathbb{C}^n\\) is a linear operator if</p> \\[ M(\\mu\\ket{x} + \\nu\\ket{y}) = \\mu M\\ket{x} + \\nu M \\ket{y} \\] <p>is satisfied for arbitrary \\(\\ket{x}, \\ket{y} \\in \\mathbb{C}^n\\) and \\(\\mu, \\nu \\in \\mathbb{C}\\).</p> <p>When we choose a set of N orthonormal \\(\\ket{i}\\), we can symbolically represent linear operator by an \\(N \\times N\\) matrix.</p> <p>Hermitian Operator</p> <p>If a linear operator \\(M\\) satisfies</p> \\[     M = M^{\\dagger} = \\left(M^T\\right)^*. \\]"},{"location":"Pot-pourri/QISE/Review/#eigenvalues-and-eigenvectors","title":"Eigenvalues and Eigenvectors","text":"<p>Eigenvalues and Eigenvectors</p> <p>If there is a ket-vector \\(\\ket{\\lambda}\\) such that</p> \\[ M\\ket{\\lambda} = \\lambda\\ket{\\lambda}, \\] <p>then \\(\\lambda\\) is called eigenvalue, while \\(\\ket{\\lambda}\\) is called eigenvector.</p> <p>Note that \\(M\\) can be represented by its eigenvalues and eigenvectors, called spectral decomposition.</p> \\[     M = \\sum\\limits_{\\lambda\\lambda'} \\ket{\\lambda} \\braket{\\lambda | M | \\lambda'} \\bra{\\lambda'} = \\sum\\limits_{\\lambda}\\ket{\\lambda}\\bra{\\lambda} = \\sum\\limits_{\\lambda}\\lambda P_\\lambda. \\]"},{"location":"Pot-pourri/QISE/Review/#principle-of-quantum-mechanics","title":"Principle of Quantum Mechanics","text":"<p>Here are some principles, or say axioms of quantum mechanics.</p> <p>Principle</p> <ol> <li>States of a system are represented by vectors in a complex vector space.</li> <li>The observables or measurable quantities of quantum mechanics are represented by linear operators \\(L\\).</li> <li>The possible results of a measurement are the eigenvalues \\(\\lambda\\) of the operator that represents the observable.</li> <li>Unambiguously distinguishable states are represented by orthogonal vectors.</li> <li> <p>If \\(\\ket{\\psi}\\) is a state-vector, and the observable \\(L\\) is measured, the probability to observe \\(\\lambda\\) i</p> \\[     P_\\lambda = \\braket{\\psi|\\lambda}\\braket{\\lambda|\\psi} = \\braket{\\psi|P_\\lambda|\\psi}, \\] <p>where \\(\\ket{\\lambda}\\) is the corresponding eigenvector.</p> </li> </ol>"},{"location":"Pot-pourri/QISE/Review/#pauli-matrices","title":"Pauli Matrices","text":"<p>The spin operators in the Stern-Glarch experiment is called Pauli matrices, represented by</p> \\[ \\sigma_x = \\begin{pmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix},\\ \\ \\sigma_y = \\begin{pmatrix} 0 &amp; -i \\\\ i &amp; 0 \\end{pmatrix},\\ \\ \\sigma_z = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{pmatrix}. \\] <p>Their eigenvalues are all \\(\\pm 1\\).</p> <p>In our classical representation, a spin is a 3D vector, but in the quantum state of a spin is a 2D complex vector. To connect with these two concepts, we use a special representation.</p> \\[ \\vec{\\sigma} = \\sigma_x \\hat{x} + \\sigma_y \\hat{y} + \\sigma_z \\hat{z}, \\] <p>For the spin along an arbitrary direction \\(\\hat{n} = (n_x, n_y, n_z)\\), the operator is</p> \\[ \\sigma_n = \\vec{\\sigma} \\cdot \\hat{n} = \\sigma_xn_x +\\sigma_yn_y + \\sigma_zn_z = \\begin{pmatrix} n_z &amp; n_x - in_y \\\\ n_x + in_y &amp; -n_z \\end{pmatrix} \\]"},{"location":"Pot-pourri/QISE/Review/#lecture-4-quantum-entanglement","title":"Lecture 4 | Quantum Entanglement","text":"<p>We represent one bit by \\(\\ket{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\) and \\(\\ket{0} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\), intuitively, we want to represent the two bit states by</p> \\[ \\ket{00} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix},\\ \\ \\ket{01} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix},\\ \\ \\ket{10} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix},\\ \\ \\ket{11} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}. \\] <p>To meet our intuition, here is tensor product.</p> <p>Tensor Product</p> <p>The tensor product of an \\(m \\times n\\) matrix \\(A\\) and a \\(p \\times q\\) matrix \\(B\\) is an \\(mp \\times nq\\) matrix in the form</p> \\[ A \\otimes B = \\begin{pmatrix}     a_{11}B &amp; a_{12}B &amp; \\cdots &amp; a_{1n}B \\\\     a_{21}B &amp; a_{22}B &amp; \\cdots &amp; a_{2n}B \\\\     \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\     a_{n1}B &amp; a_{n2}B &amp; \\cdots &amp; a_{nn}B \\end{pmatrix} \\] <p>For two vector \\(\\ket{u}\\) and \\(\\ket{v}\\), we often use abbrevation as \\(\\ket{u}\\ket{v}\\) or just \\(\\ket{uv}\\).</p>"},{"location":"Pot-pourri/QISE/Review/#entanglement","title":"Entanglement","text":"<p>Suppose we have two qubit in \\(\\alpha_1\\ket{0} + \\beta_1\\ket{1}\\) and \\(\\alpha_2\\ket{0} + \\beta_2\\ket{1}\\) respectively. Then the product state is</p> \\[ (\\alpha_1\\ket{0} + \\beta_1\\ket{1})\\otimes(\\alpha_2\\ket{0} + \\beta_2\\ket{1}) = \\alpha_1\\alpha_2\\ket{00} + \\alpha_1\\beta_2\\ket{01} + \\beta_1\\alpha_2\\ket{10} + \\beta_1\\beta_2\\ket{11}. \\] <p>If we consider each qubit respectively, we need 2 real number parameters to specify each, thus totally 4. But for the most general 2-qubit state, we need 6. This means there still some states that product states cannot represent, which is the entangled state.</p> <p>A typical entangle state is Bell states, one of which is</p> \\[ \\ket{\\Psi} = \\frac{1}{\\sqrt{2}}(\\ket{01} + \\ket{10}). \\]"},{"location":"Pot-pourri/QISE/Review/#expectation","title":"Expectation","text":"<p>Suppose an observable \\(L\\) with eigenvalues \\(\\lambda\\), for an arbitrary state \\(\\ket{\\psi}\\), we have</p> \\[ \\ket{\\psi} = \\sum_{\\lambda} \\ket{\\lambda}\\braket{\\lambda|\\psi}. \\] <p>Act \\(L\\) on \\(\\ket{\\psi}\\),</p> \\[ L\\ket{\\psi} = \\sum_\\lambda L\\ket{\\lambda}\\braket{\\lambda|\\psi} = \\sum_\\lambda \\lambda\\ket{\\lambda}\\braket{\\lambda|\\psi}. \\] <p>The expectation value of \\(L\\) of the system is defined as</p> \\[ \\braket{L} = \\braket{\\psi|L|\\psi} = \\sum_\\lambda \\lambda\\braket{\\psi|\\lambda}\\braket{\\lambda|\\psi} = \\sum_\\lambda \\lambda P(\\lambda). \\] <p>This is *the weighted sum of \\(L\\)'s eigenvalues \\(\\lambda\\) with probability \\(P(\\lambda) = \\braket{\\psi|\\lambda}\\braket{\\lambda|\\psi}\\).</p> <p>However, we can't get any information from Bell states, since if one measures the first qubit in the Bell state, the result is \\(\\braket{(\\vec{\\sigma} \\cdot \\hat{n}) \\otimes I}\\) = 0. Thus we need another ways to get the information of the entangled states.</p>"},{"location":"Pot-pourri/QISE/Review/#density-matrix","title":"Density Matrix","text":"<p>For pure state \\(\\ket{\\psi}\\), the expectation of an operator \\(L\\) is</p> \\[ \\braket{L} = \\sum_i \\braket{\\psi|L|i}\\braket{i|\\psi} = \\sum_i\\braket{i|\\psi}\\braket{\\psi|L|i} = \\text{Tr}(\\ket{\\psi}\\bra{\\psi}L) = \\text{Tr}\\rho L, \\] <p>where \\(\\rho = \\ket{\\psi}\\bra{\\psi}\\) is the density matrix.</p> <p>For mixed state, with each state \\(\\ket{\\psi_i}\\) of probability \\(P_i\\), the density matrix is</p> \\[ \\rho = \\sum_i P_i \\ket{\\psi_i}\\bra{\\psi_i}. \\]"},{"location":"Pot-pourri/QISE/Review/#reduced-density","title":"Reduced Density","text":"<p>For a separable state \\(\\ket{\\psi} = \\ket{\\lambda}_A\\ket{\\phi}_B\\), the density matrix is</p> \\[ \\rho_{AB} = \\ket{\\psi}\\bra{\\psi} = \\ket{\\lambda}\\bra{\\lambda} \\otimes \\ket{\\phi}\\bra{\\phi} = \\rho_A \\otimes \\rho_B. \\] <p>In this case, we define reduce density matrices</p> \\[ \\rho_A = \\ket{\\lambda}\\bra{\\lambda},\\ \\ \\rho_B = \\ket{\\phi}\\bra{\\phi}. \\] <p>Informally, the reduce density matrix of subsystem \\(A\\) is given by</p> \\[ \\rho_A = \\text{Tr}_B(\\rho_{AB}). \\] <p>Here, \\(\\text{Tr}_B\\) is partial trace, defined as</p> \\[ \\text{Tr}_B(\\ket{\\xi_u}\\bra{\\xi_v} \\otimes \\ket{\\chi_u}\\bra{\\chi_v}) = \\ket{\\xi_u}\\bra{\\xi_v} \\text{Tr}(\\ket{\\chi_u}\\bra{\\chi_v}), \\] <p>where \\(\\ket{\\xi_u}\\) and \\(\\ket{\\xi_v}\\) are arbitrary states in \\(A\\) and \\(\\ket{\\chi_u}\\) and \\(\\ket{\\chi_v}\\) are arbitrary states in \\(B\\).</p> <p>Remains</p> <p>Examples.</p> <p>Entanglement Entropy</p> <p>To measure the entanglement of the two subsystems quantitatively, we define entanglement entropy by</p> \\[     S_A = - \\text{Tr}_A(\\rho_A \\log_2 \\rho_A) = - \\sum_i \\lambda_i \\log_2 \\lambda_i, \\] <p>where \\(\\lambda_i\\) are the eigenvalues of \\(\\rho_A\\).</p>"},{"location":"Pot-pourri/QISE/Review/#lecture-5-time-evolution-of-closed-quantum-systems","title":"Lecture 5 | Time Evolution of Closed Quantum Systems","text":""}]}