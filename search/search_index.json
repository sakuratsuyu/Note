{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u3053\u3053\u306f \u2014\u2014 \u306a\u3093\u3068! \u00b6 A note book by @sakuratsuyu. \u5076\u5c14\u5199\u70b9\u4e1c\u897f\uff0c\u5e0c\u671b\u4f60\u559c\u6b22\u3002 \\[ \\tt{Transfering\\ content\\ from\\ other\\ places\\ these\\ days\\ } \\cdots\\ \\&\\$@\\%\\&\\$@\\%\\&\\$@\\% \\] About me All ids I used: sakuratsuyu , \u685c\u3064\u3086 , \u66e6\u9732 and Mslease . Maybe you can find me somewhere else! Sophomore as a ZJU CS Undergraduate. ZJUSCT Freshman. (\u6478\u5927\u9c7c) MADer. (MAD \u5728\u505a\u4e86\u5728\u505a\u4e86.jpg) Hope you can find anything useful and interesting here! \u2728\u30ad\u30e9\u30ad\u30e9\u76ee\u2728","title":"\u3053\u3053\u306f \u2014\u2014 \u306a\u3093\u3068!"},{"location":"#_1","text":"A note book by @sakuratsuyu. \u5076\u5c14\u5199\u70b9\u4e1c\u897f\uff0c\u5e0c\u671b\u4f60\u559c\u6b22\u3002 \\[ \\tt{Transfering\\ content\\ from\\ other\\ places\\ these\\ days\\ } \\cdots\\ \\&\\$@\\%\\&\\$@\\%\\&\\$@\\% \\] About me All ids I used: sakuratsuyu , \u685c\u3064\u3086 , \u66e6\u9732 and Mslease . Maybe you can find me somewhere else! Sophomore as a ZJU CS Undergraduate. ZJUSCT Freshman. (\u6478\u5927\u9c7c) MADer. (MAD \u5728\u505a\u4e86\u5728\u505a\u4e86.jpg) Hope you can find anything useful and interesting here! \u2728\u30ad\u30e9\u30ad\u30e9\u76ee\u2728","title":"\u3053\u3053\u306f \u2014\u2014 \u306a\u3093\u3068!"},{"location":"comming_soon/","text":"Waiting... \u00b6 Failure Things there are comming soon.","title":"Waiting..."},{"location":"comming_soon/#waiting","text":"Failure Things there are comming soon.","title":"Waiting..."},{"location":"discussion/","text":"Discussion \u00b6 Free talk here about anything! Maybe I can put some blogrolls there... Anyone wants? Blogrolls wants more friends! Isshiki \u4fee's Notebook","title":"Blogroll"},{"location":"discussion/#discussion","text":"Free talk here about anything! Maybe I can put some blogrolls there... Anyone wants? Blogrolls wants more friends! Isshiki \u4fee's Notebook","title":"Discussion"},{"location":"tasklist/","text":"Task List \u00b6 Make a logo! Mathematics Basis Discrete Mathematics | DM Numerical Analysis | NA Abstract Algrebra Computer Science Courses The Missing Semester of your CS education \u2192 Cheat Sheets / Tools Introduction to Computer Systems | ICS HPC 101 Introduction to Computer Vision | ICV Fundemental of Data Strcuture | FDS Digital Logic Design Cheat Sheets / Tools Git Markdown Latex Vim / Neovim Shell Data Wrangling Profiling Pot-pourri Simple Cryptography Guidance to Configure Manjaro + i3wm","title":"Plan"},{"location":"tasklist/#task-list","text":"Make a logo! Mathematics Basis Discrete Mathematics | DM Numerical Analysis | NA Abstract Algrebra Computer Science Courses The Missing Semester of your CS education \u2192 Cheat Sheets / Tools Introduction to Computer Systems | ICS HPC 101 Introduction to Computer Vision | ICV Fundemental of Data Strcuture | FDS Digital Logic Design Cheat Sheets / Tools Git Markdown Latex Vim / Neovim Shell Data Wrangling Profiling Pot-pourri Simple Cryptography Guidance to Configure Manjaro + i3wm","title":"Task List"},{"location":"Computer_Science_Courses/","text":"Title Page \u00b6 Abstract This section stores the notes of the courses that I've learned about computer scicence (CS) from ZJU or other platforms like MIT and Standford. It helps me to build my knowledge system and hope it helps you too. Welcome to point out anything wrong and careless mistakes!","title":"Title Page"},{"location":"Computer_Science_Courses/#title-page","text":"Abstract This section stores the notes of the courses that I've learned about computer scicence (CS) from ZJU or other platforms like MIT and Standford. It helps me to build my knowledge system and hope it helps you too. Welcome to point out anything wrong and careless mistakes!","title":"Title Page"},{"location":"Computer_Science_Courses/ICV/10_Recognition/","text":"","title":"Lecture 10"},{"location":"Computer_Science_Courses/ICV/11_3D_Deep_Learning/","text":"","title":"Lecture 11"},{"location":"Computer_Science_Courses/ICV/12_13_Computational_Photography/","text":"","title":"Lecture 12/13"},{"location":"Computer_Science_Courses/ICV/1_Introduction/","text":"Lecture 1 Introduction \u00b6 Review of Linear Algrebra \u00b6 Affine Transformations \u4eff\u5c04\u53d8\u6362 \u00b6 Affine map = linear map + translation \\[ \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix} \\] Using homogenous coordinates \uff08\u9f50\u6b21\u5750\u6807\uff09 \\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a & b & t_x \\\\ c & d & t_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\] Eigenvectors and Eigenvalues \u00b6 The eigenvalues of symmetric matrices are real numbers. The eigenvalues of positive definite matrices are positive numbers.","title":"Lecture 1"},{"location":"Computer_Science_Courses/ICV/1_Introduction/#lecture-1-introduction","text":"","title":"Lecture 1 Introduction"},{"location":"Computer_Science_Courses/ICV/1_Introduction/#review-of-linear-algrebra","text":"","title":"Review of Linear Algrebra"},{"location":"Computer_Science_Courses/ICV/1_Introduction/#affine-transformations","text":"Affine map = linear map + translation \\[ \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix} \\] Using homogenous coordinates \uff08\u9f50\u6b21\u5750\u6807\uff09 \\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a & b & t_x \\\\ c & d & t_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\]","title":"Affine Transformations \u4eff\u5c04\u53d8\u6362"},{"location":"Computer_Science_Courses/ICV/1_Introduction/#eigenvectors-and-eigenvalues","text":"The eigenvalues of symmetric matrices are real numbers. The eigenvalues of positive definite matrices are positive numbers.","title":"Eigenvectors and Eigenvalues"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/","text":"Lecture 2 Image Formation \u00b6 Camera and Lens \u00b6 Pinhole Camera \u00b6 A barrier to block off most of the rays. The opening is the aperture \u5149\u5708 . Flaws Less light gets through. Diffraction effects\uff08\u884d\u5c04\uff09 Lens \u900f\u955c \u00b6 \\[ \\frac{1}{i} + \\frac{1}{o} = \\frac{1}{f} \\] Focal length \\(f\\) \u7126\u8ddd \u00b6 if \\(o = \\infty\\) , then \\(f = i\\) . Magnification \\(m\\) \u653e\u5927\u7387 \u00b6 \\[ m = \\frac{h_i}{h_o} \\] Field of View (FOV) \u89c6\u91ce \u00b6 Factor \u00b6 Focal Length Longer focal length, Narrower angle of view. Vice versa. Note 50mm / 46\u00b0 (and full frame) is the most similar FOV with human eyes. Thus 50mm lens is called standard lens \uff08\u6807\u51c6\u955c\u5934\uff09. telephoto lens \uff08\u957f\u7126\u955c\u5934\uff0c\u671b\u8fdc\u955c\u5934\uff09\uff1a\u89c6\u91ce\u5c0f\uff0c\u653e\u5927\u7387\u5927 short focal lens \uff08\u77ed\u7126\u955c\u5934\uff0c\u5e7f\u89d2\u955c\u5934\uff09\uff1a\u89c6\u91ce\u5927\uff0c\u653e\u5927\u7387\u5c0f Sensor Size Bigger sensor size, Wider angle of view. Vice versa. Aperture \u5149\u5708 \u00b6 The representation of aperture is its Diameter \\(D\\) . F-Number \\[ N = \\frac{f}{D} \\text{ (mostly greater than 1, around 1.8 ~ 22)} \\] Lens Defocus \u00b6 Blur Circle Diameter (\u5149\u6591\u534a\u5f84) \\[ b = \\frac{D}{i'}|i' -i|, b \\propto D \\propto \\frac{1}{N} \\] Focusing \u5bf9\u7126 \u00b6 Depth of Field (DoF) \u666f\u6df1 \u00b6 \\[ {\\tt DoF} = o_2 - o_1 = \\frac{2of^2cN(o-f)}{f^4-c^2N^2(o-f)^2} \\] From the equation above DoF is almost proportional to \\(N\\) , and thus the Larger aperture, the Smaller F-Number and the Smaller DoF. How to blur the background \u00b6 Large aperture Long focal length Near foreground Far background Geometric Image Formation\uff08\u5b9a\u4f4d\uff09 \u00b6 Camera model \\[ \\begin{bmatrix} u \\\\ v \\end{bmatrix} = \\begin{bmatrix} f \\frac{x}{z} \\\\ f \\frac{y}{z} \\end{bmatrix} \\] Homogeneous Coordinates / Projective Coordinates \u00b6 Suppose that \\(\\begin{bmatrix} x \\\\ y \\\\ w \\end{bmatrix}\\) is the same as \\(\\begin{bmatrix} x/w \\\\ y/w \\\\ 1 \\end{bmatrix}\\) , then we get \\[ \\begin{bmatrix} f & 0 & 0 & 0 \\\\ 0 & f & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} fx \\\\ fy \\\\ z \\\\ \\end{bmatrix} \\cong \\begin{bmatrix} f\\frac{x}{z} \\\\ f\\frac{y}{z} \\\\ 1 \\end{bmatrix} \\] We can also put the image plane in front of the camera (opposite to the previous picture). Perspective Projection \u00b6 Preserevd - Straight lines are still straight Lost - Length and Angle Vanishing Points \u00b6 Properties Any wo parallel lines have the same vanishing point v. v can be outside the image frame or at infinity. Line from C to v is parallel to the lines. Vanishing Lines \u00b6 Multiple vanishing points The direction of the vanishing line tells us the orientation of the plane. Distortion \u00b6 Converging verticals Problem and Solution (View Camera \u79fb\u8f74\u76f8\u673a) Exterior columns appear bigger Due to lens flaws Radial distortion Due to imperfect lens \\[ \\begin{align} r^2 &= {x'}_n^{2} + {y'}_n^{2} \\\\ {x'}_d &= {x'}_n(1 + \\kappa_1r^2 + \\kappa_2r^4) \\\\ {y'}_d &= {y'}_n(1 + \\kappa_1r^2 + \\kappa_2r^4) \\\\ \\end{align} \\] Solution Take a photo of a grid at the same point and then use the mathmatics to calculate and correct radial distortion. Orthographic Projection \u00b6 \\[ \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\\\ 1 \\\\ \\end{bmatrix} \\Rightarrow (x, y) \\] Photometric Image Formation\uff08\u5b9a\u989c\u8272/\u4eae\u5ea6\uff09 \u00b6 Image Sensor \u00b6 CMOS CCD (Charge Coupled Device) Shutter \u00b6 Shutter speed controls exposure time. Color Sensing \u00b6 Color Spaces RGB HSV Practical Color Sensing: Bayer Filter Shading \u7740\u8272 \u00b6 BRDF (Bidirectional Reflectance Distribution Function) \u00b6 \\[ L_r(\\hat{\\textbf{v}_r};\\lambda) = \\int L_i(\\hat{\\textbf{i}_r};\\lambda)f_r(\\hat{\\textbf{v}_r}, \\hat{\\textbf{v}_i}, \\hat{\\textbf{n}}; \\lambda)\\cos^+\\theta_i\\ d\\hat{\\textbf{v}_i} \\] Diffuse (Lambertian) Reflection \u00b6 Shading independent of view direction Specular Term \u00b6 Intensity depends on view direction Blinn-Phong Reflection Model \u00b6 \\[ L = L_a + L_d + L_s = k_aI_a + k_d(I/r^2)\\max(0, \\textbf{n}\\cdot\\textbf{l}) + k_s(I/r^2)\\max(0, \\textbf{n}\\cdot\\textbf{h})^p \\]","title":"Lecture 2"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#lecture-2-image-formation","text":"","title":"Lecture 2 Image Formation"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#camera-and-lens","text":"","title":"Camera and Lens"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#pinhole-camera","text":"A barrier to block off most of the rays. The opening is the aperture \u5149\u5708 . Flaws Less light gets through. Diffraction effects\uff08\u884d\u5c04\uff09","title":"Pinhole Camera"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#lens","text":"\\[ \\frac{1}{i} + \\frac{1}{o} = \\frac{1}{f} \\]","title":"Lens \u900f\u955c"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#focal-length-f","text":"if \\(o = \\infty\\) , then \\(f = i\\) .","title":"Focal length \\(f\\)  \u7126\u8ddd"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#magnification-m","text":"\\[ m = \\frac{h_i}{h_o} \\]","title":"Magnification \\(m\\) \u653e\u5927\u7387"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#field-of-view-fov","text":"","title":"Field of View (FOV) \u89c6\u91ce"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#factor","text":"Focal Length Longer focal length, Narrower angle of view. Vice versa. Note 50mm / 46\u00b0 (and full frame) is the most similar FOV with human eyes. Thus 50mm lens is called standard lens \uff08\u6807\u51c6\u955c\u5934\uff09. telephoto lens \uff08\u957f\u7126\u955c\u5934\uff0c\u671b\u8fdc\u955c\u5934\uff09\uff1a\u89c6\u91ce\u5c0f\uff0c\u653e\u5927\u7387\u5927 short focal lens \uff08\u77ed\u7126\u955c\u5934\uff0c\u5e7f\u89d2\u955c\u5934\uff09\uff1a\u89c6\u91ce\u5927\uff0c\u653e\u5927\u7387\u5c0f Sensor Size Bigger sensor size, Wider angle of view. Vice versa.","title":"Factor"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#aperture","text":"The representation of aperture is its Diameter \\(D\\) . F-Number \\[ N = \\frac{f}{D} \\text{ (mostly greater than 1, around 1.8 ~ 22)} \\]","title":"Aperture \u5149\u5708"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#lens-defocus","text":"Blur Circle Diameter (\u5149\u6591\u534a\u5f84) \\[ b = \\frac{D}{i'}|i' -i|, b \\propto D \\propto \\frac{1}{N} \\]","title":"Lens Defocus"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#focusing","text":"","title":"Focusing \u5bf9\u7126"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#depth-of-field-dof","text":"\\[ {\\tt DoF} = o_2 - o_1 = \\frac{2of^2cN(o-f)}{f^4-c^2N^2(o-f)^2} \\] From the equation above DoF is almost proportional to \\(N\\) , and thus the Larger aperture, the Smaller F-Number and the Smaller DoF.","title":"Depth of Field (DoF) \u666f\u6df1"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#how-to-blur-the-background","text":"Large aperture Long focal length Near foreground Far background","title":"How to blur the background"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#geometric-image-formation","text":"Camera model \\[ \\begin{bmatrix} u \\\\ v \\end{bmatrix} = \\begin{bmatrix} f \\frac{x}{z} \\\\ f \\frac{y}{z} \\end{bmatrix} \\]","title":"Geometric Image Formation\uff08\u5b9a\u4f4d\uff09"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#homogeneous-coordinates-projective-coordinates","text":"Suppose that \\(\\begin{bmatrix} x \\\\ y \\\\ w \\end{bmatrix}\\) is the same as \\(\\begin{bmatrix} x/w \\\\ y/w \\\\ 1 \\end{bmatrix}\\) , then we get \\[ \\begin{bmatrix} f & 0 & 0 & 0 \\\\ 0 & f & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} fx \\\\ fy \\\\ z \\\\ \\end{bmatrix} \\cong \\begin{bmatrix} f\\frac{x}{z} \\\\ f\\frac{y}{z} \\\\ 1 \\end{bmatrix} \\] We can also put the image plane in front of the camera (opposite to the previous picture).","title":"Homogeneous Coordinates / Projective Coordinates"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#perspective-projection","text":"Preserevd - Straight lines are still straight Lost - Length and Angle","title":"Perspective Projection"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#vanishing-points","text":"Properties Any wo parallel lines have the same vanishing point v. v can be outside the image frame or at infinity. Line from C to v is parallel to the lines.","title":"Vanishing Points"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#vanishing-lines","text":"Multiple vanishing points The direction of the vanishing line tells us the orientation of the plane.","title":"Vanishing Lines"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#distortion","text":"Converging verticals Problem and Solution (View Camera \u79fb\u8f74\u76f8\u673a) Exterior columns appear bigger Due to lens flaws Radial distortion Due to imperfect lens \\[ \\begin{align} r^2 &= {x'}_n^{2} + {y'}_n^{2} \\\\ {x'}_d &= {x'}_n(1 + \\kappa_1r^2 + \\kappa_2r^4) \\\\ {y'}_d &= {y'}_n(1 + \\kappa_1r^2 + \\kappa_2r^4) \\\\ \\end{align} \\] Solution Take a photo of a grid at the same point and then use the mathmatics to calculate and correct radial distortion.","title":"Distortion"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#orthographic-projection","text":"\\[ \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\\\ 1 \\\\ \\end{bmatrix} \\Rightarrow (x, y) \\]","title":"Orthographic Projection"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#photometric-image-formation","text":"","title":"Photometric Image Formation\uff08\u5b9a\u989c\u8272/\u4eae\u5ea6\uff09"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#image-sensor","text":"CMOS CCD (Charge Coupled Device)","title":"Image Sensor"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#shutter","text":"Shutter speed controls exposure time.","title":"Shutter"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#color-sensing","text":"Color Spaces RGB HSV Practical Color Sensing: Bayer Filter","title":"Color Sensing"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#shading","text":"","title":"Shading \u7740\u8272"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#brdf-bidirectional-reflectance-distribution-function","text":"\\[ L_r(\\hat{\\textbf{v}_r};\\lambda) = \\int L_i(\\hat{\\textbf{i}_r};\\lambda)f_r(\\hat{\\textbf{v}_r}, \\hat{\\textbf{v}_i}, \\hat{\\textbf{n}}; \\lambda)\\cos^+\\theta_i\\ d\\hat{\\textbf{v}_i} \\]","title":"BRDF (Bidirectional Reflectance Distribution Function)"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#diffuse-lambertian-reflection","text":"Shading independent of view direction","title":"Diffuse (Lambertian) Reflection"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#specular-term","text":"Intensity depends on view direction","title":"Specular Term"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#blinn-phong-reflection-model","text":"\\[ L = L_a + L_d + L_s = k_aI_a + k_d(I/r^2)\\max(0, \\textbf{n}\\cdot\\textbf{l}) + k_s(I/r^2)\\max(0, \\textbf{n}\\cdot\\textbf{h})^p \\]","title":"Blinn-Phong Reflection Model"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/","text":"Lecture 3 Imae Processing \u00b6 Image Processing Basis \u00b6 Operations: Incresing contrast, Invert, Blur, Sharpen, Edge Detection Convolution \u00b6 \\[ (f * g)(x) = \\int_{-\\infty}^\\infty f(y)g(x-y)dy \\] discrete 2D form \\[ (f * g)(x) = \\sum_{i, j =-\\infty}^\\infty f(i, j)g(x - i, y - j) \\] Gaussian Blur \u00b6 2D Gaussian function \\[ \\large f(i, j) = \\frac{1}{2\\pi\\sigma^2}e^{-\\frac{i^2 + j^2}{2\\sigma^2}} \\] Usage Define a window size (commmly a square, say \\(n \\times n\\) , and let \\(r = \\lfloor n / 2 \\rfloor\\) ). Select a point, say \\((x, y)\\) and then a window around it. Apply the Gaussian function at each point in the window, and sum them up, namely \\(G(x, y) = \\sum\\limits_{i = x - r}^{x + r}\\sum\\limits_{j = y - r}^{y + r}f(i,j)\\) . Then the \"blurred\" value of point \\((x, y)\\) is \\(G(x, y)\\) . Sharpen \u00b6 An example of kernel matrix \\[ f = \\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 5 & -1 \\\\ 0 & -1 & 0 \\end{bmatrix} \\] An insight Let \\(I\\) be the original image, then the sharpen image \\(I' = I + (I - \\text{blur}(I))\\) , where \\(I - \\text{blur}(I)\\) can be regarded as the high frequency content. Extract Gradients \u00b6 Examples of kernel matrix \\(f = \\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1\\end{bmatrix}\\) extracts horizontal gradients. \\(f = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\) extracts vertical gradients. Bilateral Filters \u00b6 Kernel depends on image content. Better performance but lower efficiency. A trick: Separable Filter A filter is separable if it can be wriiten as the outer product of two other filters. Example \\[ \\frac19 \\begin{bmatrix} 1 & 1 & 1\\\\ 1 & 1 & 1\\\\ 1 & 1 & 1 \\end{bmatrix} = \\frac13 \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\times \\frac13 \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix} \\] Purpose / Advantages : speed up the calculation Image Sampling \u00b6 Abstract Down-sampling \u2192 Reducing image size Aliasing \u00b6 Aliasing is the artifacts due to sampling Higher frequencies need faster sampling. Undersampling creates frequency aliases. Fourier Transform \u00b6 Simly put, fourier transform represents a function as a weighted sum of sines and cosines , where the sines and consines are in various frequencies . Convolution Theorem From the view of Fourier Transform ... \u00b6 Sampling and Aliasing \u00b6 Sampling is repeating frequency contents. Aliasing is mixed frequency contents. Method to reduce aliasing Increase sampling rate Nyquist-Shannon theorem the signal can be perfectly reconstructed if sampled with a frequency larger than \\(2f_0\\) . Anti-aliasing Filtering, then sampling Example Image Magnification \u00b6 Abstract Up-sampling - Inverse of down-sampling An important method: Interpolation . 1D Interpolation \u00b6 Nearest-neighbour Interpolation Not continuous; Not smooth Linear Interpolation Continuous; Not smooth Cubic Interpolation Continuous; Smooth 2D Interpolation \u00b6 (similar to 1D cases) Nearest-neighbour Interpolation Bilinear Interpolation define \\(\\text{lerp}(x, v_0,v_1) = v_0 + x(v_1 - v_0)\\) . Suppose the point in the rectangle surrounded by four points \\(u_{00},u_{01},u_{10},u_{11}\\) . then \\(f(x, y) = \\text{lerp}(t, u_0, u_1)\\) , where \\(u_0 = \\text{lerp}(s, u_{00}, u_{10})\\) and \\(u_1 = \\text{lerp}(s, u_{01}, u_{11})\\) . Bicubic Interpolation Super-Resolution \u00b6 Remains Change Aspect Ratio \u00b6 Remains","title":"Lecture 3"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#lecture-3-imae-processing","text":"","title":"Lecture 3 Imae Processing"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#image-processing-basis","text":"Operations: Incresing contrast, Invert, Blur, Sharpen, Edge Detection","title":"Image Processing Basis"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#convolution","text":"\\[ (f * g)(x) = \\int_{-\\infty}^\\infty f(y)g(x-y)dy \\] discrete 2D form \\[ (f * g)(x) = \\sum_{i, j =-\\infty}^\\infty f(i, j)g(x - i, y - j) \\]","title":"Convolution"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#gaussian-blur","text":"2D Gaussian function \\[ \\large f(i, j) = \\frac{1}{2\\pi\\sigma^2}e^{-\\frac{i^2 + j^2}{2\\sigma^2}} \\] Usage Define a window size (commmly a square, say \\(n \\times n\\) , and let \\(r = \\lfloor n / 2 \\rfloor\\) ). Select a point, say \\((x, y)\\) and then a window around it. Apply the Gaussian function at each point in the window, and sum them up, namely \\(G(x, y) = \\sum\\limits_{i = x - r}^{x + r}\\sum\\limits_{j = y - r}^{y + r}f(i,j)\\) . Then the \"blurred\" value of point \\((x, y)\\) is \\(G(x, y)\\) .","title":"Gaussian Blur"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#sharpen","text":"An example of kernel matrix \\[ f = \\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 5 & -1 \\\\ 0 & -1 & 0 \\end{bmatrix} \\] An insight Let \\(I\\) be the original image, then the sharpen image \\(I' = I + (I - \\text{blur}(I))\\) , where \\(I - \\text{blur}(I)\\) can be regarded as the high frequency content.","title":"Sharpen"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#extract-gradients","text":"Examples of kernel matrix \\(f = \\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1\\end{bmatrix}\\) extracts horizontal gradients. \\(f = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\) extracts vertical gradients.","title":"Extract Gradients"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#bilateral-filters","text":"Kernel depends on image content. Better performance but lower efficiency. A trick: Separable Filter A filter is separable if it can be wriiten as the outer product of two other filters. Example \\[ \\frac19 \\begin{bmatrix} 1 & 1 & 1\\\\ 1 & 1 & 1\\\\ 1 & 1 & 1 \\end{bmatrix} = \\frac13 \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\times \\frac13 \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix} \\] Purpose / Advantages : speed up the calculation","title":"Bilateral Filters"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#image-sampling","text":"Abstract Down-sampling \u2192 Reducing image size","title":"Image Sampling"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#aliasing","text":"Aliasing is the artifacts due to sampling Higher frequencies need faster sampling. Undersampling creates frequency aliases.","title":"Aliasing"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#fourier-transform","text":"Simly put, fourier transform represents a function as a weighted sum of sines and cosines , where the sines and consines are in various frequencies . Convolution Theorem","title":"Fourier Transform"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#from-the-view-of-fourier-transform","text":"","title":"From the view of Fourier Transform ..."},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#sampling-and-aliasing","text":"Sampling is repeating frequency contents. Aliasing is mixed frequency contents. Method to reduce aliasing Increase sampling rate Nyquist-Shannon theorem the signal can be perfectly reconstructed if sampled with a frequency larger than \\(2f_0\\) . Anti-aliasing Filtering, then sampling Example","title":"Sampling and Aliasing"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#image-magnification","text":"Abstract Up-sampling - Inverse of down-sampling An important method: Interpolation .","title":"Image Magnification"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#1d-interpolation","text":"Nearest-neighbour Interpolation Not continuous; Not smooth Linear Interpolation Continuous; Not smooth Cubic Interpolation Continuous; Smooth","title":"1D Interpolation"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#2d-interpolation","text":"(similar to 1D cases) Nearest-neighbour Interpolation Bilinear Interpolation define \\(\\text{lerp}(x, v_0,v_1) = v_0 + x(v_1 - v_0)\\) . Suppose the point in the rectangle surrounded by four points \\(u_{00},u_{01},u_{10},u_{11}\\) . then \\(f(x, y) = \\text{lerp}(t, u_0, u_1)\\) , where \\(u_0 = \\text{lerp}(s, u_{00}, u_{10})\\) and \\(u_1 = \\text{lerp}(s, u_{01}, u_{11})\\) . Bicubic Interpolation","title":"2D Interpolation"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#super-resolution","text":"Remains","title":"Super-Resolution"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#change-aspect-ratio","text":"Remains","title":"Change Aspect Ratio"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/","text":"Lecture 4 Model Fitting and Optimization \u00b6 Optimization \u00b6 \\[ \\begin{align} & \\text{Minimize } & f_0(x) \\\\ & \\text{Subject to } &f_i(x) & \\le 0,\\ i = 1, \\dots, m \\\\ & & g_i(x) & = 0,\\ i = 1,\\dots, p \\end{align} \\] \\(x \\in \\mathbb{R}^n\\) is a vector variable to be chosen. \\(f_0\\) is the objective function to be minimized. \\(f_1, \\dots, f_m\\) are the inequality constraint functions. \\(g_1, \\dots, g_p\\) are the equality constraint functions. Model Fitting \u00b6 A typical approach: Minimize the Mean Square Error ( MSE ) \\[ \\hat x = \\mathop{\\arg\\min}\\limits_x \\sum\\limits_i(b_i - a_i^Tx)^2 \\] Reasons to choose MSE \u00b6 Key Assumptions: MSE = MLE with Gaussian noise From Maximum Likelihood Estimation ( MLE ) , the data is assumed to be with Gaussian noise . \\[ b_i = a_i^Tx + n,\\ n \\sim G(0, \\sigma) \\] The likelihood of observing \\((a_i, b_i)\\) is \\[ P[(a_i, b_i)|x] = P[b_i - a_i^Tx] \\propto \\exp\\left(-\\frac{(b_i - a_i^Tx)^2}{2\\sigma^2}\\right) \\] Note If the data points are independent , \\[ \\begin{align} P[(a_1, b_1)(a_2, b_2)\\dots|x] & = \\prod\\limits_iP[(a_i, b_i)|x] = \\prod\\limits_iP[b_i - a_{i^{Tx]}} \\\\ & \\propto \\exp\\left(-\\frac{\\sum_i(b_i - a_i^Tx)^2}{2\\sigma^2}\\right) = \\exp\\left(-\\frac{||Ax-b||^2_2}{2\\sigma^2}\\right) \\end{align} \\] \\[ \\begin{align} \\hat x &= \\mathop{\\arg\\max}\\limits_x P[(a_1, b_1)(a_2, b_2)\\dots|x] \\\\ &= \\mathop{\\arg\\max}\\limits_x \\exp\\left(-\\frac{||Ax-b||^2_2}{2\\sigma^2}\\right) = \\mathop{\\arg\\min}\\limits_x||Ax - b||_2^2 \\end{align} \\] Numerical Methods \u00b6 Analytical Solution \u89e3\u6790\u89e3 \u00b6 The derivative of \\(||Ax-b||^2_2\\) is \\(2A^T(Ax - b)\\) , let it be 0. Then we get \\(\\hat x\\) satisfying \\[ A^TAx = A^Tb \\] But, if no analytical solution ... Approximate Solution \u8fd1\u4f3c\u89e3 \u00b6 Method \\(x \\leftarrow x_0\\) \\(\\text{while not converge}\\) \\(p \\leftarrow \\text{descending_direction(x)}\\) \\(\\alpha \\leftarrow \\text{descending_step(x)}\\) \\(x \\leftarrow x + \\alpha p\\) Gradient Descent (GD) \u00b6 Steepest Descent Method \u00b6 \\[ F(x_0 + \\Delta x) \\approx F(x_0) + J_F\\Delta x \\] Direction \\(\\Delta x = -J^T_F\\) Step To minimize \\(\\phi(a)\\) Backtracking Algorithm Initialize \\(\\alpha\\) with a large value. Decrease \\(\\alpha\\) until \\(\\phi(\\alpha)\\le\\phi(0) + \\gamma\\phi'(0)\\alpha\\) . Advantage Easy to implement Perform well when far from the minimum Disadvantage Converge slowly when near the minimum, which wastes a lot of computation Newton Method \u00b6 \\[ F(x_0 + \\Delta x) \\approx F(x_0) + J_F\\Delta x + \\frac12\\Delta x^TH_F\\Delta x \\] \\(\\Delta x = -H_F^{-1}J^T_F\\) Advantage Faster convergence Disadvantage Hessian matrix requires a lot of computation Gauss-Newton Method \u00b6 For \\(\\hat x = \\mathop{\\arg\\min}\\limits_x ||Ax-b||^2_2 \\overset{\\Delta}{=\\!=}\\mathop{\\arg\\min}\\limits_x||R(x)||^2_2\\) , expand \\(R(x)\\) . \\[ \\begin{align} ||R(x_k+\\Delta x)||^2_2 &\\approx ||R(x_k) + J_R\\Delta x||^2_2 \\newline &= ||R(x_k)||^2_2 + 2R(x_k)^TJ_R\\Delta x + \\Delta x^TJ^T_RJ_R\\Delta x \\end{align} \\] \\(\\Delta x = -(J_R^TJ_R)^{-1}J_R^TR(x_k) = -(J_R^TJ_R)^{-1}J_F^T\\) Compared to Newton Method, \\(J_R^TJ_R\\) is used to approximate \\(H_F\\) . Advantage No need to compute Hessian matrix Faster to converge Disadvantage \\(J^T_RJ_R\\) may be singular. Levenberg-Marquardt (LM) \u00b6 \\(\\Delta x = -(J_R^TJ_R + \\lambda I)^{-1}J_R^TR(x_k)\\) \\(\\lambda \\rightarrow \\infty\\) Steepest Descent \\(\\lambda \\rightarrow 0\\) Gauss-Newton Advantage Start and converge quickly Local Minimum and Global Minimum Convex Optimization \u00b6 Remains Robust Estimation \u00b6 Inlier obeys the model assumption. Outlier differs significantly rom the assumption. Outlier makes MSE fail. To reduce its effect, we can use other loss functions, called robust functions. RANSAC (Random Sample Concensus) \u00b6 The most powerful method to handle outliers. Key ideas The distribution of inliers is similar while outliers differ a lot. Use data point pairs to vote ill-posed Problem \u75c5\u6001\u95ee\u9898 / \u591a\u89e3\u95ee\u9898 \u00b6 The solution is not unique. To make it unique: L2 regularization Suppress redundant variables \\(\\min_x||Ax-b||^2_2,\\ s.t. ||x||_2 \\le 1\\) (the same as \\(\\min_x||Ax-b||^2_2 + \\lambda||x||_2\\) ) L1 regularization Make \\(x\\) sparse \\(\\min_x||Ax-b||^2_2,\\ s.t. ||x||_1 \\le 1\\) (the same as \\(\\min_x||Ax-b||^2_2 + \\lambda||x||_1\\) ) Graphcut and MRF \u00b6 Note A key idea: Neighboring pixels tend to take the same label. Images as Graphs A vertex for each pixel An edge between each pair, weighted by the affinity or similarity between its two vertices Pixel Dissimilarity \\(S(\\textbf{f}_i, \\textbf{f}_j) = \\sqrt{\\sum_k(f_{ik} - f_{jk})^2}\\) Pixel Affinity \\(w(i, j) = A(\\textbf{f}_i, \\textbf{f}_j) = \\exp\\left(\\frac{-1}{2\\sigma^2}S(\\textbf{f}_i, \\textbf{f}_j)\\right)\\) Graph Notation \\(G = (V, E)\\) \\(V\\) : a set of vertices \\(E\\) : a set of edges Graph Cut \u00b6 Cut \\(C=(V_A, V_B)\\) is a parition of vertices \\(V\\) of a graph \\(G\\) into two disjoint subsets \\(V_A\\) and \\(V_B\\) . Cost of Cut \\(\\text{cut}(V_A, V_B) = \\sum_{u\\in V_A, v\\in V_B} w(u, v)\\) Problem with min-cut \u00b6 Bias to cut small, isolated segments Solution: Normalized cut Compute how strongly verices \\(V_A\\) are associated with vertices \\(V\\) \\[ \\text{assoc}(V_A, V) = \\sum_{u\\in V_A, v\\in V} w(u, v) \\] \\[ \\text{NCut}(V_A, V_B) = \\frac{\\text{cut}(V_A, V_B)}{\\text{assoc}(V_A, V)} + \\frac{\\text{cut}(V_A, V_B)}{\\text{assoc}(V_B, V)} \\] Markow Random Field ( MRF ) \u00b6 Graphcut is an exception of MRF . Question","title":"Lecture 4"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#lecture-4-model-fitting-and-optimization","text":"","title":"Lecture 4 Model Fitting and Optimization"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#optimization","text":"\\[ \\begin{align} & \\text{Minimize } & f_0(x) \\\\ & \\text{Subject to } &f_i(x) & \\le 0,\\ i = 1, \\dots, m \\\\ & & g_i(x) & = 0,\\ i = 1,\\dots, p \\end{align} \\] \\(x \\in \\mathbb{R}^n\\) is a vector variable to be chosen. \\(f_0\\) is the objective function to be minimized. \\(f_1, \\dots, f_m\\) are the inequality constraint functions. \\(g_1, \\dots, g_p\\) are the equality constraint functions.","title":"Optimization"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#model-fitting","text":"A typical approach: Minimize the Mean Square Error ( MSE ) \\[ \\hat x = \\mathop{\\arg\\min}\\limits_x \\sum\\limits_i(b_i - a_i^Tx)^2 \\]","title":"Model Fitting"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#reasons-to-choose-mse","text":"Key Assumptions: MSE = MLE with Gaussian noise From Maximum Likelihood Estimation ( MLE ) , the data is assumed to be with Gaussian noise . \\[ b_i = a_i^Tx + n,\\ n \\sim G(0, \\sigma) \\] The likelihood of observing \\((a_i, b_i)\\) is \\[ P[(a_i, b_i)|x] = P[b_i - a_i^Tx] \\propto \\exp\\left(-\\frac{(b_i - a_i^Tx)^2}{2\\sigma^2}\\right) \\] Note If the data points are independent , \\[ \\begin{align} P[(a_1, b_1)(a_2, b_2)\\dots|x] & = \\prod\\limits_iP[(a_i, b_i)|x] = \\prod\\limits_iP[b_i - a_{i^{Tx]}} \\\\ & \\propto \\exp\\left(-\\frac{\\sum_i(b_i - a_i^Tx)^2}{2\\sigma^2}\\right) = \\exp\\left(-\\frac{||Ax-b||^2_2}{2\\sigma^2}\\right) \\end{align} \\] \\[ \\begin{align} \\hat x &= \\mathop{\\arg\\max}\\limits_x P[(a_1, b_1)(a_2, b_2)\\dots|x] \\\\ &= \\mathop{\\arg\\max}\\limits_x \\exp\\left(-\\frac{||Ax-b||^2_2}{2\\sigma^2}\\right) = \\mathop{\\arg\\min}\\limits_x||Ax - b||_2^2 \\end{align} \\]","title":"Reasons to choose MSE"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#numerical-methods","text":"","title":"Numerical Methods"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#analytical-solution","text":"The derivative of \\(||Ax-b||^2_2\\) is \\(2A^T(Ax - b)\\) , let it be 0. Then we get \\(\\hat x\\) satisfying \\[ A^TAx = A^Tb \\] But, if no analytical solution ...","title":"Analytical Solution \u89e3\u6790\u89e3"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#approximate-solution","text":"Method \\(x \\leftarrow x_0\\) \\(\\text{while not converge}\\) \\(p \\leftarrow \\text{descending_direction(x)}\\) \\(\\alpha \\leftarrow \\text{descending_step(x)}\\) \\(x \\leftarrow x + \\alpha p\\)","title":"Approximate Solution \u8fd1\u4f3c\u89e3"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#gradient-descent-gd","text":"","title":"Gradient Descent (GD)"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#steepest-descent-method","text":"\\[ F(x_0 + \\Delta x) \\approx F(x_0) + J_F\\Delta x \\] Direction \\(\\Delta x = -J^T_F\\) Step To minimize \\(\\phi(a)\\) Backtracking Algorithm Initialize \\(\\alpha\\) with a large value. Decrease \\(\\alpha\\) until \\(\\phi(\\alpha)\\le\\phi(0) + \\gamma\\phi'(0)\\alpha\\) . Advantage Easy to implement Perform well when far from the minimum Disadvantage Converge slowly when near the minimum, which wastes a lot of computation","title":"Steepest Descent Method"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#newton-method","text":"\\[ F(x_0 + \\Delta x) \\approx F(x_0) + J_F\\Delta x + \\frac12\\Delta x^TH_F\\Delta x \\] \\(\\Delta x = -H_F^{-1}J^T_F\\) Advantage Faster convergence Disadvantage Hessian matrix requires a lot of computation","title":"Newton Method"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#gauss-newton-method","text":"For \\(\\hat x = \\mathop{\\arg\\min}\\limits_x ||Ax-b||^2_2 \\overset{\\Delta}{=\\!=}\\mathop{\\arg\\min}\\limits_x||R(x)||^2_2\\) , expand \\(R(x)\\) . \\[ \\begin{align} ||R(x_k+\\Delta x)||^2_2 &\\approx ||R(x_k) + J_R\\Delta x||^2_2 \\newline &= ||R(x_k)||^2_2 + 2R(x_k)^TJ_R\\Delta x + \\Delta x^TJ^T_RJ_R\\Delta x \\end{align} \\] \\(\\Delta x = -(J_R^TJ_R)^{-1}J_R^TR(x_k) = -(J_R^TJ_R)^{-1}J_F^T\\) Compared to Newton Method, \\(J_R^TJ_R\\) is used to approximate \\(H_F\\) . Advantage No need to compute Hessian matrix Faster to converge Disadvantage \\(J^T_RJ_R\\) may be singular.","title":"Gauss-Newton Method"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#levenberg-marquardt-lm","text":"\\(\\Delta x = -(J_R^TJ_R + \\lambda I)^{-1}J_R^TR(x_k)\\) \\(\\lambda \\rightarrow \\infty\\) Steepest Descent \\(\\lambda \\rightarrow 0\\) Gauss-Newton Advantage Start and converge quickly Local Minimum and Global Minimum","title":"Levenberg-Marquardt (LM)"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#convex-optimization","text":"Remains","title":"Convex Optimization"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#robust-estimation","text":"Inlier obeys the model assumption. Outlier differs significantly rom the assumption. Outlier makes MSE fail. To reduce its effect, we can use other loss functions, called robust functions.","title":"Robust Estimation"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#ransac-random-sample-concensus","text":"The most powerful method to handle outliers. Key ideas The distribution of inliers is similar while outliers differ a lot. Use data point pairs to vote","title":"RANSAC (Random Sample Concensus)"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#ill-posed-problem","text":"The solution is not unique. To make it unique: L2 regularization Suppress redundant variables \\(\\min_x||Ax-b||^2_2,\\ s.t. ||x||_2 \\le 1\\) (the same as \\(\\min_x||Ax-b||^2_2 + \\lambda||x||_2\\) ) L1 regularization Make \\(x\\) sparse \\(\\min_x||Ax-b||^2_2,\\ s.t. ||x||_1 \\le 1\\) (the same as \\(\\min_x||Ax-b||^2_2 + \\lambda||x||_1\\) )","title":"ill-posed Problem \u75c5\u6001\u95ee\u9898 / \u591a\u89e3\u95ee\u9898"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#graphcut-and-mrf","text":"Note A key idea: Neighboring pixels tend to take the same label. Images as Graphs A vertex for each pixel An edge between each pair, weighted by the affinity or similarity between its two vertices Pixel Dissimilarity \\(S(\\textbf{f}_i, \\textbf{f}_j) = \\sqrt{\\sum_k(f_{ik} - f_{jk})^2}\\) Pixel Affinity \\(w(i, j) = A(\\textbf{f}_i, \\textbf{f}_j) = \\exp\\left(\\frac{-1}{2\\sigma^2}S(\\textbf{f}_i, \\textbf{f}_j)\\right)\\) Graph Notation \\(G = (V, E)\\) \\(V\\) : a set of vertices \\(E\\) : a set of edges","title":"Graphcut and MRF"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#graph-cut","text":"Cut \\(C=(V_A, V_B)\\) is a parition of vertices \\(V\\) of a graph \\(G\\) into two disjoint subsets \\(V_A\\) and \\(V_B\\) . Cost of Cut \\(\\text{cut}(V_A, V_B) = \\sum_{u\\in V_A, v\\in V_B} w(u, v)\\)","title":"Graph Cut"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#problem-with-min-cut","text":"Bias to cut small, isolated segments Solution: Normalized cut Compute how strongly verices \\(V_A\\) are associated with vertices \\(V\\) \\[ \\text{assoc}(V_A, V) = \\sum_{u\\in V_A, v\\in V} w(u, v) \\] \\[ \\text{NCut}(V_A, V_B) = \\frac{\\text{cut}(V_A, V_B)}{\\text{assoc}(V_A, V)} + \\frac{\\text{cut}(V_A, V_B)}{\\text{assoc}(V_B, V)} \\]","title":"Problem with min-cut"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#markow-random-field-mrf","text":"Graphcut is an exception of MRF . Question","title":"Markow Random Field (MRF)"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/","text":"Lecture 5 Image Matching and Motion Estimation \u00b6 Image Matching \u00b6 Abstract Dectection Description Matching Learned based matching 1. Detection \u00b6 We want uniqueness. Corner Detection \u00b6 Local measures of uniqueness \u00b6 shifting the window in any direction causes a big change distribution of gradients Principal Component Analysis (PCA) \u4e3b\u6210\u5206\u5206\u6790 To describe the distribution of gradients by two eigenvectors. Method Compute the covariance matrix \\(H\\) at each point and compute its eigenvalues \\(\\lambda_1\\) , \\(\\lambda_2\\) . Classify Flat - \\(\\lambda_1\\) and \\(lambda_2\\) are both small. Edge - \\(\\lambda_1 \\gg \\lambda_2\\) or \\(\\lambda_1 \\ll \\lambda_2\\) . Corner - \\(\\lambda_1\\) and $lambda_2are both large. Harri Corner Detector However, computing eigenvalues are expensive. Instead, we use Harris corner detector to indicate it. Compute derivatives at each pixel. Compute covariance matrix \\(H\\) in a Gaussian window around each pixel. Compute corner response function \\(f\\) , given by \\[ f = \\frac{\\lambda_1\\lambda_2}{\\lambda_1 + \\lambda_2} = \\frac{\\det(H)}{\\text{tr}(H)} \\] Threshold \\(f\\) . Find local maxima of response function. Invariance Properties Invariant to intensity shift, translation, rotation Not invariant to intensity scaling, scaling How to find the correct scale? Try each scale and find the scale of maximum of \\(f\\) . Implementation \u2014\u2014 image pyramid with a fixed window size. Blob Detection \u00b6 convolution! Laplacian of Gaussian (LoG) Filter \u00b6 Example \\[ f = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & -4 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix} \\] Laplacian is sensitive to noise. To solve this flaw, we often Smooth image with a Gaussian filter Compute Laplacian \\[ \\nabla^2(f*g) = f*\\nabla^2g, \\text{ where $f$ is the Laplacian and $g$ is the Gaussian} \\] Scale Selection (the same problem as corner detection) \u00b6 The same solution as corner detection - Try and find maximum. Implementation: Difference of Gaussian (DoG) \u00b6 A way to acclerate computation, since LoG can be approximated by DoG. \\[ \\nabla^2G_\\sigma \\approx G_{\\sigma_1} - G_{\\sigma_2} \\] 2. Description \u00b6 We mainly focus on the SIFT (Scale Invariant Feature Transform) descriptor. Orientation Normalization \u00b6 Compute orientation histogram Select dominant orientation Normalize: rotate to fixed orientation Properites of SIFT \u00b6 Extraordinaraily robust matching technique handle changes in viewpoint Theoretically invariant to scale and rotation handle significant changes in illumination Fast Other dectectores and descriptors HOG SURF FAST ORB Fast Retina Key-point 3. Matching \u00b6 Feature matching \u00b6 Define distance function to compare two descriptors. Test all to find the minimum distance. Problem: repeated elements To find that the problem happens: Ratio Test Ratio score = \\(\\frac{||f_1 - f_2||}{||f_1 - f_2'||}\\) best match \\(f_2\\) , second best match \\(f'_2\\) . Another strategy: Mutual Nearest Neighbor \\(f_2\\) is the nearest neighbour of \\(f_1\\) in \\(I_2\\) . \\(f_1\\) is the nearest neighbour of \\(f_2\\) in \\(I_1\\) . 4. Learning based matching \u00b6 Question Motion Estimation \u00b6 Problems Feature-tracking Optical flow Method: Lucas-Kanade Method \u00b6 Assumptions \u00b6 Small motion Brightness constancy Spatial coherence Brightness constancy \u00b6 \\[ \\begin{align} I(x, y, t) &= I(x + u, y + v, t + 1) \\\\ I(x + u, y + v, t + 1) &\\approx I(x, y, t) + I_x u + I_y v + I_t \\\\ I_x u + I_y v + I_t &\\approx 0 \\\\ \\nabla I \\cdot [u, v]^T + I_t &= 0 \\end{align} \\] Aperture Problem \u00b6 Idea: To get more equations \u2192 Spatial coherence constraint Assume the pixel's neighbors have the same motion \\([u, v]\\) . If we use an \\(n\\times n\\) window, \\[ \\begin{bmatrix} I_x(\\textbf{p}_\\textbf{1}) & I_y(\\textbf{p}_\\textbf{1}) \\\\ \\vdots & \\vdots\\\\ I_x(\\textbf{p}_\\textbf{1}) & I_y(\\textbf{p}_{\\textbf{n}^\\textbf{2}}) \\\\ \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = - \\begin{bmatrix} I_t(\\textbf{p}_\\textbf{1}) \\\\ \\vdots \\\\ I_t(\\textbf{p}_{\\textbf{n}^\\textbf{2}}) \\end{bmatrix} \\Rightarrow Ad=b \\] More equations than variables. Thus we solve \\(\\min_d||Ad-b||^2\\) . Solution is given by \\((A^TA)d = A^Tb\\) , when solvable, which is \\(A^TA\\) should be invertible and well-conditioned (eigenvalues should not be too small, similarly with criteria for Harris corner detector). Thus it's easier to estimate the motion of a corner, then edge, then low texture region (flat). Small Motion Assumption \u00b6 For taylor expansion, we want the motion as small as possbile. But probably not - at least larger than one pixel. Solution \u00b6 Reduce the resolution Use the image pyramid","title":"Lecture 5"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#lecture-5-image-matching-and-motion-estimation","text":"","title":"Lecture 5 Image Matching and Motion Estimation"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#image-matching","text":"Abstract Dectection Description Matching Learned based matching","title":"Image Matching"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#1-detection","text":"We want uniqueness.","title":"1. Detection"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#corner-detection","text":"","title":"Corner Detection"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#local-measures-of-uniqueness","text":"shifting the window in any direction causes a big change distribution of gradients Principal Component Analysis (PCA) \u4e3b\u6210\u5206\u5206\u6790 To describe the distribution of gradients by two eigenvectors. Method Compute the covariance matrix \\(H\\) at each point and compute its eigenvalues \\(\\lambda_1\\) , \\(\\lambda_2\\) . Classify Flat - \\(\\lambda_1\\) and \\(lambda_2\\) are both small. Edge - \\(\\lambda_1 \\gg \\lambda_2\\) or \\(\\lambda_1 \\ll \\lambda_2\\) . Corner - \\(\\lambda_1\\) and $lambda_2are both large. Harri Corner Detector However, computing eigenvalues are expensive. Instead, we use Harris corner detector to indicate it. Compute derivatives at each pixel. Compute covariance matrix \\(H\\) in a Gaussian window around each pixel. Compute corner response function \\(f\\) , given by \\[ f = \\frac{\\lambda_1\\lambda_2}{\\lambda_1 + \\lambda_2} = \\frac{\\det(H)}{\\text{tr}(H)} \\] Threshold \\(f\\) . Find local maxima of response function. Invariance Properties Invariant to intensity shift, translation, rotation Not invariant to intensity scaling, scaling How to find the correct scale? Try each scale and find the scale of maximum of \\(f\\) . Implementation \u2014\u2014 image pyramid with a fixed window size.","title":"Local measures of uniqueness"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#blob-detection","text":"convolution!","title":"Blob Detection"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#laplacian-of-gaussian-log-filter","text":"Example \\[ f = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & -4 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix} \\] Laplacian is sensitive to noise. To solve this flaw, we often Smooth image with a Gaussian filter Compute Laplacian \\[ \\nabla^2(f*g) = f*\\nabla^2g, \\text{ where $f$ is the Laplacian and $g$ is the Gaussian} \\]","title":"Laplacian of Gaussian (LoG) Filter"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#scale-selection-the-same-problem-as-corner-detection","text":"The same solution as corner detection - Try and find maximum.","title":"Scale Selection (the same problem as corner detection)"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#implementation-difference-of-gaussian-dog","text":"A way to acclerate computation, since LoG can be approximated by DoG. \\[ \\nabla^2G_\\sigma \\approx G_{\\sigma_1} - G_{\\sigma_2} \\]","title":"Implementation: Difference of Gaussian (DoG)"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#2-description","text":"We mainly focus on the SIFT (Scale Invariant Feature Transform) descriptor.","title":"2. Description"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#orientation-normalization","text":"Compute orientation histogram Select dominant orientation Normalize: rotate to fixed orientation","title":"Orientation Normalization"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#properites-of-sift","text":"Extraordinaraily robust matching technique handle changes in viewpoint Theoretically invariant to scale and rotation handle significant changes in illumination Fast Other dectectores and descriptors HOG SURF FAST ORB Fast Retina Key-point","title":"Properites of SIFT"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#3-matching","text":"","title":"3. Matching"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#feature-matching","text":"Define distance function to compare two descriptors. Test all to find the minimum distance. Problem: repeated elements To find that the problem happens: Ratio Test Ratio score = \\(\\frac{||f_1 - f_2||}{||f_1 - f_2'||}\\) best match \\(f_2\\) , second best match \\(f'_2\\) . Another strategy: Mutual Nearest Neighbor \\(f_2\\) is the nearest neighbour of \\(f_1\\) in \\(I_2\\) . \\(f_1\\) is the nearest neighbour of \\(f_2\\) in \\(I_1\\) .","title":"Feature matching"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#4-learning-based-matching","text":"Question","title":"4. Learning based matching"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#motion-estimation","text":"Problems Feature-tracking Optical flow","title":"Motion Estimation"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#method-lucas-kanade-method","text":"","title":"Method: Lucas-Kanade Method"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#assumptions","text":"Small motion Brightness constancy Spatial coherence","title":"Assumptions"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#brightness-constancy","text":"\\[ \\begin{align} I(x, y, t) &= I(x + u, y + v, t + 1) \\\\ I(x + u, y + v, t + 1) &\\approx I(x, y, t) + I_x u + I_y v + I_t \\\\ I_x u + I_y v + I_t &\\approx 0 \\\\ \\nabla I \\cdot [u, v]^T + I_t &= 0 \\end{align} \\]","title":"Brightness constancy"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#aperture-problem","text":"Idea: To get more equations \u2192 Spatial coherence constraint Assume the pixel's neighbors have the same motion \\([u, v]\\) . If we use an \\(n\\times n\\) window, \\[ \\begin{bmatrix} I_x(\\textbf{p}_\\textbf{1}) & I_y(\\textbf{p}_\\textbf{1}) \\\\ \\vdots & \\vdots\\\\ I_x(\\textbf{p}_\\textbf{1}) & I_y(\\textbf{p}_{\\textbf{n}^\\textbf{2}}) \\\\ \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = - \\begin{bmatrix} I_t(\\textbf{p}_\\textbf{1}) \\\\ \\vdots \\\\ I_t(\\textbf{p}_{\\textbf{n}^\\textbf{2}}) \\end{bmatrix} \\Rightarrow Ad=b \\] More equations than variables. Thus we solve \\(\\min_d||Ad-b||^2\\) . Solution is given by \\((A^TA)d = A^Tb\\) , when solvable, which is \\(A^TA\\) should be invertible and well-conditioned (eigenvalues should not be too small, similarly with criteria for Harris corner detector). Thus it's easier to estimate the motion of a corner, then edge, then low texture region (flat).","title":"Aperture Problem"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#small-motion-assumption","text":"For taylor expansion, we want the motion as small as possbile. But probably not - at least larger than one pixel.","title":"Small Motion Assumption"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#solution","text":"Reduce the resolution Use the image pyramid","title":"Solution"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/","text":"Lecture 6 Image Stitching \u00b6 Image Warping \u00b6 Info Image filtering changes intensity of image. Image warping (\u7fd8\u66f2) changes shape of image. 2D Transformations \u00b6 Affine Transformations \u4eff\u5c04\u53d8\u6362 \u00b6 \\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\] \\[ \\begin{align} x' &= ax + by + c \\\\ y' &= dx + ey + f \\end{align} \\] \u81ea\u7531\u5ea6 (degree of freedom) \u4e3a 6 \uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u81f3\u5c11 3 \u7ec4\u70b9\uff0c\u4e5f\u5c31\u662f 6 \u4e2a\u65b9\u7a0b\uff0c\u4ee5\u80fd\u591f\u6c42\u89e3\u51fa \\(a, b, c, d, e, f\\) . Projective Transformation (Homography) \u5355\u5e94\u53d8\u6362 \u00b6 \\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} \\cong \\begin{bmatrix} h_{00} & h_{01} & h_{02} \\\\ h_{10} & h_{11} & h_{12} \\\\ h_{20} & h_{21} & h_{22} \\\\ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\] \\[ \\begin{align} x' = \\frac{h_{00}x + h_{01}y + h_{02}}{h_{20}x + h_{11}y + h_{12}} y' = \\frac{h_{10}x + h_{11}y + h_{12}}{h_{20}x + h_{11}y + h_{12}} \\end{align} \\] Constraint: \u4ee4 \\(||(h_{00}, h_{01}, \\dots, h_{22})||\\) (\u5411\u91cf\u8303\u6570) \u89c4\u5b9a\u4e3a \\(1\\) \uff0c\u6216\u8005\u4ee4 \\(h_{22}\\) \u4e3a \\(1\\) . \u8003\u8651\u5230\u4ee5\u4e0a\u7684\u9650\u5236\u6761\u4ef6 (constraint)\uff0c\u81ea\u7531\u5ea6 (degree of freedom) \u4e3a 8 \uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u81f3\u5c11 4 \u7ec4\u70b9\uff0c\u4e5f\u5c31\u662f 8 \u4e2a\u65b9\u7a0b\uff0c\u4ee5\u80fd\u591f\u6c42\u89e3\u51fa \\(h_{ij}\\) . Summary \u00b6 Implementation \u00b6 \u51fa\u4e8e\u65b9\u4fbf\u548c\u53ef\u64cd\u4f5c\u6027\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u8868\u5f81\u6574\u4f53\u7fd8\u66f2\u7684\u51fd\u6570 \\(T\\) \u800c\u4e0d\u662f\u5bf9\u6bcf\u4e2a\u50cf\u7d20\u70b9\u91c7\u53d6\u4e0d\u540c\u7684\u7fd8\u66f2\u7b56\u7565. Forward Warping (Not so good) \u00b6 \u5047\u8bbe\u56fe\u50cf\u51fd\u6570 \\(f:(x, y) \\rightarrow (r, g, b)\\) \u7ecf\u8fc7\u4e86\u53d8\u6362 \\(T: (x, y) \\rightarrow (x', y')\\) \u5f97\u5230\u65b0\u7684\u56fe\u50cf\u51fd\u6570 \\(g:(x', y') \\rightarrow (r, g, b)\\) . If the transformed pixed lands inside the pixels - Quite complicated to solve it. Inverse Warping \u00b6 Why not consider inversely? If the transformed pixed lands inside the pixels Solution 2D Interpolation! Image Stitching \u00b6 \\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} \\cong T \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\] Image Matching \u00b6 \u901a\u8fc7\u56fe\u50cf\u5339\u914d\u5f97\u5230\u5339\u914d\u70b9\uff0c\u6bcf\u4e2a\u5339\u914d\u70b9\u53ef\u4ee5\u63d0\u4f9b\u4e00\u4e2a\u4ee5\u4e0a\u5f62\u5f0f\u7684\u77e9\u9635\u4e58\u6cd5\u7b49\u5f0f. Find \\(T\\) for Image Wraping \u00b6 Affine Transformations \u00b6 \u5bf9\u4e8e\u6bcf\u4e2a\u5339\u914d\uff0c \\[ \\begin{align} x' = ax + by + c \\\\ y' = dx + ey + f \\end{align} \\] \u77e9\u9635\u5f62\u5f0f\u4e3a \\[ \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} x & y & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x & y & 1 \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix} \\] \u5bf9\u4e8e \\(n\\) \u4e2a\u5339\u914d\uff0c\u5f97\u5230\u4ee5\u4e0b\u65b9\u7a0b\u7ec4 \\[ \\begin{bmatrix} x_1 & y_1 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_1 & y_1 & 1 \\\\ x_2 & y_2 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_2 & y_2 & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_n & y_n & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_n & y_n & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix} = \\begin{bmatrix} x_1' \\\\ y_1' \\\\ x_2' \\\\ y_2' \\\\ \\vdots \\\\ x_n' \\\\ y_n' \\end{bmatrix} \\] \u4e5f\u5373 \\[ \\mathbf{A}_{2n \\times 6} \\times \\mathbf{t}_{6 \\times 1} = \\mathbf{b}_{2n \\times 1} \\] Note \u867d\u7136\u9009\u62e9 3 \u7ec4\u70b9\u662f\u80fd\u591f\u89e3\u51fa \\(a, b, c, d, e, f\\) \u7684\u6700\u5c11\u6570\u91cf\uff0c\u4f46\u662f\u5f88\u53ef\u80fd\u5b58\u5728\u90e8\u5206\u884c\u662f\u7ebf\u6027\u76f8\u5173\u4ece\u800c\u5bfc\u81f4\u5947\u5f02\u77e9\u9635\uff0c\u9000\u5316\u4e3a\u6ca1\u6709\u552f\u4e00\u89e3. \u6240\u4ee5\u4e3a\u4e86\u80fd\u591f\u5f97\u5230 \\(a, b, c, d, e, f\\) \u7684\u503c\uff0c\u91c7\u7528\u7684\u65b9\u5f0f\u662f\u53d6\u8f83\u591a\u7684\u70b9\u7136\u540e\u901a\u8fc7\u6700\u5c0f\u4e8c\u4e58\u6cd5\u8ba1\u7b97\uff0c\u4e5f\u5373\u8ba9 \\(\\mathbf{t}\\) \u6700\u5c0f\u5316 \\(||\\mathbf{A} \\mathbf{t} - \\mathbf{b}||\\) . Method of Least Square \\[ \\begin{align} \\mathbf{A^{T}At} &= \\mathbf{A^{T}b} \\\\ t &= \\mathbf{A^{T}A^{-1}A^{T}b} \\end{align} \\] Projective Transformations \u00b6 \u7c7b\u4f3c\u5730\uff0c\u5bf9\u4e8e\u6bcf\u4e2a\u5339\u914d\uff0c \\[ \\begin{align} x_i'(h_{20}x_i + h_{21} y_i + h_{22}) = h_{00}x_i + h_{01}y_i + h_{02} \\\\ y_i'(h_{20}x_i + h_{21} y_i + h_{22}) = h_{10}x_i + h_{11}y_i + h_{12} \\\\ \\end{align} \\] \u77e9\u9635\u5f62\u5f0f\u4e3a \\[ \\begin{bmatrix} x_i & y_i & 1 & 0 & 0 & 0 & -x_i'x_i & -x_i'y_i & -x_i'\\\\ 0 & 0 & 0 & x_i & y_i & 1 & -y_i'x_i & -y_i'y_i & -y_i' \\end{bmatrix} \\begin{bmatrix} h_{00} \\\\ h_{01} \\\\ h_{02} \\\\ h_{10} \\\\ h_{11} \\\\ h_{12} \\\\ h_{20} \\\\ h_{21} \\\\ h_{22} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\] \u5bf9\u4e8e \\(n\\) \u4e2a\u5339\u914d\uff0c\u5f97\u5230\u4ee5\u4e0b\u65b9\u7a0b\u7ec4 \\[ \\begin{bmatrix} x_1 & y_1 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_1 & y_1 & 1 \\\\ x_2 & y_2 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_2 & y_2 & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_n & y_n & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_n & y_n & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix} = \\begin{bmatrix} x_1' \\\\ y_1' \\\\ x_2' \\\\ y_2' \\\\ \\vdots \\\\ x_n' \\\\ y_n' \\end{bmatrix} \\] \u4e5f\u5373 \\[ \\mathbf{A}_{2n \\times 9} \\times \\mathbf{t}_{9 \\times 1} = \\mathbf{b}_{2n \\times 1} \\] \u540c\u6837\u5730\uff0c\u6211\u4eec\u8981\u6700\u5c0f\u5316 \\(||\\mathbf{Ah - 0}|| = \\mathbf{||Ah||}\\) . Contraint: \\(||\\mathbf{h}|| = 1\\) \u5982\u679c\u4f7f\u7528\u7684\u662f\u5411\u91cf\u8303\u6570\u4e3a \\(1\\) \u7684\u9650\u5236\uff0c\u90a3\u4e48\u6211\u4eec\u53ea\u9700\u8981\u786e\u5b9a \\(\\mathbf{h}\\) \u7684\u65b9\u5411 \\(\\mathbf{\\hat h}\\) . \u6570\u5b66\u4e0a\u53ef\u4ee5\u8bc1\u660e\u89e3 \\(\\mathbf{\\hat h}\\) \u662f \\(\\mathbf{A^{T}A}\\) \u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf. Robustness \u00b6 Outliers \u56fe\u50cf\u5339\u914d\u4e5f\u4e0d\u662f\u5b8c\u7f8e\u7684\uff0c\u53ef\u80fd\u6709\u90e8\u5206\u9519\u8bef\u7684\u5339\u914d\u70b9\u6210\u4e3a\u5b64\u7acb\u70b9 (outliner). Solution: RANSAC \u6bcf\u6b21\u968f\u673a\u53d6 \\(s\\) \u7ec4\u5339\u914d\u70b9. \u901a\u8fc7\u9009\u53d6\u7684\u5339\u914d\u70b9\u8ba1\u7b97\u51fa\u53d8\u6362\u77e9\u9635 \\(T\\) . \u5728\u6240\u6709\u7684\u5339\u914d\u70b9\u4e2d\uff0c\u7edf\u8ba1\u5728\u4e00\u5b9a\u8bef\u5dee\u8303\u56f4\u5185\u7b26\u5408\u53d8\u6362 \\(T\\) \u7684\u70b9 (inlier) \u7684\u6570\u91cf\u4f5c\u4e3a\u5f97\u5206. \u91cd\u590d \\(N\\) \u6b21\uff0c\u53d6\u5f97\u5206\u6700\u9ad8\u7684\u53d8\u6362 \\(T_0\\) . (Better Step) \u5bf9\u6240\u6709\u5927\u81f4\u7b26\u5408\u53d8\u6362 \\(T_0\\) \u7684\u5339\u914d\u70b9\u518d\u6b21\u8ba1\u7b97\u4e00\u4e2a\u5e73\u5747\u7684\u53d8\u6362\u77e9\u9635 \\(T\\) . Summary \u00b6 How to make image stitching? Input Images. Feature Matching. Compute transformation matrices \\(T\\) with RANSAC. Warp image 2 to image 1. Extension: Panaroma and Cylindrical Projection Problem: Drift Solution Small vertical errors accumulate over time. Apply correction s.t. the sum of drift is 0. \u4e5f\u5c31\u662f\u62cd\u6444\u5168\u666f\u56fe\u65f6\u5c3d\u53ef\u80fd\u51cf\u5c11\u4e0a\u4e0b\u6296\u52a8.","title":"Lecture 6"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#lecture-6-image-stitching","text":"","title":"Lecture 6 Image Stitching"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#image-warping","text":"Info Image filtering changes intensity of image. Image warping (\u7fd8\u66f2) changes shape of image.","title":"Image Warping"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#2d-transformations","text":"","title":"2D Transformations"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#affine-transformations","text":"\\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\] \\[ \\begin{align} x' &= ax + by + c \\\\ y' &= dx + ey + f \\end{align} \\] \u81ea\u7531\u5ea6 (degree of freedom) \u4e3a 6 \uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u81f3\u5c11 3 \u7ec4\u70b9\uff0c\u4e5f\u5c31\u662f 6 \u4e2a\u65b9\u7a0b\uff0c\u4ee5\u80fd\u591f\u6c42\u89e3\u51fa \\(a, b, c, d, e, f\\) .","title":"Affine Transformations \u4eff\u5c04\u53d8\u6362"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#projective-transformation-homography","text":"\\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} \\cong \\begin{bmatrix} h_{00} & h_{01} & h_{02} \\\\ h_{10} & h_{11} & h_{12} \\\\ h_{20} & h_{21} & h_{22} \\\\ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\] \\[ \\begin{align} x' = \\frac{h_{00}x + h_{01}y + h_{02}}{h_{20}x + h_{11}y + h_{12}} y' = \\frac{h_{10}x + h_{11}y + h_{12}}{h_{20}x + h_{11}y + h_{12}} \\end{align} \\] Constraint: \u4ee4 \\(||(h_{00}, h_{01}, \\dots, h_{22})||\\) (\u5411\u91cf\u8303\u6570) \u89c4\u5b9a\u4e3a \\(1\\) \uff0c\u6216\u8005\u4ee4 \\(h_{22}\\) \u4e3a \\(1\\) . \u8003\u8651\u5230\u4ee5\u4e0a\u7684\u9650\u5236\u6761\u4ef6 (constraint)\uff0c\u81ea\u7531\u5ea6 (degree of freedom) \u4e3a 8 \uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u81f3\u5c11 4 \u7ec4\u70b9\uff0c\u4e5f\u5c31\u662f 8 \u4e2a\u65b9\u7a0b\uff0c\u4ee5\u80fd\u591f\u6c42\u89e3\u51fa \\(h_{ij}\\) .","title":"Projective Transformation (Homography) \u5355\u5e94\u53d8\u6362"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#summary","text":"","title":"Summary"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#implementation","text":"\u51fa\u4e8e\u65b9\u4fbf\u548c\u53ef\u64cd\u4f5c\u6027\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u8868\u5f81\u6574\u4f53\u7fd8\u66f2\u7684\u51fd\u6570 \\(T\\) \u800c\u4e0d\u662f\u5bf9\u6bcf\u4e2a\u50cf\u7d20\u70b9\u91c7\u53d6\u4e0d\u540c\u7684\u7fd8\u66f2\u7b56\u7565.","title":"Implementation"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#forward-warping-not-so-good","text":"\u5047\u8bbe\u56fe\u50cf\u51fd\u6570 \\(f:(x, y) \\rightarrow (r, g, b)\\) \u7ecf\u8fc7\u4e86\u53d8\u6362 \\(T: (x, y) \\rightarrow (x', y')\\) \u5f97\u5230\u65b0\u7684\u56fe\u50cf\u51fd\u6570 \\(g:(x', y') \\rightarrow (r, g, b)\\) . If the transformed pixed lands inside the pixels - Quite complicated to solve it.","title":"Forward Warping (Not so good)"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#inverse-warping","text":"Why not consider inversely? If the transformed pixed lands inside the pixels Solution 2D Interpolation!","title":"Inverse Warping"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#image-stitching","text":"\\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} \\cong T \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\]","title":"Image Stitching"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#image-matching","text":"\u901a\u8fc7\u56fe\u50cf\u5339\u914d\u5f97\u5230\u5339\u914d\u70b9\uff0c\u6bcf\u4e2a\u5339\u914d\u70b9\u53ef\u4ee5\u63d0\u4f9b\u4e00\u4e2a\u4ee5\u4e0a\u5f62\u5f0f\u7684\u77e9\u9635\u4e58\u6cd5\u7b49\u5f0f.","title":"Image Matching"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#find-t-for-image-wraping","text":"","title":"Find \\(T\\) for Image Wraping"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#affine-transformations_1","text":"\u5bf9\u4e8e\u6bcf\u4e2a\u5339\u914d\uff0c \\[ \\begin{align} x' = ax + by + c \\\\ y' = dx + ey + f \\end{align} \\] \u77e9\u9635\u5f62\u5f0f\u4e3a \\[ \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} x & y & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x & y & 1 \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix} \\] \u5bf9\u4e8e \\(n\\) \u4e2a\u5339\u914d\uff0c\u5f97\u5230\u4ee5\u4e0b\u65b9\u7a0b\u7ec4 \\[ \\begin{bmatrix} x_1 & y_1 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_1 & y_1 & 1 \\\\ x_2 & y_2 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_2 & y_2 & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_n & y_n & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_n & y_n & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix} = \\begin{bmatrix} x_1' \\\\ y_1' \\\\ x_2' \\\\ y_2' \\\\ \\vdots \\\\ x_n' \\\\ y_n' \\end{bmatrix} \\] \u4e5f\u5373 \\[ \\mathbf{A}_{2n \\times 6} \\times \\mathbf{t}_{6 \\times 1} = \\mathbf{b}_{2n \\times 1} \\] Note \u867d\u7136\u9009\u62e9 3 \u7ec4\u70b9\u662f\u80fd\u591f\u89e3\u51fa \\(a, b, c, d, e, f\\) \u7684\u6700\u5c11\u6570\u91cf\uff0c\u4f46\u662f\u5f88\u53ef\u80fd\u5b58\u5728\u90e8\u5206\u884c\u662f\u7ebf\u6027\u76f8\u5173\u4ece\u800c\u5bfc\u81f4\u5947\u5f02\u77e9\u9635\uff0c\u9000\u5316\u4e3a\u6ca1\u6709\u552f\u4e00\u89e3. \u6240\u4ee5\u4e3a\u4e86\u80fd\u591f\u5f97\u5230 \\(a, b, c, d, e, f\\) \u7684\u503c\uff0c\u91c7\u7528\u7684\u65b9\u5f0f\u662f\u53d6\u8f83\u591a\u7684\u70b9\u7136\u540e\u901a\u8fc7\u6700\u5c0f\u4e8c\u4e58\u6cd5\u8ba1\u7b97\uff0c\u4e5f\u5373\u8ba9 \\(\\mathbf{t}\\) \u6700\u5c0f\u5316 \\(||\\mathbf{A} \\mathbf{t} - \\mathbf{b}||\\) . Method of Least Square \\[ \\begin{align} \\mathbf{A^{T}At} &= \\mathbf{A^{T}b} \\\\ t &= \\mathbf{A^{T}A^{-1}A^{T}b} \\end{align} \\]","title":"Affine Transformations"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#projective-transformations","text":"\u7c7b\u4f3c\u5730\uff0c\u5bf9\u4e8e\u6bcf\u4e2a\u5339\u914d\uff0c \\[ \\begin{align} x_i'(h_{20}x_i + h_{21} y_i + h_{22}) = h_{00}x_i + h_{01}y_i + h_{02} \\\\ y_i'(h_{20}x_i + h_{21} y_i + h_{22}) = h_{10}x_i + h_{11}y_i + h_{12} \\\\ \\end{align} \\] \u77e9\u9635\u5f62\u5f0f\u4e3a \\[ \\begin{bmatrix} x_i & y_i & 1 & 0 & 0 & 0 & -x_i'x_i & -x_i'y_i & -x_i'\\\\ 0 & 0 & 0 & x_i & y_i & 1 & -y_i'x_i & -y_i'y_i & -y_i' \\end{bmatrix} \\begin{bmatrix} h_{00} \\\\ h_{01} \\\\ h_{02} \\\\ h_{10} \\\\ h_{11} \\\\ h_{12} \\\\ h_{20} \\\\ h_{21} \\\\ h_{22} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\] \u5bf9\u4e8e \\(n\\) \u4e2a\u5339\u914d\uff0c\u5f97\u5230\u4ee5\u4e0b\u65b9\u7a0b\u7ec4 \\[ \\begin{bmatrix} x_1 & y_1 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_1 & y_1 & 1 \\\\ x_2 & y_2 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_2 & y_2 & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_n & y_n & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_n & y_n & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix} = \\begin{bmatrix} x_1' \\\\ y_1' \\\\ x_2' \\\\ y_2' \\\\ \\vdots \\\\ x_n' \\\\ y_n' \\end{bmatrix} \\] \u4e5f\u5373 \\[ \\mathbf{A}_{2n \\times 9} \\times \\mathbf{t}_{9 \\times 1} = \\mathbf{b}_{2n \\times 1} \\] \u540c\u6837\u5730\uff0c\u6211\u4eec\u8981\u6700\u5c0f\u5316 \\(||\\mathbf{Ah - 0}|| = \\mathbf{||Ah||}\\) . Contraint: \\(||\\mathbf{h}|| = 1\\) \u5982\u679c\u4f7f\u7528\u7684\u662f\u5411\u91cf\u8303\u6570\u4e3a \\(1\\) \u7684\u9650\u5236\uff0c\u90a3\u4e48\u6211\u4eec\u53ea\u9700\u8981\u786e\u5b9a \\(\\mathbf{h}\\) \u7684\u65b9\u5411 \\(\\mathbf{\\hat h}\\) . \u6570\u5b66\u4e0a\u53ef\u4ee5\u8bc1\u660e\u89e3 \\(\\mathbf{\\hat h}\\) \u662f \\(\\mathbf{A^{T}A}\\) \u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf.","title":"Projective Transformations"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#robustness","text":"Outliers \u56fe\u50cf\u5339\u914d\u4e5f\u4e0d\u662f\u5b8c\u7f8e\u7684\uff0c\u53ef\u80fd\u6709\u90e8\u5206\u9519\u8bef\u7684\u5339\u914d\u70b9\u6210\u4e3a\u5b64\u7acb\u70b9 (outliner). Solution: RANSAC \u6bcf\u6b21\u968f\u673a\u53d6 \\(s\\) \u7ec4\u5339\u914d\u70b9. \u901a\u8fc7\u9009\u53d6\u7684\u5339\u914d\u70b9\u8ba1\u7b97\u51fa\u53d8\u6362\u77e9\u9635 \\(T\\) . \u5728\u6240\u6709\u7684\u5339\u914d\u70b9\u4e2d\uff0c\u7edf\u8ba1\u5728\u4e00\u5b9a\u8bef\u5dee\u8303\u56f4\u5185\u7b26\u5408\u53d8\u6362 \\(T\\) \u7684\u70b9 (inlier) \u7684\u6570\u91cf\u4f5c\u4e3a\u5f97\u5206. \u91cd\u590d \\(N\\) \u6b21\uff0c\u53d6\u5f97\u5206\u6700\u9ad8\u7684\u53d8\u6362 \\(T_0\\) . (Better Step) \u5bf9\u6240\u6709\u5927\u81f4\u7b26\u5408\u53d8\u6362 \\(T_0\\) \u7684\u5339\u914d\u70b9\u518d\u6b21\u8ba1\u7b97\u4e00\u4e2a\u5e73\u5747\u7684\u53d8\u6362\u77e9\u9635 \\(T\\) .","title":"Robustness"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#summary_1","text":"How to make image stitching? Input Images. Feature Matching. Compute transformation matrices \\(T\\) with RANSAC. Warp image 2 to image 1. Extension: Panaroma and Cylindrical Projection Problem: Drift Solution Small vertical errors accumulate over time. Apply correction s.t. the sum of drift is 0. \u4e5f\u5c31\u662f\u62cd\u6444\u5168\u666f\u56fe\u65f6\u5c3d\u53ef\u80fd\u51cf\u5c11\u4e0a\u4e0b\u6296\u52a8.","title":"Summary"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/","text":"Lecture 7 Structure from Motion (SfM) \u00b6 Camera Calibration \u76f8\u673a\u6807\u5b9a \u00b6 \u76f8\u673a\u6807\u5b9a\u7684\u8fc7\u7a0b\uff0c\u5c31\u662f\u4ece\u4e16\u754c\u5750\u6807\u7cfb\u5411\u50cf\u7d20\u5750\u6807\u7cfb\u8f6c\u6362\u7684\u8fc7\u7a0b. \u9700\u8981\u6ce8\u610f\u7684\u56db\u4e2a\u5750\u6807\u7cfb\uff1a \u4e16\u754c\u5750\u6807\u7cfb (World Coordinates) \u76f8\u673a\u5750\u6807\u7cfb (Camera Coordinates) \u56fe\u50cf\u5750\u6807\u7cfb (Image Plane) \u50cf\u7d20\u5750\u6807\u7cfb (Image Sensor) \u5176\u4e2d\uff0c\u524d\u4e24\u4e2a\u5750\u6807\u7cfb\u7684\u8f6c\u6362\u77e9\u9635\u53c2\u6570\u88ab\u79f0\u4e3a \u76f8\u673a\u5916\u53c2 \uff1b\u540e\u4e09\u4e2a\u5750\u6807\u7cfb\u7684\u8f6c\u6362\u77e9\u9635\u53c2\u6570\u88ab\u79f0\u4e3a \u76f8\u673a\u5185\u53c2 . Principle Analysis \u539f\u7406\u5206\u6790 \u00b6 Coordinate Transformation \u00b6 \u4ece\u4e16\u754c\u5750\u6807\u7cfb\u5230\u76f8\u673a\u5750\u6807\u7cfb\u7684\u53d8\u6362. \u7279\u70b9 \u00b6 \u521a\u4f53\u53d8\u6362\uff0c\u53ea\u6709\u65cb\u8f6c\u548c\u5e73\u79fb\uff0c\u5bf9\u5e94\u65cb\u8f6c\u77e9\u9635 \\(R\\) \uff08 \\(R\\) \u662f \u5355\u4f4d\u6b63\u4ea4\u77e9\u9635 (Orthonormal) ) \u548c\u5e73\u79fb\u5411\u91cf \\(c_w\\) . \\[ \\mathbf{x}_c = R(\\mathbf{x}_w - \\mathbf{c}_w) = R \\mathbf{x}_w - R \\mathbf{c}_w \\overset{\\Delta}{=\\!=} R \\mathbf{x}_w + \\mathbf{t}, \\text{ where } \\mathbf{t } = -R \\mathbf{c}_w. \\] \u6545\u8fd9\u4e2a\u53d8\u6362\u4e5f\u53ef\u4ee5\u7b49\u4ef7\u5730\u7528 \\(R\\) \u548c \\(\\mathbf{t}\\) \u6765\u8868\u5f81. \\[ \\mathbf{x}_c = \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\\\ \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} \\] \u540c\u6837\u8003\u8651\u9f50\u6b21\u5750\u6807\u7cfb \\[ \\mathbf{\\tilde x}_c = \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1\\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix} \\] Extrinsic Matrix (\u5916\u53c2\u77e9\u9635) \\[ M_{ext} = \\begin{bmatrix} R_{3\\times 3} & \\mathbf{t} \\\\ \\mathbf{0}_{1\\times 3} & 1 \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\] \u4e5f\u53ef\u4ee5\u5199\u4f5c \\[ M_{ext} = \\begin{bmatrix} R_{3\\times 3} & \\mathbf{t} \\\\ \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ \\end{bmatrix} \\] Perspective Projection \u00b6 \u4ece\u76f8\u673a\u5750\u6807\u7cfb\u5230\u56fe\u50cf\u5750\u6807\u7cfb\u7684\u53d8\u6362. \u524d\u9762\u51e0\u4e2a Lecture \u4e2d\u5df2\u7ecf\u8ba8\u8bba\u8fc7\u8fd9\u4e00\u90e8\u5206 \u5185\u5bb9 . \u6b64\u5904\u4e0d\u518d\u8d58\u8ff0. \\[ \\begin{bmatrix} x_i \\\\ y_i \\\\ 1 \\end{bmatrix} \\cong \\begin{bmatrix} f & 0 & 0 & 0 \\\\ 0 & f & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1 \\end{bmatrix} \\] Image Plane to Image Sensor Mapping \u00b6 \u4ece\u56fe\u50cf\u5750\u6807\u7cfb\u5230\u50cf\u7d20\u5750\u6807\u7cfb\u7684\u53d8\u6362. \u50cf\u7d20\u70b9 pixel \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u7531\u4e8e\u76f8\u673a\u81ea\u8eab\u7684\u5236\u4f5c\u5de5\u827a\u548c\u8981\u6c42\uff0c\u50cf\u7d20\u70b9\u5e76\u4e0d\u4e00\u5b9a\u662f\u6b63\u65b9\u5f62\uff0c\u800c\u662f\u77e9\u5f62\uff0c\u4e0e\u539f\u56fe\u50cf\u76f8\u6bd4\u5177\u6709\u4e00\u5b9a\u7684 \u7f29\u653e . \u8fd9\u4e00\u70b9\u5728\u89c6\u9891\u7684\u5236\u4f5c\u4e2d\u4e5f\u6709\u6240\u4f53\u73b0. \u6b64\u5916\uff0c\u7531\u4e8e\u56fe\u50cf\u5750\u6807\u7cfb\u662f\u4ece\u4e2d\u5fc3\u4e3a\u539f\u70b9\uff0c\u800c\u50cf\u7d20\u5750\u6807\u7cfb\u662f\u4ee5\u5de6\u4e0a\u89d2\u4e3a\u539f\u70b9\u7684\uff0c\u5750\u6807\u53d8\u6362\u8fd8\u9700\u8981\u4e00\u5b9a\u7684* \u5e73\u79fb . \\[ \\begin{align} u &= m_x f \\frac{x_c}{z_c} + c_x \\\\ v &= m_y f \\frac{y_c}{z_c} + c_y \\end{align} \\] \u7ed3\u5408\u4ee5\u4e0a\u4e24\u4e2a\u53d8\u6362\uff0c\u8bb0 \\(f_x = m_x f\\) , \\(f_y = m_y f\\) . \u5219\u4ece\u76f8\u673a\u5750\u6807\u7cfb\u5230\u50cf\u7d20\u5750\u6807\u7cfb\u7684\u53d8\u6362\u4e3a \\[ \\mathbf{\\tilde{u}} = \\begin{bmatrix} u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1 \\end{bmatrix} \\] Intrinsic Matrix (\u5185\u53c2\u77e9\u9635) \\[ M_{int} = \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\] \u4e5f\u53ef\u4ee5\u5199\u4f5c \\[ K = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\] Projection Matrix \\(P\\) World to Camera \\[ \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1\\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix} \\] \\[ \\mathbf{\\tilde u} = M_{int}\\mathbf{\\tilde x}_c \\] Camera to Pixel \\[ \\begin{bmatrix} u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1 \\end{bmatrix} \\] \\[ \\mathbf{\\tilde x}_c = M_{ext}\\mathbf{\\tilde x}_w \\] \u6545 \\[ \\mathbf{\\tilde u} = M_{int}M_{ext} \\mathbf{\\tilde x}_w = P \\mathbf{\\tilde x} \\] \u5176\u4e2d\uff0c \\(P\\) \u88ab\u79f0\u4e3a\u6295\u5f71\u77e9\u9635 (Projection Matrix). \\[ \\begin{bmatrix} u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\\\ p_{21} & p_{22} & p_{23} & p_{24} \\\\ p_{31} & p_{32} & p_{33} & p_{34} \\\\ p_{41} & p_{42} & p_{43} & p_{44} \\\\ \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix} \\] Implemetation \u00b6 Step 1. Capture an image of an object with known geometry. \u00b6 Step 2. Identify correspondences (Image Matching \u56fe\u50cf\u5339\u914d) between 3D scene points and image points. \u00b6 Step 3. For each corresponding point \\(i\\) , we get \u00b6 \\[ \\begin{bmatrix} u^{(i)} \\\\ v^{(i)} \\\\ w^{(i)} \\end{bmatrix} \\cong \\begin{bmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\\\ p_{21} & p_{22} & p_{23} & p_{24} \\\\ p_{31} & p_{32} & p_{33} & p_{34} \\\\ p_{41} & p_{42} & p_{43} & p_{44} \\\\ \\end{bmatrix} \\begin{bmatrix} x_w^{(i)} \\\\ y_w^{(i)} \\\\ z_w^{(i)} \\\\ 1 \\end{bmatrix} \\] \\[ \\begin{align} u^{(i)} &= \\frac{p_{11}x_w^{(i)} + p_{12}y_w^{(i)} + p_{13}z_w^{(i)} + p_{14}}{p_{31}x_w^{(i)} + p_{32}y_w^{(i)} + p_{33}z_w^{(i)} + p_{34}} \\\\ v^{(i)} &= \\frac{p_{21}x_w^{(i)} + p_{22}y_w^{(i)} + p_{23}z_w^{(i)} + p_{24}}{p_{31}x_w^{(i)} + p_{32}y_w^{(i)} + p_{33}z_w^{(i)} + p_{34}} \\\\ \\end{align} \\] Step 4. Rearrange. \u00b6 \\[ \\scriptsize \\begin{bmatrix} x_w^{(1)} & y_w^{(1)} & z_w^{(1)} & 1 & 0 & 0 & 0 & 0 & -u_1x_w^{(1)} & -u_1y_w^{(1)} & -u_1z_w^{(1)} & -u_1 \\\\ 0 & 0 & 0 & 0 & x_w^{(1)} & y_w^{(1)} & z_w^{(1)} & 1 & -v_1x_w^{(1)} & -v_1y_w^{(1)} & -v_1z_w^{(1)} & -v_1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_w^{(i)} & y_w^{(i)} & z_w^{(i)} & 1 & 0 & 0 & 0 & 0 & -u_ix_w^{(i)} & -u_iy_w^{(i)} & -u_iz_w^{(i)} & -u_i \\\\ 0 & 0 & 0 & 0 & x_w^{(i)} & y_w^{(i)} & z_w^{(i)} & 1 & -v_ix_w^{(i)} & -v_iy_w^{(i)} & -v_iz_w^{(i)} & -v_i \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_w^{(n)} & y_w^{(n)} & z_w^{(n)} & 1 & 0 & 0 & 0 & 0 & -u_nx_w^{(n)} & -u_ny_w^{(n)} & -u_nz_w^{(n)} & -u_n \\\\ 0 & 0 & 0 & 0 & x_w^{(n)} & y_w^{(n)} & z_w^{(n)} & 1 & -v_nx_w^{(n)} & -v_ny_w^{(n)} & -v_nz_w^{(n)} & -v_n \\\\ \\end{bmatrix} \\begin{bmatrix} p_{11} \\\\ p_{12} \\\\ p_{13} \\\\ p_{14} \\\\ p_{21} \\\\ p_{22} \\\\ p_{23} \\\\ p_{24} \\\\ p_{31} \\\\ p_{32} \\\\ p_{33} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\] \\[ i.e.\\ A \\mathbf{p} = 0 \\] Step 5 Solve for \\(P\\) . \u00b6 \u7c7b\u4f3c\u5730\uff0c\u6211\u4eec\u53ef\u4ee5\u7ed9\u51fa\u4e00\u5b9a\u7684\u9650\u5236 constraint. Option 1. \u4ee4 \\(p_{34} = 1\\) . Option 2. \u4ee4 \\(||p|| = 1\\) . \u5f97\u5230\u81ea\u7531\u5ea6\u4e3a \\(11\\) \uff0c\u81f3\u5c11\u9700\u8981 6 \u7ec4\u70b9. \u5e76\u540c\u6837\u8981\u6700\u5c0f\u5316 \\(||\\mathbf{Ap - 0}|| = \\mathbf{||Ap||}\\) . \u540c\u6837\u5982\u679c\u4f7f\u7528\u7684\u662f\u5411\u91cf\u8303\u6570\u4e3a \\(1\\) \u7684\u9650\u5236\uff0c\u90a3\u4e48\u6211\u4eec\u53ea\u9700\u8981\u786e\u5b9a \\(\\mathbf{p}\\) \u7684\u65b9\u5411 \\(\\mathbf{\\hat p}\\) . \u4e14 \\(\\mathbf{\\hat p}\\) \u662f \\(\\mathbf{A^{T}A}\\) \u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf. Step 6 From \\(P\\) to Answers \u00b6 \u7531 \\[ \\begin{bmatrix} p_{11} \\\\ p_{12} \\\\ p_{13} \\\\ p_{14} \\\\ p_{21} \\\\ p_{22} \\\\ p_{23} \\\\ p_{24} \\\\ p_{31} \\\\ p_{32} \\\\ p_{33} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\] \u6ce8\u610f\u5230 \\[ \\begin{bmatrix} p_{11} \\\\ p_{12} \\\\ p_{13} \\\\ p_{21} \\\\ p_{22} \\\\ p_{23} \\\\ p_{31} \\\\ p_{32} \\\\ p_{33} \\end{bmatrix} = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\\\ \\end{bmatrix} = KR \\] \u5176\u4e2d \\(K\\) \u662f\u4e0a\u4e09\u89d2\u77e9\u9635 (Upper Right Triangular Matrix)\uff0c \\(R\\) \u662f\u5355\u4f4d\u6b63\u4ea4\u77e9\u9635 (Orthonormal Matrix). \u7531 QR \u5206\u89e3 (QR Factorization) \u53ef\u77e5\uff0c \\(K\\) \u4e0e \\(R\\) \u7684\u89e3\u662f\u552f\u4e00\u7684. \u518d\u7531 \\[ \\begin{bmatrix} p_{14} \\\\ p_{24} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} = K \\mathbf{t} \\] \u6545\u53ef\u89e3\u5f97 \\[ \\mathbf{t} = K^{-1} \\begin{bmatrix} p_{14} \\\\ p_{24} \\\\ p_{34} \\end{bmatrix} \\] Problem: Distortion \u5b9e\u9645\u4e0a\u76f8\u673a\u81ea\u8eab\u4f1a\u5177\u6709\u4e00\u5b9a\u7684\u626d\u66f2\u7cfb\u6570\uff0c\u6211\u4eec\u5728\u6b64 \u5ffd\u7565 \u5b83\u7684\u5f71\u54cd. Perspective-n-Point Problem (PnP \u95ee\u9898) \u5f31\u5316\u4e00\u4e0b\u6761\u4ef6\uff0c\u5047\u8bbe\u5df2\u77e5\u76f8\u673a\u7684\u5185\u53c2\uff08\u5373\u77e9\u9635 \\(K\\) \uff09\uff0c\u6c42\u76f8\u673a\u7684\u5916\u53c2\uff08\u5373\u77e9\u9635 \\(R\\) \u548c \\(\\mathbf{t}\\) \uff09\uff0c\u4e5f\u5373\u76f8\u673a\u59ff\u6001 (camera pose). Direct Linear Transform (DLT) \u00b6 \u5373\u4ee5\u4e0a\u5728\u76f8\u673a\u6807\u5b9a\u4e2d\u63d0\u5230\u7684\u65b9\u6cd5\uff0c\u5148\u6c42\u89e3\u77e9\u9635 \\(P\\) \uff0c\u518d\u901a\u8fc7 \\(K\\) \u5f97\u5230 \\(R\\) \u548c \\(\\mathbf{t}\\) . P3P \u00b6 \u5373\u5df2\u77e5 \\(a, b, c, A, B, C\\) \u70b9\u5750\u6807\uff0c\u6c42 \\(O\\) \u5750\u6807. \u81f3\u5c11\u9700\u8981 \\(3\\) \u7ec4\u70b9 Intermediates. \u7531\u4f59\u5f26\u5b9a\u7406 \\[ \\begin{align} OA^2 + OB^2 - 2 OA \\cdot OB \\cos \\angle AOB &= AB^2 \\\\ OA^2 + OC^2 - 2 OB \\cdot OC \\cos \\angle BOC &= BC^2 \\\\ OA^2 + OC^2 - 2 OA \\cdot OC \\cos \\angle AOC &= AC^2 \\end{align} \\] \u4ee4 \\(x = \\frac{OA}{OC}, y = \\frac{OB}{OC}, v = \\frac{AB^2}{OC^2}, u = \\frac{BC^2}{AB^2}, w = \\frac{AC^2}{AB^2}\\) \uff0c\u5316\u7b80\u5f97 \\[ \\begin{align} (1 - u)y^2 - ux^2 - cos \\angle BOC y + 2uxy \\cos \\angle AOB + 1 &= 0 \\\\ (1 - w)x^2 - wy^2 - cos \\angle AOC y + 2wxy \\cos \\angle AOB + 1 &= 0 \\end{align} \\] \u4ee5\u4e0a\u65b9\u7a0b\u7ec4\u662f\u4e8c\u5143\u4e8c\u6b21\u65b9\u7a0b\u7ec4 (Binary Quadratic Equation)\uff0c\u6709\u56db\u7ec4\u89e3. \u5176\u4e2d\u6709\u4e24\u7ec4\u89e3\u7684 \\(O\\) \u5750\u6807\u5728 \u9762 \\(ABC\\) \u4e0e \u9762 \\(abc\\) \u4e4b\u95f4\uff0c\u820d\u53bb. \u8fd8\u9700\u8981\u4e00\u7ec4\u989d\u5916\u7684\u70b9\u6765\u786e\u5b9a\u552f\u4e00\u89e3\uff0c\u6545\u4e00\u5171\u9700\u8981 \\(4\\) \u7ec4\u70b9. PnP \u00b6 \u6700\u5c0f\u5316 \u91cd\u6295\u5f71\u8bef\u5dee (Reprojection Error) \\[ \\mathop{\\arg\\min\\limits_{R, \\mathbf{t}}} \\sum\\limits_i ||(p_i, K(RP_i + \\mathbf{t})||^2 \\] \u5176\u4e2d\uff0c \\(p_i\\) \u662f\u6295\u5f71\u9762\u4e0a\u7684\u4e8c\u7ef4\u5750\u6807\uff0c \\(P_i\\) \u662f\u7a7a\u95f4\u5185\u7684\u4e09\u7ef4\u5750\u6807. \u901a\u8fc7 P3P \u521d\u59cb\u5316\uff0cGauss-Newton \u6cd5\u4f18\u5316. EPnP \u00b6 one of the most popluar methods. time complexity \\(O(N)\\) . high accuracy. \u5927\u81f4\u65b9\u5f0f\u662f\u901a\u8fc7\u56db\u7ec4\u63a7\u5236\u70b9\u7684\u7ebf\u6027\u7ec4\u5408\u6c42\u89e3. Two-frame Structure from Motion \u00b6 Stereo vision \u7acb\u4f53\u89c6\u89c9 Compute 3D structure of the scene and camera poses from views (images). Two-frame SfM \u610f\u5728\u89e3\u51b3\u8fd9\u6837\u4e00\u4e2a\u95ee\u9898\uff1a\u5df2\u77e5\u4e24\u5f20\u56fe\u50cf\u548c\u76f8\u673a\u5185\u53c2\u77e9\u9635 ( \\(K_l(3 \\times 3)\\) \u4e0e \\(K_r(3 \\times 3)\\) )\uff0c\u6c42\u76f8\u673a\u7684\u4f4d\u7f6e\u4ee5\u53ca\u6240\u62cd\u6444\u4e3b\u4f53\u7684\u4e09\u7ef4\u4f4d\u7f6e\u5750\u6807. Principle Induction \u539f\u7406\u63a8\u5bfc \u00b6 Epipolar Geometry \u5bf9\u6781\u51e0\u4f55 \u5bf9\u6781\u51e0\u4f55\u63cf\u8ff0\u4e86\u4e00\u4e2a\u4e09\u7ef4\u70b9 \\(P\\) \u5728\u4e24\u4e2a\u89c6\u89d2\u7684\u4e8c\u7ef4\u6295\u5f71 \\(u_l, u_r\\) \u4e4b\u95f4\u7684\u51e0\u4f55\u5173\u7cfb. \u5e76\u901a\u8fc7\u8fd9\u4e2a\u51e0\u4f55\u5173\u7cfb\u5efa\u7acb\u4e24\u4e2a\u6444\u50cf\u673a\u4e4b\u95f4\u7684\u53d8\u6362\u5173\u7cfb\uff08\u5373\u7ed9\u51fa\u77e9\u9635 \\(R\\) \u548c \\(\\mathbf{t}\\) \uff09. Definition Epipole \u5bf9\u6781\u70b9 \uff1a\u4e00\u53f0\u6444\u50cf\u673a\u6295\u5f71\u5728\u53e6\u4e00\u53f0\u6444\u50cf\u673a\u4e2d\u7684\u70b9. \u5982\u56fe\u4e2d\u7684 \\(e_l\\) \u548c \\(e_r\\) . \u5bf9\u7ed9\u5b9a\u7684\u4e24\u53f0\u6444\u50cf\u673a\uff0c \\(e_l\\) \u548c \\(e_r\\) \u662f\u552f\u4e00\u7684. Epipolar Plane of Scene Point P \u5173\u4e8e P \u70b9\u7684\u5bf9\u6781\u9762 \uff1a\u7531 Scene Point \\(P\\) \u70b9\u548c\u4e24\u4e2a\u6444\u50cf\u673a \\(O_l\\) \u548c \\(O_r\\) \u5f62\u6210\u7684\u5e73\u9762. \u6bcf\u4e2a scene point \u6709\u552f\u4e00\u5bf9\u5e94\u7684\u5bf9\u6781\u9762. \u5bf9\u6781\u70b9\u5728\u5bf9\u6781\u9762\u4e0a. Essential Matrix \\(E\\) \u57fa\u672c\u77e9\u9635 \u00b6 \u5bf9\u4e8e\u4e00\u4e2a\u5bf9\u6781\u9762\uff0c\u4ee5\u4e0b\u7b49\u5f0f\u5173\u7cfb\u6210\u7acb\uff1a \\[ \\begin{align} \\mathbf{n} = \\mathbf{t} \\times \\mathbf{x}_l \\\\ \\\\ \\mathbf{x}_l \\cdot (\\mathbf{t} \\times \\mathbf{x}_l) = 0 \\end{align} \\] \u5176\u4e2d \\(\\mathbf{n}\\) \u88ab\u79f0\u4e3a Normal Vector. \u5047\u8bbe\u53f3\u8fb9\u7684\u6444\u50cf\u673a\u76f8\u5bf9\u5de6\u8fb9\u7684\u6444\u50cf\u673a\u8fdb\u884c\u521a\u4f53\u53d8\u6362 + \u5e73\u79fb\uff0c\u5373\uff1a \\[ \\begin{align} \\mathbf{x}_l &= R \\mathbf{x}_r + \\mathbf{t} \\\\ \\\\ \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\end{bmatrix} &= \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ y_r \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} \\end{align} \\] \u8fdb\u4e00\u6b65\u6f14\u7ece\u63a8\u7406\uff0c\u5f97\u5230\uff1a \\[ \\begin{align} \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\begin{bmatrix} t_yz_l - t_zy_l \\\\ t_zx_l - t_xz_l \\\\ t_xy_l - t_yx_l \\end{bmatrix} = 0 & \\text{, Cross-product definition} \\\\ \\\\ \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\underbrace{ \\begin{bmatrix} 0 & -t_z & t_y \\\\ t_z & 0 & -t_x \\\\ -t_y & t_x & 0 \\end{bmatrix} }_{T_\\times} \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\end{bmatrix} = 0 & \\text{, Matrix-vector form} \\end{align} \\] \u4ee3\u5165\u53d8\u6362\u77e9\u9635 \\[ \\small \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\left( \\underbrace{ \\begin{bmatrix} 0 & -t_z & t_y \\\\ t_z & 0 & -t_x \\\\ -t_y & t_x & 0 \\end{bmatrix} }_{T_\\times} \\underbrace{ \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\end{bmatrix} }_{R} \\begin{bmatrix} x_r \\\\ y_r \\\\ y_r \\end{bmatrix} + \\underbrace{ \\begin{bmatrix} 0 & -t_z & t_y \\\\ t_z & 0 & -t_x \\\\ -t_y & t_x & 0 \\end{bmatrix} \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} }_{\\mathbf{t} \\times \\mathbf{t} = \\mathbf{0}} \\right) = 0 \\] \\[ \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\underbrace{ \\begin{bmatrix} e_{11} & e_{12} & e_{13} \\\\ e_{21} & e_{22} & e_{23} \\\\ e_{31} & e_{32} & e_{33} \\end{bmatrix} }_{\\text{Essential Matrix } E} \\begin{bmatrix} x_r \\\\ y_r \\\\ y_r \\end{bmatrix} \\text{, where } E = T_\\times R \\] \u6ce8\u610f\u5230 \\(T_\\times\\) \u662f\u53cd\u5bf9\u79f0\u77e9\u9635 (skew-symmetric matrix) \u800c \\(R\\) \u662f\u5355\u4f4d\u6b63\u4ea4\u77e9\u9635 (orthonomal matrix)\uff0c\u7531 SVD \u5206\u89e3 (Singule Value Decomposition)\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece \\(E\\) \u552f\u4e00\u5206\u89e3\u5f97\u5230 \\(T_\\times\\) \u548c \\(R\\) . Fundemental Matrix \\(F\\) \u672c\u771f\u77e9\u9635 \u00b6 \u7531\u6295\u5f71\u5173\u7cfb\u4ee5\u53ca\u5185\u53c2\u77e9\u9635 \\(K\\) \uff0c\u5f97\u5230\uff1a \\[ \\small \\begin{align} \\text{Left Camera} && \\text{Right Camera} \\\\ z_l \\begin{bmatrix} u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &= \\underbrace{ \\begin{bmatrix} f_x^{(l)} & 0 & o_x^{(l)} \\\\ 0 & f_y^{(l)} & o_y^{(l)} \\\\ 0 & 0 & 1 \\end{bmatrix} }_{K_l} \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\end{bmatrix} & z_r \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} &= \\underbrace{ \\begin{bmatrix} f_x^{(r)} & 0 & o_x^{(r)} \\\\ 0 & f_y^{(r)} & o_y^{(r)} \\\\ 0 & 0 & 1 \\end{bmatrix} }_{K_r} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\end{bmatrix} \\\\ \\mathbf{x}_l^T &= \\begin{bmatrix} u_l & v_l & 1 \\end{bmatrix} z_l (K_l^{-1})^T & \\mathbf{x}_r &= K_r^{-1} z_r \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} \\end{align} \\] \u4ee3\u5165\u4ee5\u4e0a\u5f97\u5230\u7684\uff1a \\[ \\mathbf{x}^T_l E \\mathbf{x}_r = 0 \\] \u6709 \\[ \\require{canel} \\begin{bmatrix} u_l & v_l & 1 \\end{bmatrix} \\cancel{z_l} (K_l^{-1})^T E K_r^{-1} \\cancel{z_r} \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = 0 \\] \u5373 \\[ \\begin{bmatrix} u_l & v_l & 1 \\end{bmatrix} F \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = 0 \\text{, where } E = K_l^T F K_r \\] Implementation \u5b9e\u73b0 \u00b6 Step 1. Find a set ( \\(m\\) pairs) of corresponding features. \u627e\u5230\u4e00\u7ec4\uff08 \\(m\\) \u5bf9\uff09\u5339\u914d\u70b9. \u00b6 Least Number of Points At least 8 pairs: \\((u_l^{(i)}, v_l^{(i)})\\) \u2194 \\((u_r^{(i)}, v_r^{(i)})\\) NOTE that one pair only corresponds one equation. (not the same as before) Step 2. Build linear systems and solve for \\(F\\) . \u5efa\u7acb\u65b9\u7a0b\u7ec4\u5e76\u6c42\u89e3\u77e9\u9635 \\(F\\) . \u00b6 \u5bf9\u4e8e\u6bcf\u7ec4\u5339\u914d \\(i\\) \uff0c \\[ \\begin{bmatrix} u_l^{(i)} & v_l^{(i)} & 1 \\end{bmatrix} \\begin{bmatrix} f_{11} & f_{12} & f_{13} \\\\ f_{21} & f_{22} & f_{23} \\\\ f_{31} & f_{32} & f_{33} \\end{bmatrix} \\begin{bmatrix} u_r^{(i)} \\\\ v_r^{(i)} \\\\ 1 \\end{bmatrix} = 0 \\] \u5c55\u5f00\u5f97\uff0c \\[ \\small \\left( f_{11} u_r^{(i)} + f_{12} v_r^{(i)} + f_13 \\right)u_l^{(i)} + \\left( f_{21} u_r^{(i)} + f_{22} v_r^{(i)} + f_23 \\right)v_l^{(i)} + f_{31} u_r^{(i)} + f_{32} v_r^{(i)} + f_33 = 0 \\] \u5bf9\u6240\u6709\u9009\u62e9\u7684\u70b9\uff0c\u6709 \\[ \\small \\begin{bmatrix} u_l^{(1)}u_r^{(1)} & u_l^{(1)}v_r^{(1)} & u_l^{(1)} & v_l^{(1)}u_r^{(1)} & v_l^{(1)}v_r^{(1)} & v_l^{(1)} & u_r^{(1)} & v_r^{(1)} & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ u_l^{(i)}u_r^{(i)} & u_l^{(i)}v_r^{(i)} & u_l^{(i)} & v_l^{(i)}u_r^{(i)} & v_l^{(i)}v_r^{(i)} & v_l^{(i)} & u_r^{(i)} & v_r^{(i)} & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ u_l^{(m)}u_r^{(m)} & u_l^{(m)}v_r^{(m)} & u_l^{(m)} & v_l^{(m)}u_r^{(m)} & v_l^{(m)}v_r^{(m)} & v_l^{(m)} & u_r^{(m)} & v_r^{(m)} & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} f_{11} \\\\ f_{12} \\\\ f_{13} \\\\ f_{21} \\\\ f_{22} \\\\ f_{23} \\\\ f_{31} \\\\ f_{32} \\\\ f_{33} \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\] \\[ A \\mathbf{f} = \\mathbf{0} \\] \u540c\u6837\u5730 \uff0c\u6211\u4eec\u53ef\u4ee5\u7ed9\u51fa\u4e00\u5b9a\u7684\u9650\u5236 constraint. Option 1. \u4ee4 \\(f_{33} = 1\\) . Option 2. \u4ee4 \\(||f|| = 1\\) . \u5f97\u5230\u81ea\u7531\u5ea6\u4e3a \\(8\\) \uff0c\u81f3\u5c11\u9700\u8981 8 \u7ec4\u70b9. \u5e76\u540c\u6837\u8981\u6700\u5c0f\u5316 \\(||\\mathbf{Af - 0}|| = \\mathbf{||Af||}\\) . \u540c\u6837\u5982\u679c\u4f7f\u7528\u7684\u662f\u5411\u91cf\u8303\u6570\u4e3a \\(1\\) \u7684\u9650\u5236\uff0c\u90a3\u4e48\u89e3\u5c31\u662f \\(\\mathbf{A^{T}A}\\) \u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf. Step 3. Find \\(E\\) and Extract \\(R, \\mathbf{t}\\) \u00b6 \\[ E = K_l^T F K_r \\] \u901a\u8fc7 SVD \u5206\u89e3\uff0c \\(E = T_\\times R\\) \u5f97\u5230 \\(T_\\times\\) \u548c \\(R\\) . \\(R\\) \u5373\u4e3a\u65cb\u8f6c\u77e9\u9635\uff0c \\(\\mathbf{t}\\) \u53ef\u4ee5\u901a\u8fc7 \\(T_\\times\\) \u76f4\u63a5\u5f97\u5230. Step 4. Find 3D Position of Scene Points \u00b6 \u73b0\u5728\u5df2\u7ecf\u6709\u4ee5\u4e0b\u7b49\u5f0f \\[ \\small \\begin{align} \\text{Left Camera} \\\\ \\begin{bmatrix} u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} f_x^{(l)} & 0 & o_x^{(l)} & 0 \\\\ 0 & f_y^{(l)} & o_y^{(l)} & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\\\ 1 \\end{bmatrix} ,\\ \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y \\\\ r_{31} & r_{32} & r_{33} & t_z \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\\\ \\begin{bmatrix} u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} f_x^{(l)} & 0 & o_x^{(l)} & 0 \\\\ 0 & f_y^{(l)} & o_y^{(l)} & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y \\\\ r_{31} & r_{32} & r_{33} & t_z \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\\\ \\\\ \\normalsize \\mathbf{\\tilde u}_l &= \\normalsize P_l \\mathbf{\\tilde x}_r \\\\ \\\\ \\small \\text{Right Camera} \\\\ \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} f_x^{(r)} & 0 & o_x^{(r)} & 0 \\\\ 0 & f_y^{(r)} & o_y^{(r)} & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\\\ \\\\ \\normalsize \\mathbf{\\tilde u}_l &= \\normalsize M_{int_r} \\mathbf{\\tilde x}_r \\end{align} \\] \u7531 \\[ \\small \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} m_{11} & m_{12} & m_{13} & m_{14} \\\\ m_{21} & m_{22} & m_{23} & m_{24} \\\\ m_{31} & m_{32} & m_{33} & m_{34} \\\\ \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} ,\\ \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\\\ p_{21} & p_{22} & p_{23} & p_{24} \\\\ p_{31} & p_{32} & p_{33} & p_{34} \\\\ \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\] \u6574\u7406\u5f97 \\[ \\underbrace{ \\begin{bmatrix} u_r m_{31} - m_{11} & u_r m_{32} - m_{12} & u_r m_{33} - m_{13} \\\\ v_r m_{31} - m_{21} & v_r m_{32} - m_{22} & v_r m_{33} - m_{23} \\\\ u_l p_{31} - p_{11} & u_l p_{32} - p_{12} & u_l p_{33} - p_{13} \\\\ v_l p_{31} - p_{21} & v_l p_{32} - p_{22} & v_l p_{33} - p_{23} \\end{bmatrix} }_{A_{4\\times 3}} \\underbrace{ \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\end{bmatrix} }_{\\mathbf{x}_r} = \\underbrace{ \\begin{bmatrix} m_{14} - m_{34} \\\\ m_{24} - m_{34} \\\\ p_{14} - p_{34} \\\\ p_{24} - p_{34} \\end{bmatrix} }_{\\mathbf{b}} \\] \u7531\u5206\u6790\u4e0a\u89e3\u7684\u552f\u4e00\u6027\uff0c\u7406\u8bba\u4e0a\u6709\u4e24\u884c\u662f\u7ebf\u6027\u76f8\u5173\u7684. \u4f46\u662f\u5728\u5b9e\u9645\u53d6\u70b9\u4e2d\u53ef\u80fd\u4f1a\u5b58\u5728\u4e00\u5b9a\u7684\u8bef\u5dee\uff0c\u6240\u4ee5\u4e3a\u4e86\u6700\u5927\u5316\u5229\u7528\u6570\u636e\u91cf\uff0c\u8fd8\u662f\u91c7\u7528\u6700\u5c0f\u4e8c\u4e58\u6cd5. \u7531\u6700\u5c0f\u4e8c\u4e58\u6cd5\u5f97\uff0c \\[ \\mathbf{x}_r = (A^TA)^{-1}A^T \\mathbf{b} \\] Non-linear Solution \u4e0a\u9762\u7684\u662f\u901a\u8fc7\u89e3\u7ebf\u6027\u7cfb\u7edf\u7684\u65b9\u6cd5\uff0c\u53e6\u4e00\u79cd\u60f3\u6cd5\u662f\u6700\u5c0f\u5316 \u91cd\u6620\u5c04\u8bef\u5dee (reprojection error) . \\[ \\text{cost}(P) = \\text{dist}(\\mathbf{u}_l, \\mathbf{\\hat u}_l)^2 + \\text{dist}(\\mathbf{u}_r, \\mathbf{\\hat u}_r)^2 \\] \u53e6\u5916\uff0c\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\u6211\u4eec\u4e5f\u53ef\u4ee5\u4f18\u5316 \\(R\\) \u548c \\(\\mathbf{t}\\) . Multi-frame Structure from Motion \u00b6 \u4e0e Two-frame SfM \u7c7b\u4f3c\uff0c\u5df2\u77e5 \\(m\\) \u5f20\u56fe\u50cf\uff0c \\(n\\) \u4e2a\u4e09\u7ef4\u70b9\u4ee5\u53ca\u76f8\u673a\u5185\u53c2\u77e9\u9635\uff0c\u6c42\u76f8\u673a\u7684\u4f4d\u7f6e\u4ee5\u53ca\u6240\u62cd\u6444\u4e3b\u4f53\u7684\u4e09\u7ef4\u4f4d\u7f6e\u5750\u6807. \u4e5f\u5c31\u662f\u5bf9\u4e8e\u4ee5\u4e0b\u7b49\u5f0f\uff1a \\[ \\mathbf{u}_j^{(i)} = P_{proj}^{(i)} \\mathbf{P}_j \\text{, where } i = 1, \\dots, m, j = 1, \\dots, n \\] \u7531 \\(mn\\) \u4e2a\u6295\u5f71\u7684\u4e8c\u7ef4\u70b9 \\(\\mathbf{u}_j^{(i)}\\) \u6c42\u89e3 \\(m\\) \u4e2a\u6295\u5f71\u77e9\u9635 \\(P_{proj}^{(i)}\\) \u4e0e \\(n\\) \u4e2a\u4e09\u7ef4\u70b9\u5750\u6807 \\(P_j\\) . Solution: Sequential Structrue from Motion \u00b6 Step 1. Initialize camera motion and scene structure. \u521d\u59cb\u5316 \u00b6 \u9996\u5148\u9009\u4e24\u5f20\u56fe\u505a\u4e00\u6b21 Two-frame SfM. Step 2. Deal with an addition view. \u5904\u7406\u4e0b\u4e00\u5f20\u56fe \u00b6 \u5bf9\u4e8e\u6bcf\u65b0\u589e\u7684\u4e00\u5f20\u56fe\uff0c \u5bf9\u4e8e\u5df2\u7ecf\u5efa\u7acb\u4e09\u7ef4\u5750\u6807\u7684\u70b9\uff1a PnP \u95ee\u9898 . \u5bf9\u4e8e\u5c1a\u672a\u5efa\u7acb\u4e09\u7ef4\u5750\u6807\u7684\u70b9\uff1a\u505a Two-frame SfM. Step 3. Refine structure and motion: Bundle Adjustment. \u96c6\u675f\u8c03\u6574 \u00b6 \u5904\u7406\u5b8c\u6240\u6709\u7684\u56fe\u540e\uff0c\u518d\u901a\u8fc7\u4e00\u6b21 \u96c6\u675f\u8c03\u6574 \uff08\u5177\u4f53\u5b9e\u73b0\u662f LM \u7b97\u6cd5 \uff09\u5bf9\u4e09\u7ef4\u70b9\u5750\u6807\u548c\u76f8\u673a\u53c2\u6570\u505a\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u5373\u6700\u5c0f\u5316 \u91cd\u6620\u5c04\u8bef\u5dee \uff08\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\uff09\uff1a \\[ E(P_{proj}, \\mathbf{P}) = \\sum\\limits_{i=1}^m \\sum\\limits_{j=1}^n \\text{dist} \\left( u_j^{(i)}, P_{proj}^{(i)} \\mathbf{P}_j \\right)^2 \\] COLMAP \u00b6 COLMAP is a general-purpose Structure-from-Motion (SfM) and Multi-View Stereo (MVS) pipeline with a graphical and commandline interface. It offers a wide range of features for reconstruction of ordered and unordered image collections. Pipeline","title":"Lecture 7"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#lecture-7-structure-from-motion-sfm","text":"","title":"Lecture 7 Structure from Motion (SfM)"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#camera-calibration","text":"\u76f8\u673a\u6807\u5b9a\u7684\u8fc7\u7a0b\uff0c\u5c31\u662f\u4ece\u4e16\u754c\u5750\u6807\u7cfb\u5411\u50cf\u7d20\u5750\u6807\u7cfb\u8f6c\u6362\u7684\u8fc7\u7a0b. \u9700\u8981\u6ce8\u610f\u7684\u56db\u4e2a\u5750\u6807\u7cfb\uff1a \u4e16\u754c\u5750\u6807\u7cfb (World Coordinates) \u76f8\u673a\u5750\u6807\u7cfb (Camera Coordinates) \u56fe\u50cf\u5750\u6807\u7cfb (Image Plane) \u50cf\u7d20\u5750\u6807\u7cfb (Image Sensor) \u5176\u4e2d\uff0c\u524d\u4e24\u4e2a\u5750\u6807\u7cfb\u7684\u8f6c\u6362\u77e9\u9635\u53c2\u6570\u88ab\u79f0\u4e3a \u76f8\u673a\u5916\u53c2 \uff1b\u540e\u4e09\u4e2a\u5750\u6807\u7cfb\u7684\u8f6c\u6362\u77e9\u9635\u53c2\u6570\u88ab\u79f0\u4e3a \u76f8\u673a\u5185\u53c2 .","title":"Camera Calibration \u76f8\u673a\u6807\u5b9a"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#principle-analysis","text":"","title":"Principle Analysis \u539f\u7406\u5206\u6790"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#coordinate-transformation","text":"\u4ece\u4e16\u754c\u5750\u6807\u7cfb\u5230\u76f8\u673a\u5750\u6807\u7cfb\u7684\u53d8\u6362.","title":"Coordinate Transformation"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#_1","text":"\u521a\u4f53\u53d8\u6362\uff0c\u53ea\u6709\u65cb\u8f6c\u548c\u5e73\u79fb\uff0c\u5bf9\u5e94\u65cb\u8f6c\u77e9\u9635 \\(R\\) \uff08 \\(R\\) \u662f \u5355\u4f4d\u6b63\u4ea4\u77e9\u9635 (Orthonormal) ) \u548c\u5e73\u79fb\u5411\u91cf \\(c_w\\) . \\[ \\mathbf{x}_c = R(\\mathbf{x}_w - \\mathbf{c}_w) = R \\mathbf{x}_w - R \\mathbf{c}_w \\overset{\\Delta}{=\\!=} R \\mathbf{x}_w + \\mathbf{t}, \\text{ where } \\mathbf{t } = -R \\mathbf{c}_w. \\] \u6545\u8fd9\u4e2a\u53d8\u6362\u4e5f\u53ef\u4ee5\u7b49\u4ef7\u5730\u7528 \\(R\\) \u548c \\(\\mathbf{t}\\) \u6765\u8868\u5f81. \\[ \\mathbf{x}_c = \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\\\ \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} \\] \u540c\u6837\u8003\u8651\u9f50\u6b21\u5750\u6807\u7cfb \\[ \\mathbf{\\tilde x}_c = \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1\\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix} \\] Extrinsic Matrix (\u5916\u53c2\u77e9\u9635) \\[ M_{ext} = \\begin{bmatrix} R_{3\\times 3} & \\mathbf{t} \\\\ \\mathbf{0}_{1\\times 3} & 1 \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\] \u4e5f\u53ef\u4ee5\u5199\u4f5c \\[ M_{ext} = \\begin{bmatrix} R_{3\\times 3} & \\mathbf{t} \\\\ \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ \\end{bmatrix} \\]","title":"\u7279\u70b9"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#perspective-projection","text":"\u4ece\u76f8\u673a\u5750\u6807\u7cfb\u5230\u56fe\u50cf\u5750\u6807\u7cfb\u7684\u53d8\u6362. \u524d\u9762\u51e0\u4e2a Lecture \u4e2d\u5df2\u7ecf\u8ba8\u8bba\u8fc7\u8fd9\u4e00\u90e8\u5206 \u5185\u5bb9 . \u6b64\u5904\u4e0d\u518d\u8d58\u8ff0. \\[ \\begin{bmatrix} x_i \\\\ y_i \\\\ 1 \\end{bmatrix} \\cong \\begin{bmatrix} f & 0 & 0 & 0 \\\\ 0 & f & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1 \\end{bmatrix} \\]","title":"Perspective Projection"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#image-plane-to-image-sensor-mapping","text":"\u4ece\u56fe\u50cf\u5750\u6807\u7cfb\u5230\u50cf\u7d20\u5750\u6807\u7cfb\u7684\u53d8\u6362. \u50cf\u7d20\u70b9 pixel \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u7531\u4e8e\u76f8\u673a\u81ea\u8eab\u7684\u5236\u4f5c\u5de5\u827a\u548c\u8981\u6c42\uff0c\u50cf\u7d20\u70b9\u5e76\u4e0d\u4e00\u5b9a\u662f\u6b63\u65b9\u5f62\uff0c\u800c\u662f\u77e9\u5f62\uff0c\u4e0e\u539f\u56fe\u50cf\u76f8\u6bd4\u5177\u6709\u4e00\u5b9a\u7684 \u7f29\u653e . \u8fd9\u4e00\u70b9\u5728\u89c6\u9891\u7684\u5236\u4f5c\u4e2d\u4e5f\u6709\u6240\u4f53\u73b0. \u6b64\u5916\uff0c\u7531\u4e8e\u56fe\u50cf\u5750\u6807\u7cfb\u662f\u4ece\u4e2d\u5fc3\u4e3a\u539f\u70b9\uff0c\u800c\u50cf\u7d20\u5750\u6807\u7cfb\u662f\u4ee5\u5de6\u4e0a\u89d2\u4e3a\u539f\u70b9\u7684\uff0c\u5750\u6807\u53d8\u6362\u8fd8\u9700\u8981\u4e00\u5b9a\u7684* \u5e73\u79fb . \\[ \\begin{align} u &= m_x f \\frac{x_c}{z_c} + c_x \\\\ v &= m_y f \\frac{y_c}{z_c} + c_y \\end{align} \\] \u7ed3\u5408\u4ee5\u4e0a\u4e24\u4e2a\u53d8\u6362\uff0c\u8bb0 \\(f_x = m_x f\\) , \\(f_y = m_y f\\) . \u5219\u4ece\u76f8\u673a\u5750\u6807\u7cfb\u5230\u50cf\u7d20\u5750\u6807\u7cfb\u7684\u53d8\u6362\u4e3a \\[ \\mathbf{\\tilde{u}} = \\begin{bmatrix} u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1 \\end{bmatrix} \\] Intrinsic Matrix (\u5185\u53c2\u77e9\u9635) \\[ M_{int} = \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\] \u4e5f\u53ef\u4ee5\u5199\u4f5c \\[ K = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\] Projection Matrix \\(P\\) World to Camera \\[ \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1\\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix} \\] \\[ \\mathbf{\\tilde u} = M_{int}\\mathbf{\\tilde x}_c \\] Camera to Pixel \\[ \\begin{bmatrix} u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1 \\end{bmatrix} \\] \\[ \\mathbf{\\tilde x}_c = M_{ext}\\mathbf{\\tilde x}_w \\] \u6545 \\[ \\mathbf{\\tilde u} = M_{int}M_{ext} \\mathbf{\\tilde x}_w = P \\mathbf{\\tilde x} \\] \u5176\u4e2d\uff0c \\(P\\) \u88ab\u79f0\u4e3a\u6295\u5f71\u77e9\u9635 (Projection Matrix). \\[ \\begin{bmatrix} u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\\\ p_{21} & p_{22} & p_{23} & p_{24} \\\\ p_{31} & p_{32} & p_{33} & p_{34} \\\\ p_{41} & p_{42} & p_{43} & p_{44} \\\\ \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix} \\]","title":"Image Plane to Image Sensor Mapping"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#implemetation","text":"","title":"Implemetation"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-1-capture-an-image-of-an-object-with-known-geometry","text":"","title":"Step 1. Capture an image of an object with known geometry."},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-2-identify-correspondences-image-matching-between-3d-scene-points-and-image-points","text":"","title":"Step 2. Identify correspondences (Image Matching \u56fe\u50cf\u5339\u914d) between 3D scene points and image points."},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-3-for-each-corresponding-point-i-we-get","text":"\\[ \\begin{bmatrix} u^{(i)} \\\\ v^{(i)} \\\\ w^{(i)} \\end{bmatrix} \\cong \\begin{bmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\\\ p_{21} & p_{22} & p_{23} & p_{24} \\\\ p_{31} & p_{32} & p_{33} & p_{34} \\\\ p_{41} & p_{42} & p_{43} & p_{44} \\\\ \\end{bmatrix} \\begin{bmatrix} x_w^{(i)} \\\\ y_w^{(i)} \\\\ z_w^{(i)} \\\\ 1 \\end{bmatrix} \\] \\[ \\begin{align} u^{(i)} &= \\frac{p_{11}x_w^{(i)} + p_{12}y_w^{(i)} + p_{13}z_w^{(i)} + p_{14}}{p_{31}x_w^{(i)} + p_{32}y_w^{(i)} + p_{33}z_w^{(i)} + p_{34}} \\\\ v^{(i)} &= \\frac{p_{21}x_w^{(i)} + p_{22}y_w^{(i)} + p_{23}z_w^{(i)} + p_{24}}{p_{31}x_w^{(i)} + p_{32}y_w^{(i)} + p_{33}z_w^{(i)} + p_{34}} \\\\ \\end{align} \\]","title":"Step 3. For each corresponding point \\(i\\),  we get"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-4-rearrange","text":"\\[ \\scriptsize \\begin{bmatrix} x_w^{(1)} & y_w^{(1)} & z_w^{(1)} & 1 & 0 & 0 & 0 & 0 & -u_1x_w^{(1)} & -u_1y_w^{(1)} & -u_1z_w^{(1)} & -u_1 \\\\ 0 & 0 & 0 & 0 & x_w^{(1)} & y_w^{(1)} & z_w^{(1)} & 1 & -v_1x_w^{(1)} & -v_1y_w^{(1)} & -v_1z_w^{(1)} & -v_1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_w^{(i)} & y_w^{(i)} & z_w^{(i)} & 1 & 0 & 0 & 0 & 0 & -u_ix_w^{(i)} & -u_iy_w^{(i)} & -u_iz_w^{(i)} & -u_i \\\\ 0 & 0 & 0 & 0 & x_w^{(i)} & y_w^{(i)} & z_w^{(i)} & 1 & -v_ix_w^{(i)} & -v_iy_w^{(i)} & -v_iz_w^{(i)} & -v_i \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_w^{(n)} & y_w^{(n)} & z_w^{(n)} & 1 & 0 & 0 & 0 & 0 & -u_nx_w^{(n)} & -u_ny_w^{(n)} & -u_nz_w^{(n)} & -u_n \\\\ 0 & 0 & 0 & 0 & x_w^{(n)} & y_w^{(n)} & z_w^{(n)} & 1 & -v_nx_w^{(n)} & -v_ny_w^{(n)} & -v_nz_w^{(n)} & -v_n \\\\ \\end{bmatrix} \\begin{bmatrix} p_{11} \\\\ p_{12} \\\\ p_{13} \\\\ p_{14} \\\\ p_{21} \\\\ p_{22} \\\\ p_{23} \\\\ p_{24} \\\\ p_{31} \\\\ p_{32} \\\\ p_{33} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\] \\[ i.e.\\ A \\mathbf{p} = 0 \\]","title":"Step 4. Rearrange."},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-5-solve-for-p","text":"\u7c7b\u4f3c\u5730\uff0c\u6211\u4eec\u53ef\u4ee5\u7ed9\u51fa\u4e00\u5b9a\u7684\u9650\u5236 constraint. Option 1. \u4ee4 \\(p_{34} = 1\\) . Option 2. \u4ee4 \\(||p|| = 1\\) . \u5f97\u5230\u81ea\u7531\u5ea6\u4e3a \\(11\\) \uff0c\u81f3\u5c11\u9700\u8981 6 \u7ec4\u70b9. \u5e76\u540c\u6837\u8981\u6700\u5c0f\u5316 \\(||\\mathbf{Ap - 0}|| = \\mathbf{||Ap||}\\) . \u540c\u6837\u5982\u679c\u4f7f\u7528\u7684\u662f\u5411\u91cf\u8303\u6570\u4e3a \\(1\\) \u7684\u9650\u5236\uff0c\u90a3\u4e48\u6211\u4eec\u53ea\u9700\u8981\u786e\u5b9a \\(\\mathbf{p}\\) \u7684\u65b9\u5411 \\(\\mathbf{\\hat p}\\) . \u4e14 \\(\\mathbf{\\hat p}\\) \u662f \\(\\mathbf{A^{T}A}\\) \u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf.","title":"Step 5 Solve for \\(P\\)."},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-6-from-p-to-answers","text":"\u7531 \\[ \\begin{bmatrix} p_{11} \\\\ p_{12} \\\\ p_{13} \\\\ p_{14} \\\\ p_{21} \\\\ p_{22} \\\\ p_{23} \\\\ p_{24} \\\\ p_{31} \\\\ p_{32} \\\\ p_{33} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\] \u6ce8\u610f\u5230 \\[ \\begin{bmatrix} p_{11} \\\\ p_{12} \\\\ p_{13} \\\\ p_{21} \\\\ p_{22} \\\\ p_{23} \\\\ p_{31} \\\\ p_{32} \\\\ p_{33} \\end{bmatrix} = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\\\ \\end{bmatrix} = KR \\] \u5176\u4e2d \\(K\\) \u662f\u4e0a\u4e09\u89d2\u77e9\u9635 (Upper Right Triangular Matrix)\uff0c \\(R\\) \u662f\u5355\u4f4d\u6b63\u4ea4\u77e9\u9635 (Orthonormal Matrix). \u7531 QR \u5206\u89e3 (QR Factorization) \u53ef\u77e5\uff0c \\(K\\) \u4e0e \\(R\\) \u7684\u89e3\u662f\u552f\u4e00\u7684. \u518d\u7531 \\[ \\begin{bmatrix} p_{14} \\\\ p_{24} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} = K \\mathbf{t} \\] \u6545\u53ef\u89e3\u5f97 \\[ \\mathbf{t} = K^{-1} \\begin{bmatrix} p_{14} \\\\ p_{24} \\\\ p_{34} \\end{bmatrix} \\] Problem: Distortion \u5b9e\u9645\u4e0a\u76f8\u673a\u81ea\u8eab\u4f1a\u5177\u6709\u4e00\u5b9a\u7684\u626d\u66f2\u7cfb\u6570\uff0c\u6211\u4eec\u5728\u6b64 \u5ffd\u7565 \u5b83\u7684\u5f71\u54cd. Perspective-n-Point Problem (PnP \u95ee\u9898) \u5f31\u5316\u4e00\u4e0b\u6761\u4ef6\uff0c\u5047\u8bbe\u5df2\u77e5\u76f8\u673a\u7684\u5185\u53c2\uff08\u5373\u77e9\u9635 \\(K\\) \uff09\uff0c\u6c42\u76f8\u673a\u7684\u5916\u53c2\uff08\u5373\u77e9\u9635 \\(R\\) \u548c \\(\\mathbf{t}\\) \uff09\uff0c\u4e5f\u5373\u76f8\u673a\u59ff\u6001 (camera pose).","title":"Step 6 From \\(P\\) to Answers"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#direct-linear-transform-dlt","text":"\u5373\u4ee5\u4e0a\u5728\u76f8\u673a\u6807\u5b9a\u4e2d\u63d0\u5230\u7684\u65b9\u6cd5\uff0c\u5148\u6c42\u89e3\u77e9\u9635 \\(P\\) \uff0c\u518d\u901a\u8fc7 \\(K\\) \u5f97\u5230 \\(R\\) \u548c \\(\\mathbf{t}\\) .","title":"Direct Linear Transform (DLT)"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#p3p","text":"\u5373\u5df2\u77e5 \\(a, b, c, A, B, C\\) \u70b9\u5750\u6807\uff0c\u6c42 \\(O\\) \u5750\u6807. \u81f3\u5c11\u9700\u8981 \\(3\\) \u7ec4\u70b9 Intermediates. \u7531\u4f59\u5f26\u5b9a\u7406 \\[ \\begin{align} OA^2 + OB^2 - 2 OA \\cdot OB \\cos \\angle AOB &= AB^2 \\\\ OA^2 + OC^2 - 2 OB \\cdot OC \\cos \\angle BOC &= BC^2 \\\\ OA^2 + OC^2 - 2 OA \\cdot OC \\cos \\angle AOC &= AC^2 \\end{align} \\] \u4ee4 \\(x = \\frac{OA}{OC}, y = \\frac{OB}{OC}, v = \\frac{AB^2}{OC^2}, u = \\frac{BC^2}{AB^2}, w = \\frac{AC^2}{AB^2}\\) \uff0c\u5316\u7b80\u5f97 \\[ \\begin{align} (1 - u)y^2 - ux^2 - cos \\angle BOC y + 2uxy \\cos \\angle AOB + 1 &= 0 \\\\ (1 - w)x^2 - wy^2 - cos \\angle AOC y + 2wxy \\cos \\angle AOB + 1 &= 0 \\end{align} \\] \u4ee5\u4e0a\u65b9\u7a0b\u7ec4\u662f\u4e8c\u5143\u4e8c\u6b21\u65b9\u7a0b\u7ec4 (Binary Quadratic Equation)\uff0c\u6709\u56db\u7ec4\u89e3. \u5176\u4e2d\u6709\u4e24\u7ec4\u89e3\u7684 \\(O\\) \u5750\u6807\u5728 \u9762 \\(ABC\\) \u4e0e \u9762 \\(abc\\) \u4e4b\u95f4\uff0c\u820d\u53bb. \u8fd8\u9700\u8981\u4e00\u7ec4\u989d\u5916\u7684\u70b9\u6765\u786e\u5b9a\u552f\u4e00\u89e3\uff0c\u6545\u4e00\u5171\u9700\u8981 \\(4\\) \u7ec4\u70b9.","title":"P3P"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#pnp","text":"\u6700\u5c0f\u5316 \u91cd\u6295\u5f71\u8bef\u5dee (Reprojection Error) \\[ \\mathop{\\arg\\min\\limits_{R, \\mathbf{t}}} \\sum\\limits_i ||(p_i, K(RP_i + \\mathbf{t})||^2 \\] \u5176\u4e2d\uff0c \\(p_i\\) \u662f\u6295\u5f71\u9762\u4e0a\u7684\u4e8c\u7ef4\u5750\u6807\uff0c \\(P_i\\) \u662f\u7a7a\u95f4\u5185\u7684\u4e09\u7ef4\u5750\u6807. \u901a\u8fc7 P3P \u521d\u59cb\u5316\uff0cGauss-Newton \u6cd5\u4f18\u5316.","title":"PnP"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#epnp","text":"one of the most popluar methods. time complexity \\(O(N)\\) . high accuracy. \u5927\u81f4\u65b9\u5f0f\u662f\u901a\u8fc7\u56db\u7ec4\u63a7\u5236\u70b9\u7684\u7ebf\u6027\u7ec4\u5408\u6c42\u89e3.","title":"EPnP"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#two-frame-structure-from-motion","text":"Stereo vision \u7acb\u4f53\u89c6\u89c9 Compute 3D structure of the scene and camera poses from views (images). Two-frame SfM \u610f\u5728\u89e3\u51b3\u8fd9\u6837\u4e00\u4e2a\u95ee\u9898\uff1a\u5df2\u77e5\u4e24\u5f20\u56fe\u50cf\u548c\u76f8\u673a\u5185\u53c2\u77e9\u9635 ( \\(K_l(3 \\times 3)\\) \u4e0e \\(K_r(3 \\times 3)\\) )\uff0c\u6c42\u76f8\u673a\u7684\u4f4d\u7f6e\u4ee5\u53ca\u6240\u62cd\u6444\u4e3b\u4f53\u7684\u4e09\u7ef4\u4f4d\u7f6e\u5750\u6807.","title":"Two-frame Structure from Motion"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#principle-induction","text":"Epipolar Geometry \u5bf9\u6781\u51e0\u4f55 \u5bf9\u6781\u51e0\u4f55\u63cf\u8ff0\u4e86\u4e00\u4e2a\u4e09\u7ef4\u70b9 \\(P\\) \u5728\u4e24\u4e2a\u89c6\u89d2\u7684\u4e8c\u7ef4\u6295\u5f71 \\(u_l, u_r\\) \u4e4b\u95f4\u7684\u51e0\u4f55\u5173\u7cfb. \u5e76\u901a\u8fc7\u8fd9\u4e2a\u51e0\u4f55\u5173\u7cfb\u5efa\u7acb\u4e24\u4e2a\u6444\u50cf\u673a\u4e4b\u95f4\u7684\u53d8\u6362\u5173\u7cfb\uff08\u5373\u7ed9\u51fa\u77e9\u9635 \\(R\\) \u548c \\(\\mathbf{t}\\) \uff09. Definition Epipole \u5bf9\u6781\u70b9 \uff1a\u4e00\u53f0\u6444\u50cf\u673a\u6295\u5f71\u5728\u53e6\u4e00\u53f0\u6444\u50cf\u673a\u4e2d\u7684\u70b9. \u5982\u56fe\u4e2d\u7684 \\(e_l\\) \u548c \\(e_r\\) . \u5bf9\u7ed9\u5b9a\u7684\u4e24\u53f0\u6444\u50cf\u673a\uff0c \\(e_l\\) \u548c \\(e_r\\) \u662f\u552f\u4e00\u7684. Epipolar Plane of Scene Point P \u5173\u4e8e P \u70b9\u7684\u5bf9\u6781\u9762 \uff1a\u7531 Scene Point \\(P\\) \u70b9\u548c\u4e24\u4e2a\u6444\u50cf\u673a \\(O_l\\) \u548c \\(O_r\\) \u5f62\u6210\u7684\u5e73\u9762. \u6bcf\u4e2a scene point \u6709\u552f\u4e00\u5bf9\u5e94\u7684\u5bf9\u6781\u9762. \u5bf9\u6781\u70b9\u5728\u5bf9\u6781\u9762\u4e0a.","title":"Principle Induction \u539f\u7406\u63a8\u5bfc"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#essential-matrix-e","text":"\u5bf9\u4e8e\u4e00\u4e2a\u5bf9\u6781\u9762\uff0c\u4ee5\u4e0b\u7b49\u5f0f\u5173\u7cfb\u6210\u7acb\uff1a \\[ \\begin{align} \\mathbf{n} = \\mathbf{t} \\times \\mathbf{x}_l \\\\ \\\\ \\mathbf{x}_l \\cdot (\\mathbf{t} \\times \\mathbf{x}_l) = 0 \\end{align} \\] \u5176\u4e2d \\(\\mathbf{n}\\) \u88ab\u79f0\u4e3a Normal Vector. \u5047\u8bbe\u53f3\u8fb9\u7684\u6444\u50cf\u673a\u76f8\u5bf9\u5de6\u8fb9\u7684\u6444\u50cf\u673a\u8fdb\u884c\u521a\u4f53\u53d8\u6362 + \u5e73\u79fb\uff0c\u5373\uff1a \\[ \\begin{align} \\mathbf{x}_l &= R \\mathbf{x}_r + \\mathbf{t} \\\\ \\\\ \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\end{bmatrix} &= \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ y_r \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} \\end{align} \\] \u8fdb\u4e00\u6b65\u6f14\u7ece\u63a8\u7406\uff0c\u5f97\u5230\uff1a \\[ \\begin{align} \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\begin{bmatrix} t_yz_l - t_zy_l \\\\ t_zx_l - t_xz_l \\\\ t_xy_l - t_yx_l \\end{bmatrix} = 0 & \\text{, Cross-product definition} \\\\ \\\\ \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\underbrace{ \\begin{bmatrix} 0 & -t_z & t_y \\\\ t_z & 0 & -t_x \\\\ -t_y & t_x & 0 \\end{bmatrix} }_{T_\\times} \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\end{bmatrix} = 0 & \\text{, Matrix-vector form} \\end{align} \\] \u4ee3\u5165\u53d8\u6362\u77e9\u9635 \\[ \\small \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\left( \\underbrace{ \\begin{bmatrix} 0 & -t_z & t_y \\\\ t_z & 0 & -t_x \\\\ -t_y & t_x & 0 \\end{bmatrix} }_{T_\\times} \\underbrace{ \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\end{bmatrix} }_{R} \\begin{bmatrix} x_r \\\\ y_r \\\\ y_r \\end{bmatrix} + \\underbrace{ \\begin{bmatrix} 0 & -t_z & t_y \\\\ t_z & 0 & -t_x \\\\ -t_y & t_x & 0 \\end{bmatrix} \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} }_{\\mathbf{t} \\times \\mathbf{t} = \\mathbf{0}} \\right) = 0 \\] \\[ \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\underbrace{ \\begin{bmatrix} e_{11} & e_{12} & e_{13} \\\\ e_{21} & e_{22} & e_{23} \\\\ e_{31} & e_{32} & e_{33} \\end{bmatrix} }_{\\text{Essential Matrix } E} \\begin{bmatrix} x_r \\\\ y_r \\\\ y_r \\end{bmatrix} \\text{, where } E = T_\\times R \\] \u6ce8\u610f\u5230 \\(T_\\times\\) \u662f\u53cd\u5bf9\u79f0\u77e9\u9635 (skew-symmetric matrix) \u800c \\(R\\) \u662f\u5355\u4f4d\u6b63\u4ea4\u77e9\u9635 (orthonomal matrix)\uff0c\u7531 SVD \u5206\u89e3 (Singule Value Decomposition)\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece \\(E\\) \u552f\u4e00\u5206\u89e3\u5f97\u5230 \\(T_\\times\\) \u548c \\(R\\) .","title":"Essential Matrix \\(E\\) \u57fa\u672c\u77e9\u9635"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#fundemental-matrix-f","text":"\u7531\u6295\u5f71\u5173\u7cfb\u4ee5\u53ca\u5185\u53c2\u77e9\u9635 \\(K\\) \uff0c\u5f97\u5230\uff1a \\[ \\small \\begin{align} \\text{Left Camera} && \\text{Right Camera} \\\\ z_l \\begin{bmatrix} u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &= \\underbrace{ \\begin{bmatrix} f_x^{(l)} & 0 & o_x^{(l)} \\\\ 0 & f_y^{(l)} & o_y^{(l)} \\\\ 0 & 0 & 1 \\end{bmatrix} }_{K_l} \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\end{bmatrix} & z_r \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} &= \\underbrace{ \\begin{bmatrix} f_x^{(r)} & 0 & o_x^{(r)} \\\\ 0 & f_y^{(r)} & o_y^{(r)} \\\\ 0 & 0 & 1 \\end{bmatrix} }_{K_r} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\end{bmatrix} \\\\ \\mathbf{x}_l^T &= \\begin{bmatrix} u_l & v_l & 1 \\end{bmatrix} z_l (K_l^{-1})^T & \\mathbf{x}_r &= K_r^{-1} z_r \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} \\end{align} \\] \u4ee3\u5165\u4ee5\u4e0a\u5f97\u5230\u7684\uff1a \\[ \\mathbf{x}^T_l E \\mathbf{x}_r = 0 \\] \u6709 \\[ \\require{canel} \\begin{bmatrix} u_l & v_l & 1 \\end{bmatrix} \\cancel{z_l} (K_l^{-1})^T E K_r^{-1} \\cancel{z_r} \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = 0 \\] \u5373 \\[ \\begin{bmatrix} u_l & v_l & 1 \\end{bmatrix} F \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = 0 \\text{, where } E = K_l^T F K_r \\]","title":"Fundemental Matrix \\(F\\) \u672c\u771f\u77e9\u9635"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#implementation","text":"","title":"Implementation \u5b9e\u73b0"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-1-find-a-set-m-pairs-of-corresponding-features-m","text":"Least Number of Points At least 8 pairs: \\((u_l^{(i)}, v_l^{(i)})\\) \u2194 \\((u_r^{(i)}, v_r^{(i)})\\) NOTE that one pair only corresponds one equation. (not the same as before)","title":"Step 1. Find a set (\\(m\\) pairs) of corresponding features. \u627e\u5230\u4e00\u7ec4\uff08\\(m\\) \u5bf9\uff09\u5339\u914d\u70b9."},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-2-build-linear-systems-and-solve-for-f-f","text":"\u5bf9\u4e8e\u6bcf\u7ec4\u5339\u914d \\(i\\) \uff0c \\[ \\begin{bmatrix} u_l^{(i)} & v_l^{(i)} & 1 \\end{bmatrix} \\begin{bmatrix} f_{11} & f_{12} & f_{13} \\\\ f_{21} & f_{22} & f_{23} \\\\ f_{31} & f_{32} & f_{33} \\end{bmatrix} \\begin{bmatrix} u_r^{(i)} \\\\ v_r^{(i)} \\\\ 1 \\end{bmatrix} = 0 \\] \u5c55\u5f00\u5f97\uff0c \\[ \\small \\left( f_{11} u_r^{(i)} + f_{12} v_r^{(i)} + f_13 \\right)u_l^{(i)} + \\left( f_{21} u_r^{(i)} + f_{22} v_r^{(i)} + f_23 \\right)v_l^{(i)} + f_{31} u_r^{(i)} + f_{32} v_r^{(i)} + f_33 = 0 \\] \u5bf9\u6240\u6709\u9009\u62e9\u7684\u70b9\uff0c\u6709 \\[ \\small \\begin{bmatrix} u_l^{(1)}u_r^{(1)} & u_l^{(1)}v_r^{(1)} & u_l^{(1)} & v_l^{(1)}u_r^{(1)} & v_l^{(1)}v_r^{(1)} & v_l^{(1)} & u_r^{(1)} & v_r^{(1)} & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ u_l^{(i)}u_r^{(i)} & u_l^{(i)}v_r^{(i)} & u_l^{(i)} & v_l^{(i)}u_r^{(i)} & v_l^{(i)}v_r^{(i)} & v_l^{(i)} & u_r^{(i)} & v_r^{(i)} & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ u_l^{(m)}u_r^{(m)} & u_l^{(m)}v_r^{(m)} & u_l^{(m)} & v_l^{(m)}u_r^{(m)} & v_l^{(m)}v_r^{(m)} & v_l^{(m)} & u_r^{(m)} & v_r^{(m)} & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} f_{11} \\\\ f_{12} \\\\ f_{13} \\\\ f_{21} \\\\ f_{22} \\\\ f_{23} \\\\ f_{31} \\\\ f_{32} \\\\ f_{33} \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\] \\[ A \\mathbf{f} = \\mathbf{0} \\] \u540c\u6837\u5730 \uff0c\u6211\u4eec\u53ef\u4ee5\u7ed9\u51fa\u4e00\u5b9a\u7684\u9650\u5236 constraint. Option 1. \u4ee4 \\(f_{33} = 1\\) . Option 2. \u4ee4 \\(||f|| = 1\\) . \u5f97\u5230\u81ea\u7531\u5ea6\u4e3a \\(8\\) \uff0c\u81f3\u5c11\u9700\u8981 8 \u7ec4\u70b9. \u5e76\u540c\u6837\u8981\u6700\u5c0f\u5316 \\(||\\mathbf{Af - 0}|| = \\mathbf{||Af||}\\) . \u540c\u6837\u5982\u679c\u4f7f\u7528\u7684\u662f\u5411\u91cf\u8303\u6570\u4e3a \\(1\\) \u7684\u9650\u5236\uff0c\u90a3\u4e48\u89e3\u5c31\u662f \\(\\mathbf{A^{T}A}\\) \u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf.","title":"Step 2. Build linear systems and solve for \\(F\\). \u5efa\u7acb\u65b9\u7a0b\u7ec4\u5e76\u6c42\u89e3\u77e9\u9635 \\(F\\)."},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-3-find-e-and-extract-r-mathbft","text":"\\[ E = K_l^T F K_r \\] \u901a\u8fc7 SVD \u5206\u89e3\uff0c \\(E = T_\\times R\\) \u5f97\u5230 \\(T_\\times\\) \u548c \\(R\\) . \\(R\\) \u5373\u4e3a\u65cb\u8f6c\u77e9\u9635\uff0c \\(\\mathbf{t}\\) \u53ef\u4ee5\u901a\u8fc7 \\(T_\\times\\) \u76f4\u63a5\u5f97\u5230.","title":"Step 3. Find \\(E\\) and Extract \\(R, \\mathbf{t}\\)"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-4-find-3d-position-of-scene-points","text":"\u73b0\u5728\u5df2\u7ecf\u6709\u4ee5\u4e0b\u7b49\u5f0f \\[ \\small \\begin{align} \\text{Left Camera} \\\\ \\begin{bmatrix} u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} f_x^{(l)} & 0 & o_x^{(l)} & 0 \\\\ 0 & f_y^{(l)} & o_y^{(l)} & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\\\ 1 \\end{bmatrix} ,\\ \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y \\\\ r_{31} & r_{32} & r_{33} & t_z \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\\\ \\begin{bmatrix} u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} f_x^{(l)} & 0 & o_x^{(l)} & 0 \\\\ 0 & f_y^{(l)} & o_y^{(l)} & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y \\\\ r_{31} & r_{32} & r_{33} & t_z \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\\\ \\\\ \\normalsize \\mathbf{\\tilde u}_l &= \\normalsize P_l \\mathbf{\\tilde x}_r \\\\ \\\\ \\small \\text{Right Camera} \\\\ \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} f_x^{(r)} & 0 & o_x^{(r)} & 0 \\\\ 0 & f_y^{(r)} & o_y^{(r)} & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\\\ \\\\ \\normalsize \\mathbf{\\tilde u}_l &= \\normalsize M_{int_r} \\mathbf{\\tilde x}_r \\end{align} \\] \u7531 \\[ \\small \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} m_{11} & m_{12} & m_{13} & m_{14} \\\\ m_{21} & m_{22} & m_{23} & m_{24} \\\\ m_{31} & m_{32} & m_{33} & m_{34} \\\\ \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} ,\\ \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\\\ p_{21} & p_{22} & p_{23} & p_{24} \\\\ p_{31} & p_{32} & p_{33} & p_{34} \\\\ \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\] \u6574\u7406\u5f97 \\[ \\underbrace{ \\begin{bmatrix} u_r m_{31} - m_{11} & u_r m_{32} - m_{12} & u_r m_{33} - m_{13} \\\\ v_r m_{31} - m_{21} & v_r m_{32} - m_{22} & v_r m_{33} - m_{23} \\\\ u_l p_{31} - p_{11} & u_l p_{32} - p_{12} & u_l p_{33} - p_{13} \\\\ v_l p_{31} - p_{21} & v_l p_{32} - p_{22} & v_l p_{33} - p_{23} \\end{bmatrix} }_{A_{4\\times 3}} \\underbrace{ \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\end{bmatrix} }_{\\mathbf{x}_r} = \\underbrace{ \\begin{bmatrix} m_{14} - m_{34} \\\\ m_{24} - m_{34} \\\\ p_{14} - p_{34} \\\\ p_{24} - p_{34} \\end{bmatrix} }_{\\mathbf{b}} \\] \u7531\u5206\u6790\u4e0a\u89e3\u7684\u552f\u4e00\u6027\uff0c\u7406\u8bba\u4e0a\u6709\u4e24\u884c\u662f\u7ebf\u6027\u76f8\u5173\u7684. \u4f46\u662f\u5728\u5b9e\u9645\u53d6\u70b9\u4e2d\u53ef\u80fd\u4f1a\u5b58\u5728\u4e00\u5b9a\u7684\u8bef\u5dee\uff0c\u6240\u4ee5\u4e3a\u4e86\u6700\u5927\u5316\u5229\u7528\u6570\u636e\u91cf\uff0c\u8fd8\u662f\u91c7\u7528\u6700\u5c0f\u4e8c\u4e58\u6cd5. \u7531\u6700\u5c0f\u4e8c\u4e58\u6cd5\u5f97\uff0c \\[ \\mathbf{x}_r = (A^TA)^{-1}A^T \\mathbf{b} \\] Non-linear Solution \u4e0a\u9762\u7684\u662f\u901a\u8fc7\u89e3\u7ebf\u6027\u7cfb\u7edf\u7684\u65b9\u6cd5\uff0c\u53e6\u4e00\u79cd\u60f3\u6cd5\u662f\u6700\u5c0f\u5316 \u91cd\u6620\u5c04\u8bef\u5dee (reprojection error) . \\[ \\text{cost}(P) = \\text{dist}(\\mathbf{u}_l, \\mathbf{\\hat u}_l)^2 + \\text{dist}(\\mathbf{u}_r, \\mathbf{\\hat u}_r)^2 \\] \u53e6\u5916\uff0c\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\u6211\u4eec\u4e5f\u53ef\u4ee5\u4f18\u5316 \\(R\\) \u548c \\(\\mathbf{t}\\) .","title":"Step 4. Find 3D Position of Scene Points"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#multi-frame-structure-from-motion","text":"\u4e0e Two-frame SfM \u7c7b\u4f3c\uff0c\u5df2\u77e5 \\(m\\) \u5f20\u56fe\u50cf\uff0c \\(n\\) \u4e2a\u4e09\u7ef4\u70b9\u4ee5\u53ca\u76f8\u673a\u5185\u53c2\u77e9\u9635\uff0c\u6c42\u76f8\u673a\u7684\u4f4d\u7f6e\u4ee5\u53ca\u6240\u62cd\u6444\u4e3b\u4f53\u7684\u4e09\u7ef4\u4f4d\u7f6e\u5750\u6807. \u4e5f\u5c31\u662f\u5bf9\u4e8e\u4ee5\u4e0b\u7b49\u5f0f\uff1a \\[ \\mathbf{u}_j^{(i)} = P_{proj}^{(i)} \\mathbf{P}_j \\text{, where } i = 1, \\dots, m, j = 1, \\dots, n \\] \u7531 \\(mn\\) \u4e2a\u6295\u5f71\u7684\u4e8c\u7ef4\u70b9 \\(\\mathbf{u}_j^{(i)}\\) \u6c42\u89e3 \\(m\\) \u4e2a\u6295\u5f71\u77e9\u9635 \\(P_{proj}^{(i)}\\) \u4e0e \\(n\\) \u4e2a\u4e09\u7ef4\u70b9\u5750\u6807 \\(P_j\\) .","title":"Multi-frame Structure from Motion"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#solution-sequential-structrue-from-motion","text":"","title":"Solution: Sequential Structrue from Motion"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-1-initialize-camera-motion-and-scene-structure","text":"\u9996\u5148\u9009\u4e24\u5f20\u56fe\u505a\u4e00\u6b21 Two-frame SfM.","title":"Step 1. Initialize camera motion and scene structure. \u521d\u59cb\u5316"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-2-deal-with-an-addition-view","text":"\u5bf9\u4e8e\u6bcf\u65b0\u589e\u7684\u4e00\u5f20\u56fe\uff0c \u5bf9\u4e8e\u5df2\u7ecf\u5efa\u7acb\u4e09\u7ef4\u5750\u6807\u7684\u70b9\uff1a PnP \u95ee\u9898 . \u5bf9\u4e8e\u5c1a\u672a\u5efa\u7acb\u4e09\u7ef4\u5750\u6807\u7684\u70b9\uff1a\u505a Two-frame SfM.","title":"Step 2. Deal with an addition view. \u5904\u7406\u4e0b\u4e00\u5f20\u56fe"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-3-refine-structure-and-motion-bundle-adjustment","text":"\u5904\u7406\u5b8c\u6240\u6709\u7684\u56fe\u540e\uff0c\u518d\u901a\u8fc7\u4e00\u6b21 \u96c6\u675f\u8c03\u6574 \uff08\u5177\u4f53\u5b9e\u73b0\u662f LM \u7b97\u6cd5 \uff09\u5bf9\u4e09\u7ef4\u70b9\u5750\u6807\u548c\u76f8\u673a\u53c2\u6570\u505a\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u5373\u6700\u5c0f\u5316 \u91cd\u6620\u5c04\u8bef\u5dee \uff08\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\uff09\uff1a \\[ E(P_{proj}, \\mathbf{P}) = \\sum\\limits_{i=1}^m \\sum\\limits_{j=1}^n \\text{dist} \\left( u_j^{(i)}, P_{proj}^{(i)} \\mathbf{P}_j \\right)^2 \\]","title":"Step 3. Refine structure and motion: Bundle Adjustment. \u96c6\u675f\u8c03\u6574"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#colmap","text":"COLMAP is a general-purpose Structure-from-Motion (SfM) and Multi-View Stereo (MVS) pipeline with a graphical and commandline interface. It offers a wide range of features for reconstruction of ordered and unordered image collections. Pipeline","title":"COLMAP"},{"location":"Computer_Science_Courses/ICV/8_Depth_Estimation_and_3D_Reconstruction/","text":"","title":"Lecture 8"},{"location":"Computer_Science_Courses/ICV/9_Deep_Learning/","text":"","title":"Lecture 9"},{"location":"Mathematics_Basis/","text":"Title Page \u00b6 Abstract This section stores the notes of mathematics basis. Algrebra, analysis, and any other courses or topics concerned about mathematics. Welcome to point out anything wrong and careless mistakes!","title":"Title Page"},{"location":"Mathematics_Basis/#title-page","text":"Abstract This section stores the notes of mathematics basis. Algrebra, analysis, and any other courses or topics concerned about mathematics. Welcome to point out anything wrong and careless mistakes!","title":"Title Page"},{"location":"Mathematics_Basis/NA/Chap_1/","text":"Chapter 1 | Mathematical Preliminaries \u00b6 Error \u00b6 Truncation Error \u00b6 the error involved using a truncated or finite summation. Roundoff Error \u00b6 the error produced when performing real number calculations. It occurs because the arithmetic performed in a machine involved numbers with a finite number of digits. Chopping & Rounding Given a real number \\(y = 0.d_1d_2\\dots d_kd_{k+1}\\dots \\times 10^N\\) , the floating-point representation of \\(y\\) is \\(fl(y)\\) : \\[ fl(y) = \\left\\{ \\begin{align} & 0.d_1d_2\\dots d_k \\times 10^N, & {\\tt Chopping} \\\\ & {\\tt chop}\\left(y + 5 \\times 10^{n - (k + 1)}\\right). & {\\tt Rounding} \\end{align} \\right. \\] Definition If \\(p^*\\) is an approximation to \\(p\\) , then absolute error is \\(|p - p*|\\) and relative error is \\(\\frac{|p - p^*|}{|p|}\\) . The number \\(p^*\\) is said to be approximate to \\(p\\) to \\(t\\) * significant digits if \\(t\\) is the largest nonnegative integer for which \\[ \\frac{|p - p^*|}{|p|} < 5 \\times 10^{-t} \\] Example For the floating-point representation of \\(y\\) , the relative error is \\[ \\left|\\frac{y - fl(y)}{y}\\right|. \\] for chopping representation, \\[ \\left|\\frac{y - fl(y)}{y}\\right| = \\left|\\frac{0.d_{k + 1}d_{k + 2}\\dots}{0.d_1d_2\\dots}\\right| \\times 10^{-k} \\le \\frac{1}{0.1} \\times 10^{-k} = 10^{-k + 1}. \\] for rounding representation, \\[ \\left|\\frac{y - fl(y)}{y}\\right| \\le \\frac{0.5}{0.1} \\times 10^{-k} = 0.5 \\times 10^{-k + 1}. \\] Effect of Error \u00b6 Subtraction may reduce significant digits. e.g. 0.1234 - 0.1233 = 0.001 Division by small number of multiplication by large number magnify the abosolute error without modifying the relative error. Some Solutions to Reduce Error \u00b6 Quadratic Formula the roots of \\(ax^2 + bx + c = 0\\) is \\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}. \\] Sometimes \\(b\\) is closer to \\(\\sqrt{b^2 - 4ac}\\) , which may cause the subtraction to reduce significant digits. An alternate way is to modify the formula to \\[ x = \\frac{-2c}{b \\pm \\sqrt{b^2 - 4ac}}. \\] But it may cause the division by small number. So it's a tradeoff to use one of the two formulae above. Horner's Method \u79e6\u4e5d\u97f6\u7b97\u6cd5 \\[ \\begin{align} f(x) &= a_nx^n + a_{n-1}x^{n-1} + \\dots + a_1x + a_0 \\\\ &= (\\dots((a_nx+a_{n-1})x+a_{n-2})x+\\dots+a_1)x+a_0 \\end{align} \\] Stable Algorithms and Convergence \u00b6 Definition An algorithm that satisfies that small changes in the initial data produce correspondingly small changes in the final results is called stable ; otherwise it is unstable . An algorithm is called conditionally stable if it is stable only for certain choices of initial data. Suppose \\(E_0 > 0\\) denotes an initial errors, \\(E_n\\) denotes the magnitude of an error after \\(n\\) subsequent operations Linear growth of errors \\(E_n \\approx C n E_0\\) unavoidable and acceptable Exponential growth of errors \\(E_n \\approx C^n E_0\\) unacceptable Example the recursive equation \\(p_n = \\frac{10}{3}p_{n - 1} - p_{n - 2}\\) has the solution \\[ p_n = c_1 \\left(\\frac13\\right)^n + c_23^n. \\] If \\(p_0 = 1, p_1 = \\frac13\\) , then the solution is \\[ p_n = \\left(\\frac13\\right)^n. \\] Suppose we have five-digit rounding representation, then \\(\\hat p_0 = 1.0000\\) , \\(\\hat p_1 = 0.33333\\) , and the solution is \\[ \\hat p_n = 1.0000 \\left(\\frac13\\right)^n - 0.12500 \\times 10^{-5} \\cdot 3^n. \\] Then \\[ p_n - \\hat p_n = 0.12500 \\times 10^{-5} \\cdot 3^n \\] grow exponentially with \\(n\\) . On the other hand, the recursive equation \\(p_n = 2p_{n - 1} - p_{n - 2}\\) has the solution \\[ p_n = c_1 + c_2n \\] If \\(p_0 = 1, p_1 = \\frac13\\) , then the solution is \\[ p_n = 1 - \\frac23 n. \\] Suppose we have five-digit rounding representation, then \\(\\hat p_0 = 1.0000\\) , \\(\\hat p_1 = 0.33333\\) , and the solution is \\[ \\hat p_n = 1.0000 - 0.66667 n. \\] Then \\[ p_n - \\hat p_n = \\left(0.66667 - \\frac23\\right) n \\] grow linearly with \\(n\\) . Error of floating-point number (IEEE 754 standard) Range of normal representation \u00b6 Single-Precision Smallest \\(\\text{0/1\\ 00000001\\ 00\\dots00}\\) \\(\\pm 1.0 \\times 2^{-126} \\approx \\pm 1.2 \\times 10^{-38}\\) Largest \\(\\text{0/1\\ 11111110\\ 11\\dots11}\\) \\(\\pm 2.0 \\times 2^{127} \\approx \\pm 3.4 \\times 10^{38}\\) For Double-Precision, smallest \\(\\pm 1.0 \\times 2^{-1022} \\approx \\pm 2.2 \\times 10^{-308}\\) , largest \\(\\pm 2.0 \\times 2^{1023} \\approx \\pm 1.8 \\times 10^{308}\\) Relative precision \u00b6 Single \\(2^{-23}\\) Equivalent to \\(23 \\times \\log_{10}2 \\approx 6\\) decimal digits of precision\uff086 \u4f4d\u6709\u6548\u6570\u5b57\uff09 Double \\(2^{-52}\\) Equivalent to \\(52 \\times \\log_{10}2 \\approx 16\\) decimal digits of precision\uff0816 \u4f4d\u6709\u6548\u6570\u5b57\uff09","title":"Chap 1"},{"location":"Mathematics_Basis/NA/Chap_1/#chapter-1-mathematical-preliminaries","text":"","title":"Chapter 1 | Mathematical Preliminaries"},{"location":"Mathematics_Basis/NA/Chap_1/#error","text":"","title":"Error"},{"location":"Mathematics_Basis/NA/Chap_1/#truncation-error","text":"the error involved using a truncated or finite summation.","title":"Truncation Error"},{"location":"Mathematics_Basis/NA/Chap_1/#roundoff-error","text":"the error produced when performing real number calculations. It occurs because the arithmetic performed in a machine involved numbers with a finite number of digits. Chopping & Rounding Given a real number \\(y = 0.d_1d_2\\dots d_kd_{k+1}\\dots \\times 10^N\\) , the floating-point representation of \\(y\\) is \\(fl(y)\\) : \\[ fl(y) = \\left\\{ \\begin{align} & 0.d_1d_2\\dots d_k \\times 10^N, & {\\tt Chopping} \\\\ & {\\tt chop}\\left(y + 5 \\times 10^{n - (k + 1)}\\right). & {\\tt Rounding} \\end{align} \\right. \\] Definition If \\(p^*\\) is an approximation to \\(p\\) , then absolute error is \\(|p - p*|\\) and relative error is \\(\\frac{|p - p^*|}{|p|}\\) . The number \\(p^*\\) is said to be approximate to \\(p\\) to \\(t\\) * significant digits if \\(t\\) is the largest nonnegative integer for which \\[ \\frac{|p - p^*|}{|p|} < 5 \\times 10^{-t} \\] Example For the floating-point representation of \\(y\\) , the relative error is \\[ \\left|\\frac{y - fl(y)}{y}\\right|. \\] for chopping representation, \\[ \\left|\\frac{y - fl(y)}{y}\\right| = \\left|\\frac{0.d_{k + 1}d_{k + 2}\\dots}{0.d_1d_2\\dots}\\right| \\times 10^{-k} \\le \\frac{1}{0.1} \\times 10^{-k} = 10^{-k + 1}. \\] for rounding representation, \\[ \\left|\\frac{y - fl(y)}{y}\\right| \\le \\frac{0.5}{0.1} \\times 10^{-k} = 0.5 \\times 10^{-k + 1}. \\]","title":"Roundoff Error"},{"location":"Mathematics_Basis/NA/Chap_1/#effect-of-error","text":"Subtraction may reduce significant digits. e.g. 0.1234 - 0.1233 = 0.001 Division by small number of multiplication by large number magnify the abosolute error without modifying the relative error.","title":"Effect of Error"},{"location":"Mathematics_Basis/NA/Chap_1/#some-solutions-to-reduce-error","text":"Quadratic Formula the roots of \\(ax^2 + bx + c = 0\\) is \\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}. \\] Sometimes \\(b\\) is closer to \\(\\sqrt{b^2 - 4ac}\\) , which may cause the subtraction to reduce significant digits. An alternate way is to modify the formula to \\[ x = \\frac{-2c}{b \\pm \\sqrt{b^2 - 4ac}}. \\] But it may cause the division by small number. So it's a tradeoff to use one of the two formulae above. Horner's Method \u79e6\u4e5d\u97f6\u7b97\u6cd5 \\[ \\begin{align} f(x) &= a_nx^n + a_{n-1}x^{n-1} + \\dots + a_1x + a_0 \\\\ &= (\\dots((a_nx+a_{n-1})x+a_{n-2})x+\\dots+a_1)x+a_0 \\end{align} \\]","title":"Some Solutions to Reduce Error"},{"location":"Mathematics_Basis/NA/Chap_1/#stable-algorithms-and-convergence","text":"Definition An algorithm that satisfies that small changes in the initial data produce correspondingly small changes in the final results is called stable ; otherwise it is unstable . An algorithm is called conditionally stable if it is stable only for certain choices of initial data. Suppose \\(E_0 > 0\\) denotes an initial errors, \\(E_n\\) denotes the magnitude of an error after \\(n\\) subsequent operations Linear growth of errors \\(E_n \\approx C n E_0\\) unavoidable and acceptable Exponential growth of errors \\(E_n \\approx C^n E_0\\) unacceptable Example the recursive equation \\(p_n = \\frac{10}{3}p_{n - 1} - p_{n - 2}\\) has the solution \\[ p_n = c_1 \\left(\\frac13\\right)^n + c_23^n. \\] If \\(p_0 = 1, p_1 = \\frac13\\) , then the solution is \\[ p_n = \\left(\\frac13\\right)^n. \\] Suppose we have five-digit rounding representation, then \\(\\hat p_0 = 1.0000\\) , \\(\\hat p_1 = 0.33333\\) , and the solution is \\[ \\hat p_n = 1.0000 \\left(\\frac13\\right)^n - 0.12500 \\times 10^{-5} \\cdot 3^n. \\] Then \\[ p_n - \\hat p_n = 0.12500 \\times 10^{-5} \\cdot 3^n \\] grow exponentially with \\(n\\) . On the other hand, the recursive equation \\(p_n = 2p_{n - 1} - p_{n - 2}\\) has the solution \\[ p_n = c_1 + c_2n \\] If \\(p_0 = 1, p_1 = \\frac13\\) , then the solution is \\[ p_n = 1 - \\frac23 n. \\] Suppose we have five-digit rounding representation, then \\(\\hat p_0 = 1.0000\\) , \\(\\hat p_1 = 0.33333\\) , and the solution is \\[ \\hat p_n = 1.0000 - 0.66667 n. \\] Then \\[ p_n - \\hat p_n = \\left(0.66667 - \\frac23\\right) n \\] grow linearly with \\(n\\) . Error of floating-point number (IEEE 754 standard)","title":"Stable Algorithms and Convergence"},{"location":"Mathematics_Basis/NA/Chap_1/#range-of-normal-representation","text":"Single-Precision Smallest \\(\\text{0/1\\ 00000001\\ 00\\dots00}\\) \\(\\pm 1.0 \\times 2^{-126} \\approx \\pm 1.2 \\times 10^{-38}\\) Largest \\(\\text{0/1\\ 11111110\\ 11\\dots11}\\) \\(\\pm 2.0 \\times 2^{127} \\approx \\pm 3.4 \\times 10^{38}\\) For Double-Precision, smallest \\(\\pm 1.0 \\times 2^{-1022} \\approx \\pm 2.2 \\times 10^{-308}\\) , largest \\(\\pm 2.0 \\times 2^{1023} \\approx \\pm 1.8 \\times 10^{308}\\)","title":"Range of normal representation"},{"location":"Mathematics_Basis/NA/Chap_1/#relative-precision","text":"Single \\(2^{-23}\\) Equivalent to \\(23 \\times \\log_{10}2 \\approx 6\\) decimal digits of precision\uff086 \u4f4d\u6709\u6548\u6570\u5b57\uff09 Double \\(2^{-52}\\) Equivalent to \\(52 \\times \\log_{10}2 \\approx 16\\) decimal digits of precision\uff0816 \u4f4d\u6709\u6548\u6570\u5b57\uff09","title":"Relative precision"},{"location":"Mathematics_Basis/NA/Chap_2/","text":"Chapter 2 | Solution of Equations in One Variable \u00b6 Bisection Method \u00b6 Theorem Suppose that \\(f \\in C[a, b]\\) and \\(f(a)\\cdot f(b) \\lt 0\\) . The Bisection method generates a sequence \\(\\{p_n\\}(n = 0, 1, 2,\\dots)\\) approximating a zero \\(p\\) of \\(f\\) with \\[ |p_n-p| \\le \\frac{b - a}{2^n}, \\text{when }n \\ge 1 \\] Key Points for Algorithm Implementation \u00b6 \\(mid = a + (b - a)\\ /\\ 2, \\text{but not } mid = (a + b)\\ /\\ 2\\) , for accuracy and not exceeding the limit of range. \\(sign(FA)\\cdot sign(FM)\\gt 0, \\text{but not }FA\\cdot FM \\gt 0\\) , for saving time. Pros & Cons \u00b6 Simple premise, only requires a continuous \\(f\\) . Always converges to a solution. Slow to converge, and a good intermediate approximation can be inadvertently discarded. Cannot find multiple roots and complex roots. Fixed-Point Iteration \u00b6 \\[ f(x) = 0 \\leftrightarrow x = g(x) \\] Fixed-Point Theorem Let \\(g \\in C[a,b]\\) be such that \\(g(x)\\in[a,b], \\forall x \\in [a, b]\\) . Suppose \\(g'\\) exists on \\((a,b)\\) and that a constant \\(k\\ (0\\lt k \\lt 1)\\) exists, s.t. \\[ \\forall\\ x \\in (a,b), |g'(x)| \\le k. \\] Then \\(\\forall\\ p_0 \\in[a,b]\\) , the sequence \\(\\{p_n\\}\\) defined by \\[ p_n=g(p_{n-1}) \\] converges to the unique fixed-point \\(p\\in [a,b]\\) . . Corollary \\[ \\begin{align} |p_n - p| &\\le k^n \\max\\{p_0 = a, b - p_0\\} \\\\ |p_n - p| &\\le \\frac{k^n}{1 - k}|p_1 - p_0| \\end{align} \\] \\(\\Rightarrow\\) The smaller the \\(k\\) is, the faster it converges. Newton's Method (Newton-Raphson Method) \u00b6 Newton's method is an improvement of common fixed-point iteration method above. Theorem Let \\(f\\in C^2[a,b]\\) and \\(\\exists\\ p\\in[a,b], s.t. f(p) = 0\\) and \\(f'(p) \\ne 0\\) , then \\(\\exists\\ \\delta \\gt 0, \\forall\\ p_0 \\in [p - \\epsilon, p + \\epsilon]\\) , s.t. the sequence \\(\\{p_n\\}_{n = 1}^\\infty\\) defined by \\[ p_n = p_{n-1} - \\frac{f(p_{n-1})}{f'(p_{n-1})} \\] converges to \\(p\\) . Error Analysis for Iterative Methods \u00b6 Definition Suppose \\(\\{p_n\\}\\) is a sequence that converges to \\(p\\) , and \\(\\forall n, p_n \\ne p\\) . If positive constants \\(\\alpha\\) and \\(\\lambda\\) exist with \\[ \\lim_{n \\rightarrow \\infty} \\frac{|p_{n + 1} - p|}{|p_n - p|^\\alpha} = \\lambda \\] then \\(\\{p_n\\}\\) conveges to \\(p\\) of order \\(\\alpha\\) , with asymptotic error constant \\(\\lambda\\) . Specially, If \\(\\alpha = 1\\) , the sequence is linearly convergent. If \\(\\alpha = 2\\) , the sequence is quadratically convergent. Theorem The common fixed-point iteration method ( \\(g'(p) \\ne 0\\) ) with the premise in Fixed-Point Theorem is linearly convergent. Proof \\[ \\lim_{n\\rightarrow \\infty}\\frac{|p_{n+1} - p|}{|p_n - p|} = \\lim_{n\\rightarrow \\infty}\\frac{g'(\\xi)|p_n - p|}{|p_n - p|} = |g'(p)| \\] Theorem The Newton's method \\((g'(p)=0)\\) is at least quadratically convergent. Proof \\[ \\begin{align} & 0 = f(p) = f(p_n) + f'(p_n)(p - p_n) + \\frac{f''(\\xi _n)}{2!}(p - p_n)^2 \\\\ & \\Rightarrow p = \\underbrace{p_n - \\frac{f(p_n)}{f\"(p_n)}}_{p_{n+1}} - \\frac{f''(\\xi _n)}{2!f'(p_n)}(p - p_n)^2 \\\\ & \\Rightarrow \\frac{|p_{n+1} - p|}{|p_n - p|^2} = \\frac{f''(\\xi _n)}{2f'(p_n)} \\\\ & \\lim_{n\\rightarrow \\infty}\\frac{|p_{n+1} - p|}{|p_n - p|^2} = \\frac{f''(p_n)}{2f'(p_n)} \\end{align} \\] More commonly, Theorem Let \\(p\\) be a fixed point of \\(g(x)\\) . If there exists some constant \\(\\alpha\\ge 2\\) such that \\(g\\in C^\\alpha [p-\\delta, p+\\delta],g'(p)=\\dots=g^{(\\alpha-1)}(p)=0, g^{(\\alpha)}(p)=0\\) . Then the iterations with \\(p_n = g(p_{n-1}),n\\ge 1\\) , is of order \\(\\alpha\\) . Multiple Roots Situation \u00b6 Notice that from the proof above, Newton's method is quadratically convergent if \\(f'(p_n) \\ne 0\\) . If \\(f'(p) = 0\\) , then the equation has multiple roots at \\(p\\) . If \\(f(x) = (x - p)^mq(x)\\) and \\(q(p)\\ne 0\\) , for Newton's Method, \\[ g'(p) = \\frac{f(p)f''(p)}{f'(p)^2} = \\left.\\frac{f(x)f''(x)}{f'(x)^2}\\right|_{x=p} = \\dots =1 - \\frac{1}{m} \\lt 1 \\] It is still convegent, but not quadratically . A way to speed it up Let \\[ \\mu (x) = \\frac{f(x)}{f'(x)}, \\] then \\(\\mu (x)\\) has the same root as \\(f(x)\\) but no multiple roots anymore. And \\[ g(x) = x - \\frac{\\mu (x)}{\\mu'(x)} = x - \\frac{f(x)f'(x)}{f'(x)^2 - f(x)f''(x)} \\] Newton's method can be used there again. Pros & Cons \u00b6 Quadratic convegence Requires additional calculation of \\(f''(x)\\) The denominator consists of the difference of the two numbers both close to 0. Accelerating Convergence \u00b6 Aitken's \\(\\Delta^2\\) Method \u00b6 Definition Forward Difference \\(\\Delta p_n = p_{n+1} - p_n\\) . Similarly \\(\\Delta^kp_n = \\Delta(\\Delta^{k-1}p_n)\\) Representing Aitken's \\(\\Delta^2\\) Method by forward difference, we have \\[ \\hat{p_n} = p_n - \\frac{(\\Delta p_n)^2}{\\Delta^2p_n}. \\] Theorem Suppose that \\(\\{p_n\\}\\) is a sequence, \\(\\lim_{n\\rightarrow \\infty}p_n = p\\) , \\(\\exists N\\) , s.t. \\(\\forall n > N, (p_n - p)(p_{n+1} -p) \\gt 0\\) . Then the sequence \\(\\{\\hat{p_n}\\}\\) converges to \\(p\\) faster than \\(\\{p_n\\}\\) in the sense that \\[ \\lim_{n\\rightarrow \\infty}\\frac{\\hat{p_n} - p}{p_n - p} = 0. \\] The algorithm to implement it is called Steffensen\u2019s Acceleration .","title":"Chap 2"},{"location":"Mathematics_Basis/NA/Chap_2/#chapter-2-solution-of-equations-in-one-variable","text":"","title":"Chapter 2 | Solution of Equations in One Variable"},{"location":"Mathematics_Basis/NA/Chap_2/#bisection-method","text":"Theorem Suppose that \\(f \\in C[a, b]\\) and \\(f(a)\\cdot f(b) \\lt 0\\) . The Bisection method generates a sequence \\(\\{p_n\\}(n = 0, 1, 2,\\dots)\\) approximating a zero \\(p\\) of \\(f\\) with \\[ |p_n-p| \\le \\frac{b - a}{2^n}, \\text{when }n \\ge 1 \\]","title":"Bisection Method"},{"location":"Mathematics_Basis/NA/Chap_2/#key-points-for-algorithm-implementation","text":"\\(mid = a + (b - a)\\ /\\ 2, \\text{but not } mid = (a + b)\\ /\\ 2\\) , for accuracy and not exceeding the limit of range. \\(sign(FA)\\cdot sign(FM)\\gt 0, \\text{but not }FA\\cdot FM \\gt 0\\) , for saving time.","title":"Key Points for Algorithm Implementation"},{"location":"Mathematics_Basis/NA/Chap_2/#pros-cons","text":"Simple premise, only requires a continuous \\(f\\) . Always converges to a solution. Slow to converge, and a good intermediate approximation can be inadvertently discarded. Cannot find multiple roots and complex roots.","title":"Pros &amp; Cons"},{"location":"Mathematics_Basis/NA/Chap_2/#fixed-point-iteration","text":"\\[ f(x) = 0 \\leftrightarrow x = g(x) \\] Fixed-Point Theorem Let \\(g \\in C[a,b]\\) be such that \\(g(x)\\in[a,b], \\forall x \\in [a, b]\\) . Suppose \\(g'\\) exists on \\((a,b)\\) and that a constant \\(k\\ (0\\lt k \\lt 1)\\) exists, s.t. \\[ \\forall\\ x \\in (a,b), |g'(x)| \\le k. \\] Then \\(\\forall\\ p_0 \\in[a,b]\\) , the sequence \\(\\{p_n\\}\\) defined by \\[ p_n=g(p_{n-1}) \\] converges to the unique fixed-point \\(p\\in [a,b]\\) . . Corollary \\[ \\begin{align} |p_n - p| &\\le k^n \\max\\{p_0 = a, b - p_0\\} \\\\ |p_n - p| &\\le \\frac{k^n}{1 - k}|p_1 - p_0| \\end{align} \\] \\(\\Rightarrow\\) The smaller the \\(k\\) is, the faster it converges.","title":"Fixed-Point Iteration"},{"location":"Mathematics_Basis/NA/Chap_2/#newtons-method-newton-raphson-method","text":"Newton's method is an improvement of common fixed-point iteration method above. Theorem Let \\(f\\in C^2[a,b]\\) and \\(\\exists\\ p\\in[a,b], s.t. f(p) = 0\\) and \\(f'(p) \\ne 0\\) , then \\(\\exists\\ \\delta \\gt 0, \\forall\\ p_0 \\in [p - \\epsilon, p + \\epsilon]\\) , s.t. the sequence \\(\\{p_n\\}_{n = 1}^\\infty\\) defined by \\[ p_n = p_{n-1} - \\frac{f(p_{n-1})}{f'(p_{n-1})} \\] converges to \\(p\\) .","title":"Newton's Method (Newton-Raphson Method)"},{"location":"Mathematics_Basis/NA/Chap_2/#error-analysis-for-iterative-methods","text":"Definition Suppose \\(\\{p_n\\}\\) is a sequence that converges to \\(p\\) , and \\(\\forall n, p_n \\ne p\\) . If positive constants \\(\\alpha\\) and \\(\\lambda\\) exist with \\[ \\lim_{n \\rightarrow \\infty} \\frac{|p_{n + 1} - p|}{|p_n - p|^\\alpha} = \\lambda \\] then \\(\\{p_n\\}\\) conveges to \\(p\\) of order \\(\\alpha\\) , with asymptotic error constant \\(\\lambda\\) . Specially, If \\(\\alpha = 1\\) , the sequence is linearly convergent. If \\(\\alpha = 2\\) , the sequence is quadratically convergent. Theorem The common fixed-point iteration method ( \\(g'(p) \\ne 0\\) ) with the premise in Fixed-Point Theorem is linearly convergent. Proof \\[ \\lim_{n\\rightarrow \\infty}\\frac{|p_{n+1} - p|}{|p_n - p|} = \\lim_{n\\rightarrow \\infty}\\frac{g'(\\xi)|p_n - p|}{|p_n - p|} = |g'(p)| \\] Theorem The Newton's method \\((g'(p)=0)\\) is at least quadratically convergent. Proof \\[ \\begin{align} & 0 = f(p) = f(p_n) + f'(p_n)(p - p_n) + \\frac{f''(\\xi _n)}{2!}(p - p_n)^2 \\\\ & \\Rightarrow p = \\underbrace{p_n - \\frac{f(p_n)}{f\"(p_n)}}_{p_{n+1}} - \\frac{f''(\\xi _n)}{2!f'(p_n)}(p - p_n)^2 \\\\ & \\Rightarrow \\frac{|p_{n+1} - p|}{|p_n - p|^2} = \\frac{f''(\\xi _n)}{2f'(p_n)} \\\\ & \\lim_{n\\rightarrow \\infty}\\frac{|p_{n+1} - p|}{|p_n - p|^2} = \\frac{f''(p_n)}{2f'(p_n)} \\end{align} \\] More commonly, Theorem Let \\(p\\) be a fixed point of \\(g(x)\\) . If there exists some constant \\(\\alpha\\ge 2\\) such that \\(g\\in C^\\alpha [p-\\delta, p+\\delta],g'(p)=\\dots=g^{(\\alpha-1)}(p)=0, g^{(\\alpha)}(p)=0\\) . Then the iterations with \\(p_n = g(p_{n-1}),n\\ge 1\\) , is of order \\(\\alpha\\) .","title":"Error Analysis for Iterative Methods"},{"location":"Mathematics_Basis/NA/Chap_2/#multiple-roots-situation","text":"Notice that from the proof above, Newton's method is quadratically convergent if \\(f'(p_n) \\ne 0\\) . If \\(f'(p) = 0\\) , then the equation has multiple roots at \\(p\\) . If \\(f(x) = (x - p)^mq(x)\\) and \\(q(p)\\ne 0\\) , for Newton's Method, \\[ g'(p) = \\frac{f(p)f''(p)}{f'(p)^2} = \\left.\\frac{f(x)f''(x)}{f'(x)^2}\\right|_{x=p} = \\dots =1 - \\frac{1}{m} \\lt 1 \\] It is still convegent, but not quadratically . A way to speed it up Let \\[ \\mu (x) = \\frac{f(x)}{f'(x)}, \\] then \\(\\mu (x)\\) has the same root as \\(f(x)\\) but no multiple roots anymore. And \\[ g(x) = x - \\frac{\\mu (x)}{\\mu'(x)} = x - \\frac{f(x)f'(x)}{f'(x)^2 - f(x)f''(x)} \\] Newton's method can be used there again.","title":"Multiple Roots Situation"},{"location":"Mathematics_Basis/NA/Chap_2/#pros-cons_1","text":"Quadratic convegence Requires additional calculation of \\(f''(x)\\) The denominator consists of the difference of the two numbers both close to 0.","title":"Pros &amp; Cons"},{"location":"Mathematics_Basis/NA/Chap_2/#accelerating-convergence","text":"","title":"Accelerating Convergence"},{"location":"Mathematics_Basis/NA/Chap_2/#aitkens-delta2-method","text":"Definition Forward Difference \\(\\Delta p_n = p_{n+1} - p_n\\) . Similarly \\(\\Delta^kp_n = \\Delta(\\Delta^{k-1}p_n)\\) Representing Aitken's \\(\\Delta^2\\) Method by forward difference, we have \\[ \\hat{p_n} = p_n - \\frac{(\\Delta p_n)^2}{\\Delta^2p_n}. \\] Theorem Suppose that \\(\\{p_n\\}\\) is a sequence, \\(\\lim_{n\\rightarrow \\infty}p_n = p\\) , \\(\\exists N\\) , s.t. \\(\\forall n > N, (p_n - p)(p_{n+1} -p) \\gt 0\\) . Then the sequence \\(\\{\\hat{p_n}\\}\\) converges to \\(p\\) faster than \\(\\{p_n\\}\\) in the sense that \\[ \\lim_{n\\rightarrow \\infty}\\frac{\\hat{p_n} - p}{p_n - p} = 0. \\] The algorithm to implement it is called Steffensen\u2019s Acceleration .","title":"Aitken's \\(\\Delta^2\\) Method"},{"location":"Mathematics_Basis/NA/Chap_3/","text":"Chapter 3 | Interpolation and Polynomial Approximation \u00b6 Lagrange Interpolation \u00b6 Suppose we have function \\(y = f(x)\\) with the given points \\((x_0, y_0), (x_1, y_1), \\dots, (x_n, y_n)\\) , and then construct a relatively simple approximating function \\(g(x) \\approx f(x)\\) . Theorem 3.1 If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers and \\(f\\) is a function with given values \\(f(x_0), \\dots, f(x_n)\\) , then a unique polynomial \\(P(x)\\) of degree at most \\(n\\) exists with \\[ P(x_k) = f(x_k), \\text{ for each } k = 0, 1, \\dots, n. \\] and \\[ P(x) = \\sum\\limits_{k = 0}^n f(x_k)L_{n, k}(x), \\] where, for each \\(k = 0, 1, \\dots, n\\) , \\[ L_{n, k}(x) = \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n \\frac{(x - x_i)}{(x_k - x_i)}. \\] \\(L_{n, k}(x)\\) is called the n th Lagrange interpolating polynomial . Proof First we prove for the structure of function \\(L_{n, k}(x)\\) . From the definition of \\(P(x)\\) , \\(L_{n, k}(x)\\) has the following properties \\(L_{n, k}(x_i) = 0\\) when \\(i \\ne k\\) . \\(L_{n, k}(x_k) = 1\\) . To satisfy the first property, the numerator of \\(L_{n, k}(x)\\) contains the term \\[ (x - x_0)(x - x_1) \\cdots (x - x_{k - 1})(x - x_{k + 1}) \\cdots (x - x_n) = \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n (x - x_i). \\] To satisfy the second property, the denominator of \\(L_{n, k}(x)\\) must be equal to the numerator at \\(x = x_k\\) , thus \\[ \\begin{align} L_{n, k}(x) &= \\frac{(x - x_0)(x - x_1) \\cdots (x - x_{k - 1})(x - x_{k + 1}) \\cdots (x - x_n)}{(x_k - x_0)(x - x_1) \\cdots (x_k - x_{k - 1})(x_k - x_{k + 1}) \\cdots (x_k - x_n)} \\\\ &= \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n \\frac{x - x_i}{x_k - x_i}. \\end{align} \\] For uniqueness, we prove by contradition. If not, suppose \\(P(x)\\) and \\(Q(x)\\) both satisfying the conditions, then \\(D(x) = P(x) - Q(x)\\) is a polynomial of degree \\(\\text{deg}(D(x)) \\le n\\) , but \\(D(x)\\) has \\(n + 1\\) distinct roots \\(x_0, x_1, \\dots, x_n\\) , which leads to a contradiction. Theorem 3.2 If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers in the interval \\([a, b]\\) and \\(f \\in C^{n + 1}[a, b]\\) . \\(\\forall x \\in [a, b], \\exists \\xi \\in [a, b], s.t.\\) \\[ f(x) = P(x) + \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] where \\(P(x)\\) is the Lagrange interpolating polynomial. And \\[ R(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] is the truncation error . Proof Since \\(R(x) = f(x) - P(x)\\) has at least \\(n + 1\\) roots, thus \\[ R(x) = K(x)\\prod\\limits_{i = 0}^n (x - x_i). \\] For a fixed \\(x \\ne x_k\\) , define \\(g(t)\\) in \\([a, b]\\) by \\[ g(t) = R(t) - K(x)\\prod\\limits_{i = 0}^n (t - x_i). \\] Since \\(g(t)\\) has \\(n + 2\\) distinct roots \\(x, x_0, \\dots, x_n\\) , by Generalized Rolle's Theorem, \\[ \\exists\\ \\xi \\in [a, b],\\ s.t.\\ g^{(n + 1)}(\\xi) = 0. \\] Namely, \\[ f^{(n + 1)}(\\xi) - \\underbrace{P^{n + 1}(\\xi)}_{0} - K(x)(n + 1)! = 0. \\] Thus \\[ K(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!},\\ R(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] Example Suppose a table is to be prepared for the function \\(f(x) = e^x\\) for \\(x\\) in \\([0, 1]\\) . Assume that each entry of the table is accurate up to 8 decimal places and the step size is \\(h\\) . What should \\(h\\) be for linear interpolation to give an absolute error of at most \\(10^{-6}\\) ? Solution. \\[ \\begin{align} |f(x) - P(x)| &= \\left|\\frac{f^{(2)}(\\xi)}{2!}(x - x_j)(x - x_{j + 1})\\right| \\\\ &= \\left|\\frac{e^\\xi}{2}(x - kh)(x - (k + 1)h)\\right| \\le \\frac{e}{2} \\cdot \\frac{h^2}{4} = \\frac{eh^2}{8}. \\end{align} \\] Thus let \\(\\frac{eh^2}{8} \\le 10^{-6}\\) , we have \\(h \\le 1.72 \\times 10^{-3}\\) . To make \\(N = (1 - 0) / h\\) an integer, we can simply choose \\(h = 0.001\\) . Extrapolation Suppose \\(a = \\min\\limits_i \\{x_i\\}\\) , \\(b = \\max\\limits_i \\{x_i\\}\\) . Interpolation estimates value \\(P(x)\\) , \\(x \\in [a, b]\\) , while Extrapolation estimates value \\(P(x)\\) , \\(x \\notin [a, b]\\) . In genernal, interpolation is better than extrapolation. Neville's Method \u00b6 Motivation: When we have more interpolating points, the original Lagrange interpolating method should re-calculate all \\(L_{n, k}\\) , which is not efficient. Definition Let \\(f\\) be a function defined at \\(x_0, x_1, \\dots, x_n\\) , and suppose that \\(m_1, \\dots, m_k\\) are \\(k\\) distinct integers with \\(0 \\le m_i \\le n\\) for each \\(i\\) . The Lagrange polynomial that agrees with \\(f(x)\\) at the \\(k\\) points denoted by \\(P_{m_1, m_2, \\dots, m_k}(x)\\) . Thus \\(P(x) = P_{0, 1, \\dots, n}(x)\\) , where \\(P(x)\\) is the n th Lagrange polynomials that interpolate \\(f\\) at \\(k + 1\\) points \\(x_0, \\dots, x_k\\) . Theorem 3.3 Let \\(f\\) be a function defined at \\(x_0, x_1, \\dots, x_n\\) , and \\(x_i \\ne x_j\\) , then \\[ P(x) = \\frac{(x - x_j)P_{0, 1, \\dots, j - 1, j + 1, \\dots k}(x) - (x - x_i)P_{0, 1, \\dots, i - 1, i + 1, \\dots k}(x)}{(x_i - x_j)}. \\] Denote that \\[ Q_{i, j} = P_{i - j, i - j + 1, \\dots, i - 1, i}, \\] then from the theorem above, the interpolating polynomials can be generated recursively . Newton Interpolation \u00b6 Differing from Langrange polynomials, we try to represent \\(P(x)\\) by the following form: \\[ \\begin{align} N(x) = &\\ a_0 + a_1(x - x_0) + a_2(x - x_0)(x - x_1) + \\cdots \\\\ & + a_n(x - x_0)(x - x_1) \\cdots (x - x_{n - 1}). \\end{align} \\] Definition \\(f[x_i] = f(x_i)\\) is the zeroth divided difference w.r.t. \\(x_i\\) . The k th divided difference w.r.t. \\(x_i, x_{i + 1}, x_{i + k}\\) is defined recursively by \\[ f[x_i, x_{i + 1}, \\dots, x_{i + k - 1}, x_{i + k}] = \\frac{f[x_{i + 1}, x_{i + 2}, \\dots, x_{i + k}] - f[x_i, x_{i + 1}, \\dots, x_{i + k - 1}]}{x_{i + k} - x_i} \\] Then we derive the Newton's interpolatory divided-difference formula : \\[ N(x) = f[x_0] + \\sum\\limits_{k = 1}^n f[x_0, x_1, \\dots, x_k]\\prod\\limits_{i = 0}^{k - 1}(x - x_i). \\] And the divided difference can be generated as below, which is similar to Neville's Method. Theorem 3.4 If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers in the interval \\([a, b]\\) and \\(f \\in C^{n + 1}[a, b]\\) . \\(\\forall x \\in [a, b], \\exists \\xi \\in [a, b], s.t.\\) \\[ f(x) = N(x) + f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i). \\] where \\(N(x)\\) is the Newton's interpolatory divided-difference formula. And \\[ R(x) = f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i). \\] is the truncation error . Proof By definition of divided difference, we have \\[ \\left\\{ \\begin{align} f(x) &= f[x_0] + (x - x_0)f[x, x_0], & \\textbf{Eq.1} \\\\ f[x, x_0] &= f[x_0, x_1] + (x - x_1)f[x, x_0, x_1], & \\textbf{Eq.2} \\\\ & \\vdots \\\\ f[x, x_0, \\dots, x_{n - 1}] &= f[x_0, \\dots, x_n] + (x - x_n)f[x, x_0, \\dots, x_n]. & \\textbf{Eq.n - 1} \\end{align} \\right. \\] then compute \\[ \\textbf{Eq.1} + (x - x_0) \\times \\textbf{Eq.2} + \\cdots + (x - x_0) \\cdots (x - x_{n - 1}) \\times \\textbf{Eq.n - 1}. \\] i.e. \\[ f(x) = N(x) + \\underbrace{f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i)}_{R(x)}. \\] Note Since the uniqueness of n -th interpolating polynomial, \\(N(x) \\equiv P(x)\\) . They have the same truncation error, which is \\[ f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] Theorem 3.5 Suppose that \\(f \\in C^n[a, b]\\) and \\(x_0, x_1, \\dots, x_n\\) are distinct numbers in \\([a, b]\\) . Then \\(\\exists\\ \\xi \\in (a, b)\\) , s.t. \\[ f[x_0, x_1, \\dots, x_n] = \\frac{f^{(n)}(\\xi)}{n!}. \\] Special Case: Equal Spacing \u00b6 Definition Forward Difference \\[ \\begin{align} \\Delta f_i &= f_{i + 1} - f_i \\\\ \\Delta^k f_i &= \\Delta\\left(\\Delta^{k - 1} f_i \\right) = \\Delta^{k - 1}f_{i + 1} - \\Delta^{k - 1}f_i. \\end{align} \\] Backward Difference \\[ \\begin{align} \\nabla f_i &= f_i - f_{i - 1} \\\\ \\nabla^k f_i &= \\nabla\\left(\\nabla^{k - 1} f_i \\right) = \\nabla^{k - 1}f_i - \\nabla^{k - 1}f_{i - 1}. \\end{align} \\] Property Linearity: \\(\\Delta (af(x) + bg(x)) = a \\Delta f + b \\Delta g\\) . If \\(\\text{deg}(f(x)) = m\\) , then \\[ \\text{deg}\\left(\\Delta^kf(x)\\right) = \\left\\{ \\begin{align} & m - k, & 0 \\le k \\le m \\\\ & 0, & k > m \\end{align} \\right. \\] Decompose the recursive definition, \\[ \\begin{align} \\Delta^n f_k &= \\sum\\limits_{j = 0}^n (-1)^j \\binom{n}{j} f_{n + k - j} \\\\ \\nabla^n f_k &= \\sum\\limits_{j = 0}^n (-1)^{n - j} \\binom{n}{j} f_{j + k - n} \\end{align} \\] Vice versa, \\[ f_{n + k} = \\sum\\limits_{j = 0}^n \\binom{n}{j} \\Delta^j f_k \\] Suppose \\(x_0, x_1, \\dots x_n\\) are equally spaced, namely \\(x_i = x_0 + ih\\) . And let \\(x = x_0 + sh\\) , then \\(x - x_i = (s - i)h\\) . Thus \\[ \\begin{align} N(x) &= f[x_0] + \\sum\\limits_{k = 1}^n s(s - 1) \\cdots (s - k + 1) h^k f[x_0, x_1, \\dots, x_k] \\\\ &= f[x_0] + \\sum\\limits_{k = 1}^n \\binom{s}{k} k! h^k f[x_0, x_1, \\dots, x_k] \\end{align} \\] is called Newton forward divided-difference formula . From mathematical induction, we can derive that \\[ f[x_0, x_1, \\dots, x_k] = \\frac{1}{k!h^k}\\Delta^k f(x_0). \\] Thus we get the Newton Forward-Difference Formula . \\[ N(x) = f[x_0] + \\sum\\limits_{k = 1}^n \\binom{s}{k} \\Delta^k f(x_0). \\] Inversely, \\(x = x_n + sh\\) , then \\(x - x_i = (s + n - i)h\\) , Thus \\[ \\begin{align} N(x) &= f[x_n] + \\sum\\limits_{k = 1}^n s(s + 1) \\cdots (s + k - 1) h^k f[x_n, x_{n - 1}, \\dots, x_{n - k}] \\\\ &= f[x_n] + \\sum\\limits_{k = 1}^n (-1)^k \\binom{-s}{k} k! h^k f[x_n, x_{n - 1}, \\dots, x_{n - k}]. \\end{align} \\] is called Newton backward divided-difference formula . From mathematical induction, we can derive that \\[ f[x_n, x_{n - 1}, \\dots, x_0] = \\frac{1}{k!h^k}\\nabla^k f(x_0). \\] Thus we get the Newton Backward-Difference Formula . \\[ N(x) = f[x_0] + \\sum\\limits_{k = 1}^n (-1)^k \\binom{-s}{k} \\nabla^k f(x_0). \\] Hermit Interpolation \u00b6 Definition Let \\(x_0, x_1, \\dots, x_n\\) be \\(n + 1\\) distinct numbers in \\([a, b]\\) and \\(m_i \\in \\mathbb{N}\\) . Suppose that \\(f \\in C^m[a, b]\\) , where \\(m = \\max \\{m_i\\}\\) . The osculating polynomial approximating \\(f\\) is the polynomial \\(P(x)\\) of least degree such that \\[ \\frac{d^k P(x_i)}{dx^k} = \\frac{d^k f(x_i)}{dx^k}, \\text{ for each } i = 0, 1, \\dots, n \\text{ and } k = 0, 1, \\dots, m_i. \\] From definition above, we know that when \\(m_i = 0\\) for each \\(i\\) , it's the n -th Lagrange polynomial. And the cases that \\(m_i = 1\\) for each \\(i\\) , then it's Hermit Polynomials . Theorem 3.6 If \\(f \\in C^1[a, b]\\) and \\(x_0, \\dots, x_n \\in [a, b]\\) are distinct, the unique polynomial of least degree agreeing with \\(f\\) and \\(f'\\) at \\(x_0, \\dots, x_n\\) is the Hermit Polynomial of degree at most 2n + 1 defined by \\[ H_{2n + 1}(x) = \\sum\\limits_{j = 0}^n f(x_j) H_{n ,j}(x) + \\sum\\limits_{j = 0}^n f'(x_j) \\hat H_{n, j}(x), \\] where \\[ H_{n, j}(x) = [1 - 2(x - x_j)L'_{n, j}(x_j)]L^2_{n, j}(x) \\] and \\[ \\hat H_{n, j}(x) = (x - x_j)L^2_{n, j}(x). \\] Moreover, if \\(f \\in C^{2n + 2}[a, b]\\) , then \\(\\exists\\ \\xi \\in (a, b)\\) , s.t. \\[ f(x) = H_{2n + 1}(x) + \\underbrace{\\frac{1}{(2n + 2)!}\\prod\\limits_{i = 0}^n (x - x_i)^2 f^{2n + 2}(\\xi)}_{R(x)}. \\] The theorem above gives a complete description of Hermit interpolation. But in pratice, to compute \\(H_{2n + 1}(x)\\) through the formula above is tedious . To make it compute easier, we introduce a method that is similar to Newton's interpolation. Define the sequence \\(\\{z_k\\}_{0}^{2n + 1}\\) by \\[ z_{2i} = z_{2i + 1} = x_i. \\] Based on the Theorem 3.5 , we redefine that \\[ f[z_{2i}, z_{2i + 1}] = f'(z_{2i}) = f'(x_i). \\] Then Hermite polynomial can be represented by \\[ H_{2n + 1}(x) = f[z_0] + \\sum\\limits_{k = 1}^{2n + 1} f[z_0, \\dots, z_k]\\prod\\limits_{i = 0}^{k - 1}(x - z_i). \\] Cubic Spline Interpolation \u00b6 Motivation: For osculating polynomial approximation, we can let \\(m_i\\) be bigger to get high-degree polynomials. It can somehow be better but higher degree tends to causes a fluctuation or say overfitting . An alternative approach is to divide the interval into subintervals and approximate them respectively, which is called piecewise-polynomial approximation . The most common piecewise-polynomial approximation uses cubic polynomials called cubic spline approximation . Definition Given a function \\(f\\) defined on \\([a, b]\\) and a set of nodes \\(a = x_0 < x_1 < \\cdots < x_n = b\\) , a cubic spline interpolant \\(S\\) for \\(f\\) is a function that satisfies the following conditions: \\(S(x)\\) is a cubic polynomial, denoted \\(S_j(x)\\) , on the subinterval \\([x_j, x_j + 1]\\) , for each \\(j = 0, 1, \\dots, n - 1\\) ; \\(S(x_j) = f(x_j)\\) for each \\(j = 0, 1, \\dots, n\\) ; \\(S_{j + 1}(x_{j + 1}) = S_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\) ; \\(S'_{j + 1}(x_{j + 1}) = S'_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\) ; \\(S''_{j + 1}(x_{j + 1}) = S''_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\) ; One of the following sets of boundary conditions: \\(S''(x_0) = S''(x_n) = 0\\) ( free or natural boundary ); \\(S'(x_0) = f'(x_0)\\) and \\(S'(x_n) = f'(x_n)\\) ( clamped boundary ). The spline of natural boundary is called natural spline . Theorem 3.7 The cubic spline interpolation of either natural boundary or clamped boundary is unique . (Since the coefficient matrix \\(A\\) is strictly diagonally dominant.) Suppose interpolation function in each subinterval is \\[ S_j(x) = a_j + b_j(x - x_j) + c_j(x - x_j)^2 + d_j(x - x_j)^3. \\] From the conditions in the definition above, by some algebraic process, we can derive the solution with the following equations, \\[ \\begin{align} h_j &= x_{j + 1} - x_j; \\\\ a_j &= f(x_j); \\\\ b_j &= \\frac{1}{h_j}(a_{j + 1}) - \\frac{h_j}{3}(2c_j + c_{j + 1}); \\\\ d_j &= \\frac{1}{3h_j}{c_{j + 1} - c_j}. \\end{align} \\] While \\(c_j\\) is given by solving the following linear system, \\[ A\\mathbf{x} = \\mathbf{b}, \\] where \\[ \\small A = \\begin{bmatrix} 1 & 0 & 0 & \\cdots & \\cdots & 0 \\\\ h_0 & 2(h_0 + h_1) & h_1 & \\cdots & \\cdots & \\vdots \\\\ 0 & h_1 & 2(h_1 + h_2) & h_2 & \\ddots & \\vdots \\\\ \\vdots & \\ddots & \\ddots & \\ddots & \\ddots & 0 \\\\ \\vdots & \\ddots & \\ddots & h_{n - 2} & 2(h_{n - 2} + h_{n - 1}) & h_{n - 1} \\\\ 0 & \\cdots & \\cdots & 0 & 0 & 1 \\\\ \\end{bmatrix} , \\mathbf{x} = \\begin{bmatrix} c_0 \\\\ c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\] For natural boundary, \\[ \\small \\mathbf{b} = \\begin{bmatrix} 0 \\\\ \\frac{3}{h_1}(a_2 - a_1) - \\frac{3}{h_0}(a_1 - a_0) \\\\ \\vdots \\\\ \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) - \\frac{3}{h_{n - 2}}\\ (a_{n - 1} - a_{n - 2}) \\\\ 0 \\end{bmatrix} \\] For clamped boundary, \\[ \\small \\mathbf{b} = \\begin{bmatrix} \\frac{3}{h_0}(a_1 - a_0) - 3f'(a) \\\\ \\frac{3}{h_1}(a_2 - a_1) - \\frac{3}{h_0}(a_1 - a_0) \\\\ \\vdots \\\\ \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) - \\frac{3}{h_{n - 2}}\\ (a_{n - 1} - a_{n - 2}) \\\\ 3f'(b) - \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) \\\\ \\end{bmatrix} \\] Note If \\(f \\in C[a, b]\\) and \\(\\frac{\\max h_i}{\\min h_i} \\le C < \\infty\\) . Then \\(S(x)\\ \\overset{\\text{uniform}}{\\longrightarrow}\\ f(x)\\) when \\(\\max h_i \\rightarrow 0\\) . That is the accuracy of approximation can be improved by adding more nodes without increasing the degree of the splines. Curves \u00b6 We've discussed the interpolation of functions above, but we may encounter the case to interpolate a curve. Straightforward Technique \u00b6 For given points \\((x_0, y_0), (x_1, y_1), \\dots, (x_n, y_n)\\) , we can construct two approximation functions with \\[ x_i = x(t_i),\\ y_i = y(t_i). \\] The interpolation method can be Lagrange, Hermite and Cubic spline, whatever. Bezier Curve \u00b6 In nature, it's piecewise cubic Hermite polynomial , and the curve is call Bezier curve . Similarly, suppose two function \\(x(t)\\) and \\(y(t)\\) at each interval. We have the following condtions. \\[ x(0) = x_0,\\ x(1) = x_1,\\ x'(0) = \\alpha_0,\\ x'(1) = \\alpha_1. \\] \\[ y(0) = y_0,\\ y(1) = y_1,\\ y'(0) = \\beta_0,\\ y'(1) = \\beta_1. \\] The solution is \\[ \\begin{align} x(t) &= [2(x_0 - x_1) + (\\alpha_0 + \\alpha_1)]t^3 + [3(x_1 - x_0) - (\\alpha_1 + 2\\alpha_0)]t^2 + \\alpha_0 t + x_0. \\\\ y(t) &= [2(y_0 - y_1) + (\\beta_0 + \\beta_1)]t^3 + [3(y_1 - y_0) - (\\beta_1 + 2\\beta_0)]t^2 + \\alpha_0 t + y_0. \\end{align} \\]","title":"Chap 3"},{"location":"Mathematics_Basis/NA/Chap_3/#chapter-3-interpolation-and-polynomial-approximation","text":"","title":"Chapter 3 | Interpolation and Polynomial Approximation"},{"location":"Mathematics_Basis/NA/Chap_3/#lagrange-interpolation","text":"Suppose we have function \\(y = f(x)\\) with the given points \\((x_0, y_0), (x_1, y_1), \\dots, (x_n, y_n)\\) , and then construct a relatively simple approximating function \\(g(x) \\approx f(x)\\) . Theorem 3.1 If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers and \\(f\\) is a function with given values \\(f(x_0), \\dots, f(x_n)\\) , then a unique polynomial \\(P(x)\\) of degree at most \\(n\\) exists with \\[ P(x_k) = f(x_k), \\text{ for each } k = 0, 1, \\dots, n. \\] and \\[ P(x) = \\sum\\limits_{k = 0}^n f(x_k)L_{n, k}(x), \\] where, for each \\(k = 0, 1, \\dots, n\\) , \\[ L_{n, k}(x) = \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n \\frac{(x - x_i)}{(x_k - x_i)}. \\] \\(L_{n, k}(x)\\) is called the n th Lagrange interpolating polynomial . Proof First we prove for the structure of function \\(L_{n, k}(x)\\) . From the definition of \\(P(x)\\) , \\(L_{n, k}(x)\\) has the following properties \\(L_{n, k}(x_i) = 0\\) when \\(i \\ne k\\) . \\(L_{n, k}(x_k) = 1\\) . To satisfy the first property, the numerator of \\(L_{n, k}(x)\\) contains the term \\[ (x - x_0)(x - x_1) \\cdots (x - x_{k - 1})(x - x_{k + 1}) \\cdots (x - x_n) = \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n (x - x_i). \\] To satisfy the second property, the denominator of \\(L_{n, k}(x)\\) must be equal to the numerator at \\(x = x_k\\) , thus \\[ \\begin{align} L_{n, k}(x) &= \\frac{(x - x_0)(x - x_1) \\cdots (x - x_{k - 1})(x - x_{k + 1}) \\cdots (x - x_n)}{(x_k - x_0)(x - x_1) \\cdots (x_k - x_{k - 1})(x_k - x_{k + 1}) \\cdots (x_k - x_n)} \\\\ &= \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n \\frac{x - x_i}{x_k - x_i}. \\end{align} \\] For uniqueness, we prove by contradition. If not, suppose \\(P(x)\\) and \\(Q(x)\\) both satisfying the conditions, then \\(D(x) = P(x) - Q(x)\\) is a polynomial of degree \\(\\text{deg}(D(x)) \\le n\\) , but \\(D(x)\\) has \\(n + 1\\) distinct roots \\(x_0, x_1, \\dots, x_n\\) , which leads to a contradiction. Theorem 3.2 If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers in the interval \\([a, b]\\) and \\(f \\in C^{n + 1}[a, b]\\) . \\(\\forall x \\in [a, b], \\exists \\xi \\in [a, b], s.t.\\) \\[ f(x) = P(x) + \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] where \\(P(x)\\) is the Lagrange interpolating polynomial. And \\[ R(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] is the truncation error . Proof Since \\(R(x) = f(x) - P(x)\\) has at least \\(n + 1\\) roots, thus \\[ R(x) = K(x)\\prod\\limits_{i = 0}^n (x - x_i). \\] For a fixed \\(x \\ne x_k\\) , define \\(g(t)\\) in \\([a, b]\\) by \\[ g(t) = R(t) - K(x)\\prod\\limits_{i = 0}^n (t - x_i). \\] Since \\(g(t)\\) has \\(n + 2\\) distinct roots \\(x, x_0, \\dots, x_n\\) , by Generalized Rolle's Theorem, \\[ \\exists\\ \\xi \\in [a, b],\\ s.t.\\ g^{(n + 1)}(\\xi) = 0. \\] Namely, \\[ f^{(n + 1)}(\\xi) - \\underbrace{P^{n + 1}(\\xi)}_{0} - K(x)(n + 1)! = 0. \\] Thus \\[ K(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!},\\ R(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] Example Suppose a table is to be prepared for the function \\(f(x) = e^x\\) for \\(x\\) in \\([0, 1]\\) . Assume that each entry of the table is accurate up to 8 decimal places and the step size is \\(h\\) . What should \\(h\\) be for linear interpolation to give an absolute error of at most \\(10^{-6}\\) ? Solution. \\[ \\begin{align} |f(x) - P(x)| &= \\left|\\frac{f^{(2)}(\\xi)}{2!}(x - x_j)(x - x_{j + 1})\\right| \\\\ &= \\left|\\frac{e^\\xi}{2}(x - kh)(x - (k + 1)h)\\right| \\le \\frac{e}{2} \\cdot \\frac{h^2}{4} = \\frac{eh^2}{8}. \\end{align} \\] Thus let \\(\\frac{eh^2}{8} \\le 10^{-6}\\) , we have \\(h \\le 1.72 \\times 10^{-3}\\) . To make \\(N = (1 - 0) / h\\) an integer, we can simply choose \\(h = 0.001\\) . Extrapolation Suppose \\(a = \\min\\limits_i \\{x_i\\}\\) , \\(b = \\max\\limits_i \\{x_i\\}\\) . Interpolation estimates value \\(P(x)\\) , \\(x \\in [a, b]\\) , while Extrapolation estimates value \\(P(x)\\) , \\(x \\notin [a, b]\\) . In genernal, interpolation is better than extrapolation.","title":"Lagrange Interpolation"},{"location":"Mathematics_Basis/NA/Chap_3/#nevilles-method","text":"Motivation: When we have more interpolating points, the original Lagrange interpolating method should re-calculate all \\(L_{n, k}\\) , which is not efficient. Definition Let \\(f\\) be a function defined at \\(x_0, x_1, \\dots, x_n\\) , and suppose that \\(m_1, \\dots, m_k\\) are \\(k\\) distinct integers with \\(0 \\le m_i \\le n\\) for each \\(i\\) . The Lagrange polynomial that agrees with \\(f(x)\\) at the \\(k\\) points denoted by \\(P_{m_1, m_2, \\dots, m_k}(x)\\) . Thus \\(P(x) = P_{0, 1, \\dots, n}(x)\\) , where \\(P(x)\\) is the n th Lagrange polynomials that interpolate \\(f\\) at \\(k + 1\\) points \\(x_0, \\dots, x_k\\) . Theorem 3.3 Let \\(f\\) be a function defined at \\(x_0, x_1, \\dots, x_n\\) , and \\(x_i \\ne x_j\\) , then \\[ P(x) = \\frac{(x - x_j)P_{0, 1, \\dots, j - 1, j + 1, \\dots k}(x) - (x - x_i)P_{0, 1, \\dots, i - 1, i + 1, \\dots k}(x)}{(x_i - x_j)}. \\] Denote that \\[ Q_{i, j} = P_{i - j, i - j + 1, \\dots, i - 1, i}, \\] then from the theorem above, the interpolating polynomials can be generated recursively .","title":"Neville's Method"},{"location":"Mathematics_Basis/NA/Chap_3/#newton-interpolation","text":"Differing from Langrange polynomials, we try to represent \\(P(x)\\) by the following form: \\[ \\begin{align} N(x) = &\\ a_0 + a_1(x - x_0) + a_2(x - x_0)(x - x_1) + \\cdots \\\\ & + a_n(x - x_0)(x - x_1) \\cdots (x - x_{n - 1}). \\end{align} \\] Definition \\(f[x_i] = f(x_i)\\) is the zeroth divided difference w.r.t. \\(x_i\\) . The k th divided difference w.r.t. \\(x_i, x_{i + 1}, x_{i + k}\\) is defined recursively by \\[ f[x_i, x_{i + 1}, \\dots, x_{i + k - 1}, x_{i + k}] = \\frac{f[x_{i + 1}, x_{i + 2}, \\dots, x_{i + k}] - f[x_i, x_{i + 1}, \\dots, x_{i + k - 1}]}{x_{i + k} - x_i} \\] Then we derive the Newton's interpolatory divided-difference formula : \\[ N(x) = f[x_0] + \\sum\\limits_{k = 1}^n f[x_0, x_1, \\dots, x_k]\\prod\\limits_{i = 0}^{k - 1}(x - x_i). \\] And the divided difference can be generated as below, which is similar to Neville's Method. Theorem 3.4 If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers in the interval \\([a, b]\\) and \\(f \\in C^{n + 1}[a, b]\\) . \\(\\forall x \\in [a, b], \\exists \\xi \\in [a, b], s.t.\\) \\[ f(x) = N(x) + f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i). \\] where \\(N(x)\\) is the Newton's interpolatory divided-difference formula. And \\[ R(x) = f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i). \\] is the truncation error . Proof By definition of divided difference, we have \\[ \\left\\{ \\begin{align} f(x) &= f[x_0] + (x - x_0)f[x, x_0], & \\textbf{Eq.1} \\\\ f[x, x_0] &= f[x_0, x_1] + (x - x_1)f[x, x_0, x_1], & \\textbf{Eq.2} \\\\ & \\vdots \\\\ f[x, x_0, \\dots, x_{n - 1}] &= f[x_0, \\dots, x_n] + (x - x_n)f[x, x_0, \\dots, x_n]. & \\textbf{Eq.n - 1} \\end{align} \\right. \\] then compute \\[ \\textbf{Eq.1} + (x - x_0) \\times \\textbf{Eq.2} + \\cdots + (x - x_0) \\cdots (x - x_{n - 1}) \\times \\textbf{Eq.n - 1}. \\] i.e. \\[ f(x) = N(x) + \\underbrace{f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i)}_{R(x)}. \\] Note Since the uniqueness of n -th interpolating polynomial, \\(N(x) \\equiv P(x)\\) . They have the same truncation error, which is \\[ f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] Theorem 3.5 Suppose that \\(f \\in C^n[a, b]\\) and \\(x_0, x_1, \\dots, x_n\\) are distinct numbers in \\([a, b]\\) . Then \\(\\exists\\ \\xi \\in (a, b)\\) , s.t. \\[ f[x_0, x_1, \\dots, x_n] = \\frac{f^{(n)}(\\xi)}{n!}. \\]","title":"Newton Interpolation"},{"location":"Mathematics_Basis/NA/Chap_3/#special-case-equal-spacing","text":"Definition Forward Difference \\[ \\begin{align} \\Delta f_i &= f_{i + 1} - f_i \\\\ \\Delta^k f_i &= \\Delta\\left(\\Delta^{k - 1} f_i \\right) = \\Delta^{k - 1}f_{i + 1} - \\Delta^{k - 1}f_i. \\end{align} \\] Backward Difference \\[ \\begin{align} \\nabla f_i &= f_i - f_{i - 1} \\\\ \\nabla^k f_i &= \\nabla\\left(\\nabla^{k - 1} f_i \\right) = \\nabla^{k - 1}f_i - \\nabla^{k - 1}f_{i - 1}. \\end{align} \\] Property Linearity: \\(\\Delta (af(x) + bg(x)) = a \\Delta f + b \\Delta g\\) . If \\(\\text{deg}(f(x)) = m\\) , then \\[ \\text{deg}\\left(\\Delta^kf(x)\\right) = \\left\\{ \\begin{align} & m - k, & 0 \\le k \\le m \\\\ & 0, & k > m \\end{align} \\right. \\] Decompose the recursive definition, \\[ \\begin{align} \\Delta^n f_k &= \\sum\\limits_{j = 0}^n (-1)^j \\binom{n}{j} f_{n + k - j} \\\\ \\nabla^n f_k &= \\sum\\limits_{j = 0}^n (-1)^{n - j} \\binom{n}{j} f_{j + k - n} \\end{align} \\] Vice versa, \\[ f_{n + k} = \\sum\\limits_{j = 0}^n \\binom{n}{j} \\Delta^j f_k \\] Suppose \\(x_0, x_1, \\dots x_n\\) are equally spaced, namely \\(x_i = x_0 + ih\\) . And let \\(x = x_0 + sh\\) , then \\(x - x_i = (s - i)h\\) . Thus \\[ \\begin{align} N(x) &= f[x_0] + \\sum\\limits_{k = 1}^n s(s - 1) \\cdots (s - k + 1) h^k f[x_0, x_1, \\dots, x_k] \\\\ &= f[x_0] + \\sum\\limits_{k = 1}^n \\binom{s}{k} k! h^k f[x_0, x_1, \\dots, x_k] \\end{align} \\] is called Newton forward divided-difference formula . From mathematical induction, we can derive that \\[ f[x_0, x_1, \\dots, x_k] = \\frac{1}{k!h^k}\\Delta^k f(x_0). \\] Thus we get the Newton Forward-Difference Formula . \\[ N(x) = f[x_0] + \\sum\\limits_{k = 1}^n \\binom{s}{k} \\Delta^k f(x_0). \\] Inversely, \\(x = x_n + sh\\) , then \\(x - x_i = (s + n - i)h\\) , Thus \\[ \\begin{align} N(x) &= f[x_n] + \\sum\\limits_{k = 1}^n s(s + 1) \\cdots (s + k - 1) h^k f[x_n, x_{n - 1}, \\dots, x_{n - k}] \\\\ &= f[x_n] + \\sum\\limits_{k = 1}^n (-1)^k \\binom{-s}{k} k! h^k f[x_n, x_{n - 1}, \\dots, x_{n - k}]. \\end{align} \\] is called Newton backward divided-difference formula . From mathematical induction, we can derive that \\[ f[x_n, x_{n - 1}, \\dots, x_0] = \\frac{1}{k!h^k}\\nabla^k f(x_0). \\] Thus we get the Newton Backward-Difference Formula . \\[ N(x) = f[x_0] + \\sum\\limits_{k = 1}^n (-1)^k \\binom{-s}{k} \\nabla^k f(x_0). \\]","title":"Special Case: Equal Spacing"},{"location":"Mathematics_Basis/NA/Chap_3/#hermit-interpolation","text":"Definition Let \\(x_0, x_1, \\dots, x_n\\) be \\(n + 1\\) distinct numbers in \\([a, b]\\) and \\(m_i \\in \\mathbb{N}\\) . Suppose that \\(f \\in C^m[a, b]\\) , where \\(m = \\max \\{m_i\\}\\) . The osculating polynomial approximating \\(f\\) is the polynomial \\(P(x)\\) of least degree such that \\[ \\frac{d^k P(x_i)}{dx^k} = \\frac{d^k f(x_i)}{dx^k}, \\text{ for each } i = 0, 1, \\dots, n \\text{ and } k = 0, 1, \\dots, m_i. \\] From definition above, we know that when \\(m_i = 0\\) for each \\(i\\) , it's the n -th Lagrange polynomial. And the cases that \\(m_i = 1\\) for each \\(i\\) , then it's Hermit Polynomials . Theorem 3.6 If \\(f \\in C^1[a, b]\\) and \\(x_0, \\dots, x_n \\in [a, b]\\) are distinct, the unique polynomial of least degree agreeing with \\(f\\) and \\(f'\\) at \\(x_0, \\dots, x_n\\) is the Hermit Polynomial of degree at most 2n + 1 defined by \\[ H_{2n + 1}(x) = \\sum\\limits_{j = 0}^n f(x_j) H_{n ,j}(x) + \\sum\\limits_{j = 0}^n f'(x_j) \\hat H_{n, j}(x), \\] where \\[ H_{n, j}(x) = [1 - 2(x - x_j)L'_{n, j}(x_j)]L^2_{n, j}(x) \\] and \\[ \\hat H_{n, j}(x) = (x - x_j)L^2_{n, j}(x). \\] Moreover, if \\(f \\in C^{2n + 2}[a, b]\\) , then \\(\\exists\\ \\xi \\in (a, b)\\) , s.t. \\[ f(x) = H_{2n + 1}(x) + \\underbrace{\\frac{1}{(2n + 2)!}\\prod\\limits_{i = 0}^n (x - x_i)^2 f^{2n + 2}(\\xi)}_{R(x)}. \\] The theorem above gives a complete description of Hermit interpolation. But in pratice, to compute \\(H_{2n + 1}(x)\\) through the formula above is tedious . To make it compute easier, we introduce a method that is similar to Newton's interpolation. Define the sequence \\(\\{z_k\\}_{0}^{2n + 1}\\) by \\[ z_{2i} = z_{2i + 1} = x_i. \\] Based on the Theorem 3.5 , we redefine that \\[ f[z_{2i}, z_{2i + 1}] = f'(z_{2i}) = f'(x_i). \\] Then Hermite polynomial can be represented by \\[ H_{2n + 1}(x) = f[z_0] + \\sum\\limits_{k = 1}^{2n + 1} f[z_0, \\dots, z_k]\\prod\\limits_{i = 0}^{k - 1}(x - z_i). \\]","title":"Hermit Interpolation"},{"location":"Mathematics_Basis/NA/Chap_3/#cubic-spline-interpolation","text":"Motivation: For osculating polynomial approximation, we can let \\(m_i\\) be bigger to get high-degree polynomials. It can somehow be better but higher degree tends to causes a fluctuation or say overfitting . An alternative approach is to divide the interval into subintervals and approximate them respectively, which is called piecewise-polynomial approximation . The most common piecewise-polynomial approximation uses cubic polynomials called cubic spline approximation . Definition Given a function \\(f\\) defined on \\([a, b]\\) and a set of nodes \\(a = x_0 < x_1 < \\cdots < x_n = b\\) , a cubic spline interpolant \\(S\\) for \\(f\\) is a function that satisfies the following conditions: \\(S(x)\\) is a cubic polynomial, denoted \\(S_j(x)\\) , on the subinterval \\([x_j, x_j + 1]\\) , for each \\(j = 0, 1, \\dots, n - 1\\) ; \\(S(x_j) = f(x_j)\\) for each \\(j = 0, 1, \\dots, n\\) ; \\(S_{j + 1}(x_{j + 1}) = S_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\) ; \\(S'_{j + 1}(x_{j + 1}) = S'_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\) ; \\(S''_{j + 1}(x_{j + 1}) = S''_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\) ; One of the following sets of boundary conditions: \\(S''(x_0) = S''(x_n) = 0\\) ( free or natural boundary ); \\(S'(x_0) = f'(x_0)\\) and \\(S'(x_n) = f'(x_n)\\) ( clamped boundary ). The spline of natural boundary is called natural spline . Theorem 3.7 The cubic spline interpolation of either natural boundary or clamped boundary is unique . (Since the coefficient matrix \\(A\\) is strictly diagonally dominant.) Suppose interpolation function in each subinterval is \\[ S_j(x) = a_j + b_j(x - x_j) + c_j(x - x_j)^2 + d_j(x - x_j)^3. \\] From the conditions in the definition above, by some algebraic process, we can derive the solution with the following equations, \\[ \\begin{align} h_j &= x_{j + 1} - x_j; \\\\ a_j &= f(x_j); \\\\ b_j &= \\frac{1}{h_j}(a_{j + 1}) - \\frac{h_j}{3}(2c_j + c_{j + 1}); \\\\ d_j &= \\frac{1}{3h_j}{c_{j + 1} - c_j}. \\end{align} \\] While \\(c_j\\) is given by solving the following linear system, \\[ A\\mathbf{x} = \\mathbf{b}, \\] where \\[ \\small A = \\begin{bmatrix} 1 & 0 & 0 & \\cdots & \\cdots & 0 \\\\ h_0 & 2(h_0 + h_1) & h_1 & \\cdots & \\cdots & \\vdots \\\\ 0 & h_1 & 2(h_1 + h_2) & h_2 & \\ddots & \\vdots \\\\ \\vdots & \\ddots & \\ddots & \\ddots & \\ddots & 0 \\\\ \\vdots & \\ddots & \\ddots & h_{n - 2} & 2(h_{n - 2} + h_{n - 1}) & h_{n - 1} \\\\ 0 & \\cdots & \\cdots & 0 & 0 & 1 \\\\ \\end{bmatrix} , \\mathbf{x} = \\begin{bmatrix} c_0 \\\\ c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\] For natural boundary, \\[ \\small \\mathbf{b} = \\begin{bmatrix} 0 \\\\ \\frac{3}{h_1}(a_2 - a_1) - \\frac{3}{h_0}(a_1 - a_0) \\\\ \\vdots \\\\ \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) - \\frac{3}{h_{n - 2}}\\ (a_{n - 1} - a_{n - 2}) \\\\ 0 \\end{bmatrix} \\] For clamped boundary, \\[ \\small \\mathbf{b} = \\begin{bmatrix} \\frac{3}{h_0}(a_1 - a_0) - 3f'(a) \\\\ \\frac{3}{h_1}(a_2 - a_1) - \\frac{3}{h_0}(a_1 - a_0) \\\\ \\vdots \\\\ \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) - \\frac{3}{h_{n - 2}}\\ (a_{n - 1} - a_{n - 2}) \\\\ 3f'(b) - \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) \\\\ \\end{bmatrix} \\] Note If \\(f \\in C[a, b]\\) and \\(\\frac{\\max h_i}{\\min h_i} \\le C < \\infty\\) . Then \\(S(x)\\ \\overset{\\text{uniform}}{\\longrightarrow}\\ f(x)\\) when \\(\\max h_i \\rightarrow 0\\) . That is the accuracy of approximation can be improved by adding more nodes without increasing the degree of the splines.","title":"Cubic Spline Interpolation"},{"location":"Mathematics_Basis/NA/Chap_3/#curves","text":"We've discussed the interpolation of functions above, but we may encounter the case to interpolate a curve.","title":"Curves"},{"location":"Mathematics_Basis/NA/Chap_3/#straightforward-technique","text":"For given points \\((x_0, y_0), (x_1, y_1), \\dots, (x_n, y_n)\\) , we can construct two approximation functions with \\[ x_i = x(t_i),\\ y_i = y(t_i). \\] The interpolation method can be Lagrange, Hermite and Cubic spline, whatever.","title":"Straightforward Technique"},{"location":"Mathematics_Basis/NA/Chap_3/#bezier-curve","text":"In nature, it's piecewise cubic Hermite polynomial , and the curve is call Bezier curve . Similarly, suppose two function \\(x(t)\\) and \\(y(t)\\) at each interval. We have the following condtions. \\[ x(0) = x_0,\\ x(1) = x_1,\\ x'(0) = \\alpha_0,\\ x'(1) = \\alpha_1. \\] \\[ y(0) = y_0,\\ y(1) = y_1,\\ y'(0) = \\beta_0,\\ y'(1) = \\beta_1. \\] The solution is \\[ \\begin{align} x(t) &= [2(x_0 - x_1) + (\\alpha_0 + \\alpha_1)]t^3 + [3(x_1 - x_0) - (\\alpha_1 + 2\\alpha_0)]t^2 + \\alpha_0 t + x_0. \\\\ y(t) &= [2(y_0 - y_1) + (\\beta_0 + \\beta_1)]t^3 + [3(y_1 - y_0) - (\\beta_1 + 2\\beta_0)]t^2 + \\alpha_0 t + y_0. \\end{align} \\]","title":"Bezier Curve"},{"location":"Mathematics_Basis/NA/Chap_4/","text":"","title":"Chap 4"},{"location":"Mathematics_Basis/NA/Chap_5/","text":"","title":"Chap 5"},{"location":"Mathematics_Basis/NA/Chap_6/","text":"Chapter 6 | Direct Methods for Solving Linear Systems \u00b6 Linear Systems of Equations \u00b6 Gaussian Elimination with Backward Substitution \u00b6 \\[ A\\mathbf{x} = \\mathbf{b} \\] Let \\(A^{(1)} = A, \\mathbf{b}^{(1)} = \\mathbf{b}\\) , For Step \\(k\\ (1\\le k \\le n-1)\\) , if \\(a^{(k)}_{kk}\\ne0\\) ( pivot element ), compute \\(m_{ik} = \\frac{a^{(k)}_{ik}}{a^{(k)}_{kk}}\\) and \\[ \\left\\{ \\begin{align} a_{ij}^{(k+1)} = a_{ij}^{(k)} - m_{ik}a_{kj}^{(k)} \\\\ b_i^{(k+1)} = b_i^{(k)} - m_{ik}b_k^{(k)} \\end{align} \\text{, where }i, j = k+1, \\dots, n \\right. \\] Elimination \u00b6 After \\(n-1\\) steps, \\[ \\begin{bmatrix} a^{(1)}_{11} & a^{(1)}_{12} & \\cdots & a^{(1)}_{1n} \\\\ & a^{(2)}_{22} & \\cdots & a^{(2)}_{2n} \\\\ & & \\cdots & \\vdots \\\\ & & & a^{(n)}_{nn} \\\\ \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix} b^{(1)}_1 \\\\ b^{(2)}_2 \\\\ \\vdots \\\\ b^{(n)}_n \\end{bmatrix} \\] Backward Substitution \u00b6 Then, \\[ x_n = \\frac{b_n^{(n)}}{a^{(n)}_{nn}}, x_i = \\frac{1}{a_{ii}^{(i)}}\\left(b^{(i)}_i - \\sum_{j = i + 1}^n a^{(i)}_{ij} x_j\\right), i = n-1, \\dots, 1 \\] Complexity \u00b6 For Elimination, \\[ \\sum_{k=1}^{n-1}(n - k)(n - k + 2) = \\frac{n^3}{3} + \\frac{n^2}{2} - \\frac{5}{6}n. \\] For Backward-substitution, \\[ 1 + \\sum_{k=1}^{n-1}(n - k + 1) = \\frac{n^2}{2} + \\frac{n}{2}. \\] In total, \\[ \\frac{n^3}{3} + n^2 - \\frac{n}{3}. \\] Pivoting Strategies \u00b6 Motivation: For Gaussian Elimination with Backward Substituion, if the pivot element \\(a_{kk}^{(k)}\\) is small compared to \\(a_{ik}^{(k)}\\) , then \\(m_{ik}\\) is large with high roundoff error . Thus we need some transformation to improve the accuracy. Partial Pivoting (a.k.a Maximal Column Pivoting) \u00b6 Determine the smallest \\(p \\ge k\\) such that \\[ \\left|a_{pk}^{(k)}\\right| = \\max_{k \\le i \\le n} \\left|a_{ik}^{(k)}\\right| \\] and interchange row \\(p\\) and row \\(k\\) . Requires \\(O(N^2)\\) additional comparisons . Scaled Partial Pivoting (a.k.a Scaled-Column Pivoting) \u00b6 Determine the smallest \\(p \\ge k\\) such that \\[ \\frac{\\left|a_{pk}^{(k)}\\right|}{s_p} = \\max\\limits_{k \\le i \\le n} \\frac{\\left|a_{ik}^{(k)}\\right|}{s_i} \\] and interchange row \\(p\\) and row \\(k\\) , where \\(s_i = \\max\\limits_{1 \\le j \\le n} \\left|a_{ij}\\right|\\) . (Simply put, Place the element in the pivot position that is largest relative to the entries in its row.) Requires \\(O(N^2)\\) additional comparisons and \\(O(N^2)\\) divisions . Complete Pivoting (a.k.a Maximal Pivoting) \u00b6 Search all the entries \\(a_{ij}\\) for \\(i,j = k, \\dots,n\\) to find the entry with the largest magnitude. Both row and column interchanges are performed to bring this entry to the pivot position. Requires \\(O\\left(\\frac{1}{3}N^3\\right)\\) additional comparisons . Matrix Factorization (LU Factorization) \u00b6 Considering the matrix form of Gaussian Elimination, for total \\(n-1\\) steps, \\[ L_{n-1}L_{n-2}\\dots L_1[A\\ \\textbf{b}] = \\begin{bmatrix} a^{(1)}_{11} & a^{(1)}_{12} & \\cdots & a^{(1)}_{1n} & b_1^{(1)} \\\\ & a^{(2)}_{22} & \\cdots & a^{(2)}_{2n} & b_2^{(2)} \\\\ & & \\cdots & \\vdots & \\vdots \\\\ & & & a^{(n)}_{nn} & b_n^{(n)} \\\\ \\end{bmatrix}, \\] where \\[ L_k = \\begin{bmatrix} 1 & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\ 0 & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \\vdots & \\ddots & 1 & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots & -m_{k+1, k} & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots& \\vdots & \\ddots & \\ddots & 0 \\\\ 0 & \\cdots & -m_{n, k} & \\cdots & \\cdots & 1 \\end{bmatrix} \\] It's simple to compute that \\[ L_k^{-1} = \\begin{bmatrix} 1 & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\ 0 & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \\vdots & \\ddots & 1 & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots & m_{k+1, k} & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots& \\vdots & \\ddots & \\ddots & 0 \\\\ 0 & \\cdots & m_{n, k} & \\cdots & \\cdots & 1 \\end{bmatrix}. \\] Thus we let \\[ L_1^{-1}L_2^{-1}\\dots L_{n-1}^{-1} = \\begin{bmatrix} 1 & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\ m_{1,2} & 1 & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \\vdots & \\ddots & 1 & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots &\\ddots& \\ddots & 1 & 0\\\\ m_{n,1} & \\cdots & \\cdots & \\cdots & m_{n, n-1} & 1 \\end{bmatrix} = L, \\] and \\[ U = \\begin{bmatrix} a^{(1)}_{11} & a^{(1)}_{12} & \\cdots & a^{(1)}_{1n} \\\\ & a^{(2)}_{22} & \\cdots & a^{(2)}_{2n} \\\\ & & \\cdots & \\vdots \\\\ & & & a^{(n)}_{nn} \\\\ \\end{bmatrix}. \\] Then we get \\[ A = LU \\] Theorem If Gaussian elimination can be performed on the linear system \\(A\\mathbf{x} = \\mathbf{b}\\) without row interchanges, then the matrix \\(A\\) can be factored into the product of a lower-triangular matrix \\(L\\) and an upper-triangular matrix \\(U\\) . If \\(L\\) has to be unitary , then the factorization is unique . Special Types of Matrices \u00b6 Strictly Diagonally Dominant Matrix \u4e25\u683c\u4e3b\u5bf9\u89d2\u5360\u4f18\u77e9\u9635 \u00b6 Definition The \\(n \\times n\\) matrix \\(A\\) is said to be strictly diagnoally dominant when \\[ |a_{ii}| \\gt \\sum_{j=1,j\\ne i}^{n}|a_{ij}|,\\text{ for each } i = 1, \\dots, n \\] Theorem A strictly diagonally dominant matrix is nonsingular . And Gaussian elimination can be performed without row or column interchanges, and computations will be stable with respect to the growth of roundoff errors.\uff08\u6ee1\u79e9\u3001\u65e0\u9700\u4ea4\u6362\u884c\u5217\u3001\u8bef\u5dee\u7a33\u5b9a\uff09 Positive Definite Matrix \u6b63\u5b9a\u77e9\u9635 \u00b6 Definition (Recap) A matrix \\(A\\) is positive definite if it's symmetric and if \\(\\mathbf{x}^tA\\mathbf{x} > 0\\) for every \\(n\\) -dimensional vector \\(\\mathbf{x} \\ne \\mathbf{0}\\) . Theorem If \\(A\\) is an \\(n \\times n\\) positive definite matrix, then \\(A\\) is nonsingular; \\(a_{ii} > 0\\) , for each \\(i = 1, 2, \\dots, n\\) ; \\(\\max\\limits_{1 \\le k, j \\le n}|a_{kj}| \\le \\max\\limits_{1 \\le i \\le n}|a_{ii}|\\) ; \\((a_{ij})^2 < a_{ii}a_{jj}\\) , for each \\(i \\ne j\\) . Choleski's Method (LDLt factorization) \u00b6 Further decompose \\(U\\) to \\(D\\tilde U\\) . \\(A\\) is symmetric \\(\\Rightarrow\\) \\(L = \\tilde U^t\\) . Thus \\[ A = LU = LD\\tilde U = LDL^t.\\ \\ (\u4e0b\u4e09\u89d2\u77e9\u9635\u4e58\u5bf9\u89d2\u77e9\u9635\u518d\u4e58\u5176\u8f6c\u7f6e) \\] Let \\[ D^{1/2} = \\begin{bmatrix} \\sqrt{u_{11}} & & & \\\\ & \\sqrt{u_{22}} & & \\\\ & & \\ddots & \\\\ & & & \\sqrt{u_{nn}} \\end{bmatrix}, \\] and \\(\\widetilde{L} = LD^{1/2}\\) . Then \\[ A = \\tilde L \\tilde L^t.\\ \\ (\u4e0b\u4e09\u89d2\u77e9\u9635\u4e58\u5176\u8f6c\u7f6e) \\] Tridiagonal Linear System \u00b6 Definition An \\(n \\times n\\) matrix \\(A\\) is called a band matrix if \\(\\exists p, q (1 < p, q < n)\\) , s.t. whenever \\(i + p \\le j\\) or \\(j + q \\le i\\) , \\(a_{ij} = 0\\) . And \\(w = p + q - 1\\) is called the bandwidth . Specially, if \\(p = q = 2\\) , then \\(A\\) is called tridiagonal , with the following form, \\[ \\begin{bmatrix} b_1 & c_1 & & & \\\\ a_2 & b_2 & c_2 & & \\\\ & \\ddots & \\ddots & \\ddots \\\\ & & a_{n-1} & b_{n-1} & c_{n-1} \\\\ & & & a_n & b_n \\\\ \\end{bmatrix} \\] Crout Factorization \u00b6 \\[ A = LU = \\begin{bmatrix} l_{11} \\\\ l_{21} & l_{22} \\\\ & \\ddots & \\ddots \\\\ & & \\ddots & \\ddots \\\\ & & & l_{n, n- 1} & l_{n, n} \\end{bmatrix} \\begin{bmatrix} 1 & u_{12}\\\\ & 1 & u_{23} \\\\ & & \\ddots & \\ddots \\\\ & & & \\ddots & u_{n-1,n}\\\\ & & & & 1 \\end{bmatrix} \\] the time complexity is \\(O(N)\\) .","title":"Chap 6"},{"location":"Mathematics_Basis/NA/Chap_6/#chapter-6-direct-methods-for-solving-linear-systems","text":"","title":"Chapter 6 | Direct Methods for Solving Linear Systems"},{"location":"Mathematics_Basis/NA/Chap_6/#linear-systems-of-equations","text":"","title":"Linear Systems of Equations"},{"location":"Mathematics_Basis/NA/Chap_6/#gaussian-elimination-with-backward-substitution","text":"\\[ A\\mathbf{x} = \\mathbf{b} \\] Let \\(A^{(1)} = A, \\mathbf{b}^{(1)} = \\mathbf{b}\\) , For Step \\(k\\ (1\\le k \\le n-1)\\) , if \\(a^{(k)}_{kk}\\ne0\\) ( pivot element ), compute \\(m_{ik} = \\frac{a^{(k)}_{ik}}{a^{(k)}_{kk}}\\) and \\[ \\left\\{ \\begin{align} a_{ij}^{(k+1)} = a_{ij}^{(k)} - m_{ik}a_{kj}^{(k)} \\\\ b_i^{(k+1)} = b_i^{(k)} - m_{ik}b_k^{(k)} \\end{align} \\text{, where }i, j = k+1, \\dots, n \\right. \\]","title":"Gaussian Elimination with Backward Substitution"},{"location":"Mathematics_Basis/NA/Chap_6/#elimination","text":"After \\(n-1\\) steps, \\[ \\begin{bmatrix} a^{(1)}_{11} & a^{(1)}_{12} & \\cdots & a^{(1)}_{1n} \\\\ & a^{(2)}_{22} & \\cdots & a^{(2)}_{2n} \\\\ & & \\cdots & \\vdots \\\\ & & & a^{(n)}_{nn} \\\\ \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix} b^{(1)}_1 \\\\ b^{(2)}_2 \\\\ \\vdots \\\\ b^{(n)}_n \\end{bmatrix} \\]","title":"Elimination"},{"location":"Mathematics_Basis/NA/Chap_6/#backward-substitution","text":"Then, \\[ x_n = \\frac{b_n^{(n)}}{a^{(n)}_{nn}}, x_i = \\frac{1}{a_{ii}^{(i)}}\\left(b^{(i)}_i - \\sum_{j = i + 1}^n a^{(i)}_{ij} x_j\\right), i = n-1, \\dots, 1 \\]","title":"Backward Substitution"},{"location":"Mathematics_Basis/NA/Chap_6/#complexity","text":"For Elimination, \\[ \\sum_{k=1}^{n-1}(n - k)(n - k + 2) = \\frac{n^3}{3} + \\frac{n^2}{2} - \\frac{5}{6}n. \\] For Backward-substitution, \\[ 1 + \\sum_{k=1}^{n-1}(n - k + 1) = \\frac{n^2}{2} + \\frac{n}{2}. \\] In total, \\[ \\frac{n^3}{3} + n^2 - \\frac{n}{3}. \\]","title":"Complexity"},{"location":"Mathematics_Basis/NA/Chap_6/#pivoting-strategies","text":"Motivation: For Gaussian Elimination with Backward Substituion, if the pivot element \\(a_{kk}^{(k)}\\) is small compared to \\(a_{ik}^{(k)}\\) , then \\(m_{ik}\\) is large with high roundoff error . Thus we need some transformation to improve the accuracy.","title":"Pivoting Strategies"},{"location":"Mathematics_Basis/NA/Chap_6/#partial-pivoting-aka-maximal-column-pivoting","text":"Determine the smallest \\(p \\ge k\\) such that \\[ \\left|a_{pk}^{(k)}\\right| = \\max_{k \\le i \\le n} \\left|a_{ik}^{(k)}\\right| \\] and interchange row \\(p\\) and row \\(k\\) . Requires \\(O(N^2)\\) additional comparisons .","title":"Partial Pivoting (a.k.a Maximal Column Pivoting)"},{"location":"Mathematics_Basis/NA/Chap_6/#scaled-partial-pivoting-aka-scaled-column-pivoting","text":"Determine the smallest \\(p \\ge k\\) such that \\[ \\frac{\\left|a_{pk}^{(k)}\\right|}{s_p} = \\max\\limits_{k \\le i \\le n} \\frac{\\left|a_{ik}^{(k)}\\right|}{s_i} \\] and interchange row \\(p\\) and row \\(k\\) , where \\(s_i = \\max\\limits_{1 \\le j \\le n} \\left|a_{ij}\\right|\\) . (Simply put, Place the element in the pivot position that is largest relative to the entries in its row.) Requires \\(O(N^2)\\) additional comparisons and \\(O(N^2)\\) divisions .","title":"Scaled Partial Pivoting (a.k.a Scaled-Column Pivoting)"},{"location":"Mathematics_Basis/NA/Chap_6/#complete-pivoting-aka-maximal-pivoting","text":"Search all the entries \\(a_{ij}\\) for \\(i,j = k, \\dots,n\\) to find the entry with the largest magnitude. Both row and column interchanges are performed to bring this entry to the pivot position. Requires \\(O\\left(\\frac{1}{3}N^3\\right)\\) additional comparisons .","title":"Complete Pivoting (a.k.a Maximal Pivoting)"},{"location":"Mathematics_Basis/NA/Chap_6/#matrix-factorization-lu-factorization","text":"Considering the matrix form of Gaussian Elimination, for total \\(n-1\\) steps, \\[ L_{n-1}L_{n-2}\\dots L_1[A\\ \\textbf{b}] = \\begin{bmatrix} a^{(1)}_{11} & a^{(1)}_{12} & \\cdots & a^{(1)}_{1n} & b_1^{(1)} \\\\ & a^{(2)}_{22} & \\cdots & a^{(2)}_{2n} & b_2^{(2)} \\\\ & & \\cdots & \\vdots & \\vdots \\\\ & & & a^{(n)}_{nn} & b_n^{(n)} \\\\ \\end{bmatrix}, \\] where \\[ L_k = \\begin{bmatrix} 1 & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\ 0 & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \\vdots & \\ddots & 1 & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots & -m_{k+1, k} & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots& \\vdots & \\ddots & \\ddots & 0 \\\\ 0 & \\cdots & -m_{n, k} & \\cdots & \\cdots & 1 \\end{bmatrix} \\] It's simple to compute that \\[ L_k^{-1} = \\begin{bmatrix} 1 & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\ 0 & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \\vdots & \\ddots & 1 & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots & m_{k+1, k} & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots& \\vdots & \\ddots & \\ddots & 0 \\\\ 0 & \\cdots & m_{n, k} & \\cdots & \\cdots & 1 \\end{bmatrix}. \\] Thus we let \\[ L_1^{-1}L_2^{-1}\\dots L_{n-1}^{-1} = \\begin{bmatrix} 1 & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\ m_{1,2} & 1 & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \\vdots & \\ddots & 1 & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots &\\ddots& \\ddots & 1 & 0\\\\ m_{n,1} & \\cdots & \\cdots & \\cdots & m_{n, n-1} & 1 \\end{bmatrix} = L, \\] and \\[ U = \\begin{bmatrix} a^{(1)}_{11} & a^{(1)}_{12} & \\cdots & a^{(1)}_{1n} \\\\ & a^{(2)}_{22} & \\cdots & a^{(2)}_{2n} \\\\ & & \\cdots & \\vdots \\\\ & & & a^{(n)}_{nn} \\\\ \\end{bmatrix}. \\] Then we get \\[ A = LU \\] Theorem If Gaussian elimination can be performed on the linear system \\(A\\mathbf{x} = \\mathbf{b}\\) without row interchanges, then the matrix \\(A\\) can be factored into the product of a lower-triangular matrix \\(L\\) and an upper-triangular matrix \\(U\\) . If \\(L\\) has to be unitary , then the factorization is unique .","title":"Matrix Factorization (LU Factorization)"},{"location":"Mathematics_Basis/NA/Chap_6/#special-types-of-matrices","text":"","title":"Special Types of Matrices"},{"location":"Mathematics_Basis/NA/Chap_6/#strictly-diagonally-dominant-matrix","text":"Definition The \\(n \\times n\\) matrix \\(A\\) is said to be strictly diagnoally dominant when \\[ |a_{ii}| \\gt \\sum_{j=1,j\\ne i}^{n}|a_{ij}|,\\text{ for each } i = 1, \\dots, n \\] Theorem A strictly diagonally dominant matrix is nonsingular . And Gaussian elimination can be performed without row or column interchanges, and computations will be stable with respect to the growth of roundoff errors.\uff08\u6ee1\u79e9\u3001\u65e0\u9700\u4ea4\u6362\u884c\u5217\u3001\u8bef\u5dee\u7a33\u5b9a\uff09","title":"Strictly Diagonally Dominant Matrix \u4e25\u683c\u4e3b\u5bf9\u89d2\u5360\u4f18\u77e9\u9635"},{"location":"Mathematics_Basis/NA/Chap_6/#positive-definite-matrix","text":"Definition (Recap) A matrix \\(A\\) is positive definite if it's symmetric and if \\(\\mathbf{x}^tA\\mathbf{x} > 0\\) for every \\(n\\) -dimensional vector \\(\\mathbf{x} \\ne \\mathbf{0}\\) . Theorem If \\(A\\) is an \\(n \\times n\\) positive definite matrix, then \\(A\\) is nonsingular; \\(a_{ii} > 0\\) , for each \\(i = 1, 2, \\dots, n\\) ; \\(\\max\\limits_{1 \\le k, j \\le n}|a_{kj}| \\le \\max\\limits_{1 \\le i \\le n}|a_{ii}|\\) ; \\((a_{ij})^2 < a_{ii}a_{jj}\\) , for each \\(i \\ne j\\) .","title":"Positive Definite Matrix \u6b63\u5b9a\u77e9\u9635"},{"location":"Mathematics_Basis/NA/Chap_6/#choleskis-method-ldlt-factorization","text":"Further decompose \\(U\\) to \\(D\\tilde U\\) . \\(A\\) is symmetric \\(\\Rightarrow\\) \\(L = \\tilde U^t\\) . Thus \\[ A = LU = LD\\tilde U = LDL^t.\\ \\ (\u4e0b\u4e09\u89d2\u77e9\u9635\u4e58\u5bf9\u89d2\u77e9\u9635\u518d\u4e58\u5176\u8f6c\u7f6e) \\] Let \\[ D^{1/2} = \\begin{bmatrix} \\sqrt{u_{11}} & & & \\\\ & \\sqrt{u_{22}} & & \\\\ & & \\ddots & \\\\ & & & \\sqrt{u_{nn}} \\end{bmatrix}, \\] and \\(\\widetilde{L} = LD^{1/2}\\) . Then \\[ A = \\tilde L \\tilde L^t.\\ \\ (\u4e0b\u4e09\u89d2\u77e9\u9635\u4e58\u5176\u8f6c\u7f6e) \\]","title":"Choleski's Method (LDLt factorization)"},{"location":"Mathematics_Basis/NA/Chap_6/#tridiagonal-linear-system","text":"Definition An \\(n \\times n\\) matrix \\(A\\) is called a band matrix if \\(\\exists p, q (1 < p, q < n)\\) , s.t. whenever \\(i + p \\le j\\) or \\(j + q \\le i\\) , \\(a_{ij} = 0\\) . And \\(w = p + q - 1\\) is called the bandwidth . Specially, if \\(p = q = 2\\) , then \\(A\\) is called tridiagonal , with the following form, \\[ \\begin{bmatrix} b_1 & c_1 & & & \\\\ a_2 & b_2 & c_2 & & \\\\ & \\ddots & \\ddots & \\ddots \\\\ & & a_{n-1} & b_{n-1} & c_{n-1} \\\\ & & & a_n & b_n \\\\ \\end{bmatrix} \\]","title":"Tridiagonal Linear System"},{"location":"Mathematics_Basis/NA/Chap_6/#crout-factorization","text":"\\[ A = LU = \\begin{bmatrix} l_{11} \\\\ l_{21} & l_{22} \\\\ & \\ddots & \\ddots \\\\ & & \\ddots & \\ddots \\\\ & & & l_{n, n- 1} & l_{n, n} \\end{bmatrix} \\begin{bmatrix} 1 & u_{12}\\\\ & 1 & u_{23} \\\\ & & \\ddots & \\ddots \\\\ & & & \\ddots & u_{n-1,n}\\\\ & & & & 1 \\end{bmatrix} \\] the time complexity is \\(O(N)\\) .","title":"Crout Factorization"},{"location":"Mathematics_Basis/NA/Chap_7/","text":"Chapter 7 | Iterative Techniques in Matrix Algebra \u00b6 Norms of Vectors and Matrices \u00b6 Vector Norms \u00b6 Definition A vector norm on \\(\\mathbb{R}^n\\) is a function \\(||\\cdot||\\) , from \\(\\mathbb{R}^n\\) into \\(\\mathbb{R}\\) with the following properties. \\[ \\begin{align} \\forall \\mathbf{x, y} \\in \\mathbb{R}^n, \\alpha \\in \\mathbb{R}, & (1)\\ ||\\mathbf{x}|| \\ge 0;\\ ||\\mathbf{x}|| = 0 \\Leftrightarrow \\mathbf{x} = \\mathbf{0}. \\\\ & (2)\\ ||\\alpha \\mathbf{x}|| = |\\alpha| \\cdot ||\\mathbf{x}|| \\\\ & (3)\\ ||\\mathbf{x} + \\mathbf{y}|| \\le ||\\mathbf{x}|| + ||\\mathbf{y}|| \\\\ \\end{align} \\] Commonly used examples L1 Norm: \\(||\\mathbf{x}||_1 = \\sum\\limits_{i = 1}^n|x_i|\\) . L2 Norm / Euclidean Norm: \\(||\\mathbf{x}||_2 = \\sqrt{\\sum\\limits_{i = 1}^n|x_i|^2}\\) . p-Norm: \\(||\\mathbf{x}||_p = \\left(\\sum\\limits_{i = 1}^n|x_i|^p\\right)^{1/p}\\) . Infinity Norm: \\(||\\mathbf{x}||_\\infty = \\max\\limits_{1 \\le i \\le n} |x_i|\\) . Convergence of Vector \u00b6 Similarly with a scalar, a sequence of vectors \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) in \\(\\mathbb{R}^n\\) is said to converge to \\(\\mathbf{x}\\) with respect to norm \\(||\\cdot||\\) , if \\(\\forall \\epsilon \\gt 0,\\exists N \\in \\mathbb{N}\\) , s.t \\(\\forall k \\gt N, ||\\mathbf{x}^{(k)} - \\mathbf{x}|| \\lt \\epsilon\\) . Theorem \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) in \\(\\mathbb{R}^n\\) converges to \\(\\mathbf{x}\\) with respect to norm \\(||\\cdot||_\\infty\\) if and only if \\(\\forall i,\\lim_{k \\rightarrow \\infty}x_i^{(k)} = x_i\\) . Equivalence \u00b6 Definition If \\(\\exists C_1, C_2,\\ s.t.\\ C_1||\\mathbf{x}||_B \\le ||\\mathbf{x}||_A \\le C_2||\\mathbf{x}||_B\\) , then \\(||\\mathbf{x}||_A\\) and \\(||\\mathbf{x}||_B\\) are equivalent . Theorem All the vector norms on \\(\\mathbb{R}^n\\) are equivalent. Matrix Norms \u00b6 Definition A matrix norm on \\(\\mathbb{R}^n\\) is a function \\(||\\cdot||\\) , from \\(M_n(\\mathbb{R})\\) matrices into \\(\\mathbb{R}\\) with the following properties. \\[ \\begin{align} \\forall A, B \\in M_n(\\mathbb{R}), \\alpha \\in \\mathbb{R}, & (1)\\ ||A|| \\ge 0;\\ ||A|| = 0 \\Leftrightarrow A = \\mathbf{O}. \\\\ & (2)\\ ||\\alpha A|| = |\\alpha| \\cdot ||A|| \\\\ & (3)\\ ||A + B|| \\le ||A|| + ||B|| \\\\ & (4)\\ ||AB|| \\le ||A|| \\cdot ||B|| \\end{align} \\] Commonly used examples Frobenius Norm: \\(||A||_F = \\sqrt{\\sum\\limits_{i=1}^n\\sum_{j=1}^n|a_{ij}|^2}\\) . Natural Norm: \\(||A||_p = \\max\\limits_{\\mathbf{x}\\ne \\mathbf{0}} \\frac{||A\\mathbf{x}||_p}{||\\mathbf{x}||_p} = \\max\\limits_{||\\mathbf{x}_p|| = 1} ||A\\mathbf{x}||_p\\) , where \\(||\\cdot||_p\\) is the vector norm. \\(||A||_\\infty = \\max\\limits_{1\\le i \\le n}\\sum\\limits_{j=1}^n|a_{ij}|\\) . \\(||A||_1= \\max\\limits_{1\\le j \\le n}\\sum\\limits_{i=1}^n|a_{ij}|\\) . (Spectral Norm) \\(||A||_2= \\sqrt{\\lambda_{max}(A^TA)}\\) . Corollary For any vector \\(\\mathbf{x} \\ne 0\\) , matrix \\(A\\) , and any natural norm \\(||\\cdot||\\) , we have \\[ ||A\\mathbf{x}|| \\le ||A|| \\cdot ||\\mathbf{x}||. \\] Eigenvalues and Eigenvectors \u00b6 Definition (Recap) If \\(A\\) is a square matrix, the **characteristic polynomial$ of \\(A\\) is defined by \\[ p(\\lambda) = \\text{det}(A - \\lambda I). \\] The roots of \\(p\\) are eigenvalues . If \\(\\lambda\\) is an eigenvalue and \\(\\mathbf{x} \\ne 0\\) satisfies \\((A - \\lambda I)\\mathbf{x} = \\mathbf{0}\\) , then \\(\\mathbf{x}\\) is an eigenvector . Spectral Radius \u00b6 Definition The spectral radius of a matrix \\(A\\) is defined by \\[ \\rho(A) = \\max|\\lambda| \\text{, where $\\lambda$ is an eigenvalue of $A$}. \\] (Recap that for complex \\(\\lambda = \\alpha + \\beta i\\) , \\(|\\lambda| = \\sqrt{\\alpha^2 + \\beta^2}\\) .) Theorem \\(\\forall A \\in M_n(\\mathbb{R})\\) , \\(||A||_2 = \\sqrt{\\rho(A^tA)}\\) . \\(\\rho(A) \\le ||A||\\) , for any natural norm \\(||\\cdot||\\) . Proof A proof for the second property. Suppose \\(\\lambda\\) is an eigenvalue of \\(A\\) with eigenvector \\(\\mathbf{x}\\) and \\(||\\mathbf{x}|| = 1\\) , \\[ |\\lambda| = |\\lambda| \\cdot ||\\mathbf{x}|| = ||\\lambda \\mathbf{x}|| \\le ||A|| \\cdot ||\\mathbf{x}|| = ||A||. \\] Thus, \\[ \\rho(A) = \\max|\\lambda| \\le ||A||. \\] Convergence of Matrix \u00b6 Definition \\(A \\in M_n(\\mathbb{R}))\\) is convergent if \\[ \\lim_{k \\rightarrow \\infty}\\left(A^k\\right)_{ij} = 0 \\text{ , for each } i = 1, 2, \\dots, n \\text{ and } j = 1, 2, \\dots, n. \\] Theorem The following statements are equivalent. \\(A\\) is a convergent matrix. \\(\\lim\\limits_{n \\rightarrow \\infty} ||A^n|| = 0\\) , for some natural norm. \\(\\lim\\limits_{n \\rightarrow \\infty} ||A^n|| = 0\\) , for all natural norms. \\(\\rho(A) < 1\\) . \\(\\forall \\mathbf{x}, \\lim\\limits_{n \\rightarrow \\infty} ||A^n \\mathbf{x}|| = \\mathbf{0}\\) . Iterative Techniques for Solving Linear Systems \u00b6 \\[ A\\mathbf{x} = \\mathbf{b} \\Leftrightarrow (D - L - U)\\mathbf{x} = \\mathbf{b} \\Leftrightarrow D\\mathbf{x} = (L + U)\\mathbf{x} + \\mathbf{b} \\\\ \\] Thus, \\[ \\mathbf{x} = D^{-1}(L + U)\\mathbf{x} + D^{-1}\\mathbf{b} \\] Jacobi Iterative Method \u00b6 Let \\(T_j = D^{-1}(L+U)\\) and \\(\\mathbf{c}_\\mathbf{j} = D^{-1}\\mathbf{b}\\) , then \\[ \\mathbf{x}^{(k)} = T_j\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\mathbf{j}. \\] Gauss-Seidel Iterative Method \u00b6 \\[ \\small \\mathbf{x}^{(k)} = D^{-1}(L\\mathbf{x}^{(k)} + U\\mathbf{x}^{(k - 1)}) + D^{-1}\\mathbf{b} \\Leftrightarrow \\mathbf{x}^{(k)} = (D - L)^{-1}U\\mathbf{x}^{(k - 1)} + (D - L)^{-1}\\mathbf{b} \\] Let \\(T_g = (D - L)^{-1}U\\) and \\(\\mathbf{c}_\\mathbf{g} = (D - L)^{-1}\\mathbf{b}\\) , then \\[ \\mathbf{x}^{(k)} = T_g\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\mathbf{g}. \\] Convergence of Iterative Methods \u00b6 Consider the following formula \\[ \\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c} \\] where \\(\\mathbf{x}^{(0)}\\) is arbitrary. Lemma If \\(\\rho(T) \\lt 1\\) , then \\((I - T)^{-1}\\) exists and \\[ (I - T)^{-1} = \\sum\\limits_{j = 0}^\\infty T^j. \\] Proof Suppose \\(\\lambda\\) is an eigenvalue of \\(T\\) with eigenvector \\(\\mathbf{x}\\) , then since \\(T\\mathbf{x} = \\lambda \\mathbf{x} \\Leftrightarrow (I - T)\\mathbf{x} = (1 - \\lambda)\\mathbf{x}\\) , thus \\(1 - \\lambda\\) is an eigenvalue of \\(I - T\\) . Since \\(|\\lambda| \\le \\rho(T) < 1\\) , thus \\(\\lambda = 1\\) is not an eigenvalue of \\(T\\) and \\(0\\) is not an eigenvvalue of \\(I - T\\) . Hence, \\((I - T)^{-1}\\) exists. Let \\(S_m = I + T + T^2 + \\cdots + T^m\\) , then \\[ (I - T)S_m = (1 + T + \\cdots + T^m) - (T + T^2 + \\cdots + T^{m + 1}) = I - T^{m + 1}. \\] Since \\(T\\) is convergent, thus \\[ \\lim\\limits_{m \\rightarrow \\infty} (I - T)S_m = \\lim\\limits_{m \\rightarrow \\infty}(I - T^{m + 1}) = I. \\] Thus, $(I - T)^{-1} = \\lim\\limits_{m \\rightarrow \\infty}S_m = \\sum\\limits_{j = 0}^\\infty T^j. Theorem \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) defined by \\(\\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c}\\) converges to the unique solution of \\(\\mathbf{x} = T\\mathbf{x} + \\mathbf{c}\\) if and only if \\(\\rho(T) \\lt 1\\) . Proof \\(\\Rightarrow\\) : define error \\(\\mathbf{e}^{(k)} = \\mathbf{x} - \\mathbf{x}^{(k)}\\) , then \\[ \\mathbf{e}^{(k)} = (T\\mathbf{x} + c) - (T\\mathbf{x}^{(k - 1)} + c) = T(\\mathbf{x} - \\mathbf{x}^{(k - 1)})T\\mathbf{e}^{(k - 1)} \\Rightarrow \\mathbf{e}^{(k)} = T^k \\mathbf{e}^{(0)} \\] Since it converges, thus \\[ \\lim\\limits_{k \\rightarrow \\infty} \\mathbf{e}^{(k)} = 0 \\Rightarrow \\lim\\limits_{k \\rightarrow \\infty} T^k \\mathbf{e}^{(0)} = 0, \\forall \\mathbf{e^{(0)}} \\] \\[ \\Leftrightarrow \\rho(T) < 1 \\] \\(\\Leftarrow\\) : \\[ \\mathbf{x}^{(k)} = T^{(k)}\\mathbf{x}^(0) + (T^{k - 1} + \\cdots + T + I) \\mathbf{c}. \\] Since \\(\\rho(T) < 1\\) , \\(T\\) is convergent and \\[ \\lim\\limits_{k \\rightarrow \\infty} T^k x^{(0)} = \\mathbf{0}. \\] From the Lemma above, we have \\[ \\lim\\limits_{k \\rightarrow \\infty} \\mathbf{x}^{(k)} = \\lim\\limits_{k \\rightarrow \\infty} T^k\\mathbf{x}^{(0)} + \\left(\\sum\\limits_{j = 0}^\\infty T^j \\right)\\mathbf{c} = \\mathbf{0} + (I - T)^{-1}\\mathbf{c} = (I - T)^{-1}\\mathbf{c}. \\] Thus \\(\\{\\mathbf{x}^{(k)}\\}\\) converges to \\(\\mathbf{x} \\equiv (I - T)^{-1} \\Leftrightarrow \\mathbf{x} = T\\mathbf{x} + c\\) . Corollary If \\(||T||\\lt 1\\) for any matrix norm and \\(\\mathbf{c}\\) is a given vector, then for all \\(\\mathbf{x}^{(0)}\\in \\mathbb{R}^n\\) , \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) defined by \\(\\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c}\\) converges to \\(\\mathbf{x}\\) , and the following error bounds hold \\(||\\mathbf{x} - \\mathbf{x}^{(k)}|| \\le ||T||^k||\\mathbf{x}^{(0)} - \\mathbf{x}||\\) . \\(||\\mathbf{x} - \\mathbf{x}^{(k)}|| \\le \\frac{||T||^k}{1 - ||T||}||\\mathbf{x}^{(1)} - \\mathbf{x}||\\) . Theorem \\(\\forall A\\) is strictly diagonally dominant, \\(\\forall \\text{\\bf{x}}^{(0)}\\) , both Jacobi and Gauss-Seidel methods give \\(\\{\\mathbf{x}^{(k)}\\}_{k=0}^\\infty\\) that converge to the unique solution of \\(A\\mathbf{x} = \\mathbf{b}\\) . Relaxation Methods \u00b6 Definition Suppose \\(\\mathbf{\\tilde x}\\) is an approximation to the solution of \\(A\\mathbf{x} = \\mathbf{b}\\) , then the residual vector for \\(\\mathbf{\\tilde x}\\) w.r.t this linear system is \\(\\mathbf{r} = \\mathbf{b} - A\\mathbf{\\tilde x}\\) . Further examine Gauss-Seidel method, \\[ x_i^{(k)} = x_i^{(k - 1)} + \\frac{r_i^{(k)}}{a_{ii}}, \\text{ where } r_i^{(k)} = b_i - \\sum_{j \\lt i} a_{ij}x_j^{(k)} - \\sum_{j \\ge i} a_{ij}x_j^{(k - 1)} \\] Let \\(x_i^{(k)} = x_i^{(k - 1)} + \\omega\\frac{r_i^{(k)}}{a_{ii}}\\) , by modifying the value of \\(\\omega\\) , we can somehow get faster convergence. \\(0 \\lt \\omega \\le 1\\) Under-Relaxation Method \\(\\omega = 1\\) Gauss-Seidel Method \\(\\omega \\gt 1\\) Successive Over-Relaxation Method (SOR) In matrix form, \\[ \\mathbf{x}^{(k)} = (D - \\omega L)^{-1}[(1 - \\omega)D + \\omega U]\\mathbf{x}^{(k - 1)} + (D - \\omega L)^{-1}\\mathbf{b} \\] Let \\(T_\\omega = (D - \\omega L)^{-1}[(1 - \\omega)D + \\omega U]\\) and \\(\\mathbf{c}_\\omega = (D - \\omega L)^{-1}\\mathbf{b}\\) , then \\[ \\mathbf{x}^{(k)} = T_\\omega\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\omega. \\] Theorem (Kahan) If \\(a_{ii} \\ne 0\\) , then \\(\\rho(T_\\omega)\\ge |\\omega -1 |\\) , which implies \\(0 \\lt \\omega \\lt 2\\) . Theorem (Ostrowski-Reich) If \\(A\\) is positive definite and \\(0 \\lt \\omega \\lt 2\\) , the SOR method converges for any choice of initial approximation. Theorem If \\(A\\) is positive definite and tridiagonal, then \\(\\rho(T_g) = \\rho(T_j)^2\\lt1\\) , and the optimal choice of \\(\\omega\\) for the SOR method is \\[ \\omega = \\frac{2}{1 + \\sqrt{1 - \\rho(T_j)^2}}. \\] Error Bounds and Iterative Refinement \u00b6 Definition The conditional number of the nonsigular matrix \\(A\\) relative to a norm \\(||\\cdot||\\) $ is \\[ K(A) = ||A|| \\cdot ||A^{-1}||. \\] A matrix \\(A\\) is well-conditioned if \\(K(A)\\) is close to 1, and is ill-conditioned when \\(K(A)\\) is significantly greater than 1. Proposition If \\(A\\) is symmetric, then \\(K(A)_2 = \\frac{\\max|\\lambda|}{\\min|\\lambda|}\\) . \\(K(A)_2 = 1\\) if \\(A\\) is orthogonal. \\(\\forall \\text{ orthogonal matrix }R, K(RA)_2 = K(AR)_2 = K(A)_2.\\) \\(\\forall \\text{ natural norm } ||\\cdot||_p,\\ K(A)_p \\ge 1.\\) \\(K(\\alpha A) = K(A).\\) Theorem For any natural norm, \\[ ||\\mathbf{x} - \\mathbf{\\tilde x}|| \\le ||\\mathbf{r}|| \\cdot ||A^{-1}|| \\] and if \\(\\mathbf{x} \\ne \\mathbf{0}\\) and \\(\\mathbf{b} \\ne \\mathbf{0}\\) , \\[ \\frac{||\\mathbf{x} - \\mathbf{\\tilde x}||}{||\\mathbf{x}||} \\le ||A||\\cdot||A^{-1}|| \\frac{||\\mathbf{r}||}{||\\mathbf{b}||} = K(A)\\frac{||\\mathbf{r}||}{||\\mathbf{b}||}. \\] Iterative Refinement Step.1 Solve \\(A\\mathbf{x} = \\mathbf{b}\\) and get an approximation solution \\(\\mathbf{x}_{0}\\) . Let \\(i = 1\\) . Step.2 Let \\(\\mathbf{r} = \\mathbf{b} - A\\mathbf{x}_{i - 1}\\) . Step.3 Solve \\(A\\mathbf{d} = \\mathbf{r}\\) and get the solution \\(\\mathbf{d}\\) . Step.4 The better approximation is \\(\\mathbf{x}_{i} = \\mathbf{x}_{i - 1} + \\mathbf{d}.\\) Step.5 Judge whether it's precise enough. If not, let \\(i = i + 1\\) and then repeat from Step.2 . In reality, \\(A\\) and \\(\\mathbf{b}\\) may be perturbed by an amount \\(\\delta A\\) and \\(\\delta \\mathbf{b}\\) . For \\(A(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b} + \\delta\\mathbf{b}\\) \\[ \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le ||A|| \\cdot ||A^{-1}|| \\cdot \\frac{||\\delta \\mathbf{b}||}{||\\mathbf{b}||} \\] For \\((A + \\delta A)(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b}\\) \\[ \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le \\frac{||A^{-1}|| \\cdot ||\\delta A||}{1 - ||A^{-1}|| \\cdot ||\\delta A||} = \\frac{||A|| \\cdot ||A^{-1}|| \\cdot \\frac{||\\delta A||}{||A||}}{1 - ||A|| \\cdot ||A^{-1}|| \\cdot\\frac{||\\delta A||}{||A||}} \\] Theorem If \\(A\\) is nonsingular and \\[ ||\\delta A|| \\lt \\frac{1}{||A^{-1}||} \\] then \\((A + \\delta A)(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b} + \\delta\\mathbf{b}\\) with the error estimate \\[ \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le \\frac{K(A)}{1 - K(A)\\frac{||\\delta A||}{||A||}}\\left(\\frac{||\\delta A||}{||A||} + \\frac{||\\delta\\mathbf{b}||}{||\\mathbf{b}||}\\right) \\]","title":"Chap 7"},{"location":"Mathematics_Basis/NA/Chap_7/#chapter-7-iterative-techniques-in-matrix-algebra","text":"","title":"Chapter 7 | Iterative Techniques in Matrix Algebra"},{"location":"Mathematics_Basis/NA/Chap_7/#norms-of-vectors-and-matrices","text":"","title":"Norms of Vectors and Matrices"},{"location":"Mathematics_Basis/NA/Chap_7/#vector-norms","text":"Definition A vector norm on \\(\\mathbb{R}^n\\) is a function \\(||\\cdot||\\) , from \\(\\mathbb{R}^n\\) into \\(\\mathbb{R}\\) with the following properties. \\[ \\begin{align} \\forall \\mathbf{x, y} \\in \\mathbb{R}^n, \\alpha \\in \\mathbb{R}, & (1)\\ ||\\mathbf{x}|| \\ge 0;\\ ||\\mathbf{x}|| = 0 \\Leftrightarrow \\mathbf{x} = \\mathbf{0}. \\\\ & (2)\\ ||\\alpha \\mathbf{x}|| = |\\alpha| \\cdot ||\\mathbf{x}|| \\\\ & (3)\\ ||\\mathbf{x} + \\mathbf{y}|| \\le ||\\mathbf{x}|| + ||\\mathbf{y}|| \\\\ \\end{align} \\] Commonly used examples L1 Norm: \\(||\\mathbf{x}||_1 = \\sum\\limits_{i = 1}^n|x_i|\\) . L2 Norm / Euclidean Norm: \\(||\\mathbf{x}||_2 = \\sqrt{\\sum\\limits_{i = 1}^n|x_i|^2}\\) . p-Norm: \\(||\\mathbf{x}||_p = \\left(\\sum\\limits_{i = 1}^n|x_i|^p\\right)^{1/p}\\) . Infinity Norm: \\(||\\mathbf{x}||_\\infty = \\max\\limits_{1 \\le i \\le n} |x_i|\\) .","title":"Vector Norms"},{"location":"Mathematics_Basis/NA/Chap_7/#convergence-of-vector","text":"Similarly with a scalar, a sequence of vectors \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) in \\(\\mathbb{R}^n\\) is said to converge to \\(\\mathbf{x}\\) with respect to norm \\(||\\cdot||\\) , if \\(\\forall \\epsilon \\gt 0,\\exists N \\in \\mathbb{N}\\) , s.t \\(\\forall k \\gt N, ||\\mathbf{x}^{(k)} - \\mathbf{x}|| \\lt \\epsilon\\) . Theorem \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) in \\(\\mathbb{R}^n\\) converges to \\(\\mathbf{x}\\) with respect to norm \\(||\\cdot||_\\infty\\) if and only if \\(\\forall i,\\lim_{k \\rightarrow \\infty}x_i^{(k)} = x_i\\) .","title":"Convergence of Vector"},{"location":"Mathematics_Basis/NA/Chap_7/#equivalence","text":"Definition If \\(\\exists C_1, C_2,\\ s.t.\\ C_1||\\mathbf{x}||_B \\le ||\\mathbf{x}||_A \\le C_2||\\mathbf{x}||_B\\) , then \\(||\\mathbf{x}||_A\\) and \\(||\\mathbf{x}||_B\\) are equivalent . Theorem All the vector norms on \\(\\mathbb{R}^n\\) are equivalent.","title":"Equivalence"},{"location":"Mathematics_Basis/NA/Chap_7/#matrix-norms","text":"Definition A matrix norm on \\(\\mathbb{R}^n\\) is a function \\(||\\cdot||\\) , from \\(M_n(\\mathbb{R})\\) matrices into \\(\\mathbb{R}\\) with the following properties. \\[ \\begin{align} \\forall A, B \\in M_n(\\mathbb{R}), \\alpha \\in \\mathbb{R}, & (1)\\ ||A|| \\ge 0;\\ ||A|| = 0 \\Leftrightarrow A = \\mathbf{O}. \\\\ & (2)\\ ||\\alpha A|| = |\\alpha| \\cdot ||A|| \\\\ & (3)\\ ||A + B|| \\le ||A|| + ||B|| \\\\ & (4)\\ ||AB|| \\le ||A|| \\cdot ||B|| \\end{align} \\] Commonly used examples Frobenius Norm: \\(||A||_F = \\sqrt{\\sum\\limits_{i=1}^n\\sum_{j=1}^n|a_{ij}|^2}\\) . Natural Norm: \\(||A||_p = \\max\\limits_{\\mathbf{x}\\ne \\mathbf{0}} \\frac{||A\\mathbf{x}||_p}{||\\mathbf{x}||_p} = \\max\\limits_{||\\mathbf{x}_p|| = 1} ||A\\mathbf{x}||_p\\) , where \\(||\\cdot||_p\\) is the vector norm. \\(||A||_\\infty = \\max\\limits_{1\\le i \\le n}\\sum\\limits_{j=1}^n|a_{ij}|\\) . \\(||A||_1= \\max\\limits_{1\\le j \\le n}\\sum\\limits_{i=1}^n|a_{ij}|\\) . (Spectral Norm) \\(||A||_2= \\sqrt{\\lambda_{max}(A^TA)}\\) . Corollary For any vector \\(\\mathbf{x} \\ne 0\\) , matrix \\(A\\) , and any natural norm \\(||\\cdot||\\) , we have \\[ ||A\\mathbf{x}|| \\le ||A|| \\cdot ||\\mathbf{x}||. \\]","title":"Matrix Norms"},{"location":"Mathematics_Basis/NA/Chap_7/#eigenvalues-and-eigenvectors","text":"Definition (Recap) If \\(A\\) is a square matrix, the **characteristic polynomial$ of \\(A\\) is defined by \\[ p(\\lambda) = \\text{det}(A - \\lambda I). \\] The roots of \\(p\\) are eigenvalues . If \\(\\lambda\\) is an eigenvalue and \\(\\mathbf{x} \\ne 0\\) satisfies \\((A - \\lambda I)\\mathbf{x} = \\mathbf{0}\\) , then \\(\\mathbf{x}\\) is an eigenvector .","title":"Eigenvalues and Eigenvectors"},{"location":"Mathematics_Basis/NA/Chap_7/#spectral-radius","text":"Definition The spectral radius of a matrix \\(A\\) is defined by \\[ \\rho(A) = \\max|\\lambda| \\text{, where $\\lambda$ is an eigenvalue of $A$}. \\] (Recap that for complex \\(\\lambda = \\alpha + \\beta i\\) , \\(|\\lambda| = \\sqrt{\\alpha^2 + \\beta^2}\\) .) Theorem \\(\\forall A \\in M_n(\\mathbb{R})\\) , \\(||A||_2 = \\sqrt{\\rho(A^tA)}\\) . \\(\\rho(A) \\le ||A||\\) , for any natural norm \\(||\\cdot||\\) . Proof A proof for the second property. Suppose \\(\\lambda\\) is an eigenvalue of \\(A\\) with eigenvector \\(\\mathbf{x}\\) and \\(||\\mathbf{x}|| = 1\\) , \\[ |\\lambda| = |\\lambda| \\cdot ||\\mathbf{x}|| = ||\\lambda \\mathbf{x}|| \\le ||A|| \\cdot ||\\mathbf{x}|| = ||A||. \\] Thus, \\[ \\rho(A) = \\max|\\lambda| \\le ||A||. \\]","title":"Spectral Radius"},{"location":"Mathematics_Basis/NA/Chap_7/#convergence-of-matrix","text":"Definition \\(A \\in M_n(\\mathbb{R}))\\) is convergent if \\[ \\lim_{k \\rightarrow \\infty}\\left(A^k\\right)_{ij} = 0 \\text{ , for each } i = 1, 2, \\dots, n \\text{ and } j = 1, 2, \\dots, n. \\] Theorem The following statements are equivalent. \\(A\\) is a convergent matrix. \\(\\lim\\limits_{n \\rightarrow \\infty} ||A^n|| = 0\\) , for some natural norm. \\(\\lim\\limits_{n \\rightarrow \\infty} ||A^n|| = 0\\) , for all natural norms. \\(\\rho(A) < 1\\) . \\(\\forall \\mathbf{x}, \\lim\\limits_{n \\rightarrow \\infty} ||A^n \\mathbf{x}|| = \\mathbf{0}\\) .","title":"Convergence of Matrix"},{"location":"Mathematics_Basis/NA/Chap_7/#iterative-techniques-for-solving-linear-systems","text":"\\[ A\\mathbf{x} = \\mathbf{b} \\Leftrightarrow (D - L - U)\\mathbf{x} = \\mathbf{b} \\Leftrightarrow D\\mathbf{x} = (L + U)\\mathbf{x} + \\mathbf{b} \\\\ \\] Thus, \\[ \\mathbf{x} = D^{-1}(L + U)\\mathbf{x} + D^{-1}\\mathbf{b} \\]","title":"Iterative Techniques for Solving Linear Systems"},{"location":"Mathematics_Basis/NA/Chap_7/#jacobi-iterative-method","text":"Let \\(T_j = D^{-1}(L+U)\\) and \\(\\mathbf{c}_\\mathbf{j} = D^{-1}\\mathbf{b}\\) , then \\[ \\mathbf{x}^{(k)} = T_j\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\mathbf{j}. \\]","title":"Jacobi Iterative Method"},{"location":"Mathematics_Basis/NA/Chap_7/#gauss-seidel-iterative-method","text":"\\[ \\small \\mathbf{x}^{(k)} = D^{-1}(L\\mathbf{x}^{(k)} + U\\mathbf{x}^{(k - 1)}) + D^{-1}\\mathbf{b} \\Leftrightarrow \\mathbf{x}^{(k)} = (D - L)^{-1}U\\mathbf{x}^{(k - 1)} + (D - L)^{-1}\\mathbf{b} \\] Let \\(T_g = (D - L)^{-1}U\\) and \\(\\mathbf{c}_\\mathbf{g} = (D - L)^{-1}\\mathbf{b}\\) , then \\[ \\mathbf{x}^{(k)} = T_g\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\mathbf{g}. \\]","title":"Gauss-Seidel Iterative Method"},{"location":"Mathematics_Basis/NA/Chap_7/#convergence-of-iterative-methods","text":"Consider the following formula \\[ \\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c} \\] where \\(\\mathbf{x}^{(0)}\\) is arbitrary. Lemma If \\(\\rho(T) \\lt 1\\) , then \\((I - T)^{-1}\\) exists and \\[ (I - T)^{-1} = \\sum\\limits_{j = 0}^\\infty T^j. \\] Proof Suppose \\(\\lambda\\) is an eigenvalue of \\(T\\) with eigenvector \\(\\mathbf{x}\\) , then since \\(T\\mathbf{x} = \\lambda \\mathbf{x} \\Leftrightarrow (I - T)\\mathbf{x} = (1 - \\lambda)\\mathbf{x}\\) , thus \\(1 - \\lambda\\) is an eigenvalue of \\(I - T\\) . Since \\(|\\lambda| \\le \\rho(T) < 1\\) , thus \\(\\lambda = 1\\) is not an eigenvalue of \\(T\\) and \\(0\\) is not an eigenvvalue of \\(I - T\\) . Hence, \\((I - T)^{-1}\\) exists. Let \\(S_m = I + T + T^2 + \\cdots + T^m\\) , then \\[ (I - T)S_m = (1 + T + \\cdots + T^m) - (T + T^2 + \\cdots + T^{m + 1}) = I - T^{m + 1}. \\] Since \\(T\\) is convergent, thus \\[ \\lim\\limits_{m \\rightarrow \\infty} (I - T)S_m = \\lim\\limits_{m \\rightarrow \\infty}(I - T^{m + 1}) = I. \\] Thus, $(I - T)^{-1} = \\lim\\limits_{m \\rightarrow \\infty}S_m = \\sum\\limits_{j = 0}^\\infty T^j. Theorem \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) defined by \\(\\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c}\\) converges to the unique solution of \\(\\mathbf{x} = T\\mathbf{x} + \\mathbf{c}\\) if and only if \\(\\rho(T) \\lt 1\\) . Proof \\(\\Rightarrow\\) : define error \\(\\mathbf{e}^{(k)} = \\mathbf{x} - \\mathbf{x}^{(k)}\\) , then \\[ \\mathbf{e}^{(k)} = (T\\mathbf{x} + c) - (T\\mathbf{x}^{(k - 1)} + c) = T(\\mathbf{x} - \\mathbf{x}^{(k - 1)})T\\mathbf{e}^{(k - 1)} \\Rightarrow \\mathbf{e}^{(k)} = T^k \\mathbf{e}^{(0)} \\] Since it converges, thus \\[ \\lim\\limits_{k \\rightarrow \\infty} \\mathbf{e}^{(k)} = 0 \\Rightarrow \\lim\\limits_{k \\rightarrow \\infty} T^k \\mathbf{e}^{(0)} = 0, \\forall \\mathbf{e^{(0)}} \\] \\[ \\Leftrightarrow \\rho(T) < 1 \\] \\(\\Leftarrow\\) : \\[ \\mathbf{x}^{(k)} = T^{(k)}\\mathbf{x}^(0) + (T^{k - 1} + \\cdots + T + I) \\mathbf{c}. \\] Since \\(\\rho(T) < 1\\) , \\(T\\) is convergent and \\[ \\lim\\limits_{k \\rightarrow \\infty} T^k x^{(0)} = \\mathbf{0}. \\] From the Lemma above, we have \\[ \\lim\\limits_{k \\rightarrow \\infty} \\mathbf{x}^{(k)} = \\lim\\limits_{k \\rightarrow \\infty} T^k\\mathbf{x}^{(0)} + \\left(\\sum\\limits_{j = 0}^\\infty T^j \\right)\\mathbf{c} = \\mathbf{0} + (I - T)^{-1}\\mathbf{c} = (I - T)^{-1}\\mathbf{c}. \\] Thus \\(\\{\\mathbf{x}^{(k)}\\}\\) converges to \\(\\mathbf{x} \\equiv (I - T)^{-1} \\Leftrightarrow \\mathbf{x} = T\\mathbf{x} + c\\) . Corollary If \\(||T||\\lt 1\\) for any matrix norm and \\(\\mathbf{c}\\) is a given vector, then for all \\(\\mathbf{x}^{(0)}\\in \\mathbb{R}^n\\) , \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) defined by \\(\\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c}\\) converges to \\(\\mathbf{x}\\) , and the following error bounds hold \\(||\\mathbf{x} - \\mathbf{x}^{(k)}|| \\le ||T||^k||\\mathbf{x}^{(0)} - \\mathbf{x}||\\) . \\(||\\mathbf{x} - \\mathbf{x}^{(k)}|| \\le \\frac{||T||^k}{1 - ||T||}||\\mathbf{x}^{(1)} - \\mathbf{x}||\\) . Theorem \\(\\forall A\\) is strictly diagonally dominant, \\(\\forall \\text{\\bf{x}}^{(0)}\\) , both Jacobi and Gauss-Seidel methods give \\(\\{\\mathbf{x}^{(k)}\\}_{k=0}^\\infty\\) that converge to the unique solution of \\(A\\mathbf{x} = \\mathbf{b}\\) .","title":"Convergence of Iterative Methods"},{"location":"Mathematics_Basis/NA/Chap_7/#relaxation-methods","text":"Definition Suppose \\(\\mathbf{\\tilde x}\\) is an approximation to the solution of \\(A\\mathbf{x} = \\mathbf{b}\\) , then the residual vector for \\(\\mathbf{\\tilde x}\\) w.r.t this linear system is \\(\\mathbf{r} = \\mathbf{b} - A\\mathbf{\\tilde x}\\) . Further examine Gauss-Seidel method, \\[ x_i^{(k)} = x_i^{(k - 1)} + \\frac{r_i^{(k)}}{a_{ii}}, \\text{ where } r_i^{(k)} = b_i - \\sum_{j \\lt i} a_{ij}x_j^{(k)} - \\sum_{j \\ge i} a_{ij}x_j^{(k - 1)} \\] Let \\(x_i^{(k)} = x_i^{(k - 1)} + \\omega\\frac{r_i^{(k)}}{a_{ii}}\\) , by modifying the value of \\(\\omega\\) , we can somehow get faster convergence. \\(0 \\lt \\omega \\le 1\\) Under-Relaxation Method \\(\\omega = 1\\) Gauss-Seidel Method \\(\\omega \\gt 1\\) Successive Over-Relaxation Method (SOR) In matrix form, \\[ \\mathbf{x}^{(k)} = (D - \\omega L)^{-1}[(1 - \\omega)D + \\omega U]\\mathbf{x}^{(k - 1)} + (D - \\omega L)^{-1}\\mathbf{b} \\] Let \\(T_\\omega = (D - \\omega L)^{-1}[(1 - \\omega)D + \\omega U]\\) and \\(\\mathbf{c}_\\omega = (D - \\omega L)^{-1}\\mathbf{b}\\) , then \\[ \\mathbf{x}^{(k)} = T_\\omega\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\omega. \\] Theorem (Kahan) If \\(a_{ii} \\ne 0\\) , then \\(\\rho(T_\\omega)\\ge |\\omega -1 |\\) , which implies \\(0 \\lt \\omega \\lt 2\\) . Theorem (Ostrowski-Reich) If \\(A\\) is positive definite and \\(0 \\lt \\omega \\lt 2\\) , the SOR method converges for any choice of initial approximation. Theorem If \\(A\\) is positive definite and tridiagonal, then \\(\\rho(T_g) = \\rho(T_j)^2\\lt1\\) , and the optimal choice of \\(\\omega\\) for the SOR method is \\[ \\omega = \\frac{2}{1 + \\sqrt{1 - \\rho(T_j)^2}}. \\]","title":"Relaxation Methods"},{"location":"Mathematics_Basis/NA/Chap_7/#error-bounds-and-iterative-refinement","text":"Definition The conditional number of the nonsigular matrix \\(A\\) relative to a norm \\(||\\cdot||\\) $ is \\[ K(A) = ||A|| \\cdot ||A^{-1}||. \\] A matrix \\(A\\) is well-conditioned if \\(K(A)\\) is close to 1, and is ill-conditioned when \\(K(A)\\) is significantly greater than 1. Proposition If \\(A\\) is symmetric, then \\(K(A)_2 = \\frac{\\max|\\lambda|}{\\min|\\lambda|}\\) . \\(K(A)_2 = 1\\) if \\(A\\) is orthogonal. \\(\\forall \\text{ orthogonal matrix }R, K(RA)_2 = K(AR)_2 = K(A)_2.\\) \\(\\forall \\text{ natural norm } ||\\cdot||_p,\\ K(A)_p \\ge 1.\\) \\(K(\\alpha A) = K(A).\\) Theorem For any natural norm, \\[ ||\\mathbf{x} - \\mathbf{\\tilde x}|| \\le ||\\mathbf{r}|| \\cdot ||A^{-1}|| \\] and if \\(\\mathbf{x} \\ne \\mathbf{0}\\) and \\(\\mathbf{b} \\ne \\mathbf{0}\\) , \\[ \\frac{||\\mathbf{x} - \\mathbf{\\tilde x}||}{||\\mathbf{x}||} \\le ||A||\\cdot||A^{-1}|| \\frac{||\\mathbf{r}||}{||\\mathbf{b}||} = K(A)\\frac{||\\mathbf{r}||}{||\\mathbf{b}||}. \\] Iterative Refinement Step.1 Solve \\(A\\mathbf{x} = \\mathbf{b}\\) and get an approximation solution \\(\\mathbf{x}_{0}\\) . Let \\(i = 1\\) . Step.2 Let \\(\\mathbf{r} = \\mathbf{b} - A\\mathbf{x}_{i - 1}\\) . Step.3 Solve \\(A\\mathbf{d} = \\mathbf{r}\\) and get the solution \\(\\mathbf{d}\\) . Step.4 The better approximation is \\(\\mathbf{x}_{i} = \\mathbf{x}_{i - 1} + \\mathbf{d}.\\) Step.5 Judge whether it's precise enough. If not, let \\(i = i + 1\\) and then repeat from Step.2 . In reality, \\(A\\) and \\(\\mathbf{b}\\) may be perturbed by an amount \\(\\delta A\\) and \\(\\delta \\mathbf{b}\\) . For \\(A(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b} + \\delta\\mathbf{b}\\) \\[ \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le ||A|| \\cdot ||A^{-1}|| \\cdot \\frac{||\\delta \\mathbf{b}||}{||\\mathbf{b}||} \\] For \\((A + \\delta A)(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b}\\) \\[ \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le \\frac{||A^{-1}|| \\cdot ||\\delta A||}{1 - ||A^{-1}|| \\cdot ||\\delta A||} = \\frac{||A|| \\cdot ||A^{-1}|| \\cdot \\frac{||\\delta A||}{||A||}}{1 - ||A|| \\cdot ||A^{-1}|| \\cdot\\frac{||\\delta A||}{||A||}} \\] Theorem If \\(A\\) is nonsingular and \\[ ||\\delta A|| \\lt \\frac{1}{||A^{-1}||} \\] then \\((A + \\delta A)(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b} + \\delta\\mathbf{b}\\) with the error estimate \\[ \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le \\frac{K(A)}{1 - K(A)\\frac{||\\delta A||}{||A||}}\\left(\\frac{||\\delta A||}{||A||} + \\frac{||\\delta\\mathbf{b}||}{||\\mathbf{b}||}\\right) \\]","title":"Error Bounds and Iterative Refinement"},{"location":"Mathematics_Basis/NA/Chap_8/","text":"Chapter 8 | Approximation Theory \u00b6","title":"Chap 8"},{"location":"Mathematics_Basis/NA/Chap_8/#chapter-8-approximation-theory","text":"","title":"Chapter 8 | Approximation Theory"},{"location":"Mathematics_Basis/NA/Chap_9/","text":"Chapter 9 | Approximating Eigenvalues \u00b6 Power Method \u00b6 The Power Method is an iterative technique used to determine the dominant eigenvalue of a matrix (the eigenvalue with the largest magnitude). Suppose \\(A \\in M_n(\\mathbb{R})\\) with eigenvalues satisfying \\(|\\lambda_1| \\gt |\\lambda_2| \\ge \\dots \\ge |\\lambda_n|\\) and their corresponding linearly independent eigenvectors \\(\\mathbf{v}_\\mathbf{1}, \\dots, \\mathbf{v}_\\mathbf{n}\\) . Original Method Start from any \\(\\mathbf{x}^{(0)} \\ne \\mathbf{0}\\) and \\((\\mathbf{x}^{(0)}, \\mathbf{v}_\\mathbf{1}) \\ne 0\\) , and suppose \\(\\mathbf{x}^{(0)} = \\sum\\limits_{j = 1}^n\\beta_j\\mathbf{v}_\\mathbf{j}\\) , \\(\\beta_1 \\ne 0.\\) \\(\\mathbf{x}^{(k)} = A \\mathbf{x}^{(k - 1)} = \\sum\\limits_{j = 1}^n\\beta_j\\lambda_j^k\\mathbf{v}_\\mathbf{j} = \\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}\\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\lambda_1^k\\beta_1\\mathbf{v}_\\mathbf{1}\\) . \\(A\\mathbf{x}^{(k)} \\approx \\lambda_1^k\\beta_1A\\mathbf{v}_\\mathbf{1} \\approx \\lambda_1\\mathbf{x}^{(k)}\\) , thus \\[ \\lambda_1 = \\frac{\\mathbf{x}^{(k + 1)}_i}{\\mathbf{x}^{(k)}_i}, \\mathbf{v}_\\mathbf{1} = \\frac{\\mathbf{x}^{(k)}}{\\lambda_1^k\\beta_1}. \\] Motivation for Normalization: In the original method, if \\(|\\lambda| > 1\\) then \\(\\mathbf{x}^{(k)}\\) diverges. If \\(|\\lambda| < 1\\) then \\(\\mathbf{x}^{(k)}\\) converges to 0. Both cases are not suitable for actual computing. Thus we need normalization to make sure \\(||\\mathbf{x}||_\\infty = 1\\) at each step to guarantee the stableness . Normalization Let \\(\\mathbf{u}^{(k-1)} = \\frac{\\mathbf{x}^{(k-1)}}{||\\mathbf{x}^{(k-1)}||_\\infty}, \\mathbf{x}^{(k)} = A\\mathbf{u}^{(k - 1)}\\) , then \\[ \\begin{align} & \\mathbf{u}^{(k)} = \\frac{\\mathbf{x}^{(k)}}{||\\mathbf{x}^{(k)}||_\\infty} = \\frac{\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}}{\\left|\\left|\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}\\right|\\right|_\\infty} \\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty} \\\\ & \\mathbf{x}^{(k)} = \\frac{A^k\\mathbf{x}_\\mathbf{0}}{||A^{k - 1}\\mathbf{x}_\\mathbf{0}||} = \\frac{\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}}{\\left|\\left|\\lambda_1^{k - 1} \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^{k - 1}\\mathbf{v}_\\mathbf{j}\\right|\\right|_\\infty} \\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\lambda_1 \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty}=\\lambda_1 \\mathbf{u}^{(k)}. \\end{align} \\] Thus \\[ \\lambda_1 = \\frac{\\mathbf{x}_i^{(k)}}{\\mathbf{u}_i^{(k)}}, \\hat{\\mathbf{v}_\\mathbf{1}} = \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty}= \\mathbf{u}^{(k)}. \\] Remark For multiple eigenvalues \\(\\lambda_1 = \\lambda_2 = \\dots = \\lambda_r\\) , \\[ \\mathbf{x}^{(k)} = \\lambda_1^k\\left( \\sum_{j = 1}^r\\beta_j\\mathbf{v}_\\mathbf{j} + \\sum_{j = r + 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j} \\right)\\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\lambda_1^k\\sum_{j = 1}^r\\beta_j\\mathbf{v}_\\mathbf{j} \\] The method fails to converge if \\(\\lambda_1 = -\\lambda_2\\) . Aitken's \\(\\Delta^2\\) procedure can be used to speed up the convergence. Rate of Convergence \u00b6 Examine \\(\\textbf{x}^{(k)} = \\lambda_1^k \\sum_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\textbf{v}_\\textbf{j}\\) > We find that \\(\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)\\) determines the rate of convergence, especially \\(\\left|\\frac{\\lambda_2}{\\lambda_1}\\right|\\) . Target: Make \\(\\left|\\frac{\\lambda_2}{\\lambda_1}\\right|\\) as small as possible. Example Let \\(p = \\frac12 (\\lambda_2 + \\lambda_n)\\) , and \\(B = A - pI\\) , then \\(B\\) has the eigenvalues \\[ \\lambda_1 - p, \\lambda_2 - p, \\dots, \\lambda_n - p. \\] And since \\[ \\left|\\frac{\\lambda_2 - p}{\\lambda_1 - p}\\right| < \\left|\\frac{\\lambda_2}{\\lambda_1}\\right|, \\] the iteration for finding the eigenvalues of \\(B\\) converges much faster than that of \\(A\\) . But how to find \\(p\\) ? ... Inverse Power Method \u00b6 If \\(A\\) has eigenvalues satisfying \\(|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\gt |\\lambda_n|\\) , then \\(A^{-1}\\) has \\[ \\left|\\frac{1}{\\lambda_n}\\right| \\gt \\left|\\frac{1}{\\lambda_{n-1}}\\right| \\ge \\cdots \\ge \\left|\\frac{1}{\\lambda_1}\\right|. \\] The dominant eigenvalue of \\(A^{-1}\\) is the eigenvalue with the smallest magnitude of \\(A\\) . A way to find eigenvalue \\(\\lambda\\) closest to a given value \\(p\\) Use power method for matrix \\((A - qI)^{-1}\\) , which has the eigenvalues \\[ \\left|\\frac{1}{\\lambda_1 - q}\\right| \\gt \\left|\\frac{1}{\\lambda_2 - q}\\right| \\ge \\cdots \\ge \\left|\\frac{1}{\\lambda_n - q}\\right|. \\] where \\(\\lambda_1\\) is the closest to \\(p\\) .","title":"Chap 9"},{"location":"Mathematics_Basis/NA/Chap_9/#chapter-9-approximating-eigenvalues","text":"","title":"Chapter 9 | Approximating Eigenvalues"},{"location":"Mathematics_Basis/NA/Chap_9/#power-method","text":"The Power Method is an iterative technique used to determine the dominant eigenvalue of a matrix (the eigenvalue with the largest magnitude). Suppose \\(A \\in M_n(\\mathbb{R})\\) with eigenvalues satisfying \\(|\\lambda_1| \\gt |\\lambda_2| \\ge \\dots \\ge |\\lambda_n|\\) and their corresponding linearly independent eigenvectors \\(\\mathbf{v}_\\mathbf{1}, \\dots, \\mathbf{v}_\\mathbf{n}\\) . Original Method Start from any \\(\\mathbf{x}^{(0)} \\ne \\mathbf{0}\\) and \\((\\mathbf{x}^{(0)}, \\mathbf{v}_\\mathbf{1}) \\ne 0\\) , and suppose \\(\\mathbf{x}^{(0)} = \\sum\\limits_{j = 1}^n\\beta_j\\mathbf{v}_\\mathbf{j}\\) , \\(\\beta_1 \\ne 0.\\) \\(\\mathbf{x}^{(k)} = A \\mathbf{x}^{(k - 1)} = \\sum\\limits_{j = 1}^n\\beta_j\\lambda_j^k\\mathbf{v}_\\mathbf{j} = \\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}\\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\lambda_1^k\\beta_1\\mathbf{v}_\\mathbf{1}\\) . \\(A\\mathbf{x}^{(k)} \\approx \\lambda_1^k\\beta_1A\\mathbf{v}_\\mathbf{1} \\approx \\lambda_1\\mathbf{x}^{(k)}\\) , thus \\[ \\lambda_1 = \\frac{\\mathbf{x}^{(k + 1)}_i}{\\mathbf{x}^{(k)}_i}, \\mathbf{v}_\\mathbf{1} = \\frac{\\mathbf{x}^{(k)}}{\\lambda_1^k\\beta_1}. \\] Motivation for Normalization: In the original method, if \\(|\\lambda| > 1\\) then \\(\\mathbf{x}^{(k)}\\) diverges. If \\(|\\lambda| < 1\\) then \\(\\mathbf{x}^{(k)}\\) converges to 0. Both cases are not suitable for actual computing. Thus we need normalization to make sure \\(||\\mathbf{x}||_\\infty = 1\\) at each step to guarantee the stableness . Normalization Let \\(\\mathbf{u}^{(k-1)} = \\frac{\\mathbf{x}^{(k-1)}}{||\\mathbf{x}^{(k-1)}||_\\infty}, \\mathbf{x}^{(k)} = A\\mathbf{u}^{(k - 1)}\\) , then \\[ \\begin{align} & \\mathbf{u}^{(k)} = \\frac{\\mathbf{x}^{(k)}}{||\\mathbf{x}^{(k)}||_\\infty} = \\frac{\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}}{\\left|\\left|\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}\\right|\\right|_\\infty} \\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty} \\\\ & \\mathbf{x}^{(k)} = \\frac{A^k\\mathbf{x}_\\mathbf{0}}{||A^{k - 1}\\mathbf{x}_\\mathbf{0}||} = \\frac{\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}}{\\left|\\left|\\lambda_1^{k - 1} \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^{k - 1}\\mathbf{v}_\\mathbf{j}\\right|\\right|_\\infty} \\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\lambda_1 \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty}=\\lambda_1 \\mathbf{u}^{(k)}. \\end{align} \\] Thus \\[ \\lambda_1 = \\frac{\\mathbf{x}_i^{(k)}}{\\mathbf{u}_i^{(k)}}, \\hat{\\mathbf{v}_\\mathbf{1}} = \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty}= \\mathbf{u}^{(k)}. \\] Remark For multiple eigenvalues \\(\\lambda_1 = \\lambda_2 = \\dots = \\lambda_r\\) , \\[ \\mathbf{x}^{(k)} = \\lambda_1^k\\left( \\sum_{j = 1}^r\\beta_j\\mathbf{v}_\\mathbf{j} + \\sum_{j = r + 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j} \\right)\\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\lambda_1^k\\sum_{j = 1}^r\\beta_j\\mathbf{v}_\\mathbf{j} \\] The method fails to converge if \\(\\lambda_1 = -\\lambda_2\\) . Aitken's \\(\\Delta^2\\) procedure can be used to speed up the convergence.","title":"Power Method"},{"location":"Mathematics_Basis/NA/Chap_9/#rate-of-convergence","text":"Examine \\(\\textbf{x}^{(k)} = \\lambda_1^k \\sum_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\textbf{v}_\\textbf{j}\\) > We find that \\(\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)\\) determines the rate of convergence, especially \\(\\left|\\frac{\\lambda_2}{\\lambda_1}\\right|\\) . Target: Make \\(\\left|\\frac{\\lambda_2}{\\lambda_1}\\right|\\) as small as possible. Example Let \\(p = \\frac12 (\\lambda_2 + \\lambda_n)\\) , and \\(B = A - pI\\) , then \\(B\\) has the eigenvalues \\[ \\lambda_1 - p, \\lambda_2 - p, \\dots, \\lambda_n - p. \\] And since \\[ \\left|\\frac{\\lambda_2 - p}{\\lambda_1 - p}\\right| < \\left|\\frac{\\lambda_2}{\\lambda_1}\\right|, \\] the iteration for finding the eigenvalues of \\(B\\) converges much faster than that of \\(A\\) . But how to find \\(p\\) ? ...","title":"Rate of Convergence"},{"location":"Mathematics_Basis/NA/Chap_9/#inverse-power-method","text":"If \\(A\\) has eigenvalues satisfying \\(|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\gt |\\lambda_n|\\) , then \\(A^{-1}\\) has \\[ \\left|\\frac{1}{\\lambda_n}\\right| \\gt \\left|\\frac{1}{\\lambda_{n-1}}\\right| \\ge \\cdots \\ge \\left|\\frac{1}{\\lambda_1}\\right|. \\] The dominant eigenvalue of \\(A^{-1}\\) is the eigenvalue with the smallest magnitude of \\(A\\) . A way to find eigenvalue \\(\\lambda\\) closest to a given value \\(p\\) Use power method for matrix \\((A - qI)^{-1}\\) , which has the eigenvalues \\[ \\left|\\frac{1}{\\lambda_1 - q}\\right| \\gt \\left|\\frac{1}{\\lambda_2 - q}\\right| \\ge \\cdots \\ge \\left|\\frac{1}{\\lambda_n - q}\\right|. \\] where \\(\\lambda_1\\) is the closest to \\(p\\) .","title":"Inverse Power Method"},{"location":"Mathematics_Basis/NA/class_notes/","text":"Class Notes \u00b6 Lecturer \u8bb8\u5a01\u5a01 Textbook \u00b6 Lecture Grade (100 %) \u00b6 Laboratory Projects (36%) 8 labs. Release at a time. Classroom Quiz (4%) Twice, 2% for each test Can also answer questions in class (1% for each) Research Topics (15%) Done in groups 18 topics to choose from In class presentations (5~10 minutes) Homework (5%) Final exam (40%)","title":"Class Notes"},{"location":"Mathematics_Basis/NA/class_notes/#class-notes","text":"Lecturer \u8bb8\u5a01\u5a01","title":"Class Notes"},{"location":"Mathematics_Basis/NA/class_notes/#textbook","text":"","title":"Textbook"},{"location":"Mathematics_Basis/NA/class_notes/#lecture-grade-100","text":"Laboratory Projects (36%) 8 labs. Release at a time. Classroom Quiz (4%) Twice, 2% for each test Can also answer questions in class (1% for each) Research Topics (15%) Done in groups 18 topics to choose from In class presentations (5~10 minutes) Homework (5%) Final exam (40%)","title":"Lecture Grade (100 %)"}]}