{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u3053\u3053\u306f \u2014\u2014 \u306a\u3093\u3068! \u00b6 A note book by @sakuratsuyu. \u5076\u5c14\u5199\u70b9\u4e1c\u897f\uff0c\u5e0c\u671b\u4f60\u559c\u6b22\u3002 \\[ \\tt{Transfering\\ content\\ from\\ other\\ places\\ these\\ days\\ } \\cdots\\ \\&\\$@\\%\\&\\$@\\%\\&\\$@\\% \\] @sakuratsuyu \u306b\u3064\u3044\u3067 \u2014 Sophomore as a ZJU CS Undergraduate. ZJUSCT Freshman. (\u6478\u5927\u9c7c) MADer. (MAD \u5728\u505a\u4e86\u5728\u505a\u4e86.jpg) Hope you can find anything useful and interesting here! \u2728\u30ad\u30e9\u30ad\u30e9\u76ee\u2728","title":"\u3053\u3053\u306f \u2014\u2014 \u306a\u3093\u3068!"},{"location":"#_1","text":"A note book by @sakuratsuyu. \u5076\u5c14\u5199\u70b9\u4e1c\u897f\uff0c\u5e0c\u671b\u4f60\u559c\u6b22\u3002 \\[ \\tt{Transfering\\ content\\ from\\ other\\ places\\ these\\ days\\ } \\cdots\\ \\&\\$@\\%\\&\\$@\\%\\&\\$@\\% \\] @sakuratsuyu \u306b\u3064\u3044\u3067 \u2014 Sophomore as a ZJU CS Undergraduate. ZJUSCT Freshman. (\u6478\u5927\u9c7c) MADer. (MAD \u5728\u505a\u4e86\u5728\u505a\u4e86.jpg) Hope you can find anything useful and interesting here! \u2728\u30ad\u30e9\u30ad\u30e9\u76ee\u2728","title":"\u3053\u3053\u306f \u2014\u2014 \u306a\u3093\u3068!"},{"location":"comming_soon/","text":"Waiting... \u00b6 Failure Things there are comming soon.","title":"Waiting..."},{"location":"comming_soon/#waiting","text":"Failure Things there are comming soon.","title":"Waiting..."},{"location":"discussion/","text":"Discussion \u00b6 Free talk here about anything! Maybe I can put some blogrolls there... Anyone wants? Blogrolls wants more friends!","title":"Blogroll"},{"location":"discussion/#discussion","text":"Free talk here about anything! Maybe I can put some blogrolls there... Anyone wants? Blogrolls wants more friends!","title":"Discussion"},{"location":"tasklist/","text":"Task List \u00b6 Make a logo! Mathematics Basis Discrete Mathematics | DM Numerical Analysis | NA Abstract Algrebra Computer Science Courses The Missing Semester of your CS education \u2192 Cheat Sheets / Tools Introduction to Computer Systems | ICS HPC 101 Introduction to Computer Vision | ICV Fundemental of Data Strcuture | FDS Digital Logic Design Cheat Sheets / Tools Git Markdown Latex Vim / Neovim Shell Data Wrangling Profiling Pot-pourri Simple Cryptography Guidance to Configure Manjaro + i3wm","title":"Plan"},{"location":"tasklist/#task-list","text":"Make a logo! Mathematics Basis Discrete Mathematics | DM Numerical Analysis | NA Abstract Algrebra Computer Science Courses The Missing Semester of your CS education \u2192 Cheat Sheets / Tools Introduction to Computer Systems | ICS HPC 101 Introduction to Computer Vision | ICV Fundemental of Data Strcuture | FDS Digital Logic Design Cheat Sheets / Tools Git Markdown Latex Vim / Neovim Shell Data Wrangling Profiling Pot-pourri Simple Cryptography Guidance to Configure Manjaro + i3wm","title":"Task List"},{"location":"Computer_Science_Courses/","text":"Title Page \u00b6 Abstract This section stores the notes of the courses that I've learned from ZJU or other platforms like MIT and Standford. It helps me to build my knowledge system and hope it helps you too. Welcome to point out anything wrong and careless mistakes!","title":"Title Page"},{"location":"Computer_Science_Courses/#title-page","text":"Abstract This section stores the notes of the courses that I've learned from ZJU or other platforms like MIT and Standford. It helps me to build my knowledge system and hope it helps you too. Welcome to point out anything wrong and careless mistakes!","title":"Title Page"},{"location":"Computer_Science_Courses/ICV/1_Introduction/","text":"Lecture 1 Introduction \u00b6 Review of Linear Algrebra \u00b6 Affine Transformations \u4eff\u5c04\u53d8\u6362 \u00b6 Affine map = linear map + translation \\[ \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix} \\] Using homogenous coordinates \uff08\u9f50\u6b21\u5750\u6807\uff09 \\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a & b & t_x \\\\ c & d & t_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\] Eigenvectors and Eigenvalues \u00b6 The eigenvalues of symmetric matrices are real numbers. The eigenvalues of positive definite matrices are positive numbers.","title":"Lecture 1"},{"location":"Computer_Science_Courses/ICV/1_Introduction/#lecture-1-introduction","text":"","title":"Lecture 1 Introduction"},{"location":"Computer_Science_Courses/ICV/1_Introduction/#review-of-linear-algrebra","text":"","title":"Review of Linear Algrebra"},{"location":"Computer_Science_Courses/ICV/1_Introduction/#affine-transformations","text":"Affine map = linear map + translation \\[ \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix} \\] Using homogenous coordinates \uff08\u9f50\u6b21\u5750\u6807\uff09 \\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a & b & t_x \\\\ c & d & t_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\]","title":"Affine Transformations \u4eff\u5c04\u53d8\u6362"},{"location":"Computer_Science_Courses/ICV/1_Introduction/#eigenvectors-and-eigenvalues","text":"The eigenvalues of symmetric matrices are real numbers. The eigenvalues of positive definite matrices are positive numbers.","title":"Eigenvectors and Eigenvalues"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/","text":"Lecture 2 Image Formation \u00b6 Camera and Lens \u00b6 Pinhole Camera \u00b6 A barrier to block off most of the rays. The opening is the aperture \u5149\u5708 . Flaws Less light gets through. Diffraction effects\uff08\u884d\u5c04\uff09 Lens \u900f\u955c \u00b6 \\[ \\frac{1}{i} + \\frac{1}{o} = \\frac{1}{f} \\] Focal length \\(f\\) \u7126\u8ddd \u00b6 if \\(o = \\infty\\) , then \\(f = i\\) . Magnification \\(m\\) \u653e\u5927\u7387 \u00b6 \\[ m = \\frac{h_i}{h_o} \\] Field of View (FOV) \u89c6\u91ce \u00b6 Factor \u00b6 Focal Length Longer focal length, Narrower angle of view. Vice versa. Note 50mm / 46\u00b0 (and full frame) is the most similar FOV with human eyes. Thus 50mm lens is called standard lens \uff08\u6807\u51c6\u955c\u5934\uff09. telephoto lens \uff08\u957f\u7126\u955c\u5934\uff0c\u671b\u8fdc\u955c\u5934\uff09\uff1a\u89c6\u91ce\u5c0f\uff0c\u653e\u5927\u7387\u5927 short focal lens \uff08\u77ed\u7126\u955c\u5934\uff0c\u5e7f\u89d2\u955c\u5934\uff09\uff1a\u89c6\u91ce\u5927\uff0c\u653e\u5927\u7387\u5c0f Sensor Size Bigger sensor size, Wider angle of view. Vice versa. Aperture \u5149\u5708 \u00b6 The representation of aperture is its Diameter \\(D\\) . F-Number \\[ N = \\frac{f}{D} \\text{ (mostly greater than 1, around 1.8 ~ 22)} \\] Lens Defocus \u00b6 Blur Circle Diameter (\u5149\u6591\u534a\u5f84) \\[ b = \\frac{D}{i'}|i' -i|, b \\propto D \\propto \\frac{1}{N} \\] Focusing \u5bf9\u7126 \u00b6 Depth of Field (DoF) \u666f\u6df1 \u00b6 \\[ {\\tt DoF} = o_2 - o_1 = \\frac{2of^2cN(o-f)}{f^4-c^2N^2(o-f)^2} \\] From the equation above DoF is almost proportional to \\(N\\) , and thus the Larger aperture, the Smaller F-Number and the Smaller DoF. How to blur the background \u00b6 Large aperture Long focal length Near foreground Far background Geometric Image Formation\uff08\u5b9a\u4f4d\uff09 \u00b6 Camera model \\[ \\begin{bmatrix} u \\\\ v \\end{bmatrix} = \\begin{bmatrix} f \\frac{x}{z} \\\\ f \\frac{y}{z} \\end{bmatrix} \\] Homogeneous Coordinates / Projective Coordinates \u00b6 Suppose that \\(\\begin{bmatrix} x \\\\ y \\\\ w \\end{bmatrix}\\) is the same as \\(\\begin{bmatrix} x/w \\\\ y/w \\\\ 1 \\end{bmatrix}\\) , then we get \\[ \\begin{bmatrix} f & 0 & 0 & 0 \\\\ 0 & f & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} fx \\\\ fy \\\\ z \\\\ \\end{bmatrix} \\cong \\begin{bmatrix} f\\frac{x}{z} \\\\ f\\frac{y}{z} \\\\ 1 \\end{bmatrix} \\] We can also put the image plane in front of the camera (opposite to the previous picture). Perspective Projection \u00b6 Preserevd - Straight lines are still straight Lost - Length and Angle Vanishing Points \u00b6 Properties Any wo parallel lines have the same vanishing point v. v can be outside the image frame or at infinity. Line from C to v is parallel to the lines. Vanishing Lines \u00b6 Multiple vanishing points The direction of the vanishing line tells us the orientation of the plane. Distortion \u00b6 Converging verticals Problem and Solution (View Camera \u79fb\u8f74\u76f8\u673a) Exterior columns appear bigger Due to lens flaws Radial distortion Due to imperfect lens \\[ \\begin{align} r^2 &= {x'}_n^{2} + {y'}_n^{2} \\\\ {x'}_d &= {x'}_n(1 + \\kappa_1r^2 + \\kappa_2r^4) \\\\ {y'}_d &= {y'}_n(1 + \\kappa_1r^2 + \\kappa_2r^4) \\\\ \\end{align} \\] Solution Take a photo of a grid at the same point and then use the mathmatics to calculate and correct radial distortion. Orthographic Projection \u00b6 \\[ \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\\\ 1 \\\\ \\end{bmatrix} \\Rightarrow (x, y) \\] Photometric Image Formation\uff08\u5b9a\u989c\u8272/\u4eae\u5ea6\uff09 \u00b6 Image Sensor \u00b6 CMOS CCD (Charge Coupled Device) Shutter \u00b6 Shutter speed controls exposure time. Color Sensing \u00b6 Color Spaces RGB HSV Practical Color Sensing: Bayer Filter Shading \u7740\u8272 \u00b6 BRDF (Bidirectional Reflectance Distribution Function) \u00b6 \\[ L_r(\\hat{\\textbf{v}_r};\\lambda) = \\int L_i(\\hat{\\textbf{i}_r};\\lambda)f_r(\\hat{\\textbf{v}_r}, \\hat{\\textbf{v}_i}, \\hat{\\textbf{n}}; \\lambda)\\cos^+\\theta_i\\ d\\hat{\\textbf{v}_i} \\] Diffuse (Lambertian) Reflection \u00b6 Shading independent of view direction Specular Term \u00b6 Intensity depends on view direction Blinn-Phong Reflection Model \u00b6 \\[ L = L_a + L_d + L_s = k_aI_a + k_d(I/r^2)\\max(0, \\textbf{n}\\cdot\\textbf{l}) + k_s(I/r^2)\\max(0, \\textbf{n}\\cdot\\textbf{h})^p \\]","title":"Lecture 2"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#lecture-2-image-formation","text":"","title":"Lecture 2 Image Formation"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#camera-and-lens","text":"","title":"Camera and Lens"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#pinhole-camera","text":"A barrier to block off most of the rays. The opening is the aperture \u5149\u5708 . Flaws Less light gets through. Diffraction effects\uff08\u884d\u5c04\uff09","title":"Pinhole Camera"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#lens","text":"\\[ \\frac{1}{i} + \\frac{1}{o} = \\frac{1}{f} \\]","title":"Lens \u900f\u955c"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#focal-length-f","text":"if \\(o = \\infty\\) , then \\(f = i\\) .","title":"Focal length \\(f\\)  \u7126\u8ddd"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#magnification-m","text":"\\[ m = \\frac{h_i}{h_o} \\]","title":"Magnification \\(m\\) \u653e\u5927\u7387"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#field-of-view-fov","text":"","title":"Field of View (FOV) \u89c6\u91ce"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#factor","text":"Focal Length Longer focal length, Narrower angle of view. Vice versa. Note 50mm / 46\u00b0 (and full frame) is the most similar FOV with human eyes. Thus 50mm lens is called standard lens \uff08\u6807\u51c6\u955c\u5934\uff09. telephoto lens \uff08\u957f\u7126\u955c\u5934\uff0c\u671b\u8fdc\u955c\u5934\uff09\uff1a\u89c6\u91ce\u5c0f\uff0c\u653e\u5927\u7387\u5927 short focal lens \uff08\u77ed\u7126\u955c\u5934\uff0c\u5e7f\u89d2\u955c\u5934\uff09\uff1a\u89c6\u91ce\u5927\uff0c\u653e\u5927\u7387\u5c0f Sensor Size Bigger sensor size, Wider angle of view. Vice versa.","title":"Factor"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#aperture","text":"The representation of aperture is its Diameter \\(D\\) . F-Number \\[ N = \\frac{f}{D} \\text{ (mostly greater than 1, around 1.8 ~ 22)} \\]","title":"Aperture \u5149\u5708"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#lens-defocus","text":"Blur Circle Diameter (\u5149\u6591\u534a\u5f84) \\[ b = \\frac{D}{i'}|i' -i|, b \\propto D \\propto \\frac{1}{N} \\]","title":"Lens Defocus"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#focusing","text":"","title":"Focusing \u5bf9\u7126"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#depth-of-field-dof","text":"\\[ {\\tt DoF} = o_2 - o_1 = \\frac{2of^2cN(o-f)}{f^4-c^2N^2(o-f)^2} \\] From the equation above DoF is almost proportional to \\(N\\) , and thus the Larger aperture, the Smaller F-Number and the Smaller DoF.","title":"Depth of Field (DoF) \u666f\u6df1"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#how-to-blur-the-background","text":"Large aperture Long focal length Near foreground Far background","title":"How to blur the background"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#geometric-image-formation","text":"Camera model \\[ \\begin{bmatrix} u \\\\ v \\end{bmatrix} = \\begin{bmatrix} f \\frac{x}{z} \\\\ f \\frac{y}{z} \\end{bmatrix} \\]","title":"Geometric Image Formation\uff08\u5b9a\u4f4d\uff09"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#homogeneous-coordinates-projective-coordinates","text":"Suppose that \\(\\begin{bmatrix} x \\\\ y \\\\ w \\end{bmatrix}\\) is the same as \\(\\begin{bmatrix} x/w \\\\ y/w \\\\ 1 \\end{bmatrix}\\) , then we get \\[ \\begin{bmatrix} f & 0 & 0 & 0 \\\\ 0 & f & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} fx \\\\ fy \\\\ z \\\\ \\end{bmatrix} \\cong \\begin{bmatrix} f\\frac{x}{z} \\\\ f\\frac{y}{z} \\\\ 1 \\end{bmatrix} \\] We can also put the image plane in front of the camera (opposite to the previous picture).","title":"Homogeneous Coordinates / Projective Coordinates"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#perspective-projection","text":"Preserevd - Straight lines are still straight Lost - Length and Angle","title":"Perspective Projection"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#vanishing-points","text":"Properties Any wo parallel lines have the same vanishing point v. v can be outside the image frame or at infinity. Line from C to v is parallel to the lines.","title":"Vanishing Points"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#vanishing-lines","text":"Multiple vanishing points The direction of the vanishing line tells us the orientation of the plane.","title":"Vanishing Lines"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#distortion","text":"Converging verticals Problem and Solution (View Camera \u79fb\u8f74\u76f8\u673a) Exterior columns appear bigger Due to lens flaws Radial distortion Due to imperfect lens \\[ \\begin{align} r^2 &= {x'}_n^{2} + {y'}_n^{2} \\\\ {x'}_d &= {x'}_n(1 + \\kappa_1r^2 + \\kappa_2r^4) \\\\ {y'}_d &= {y'}_n(1 + \\kappa_1r^2 + \\kappa_2r^4) \\\\ \\end{align} \\] Solution Take a photo of a grid at the same point and then use the mathmatics to calculate and correct radial distortion.","title":"Distortion"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#orthographic-projection","text":"\\[ \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\\\ 1 \\\\ \\end{bmatrix} \\Rightarrow (x, y) \\]","title":"Orthographic Projection"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#photometric-image-formation","text":"","title":"Photometric Image Formation\uff08\u5b9a\u989c\u8272/\u4eae\u5ea6\uff09"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#image-sensor","text":"CMOS CCD (Charge Coupled Device)","title":"Image Sensor"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#shutter","text":"Shutter speed controls exposure time.","title":"Shutter"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#color-sensing","text":"Color Spaces RGB HSV Practical Color Sensing: Bayer Filter","title":"Color Sensing"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#shading","text":"","title":"Shading \u7740\u8272"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#brdf-bidirectional-reflectance-distribution-function","text":"\\[ L_r(\\hat{\\textbf{v}_r};\\lambda) = \\int L_i(\\hat{\\textbf{i}_r};\\lambda)f_r(\\hat{\\textbf{v}_r}, \\hat{\\textbf{v}_i}, \\hat{\\textbf{n}}; \\lambda)\\cos^+\\theta_i\\ d\\hat{\\textbf{v}_i} \\]","title":"BRDF (Bidirectional Reflectance Distribution Function)"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#diffuse-lambertian-reflection","text":"Shading independent of view direction","title":"Diffuse (Lambertian) Reflection"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#specular-term","text":"Intensity depends on view direction","title":"Specular Term"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#blinn-phong-reflection-model","text":"\\[ L = L_a + L_d + L_s = k_aI_a + k_d(I/r^2)\\max(0, \\textbf{n}\\cdot\\textbf{l}) + k_s(I/r^2)\\max(0, \\textbf{n}\\cdot\\textbf{h})^p \\]","title":"Blinn-Phong Reflection Model"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/","text":"Lecture 3 Imae Processing \u00b6 Image Processing Basis \u00b6 Operations: Incresing contrast, Invert, Blur, Sharpen, Edge Detection Convolution \u00b6 \\[ (f * g)(x) = \\int_{-\\infty}^\\infty f(y)g(x-y)dy \\] discrete 2D form \\[ (f * g)(x) = \\sum_{i, j =-\\infty}^\\infty f(i, j)g(x - i, y - j) \\] Gaussian Blur \u00b6 2D Gaussian function \\[ \\large f(i, j) = \\frac{1}{2\\pi\\sigma^2}e^{-\\frac{i^2 + j^2}{2\\sigma^2}} \\] Usage Define a window size (commmly a square, say \\(n \\times n\\) , and let \\(r = \\lfloor n / 2 \\rfloor\\) ). Select a point, say \\((x, y)\\) and then a window around it. Apply the Gaussian function at each point in the window, and sum them up, namely \\(G(x, y) = \\sum\\limits_{i = x - r}^{x + r}\\sum\\limits_{j = y - r}^{y + r}f(i,j)\\) . Then the \"blurred\" value of point \\((x, y)\\) is \\(G(x, y)\\) . Sharpen \u00b6 An example of kernel matrix \\[ f = \\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 5 & -1 \\\\ 0 & -1 & 0 \\end{bmatrix} \\] An insight Let \\(I\\) be the original image, then the sharpen image \\(I' = I + (I - \\text{blur}(I))\\) , where \\(I - \\text{blur}(I)\\) can be regarded as the high frequency content. Extract Gradients \u00b6 Examples of kernel matrix \\(f = \\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1\\end{bmatrix}\\) extracts horizontal gradients. \\(f = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\) extracts vertical gradients. Bilateral Filters \u00b6 Kernel depends on image content. Better performance but lower efficiency. A trick: Separable Filter A filter is separable if it can be wriiten as the outer product of two other filters. Example \\[ \\frac19 \\begin{bmatrix} 1 & 1 & 1\\\\ 1 & 1 & 1\\\\ 1 & 1 & 1 \\end{bmatrix} = \\frac13 \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\times \\frac13 \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix} \\] Purpose / Advantages : speed up the calculation Image Sampling \u00b6 Abstract Down-sampling \u2192 Reducing image size Aliasing \u00b6 Aliasing is the artifacts due to sampling Higher frequencies need faster sampling. Undersampling creates frequency aliases. Fourier Transform \u00b6 Simly put, fourier transform represents a function as a weighted sum of sines and cosines , where the sines and consines are in various frequencies . Convolution Theorem From the view of Fourier Transform ... \u00b6 Sampling and Aliasing \u00b6 Sampling is repeating frequency contents. Aliasing is mixed frequency contents. Method to reduce aliasing Increase sampling rate Nyquist-Shannon theorem the signal can be perfectly reconstructed if sampled with a frequency larger than \\(2f_0\\) . Anti-aliasing Filtering, then sampling Example Image Magnification \u00b6 Abstract Up-sampling - Inverse of down-sampling An important method: Interpolation . 1D Interpolation \u00b6 Nearest-neighbour Interpolation Not continuous; Not smooth Linear Interpolation Continuous; Not smooth Cubic Interpolation Continuous; Smooth 2D Interpolation \u00b6 (similar to 1D cases) Nearest-neighbour Interpolation Bilinear Interpolation define \\(\\text{lerp}(x, v_0,v_1) = v_0 + x(v_1 - v_0)\\) . Suppose the point in the rectangle surrounded by four points \\(u_{00},u_{01},u_{10},u_{11}\\) . then \\(f(x, y) = \\text{lerp}(t, u_0, u_1)\\) , where \\(u_0 = \\text{lerp}(s, u_{00}, u_{10})\\) and \\(u_1 = \\text{lerp}(s, u_{01}, u_{11})\\) . Bicubic Interpolation Super-Resolution \u00b6 Question Change Aspect Ratio \u00b6 Question","title":"Lecture 3"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#lecture-3-imae-processing","text":"","title":"Lecture 3 Imae Processing"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#image-processing-basis","text":"Operations: Incresing contrast, Invert, Blur, Sharpen, Edge Detection","title":"Image Processing Basis"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#convolution","text":"\\[ (f * g)(x) = \\int_{-\\infty}^\\infty f(y)g(x-y)dy \\] discrete 2D form \\[ (f * g)(x) = \\sum_{i, j =-\\infty}^\\infty f(i, j)g(x - i, y - j) \\]","title":"Convolution"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#gaussian-blur","text":"2D Gaussian function \\[ \\large f(i, j) = \\frac{1}{2\\pi\\sigma^2}e^{-\\frac{i^2 + j^2}{2\\sigma^2}} \\] Usage Define a window size (commmly a square, say \\(n \\times n\\) , and let \\(r = \\lfloor n / 2 \\rfloor\\) ). Select a point, say \\((x, y)\\) and then a window around it. Apply the Gaussian function at each point in the window, and sum them up, namely \\(G(x, y) = \\sum\\limits_{i = x - r}^{x + r}\\sum\\limits_{j = y - r}^{y + r}f(i,j)\\) . Then the \"blurred\" value of point \\((x, y)\\) is \\(G(x, y)\\) .","title":"Gaussian Blur"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#sharpen","text":"An example of kernel matrix \\[ f = \\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 5 & -1 \\\\ 0 & -1 & 0 \\end{bmatrix} \\] An insight Let \\(I\\) be the original image, then the sharpen image \\(I' = I + (I - \\text{blur}(I))\\) , where \\(I - \\text{blur}(I)\\) can be regarded as the high frequency content.","title":"Sharpen"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#extract-gradients","text":"Examples of kernel matrix \\(f = \\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1\\end{bmatrix}\\) extracts horizontal gradients. \\(f = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\) extracts vertical gradients.","title":"Extract Gradients"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#bilateral-filters","text":"Kernel depends on image content. Better performance but lower efficiency. A trick: Separable Filter A filter is separable if it can be wriiten as the outer product of two other filters. Example \\[ \\frac19 \\begin{bmatrix} 1 & 1 & 1\\\\ 1 & 1 & 1\\\\ 1 & 1 & 1 \\end{bmatrix} = \\frac13 \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\times \\frac13 \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix} \\] Purpose / Advantages : speed up the calculation","title":"Bilateral Filters"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#image-sampling","text":"Abstract Down-sampling \u2192 Reducing image size","title":"Image Sampling"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#aliasing","text":"Aliasing is the artifacts due to sampling Higher frequencies need faster sampling. Undersampling creates frequency aliases.","title":"Aliasing"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#fourier-transform","text":"Simly put, fourier transform represents a function as a weighted sum of sines and cosines , where the sines and consines are in various frequencies . Convolution Theorem","title":"Fourier Transform"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#from-the-view-of-fourier-transform","text":"","title":"From the view of Fourier Transform ..."},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#sampling-and-aliasing","text":"Sampling is repeating frequency contents. Aliasing is mixed frequency contents. Method to reduce aliasing Increase sampling rate Nyquist-Shannon theorem the signal can be perfectly reconstructed if sampled with a frequency larger than \\(2f_0\\) . Anti-aliasing Filtering, then sampling Example","title":"Sampling and Aliasing"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#image-magnification","text":"Abstract Up-sampling - Inverse of down-sampling An important method: Interpolation .","title":"Image Magnification"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#1d-interpolation","text":"Nearest-neighbour Interpolation Not continuous; Not smooth Linear Interpolation Continuous; Not smooth Cubic Interpolation Continuous; Smooth","title":"1D Interpolation"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#2d-interpolation","text":"(similar to 1D cases) Nearest-neighbour Interpolation Bilinear Interpolation define \\(\\text{lerp}(x, v_0,v_1) = v_0 + x(v_1 - v_0)\\) . Suppose the point in the rectangle surrounded by four points \\(u_{00},u_{01},u_{10},u_{11}\\) . then \\(f(x, y) = \\text{lerp}(t, u_0, u_1)\\) , where \\(u_0 = \\text{lerp}(s, u_{00}, u_{10})\\) and \\(u_1 = \\text{lerp}(s, u_{01}, u_{11})\\) . Bicubic Interpolation","title":"2D Interpolation"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#super-resolution","text":"Question","title":"Super-Resolution"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#change-aspect-ratio","text":"Question","title":"Change Aspect Ratio"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/","text":"Lecture 4 Model Fitting and Optimization \u00b6 Optimization \u00b6 \\[ \\begin{align} & \\text{Minimize } & f_0(x) \\\\ & \\text{Subject to } &f_i(x) & \\le 0,\\ i = 1, \\dots, m \\\\ & & g_i(x) & = 0,\\ i = 1,\\dots, p \\end{align} \\] \\(x \\in \\mathbb{R}^n\\) is a vector variable to be chosen. \\(f_0\\) is the objective function to be minimized. \\(f_1, \\dots, f_m\\) are the inequality constraint functions. \\(g_1, \\dots, g_p\\) are the equality constraint functions. Model Fitting \u00b6 A typical approach: Minimize the Mean Square Error ( MSE ) \\[ \\hat x = \\mathop{\\arg\\min}\\limits_x \\sum\\limits_i(b_i - a_i^Tx)^2 \\] Reasons to choose MSE \u00b6 Key Assumptions: MSE = MLE with Gaussian noise From Maximum Likelihood Estimation ( MLE ) , the data is assumed to be with Gaussian noise . \\[ b_i = a_i^Tx + n,\\ n \\sim G(0, \\sigma) \\] The likelihood of observing \\((a_i, b_i)\\) is \\[ P[(a_i, b_i)|x] = P[b_i - a_i^Tx] \\propto \\exp\\left(-\\frac{(b_i - a_i^Tx)^2}{2\\sigma^2}\\right) \\] Note If the data points are independent , \\[ \\begin{align} P[(a_1, b_1)(a_2, b_2)\\dots|x] & = \\prod\\limits_iP[(a_i, b_i)|x] = \\prod\\limits_iP[b_i - a_{i^{Tx]}} \\\\ & \\propto \\exp\\left(-\\frac{\\sum_i(b_i - a_i^Tx)^2}{2\\sigma^2}\\right) = \\exp\\left(-\\frac{||Ax-b||^2_2}{2\\sigma^2}\\right) \\end{align} \\] \\[ \\begin{align} \\hat x &= \\mathop{\\arg\\max}\\limits_x P[(a_1, b_1)(a_2, b_2)\\dots|x] \\\\ &= \\mathop{\\arg\\max}\\limits_x \\exp\\left(-\\frac{||Ax-b||^2_2}{2\\sigma^2}\\right) = \\mathop{\\arg\\min}\\limits_x||Ax - b||_2^2 \\end{align} \\] Numerical Methods \u00b6 Analytical Solution \u89e3\u6790\u89e3 \u00b6 The derivative of \\(||Ax-b||^2_2\\) is \\(2A^T(Ax - b)\\) , let it be 0. Then we get \\(\\hat x\\) satisfying \\[ A^TAx = A^Tb \\] But, if no analytical solution ... Approximate Solution \u8fd1\u4f3c\u89e3 \u00b6 Method \\(x \\leftarrow x_0\\) \\(\\text{while not converge}\\) \\(p \\leftarrow \\text{descending_direction(x)}\\) \\(\\alpha \\leftarrow \\text{descending_step(x)}\\) \\(x \\leftarrow x + \\alpha p\\) Gradient Descent (GD) \u00b6 Steepest Descent Method \u00b6 \\[ F(x_0 + \\Delta x) \\approx F(x_0) + J_F\\Delta x \\] Direction \\(\\Delta x = -J^T_F\\) Step To minimize \\(\\phi(a)\\) Backtracking Algorithm Initialize \\(\\alpha\\) with a large value. Decrease \\(\\alpha\\) until \\(\\phi(\\alpha)\\le\\phi(0) + \\gamma\\phi'(0)\\alpha\\) . Advantage Easy to implement Perform well when far from the minimum Disadvantage Converge slowly when near the minimum, which wastes a lot of computation Newton Method \u00b6 \\[ F(x_0 + \\Delta x) \\approx F(x_0) + J_F\\Delta x + \\frac12\\Delta x^TH_F\\Delta x \\] \\(\\Delta x = -H_F^{-1}J^T_F\\) Advantage Faster convergence Disadvantage Hessian matrix requires a lot of computation Gauss-Newton Method \u00b6 For \\(\\hat x = \\mathop{\\arg\\min}\\limits_x ||Ax-b||^2_2 \\overset{\\Delta}{=\\!=}\\mathop{\\arg\\min}\\limits_x||R(x)||^2_2\\) , expand \\(R(x)\\) . \\[ \\begin{align} ||R(x_k+\\Delta x)||^2_2 &\\approx ||R(x_k) + J_R\\Delta x||^2_2 \\newline &= ||R(x_k)||^2_2 + 2R(x_k)^TJ_R\\Delta x + \\Delta x^TJ^T_RJ_R\\Delta x \\end{align} \\] \\(\\Delta x = -(J_R^TJ_R)^{-1}J_R^TR(x_k) = -(J_R^TJ_R)^{-1}J_F^T\\) Compared to Newton Method, \\(J_R^TJ_R\\) is used to approximate \\(H_F\\) . Advantage No need to compute Hessian matrix Faster to converge Disadvantage \\(J^T_RJ_R\\) may be singular. Levenberg-Marquardt (LM) \u00b6 \\(\\Delta x = -(J_R^TJ_R + \\lambda I)^{-1}J_R^TR(x_k)\\) \\(\\lambda \\rightarrow \\infty\\) Steepest Descent \\(\\lambda \\rightarrow 0\\) Gauss-Newton Advantage Start and converge quickly Local Minimum and Global Minimum Convex Optimization \u00b6 Question Robust Estimation \u00b6 Inlier obeys the model assumption. Outlier differs significantly rom the assumption. Outlier makes MSE fail. To reduce its effect, we can use other loss functions, called robust functions. RANSAC (Random Sample Concensus) \u00b6 The most powerful method to handle outliers. Key ideas The distribution of inliers is similar while outliers differ a lot. Use data point pairs to vote ill-posed Problem \u75c5\u6001\u95ee\u9898 / \u591a\u89e3\u95ee\u9898 \u00b6 The solution is not unique. To make it unique: L2 regularization Suppress redundant variables \\(\\min_x||Ax-b||^2_2,\\ s.t. ||x||_2 \\le 1\\) (the same as \\(\\min_x||Ax-b||^2_2 + \\lambda||x||_2\\) ) L1 regularization Make \\(x\\) sparse \\(\\min_x||Ax-b||^2_2,\\ s.t. ||x||_1 \\le 1\\) (the same as \\(\\min_x||Ax-b||^2_2 + \\lambda||x||_1\\) ) Graphcut and MRF \u00b6 Note A key idea: Neighboring pixels tend to take the same label. Images as Graphs A vertex for each pixel An edge between each pair, weighted by the affinity or similarity between its two vertices Pixel Dissimilarity \\(S(\\textbf{f}_i, \\textbf{f}_j) = \\sqrt{\\sum_k(f_{ik} - f_{jk})^2}\\) Pixel Affinity \\(w(i, j) = A(\\textbf{f}_i, \\textbf{f}_j) = \\exp\\left(\\frac{-1}{2\\sigma^2}S(\\textbf{f}_i, \\textbf{f}_j)\\right)\\) Graph Notation \\(G = (V, E)\\) \\(V\\) : a set of vertices \\(E\\) : a set of edges Graph Cut \u00b6 Cut \\(C=(V_A, V_B)\\) is a parition of vertices \\(V\\) of a graph \\(G\\) into two disjoint subsets \\(V_A\\) and \\(V_B\\) . Cost of Cut \\(\\text{cut}(V_A, V_B) = \\sum_{u\\in V_A, v\\in V_B} w(u, v)\\) Problem with min-cut \u00b6 Bias to cut small, isolated segments Solution: Normalized cut Compute how strongly verices \\(V_A\\) are associated with vertices \\(V\\) \\[ \\text{assoc}(V_A, V) = \\sum_{u\\in V_A, v\\in V} w(u, v) \\] \\[ \\text{NCut}(V_A, V_B) = \\frac{\\text{cut}(V_A, V_B)}{\\text{assoc}(V_A, V)} + \\frac{\\text{cut}(V_A, V_B)}{\\text{assoc}(V_B, V)} \\] Markow Random Field ( MRF ) \u00b6 Graphcut is an exception of MRF . Question","title":"Lecture 4"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#lecture-4-model-fitting-and-optimization","text":"","title":"Lecture 4 Model Fitting and Optimization"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#optimization","text":"\\[ \\begin{align} & \\text{Minimize } & f_0(x) \\\\ & \\text{Subject to } &f_i(x) & \\le 0,\\ i = 1, \\dots, m \\\\ & & g_i(x) & = 0,\\ i = 1,\\dots, p \\end{align} \\] \\(x \\in \\mathbb{R}^n\\) is a vector variable to be chosen. \\(f_0\\) is the objective function to be minimized. \\(f_1, \\dots, f_m\\) are the inequality constraint functions. \\(g_1, \\dots, g_p\\) are the equality constraint functions.","title":"Optimization"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#model-fitting","text":"A typical approach: Minimize the Mean Square Error ( MSE ) \\[ \\hat x = \\mathop{\\arg\\min}\\limits_x \\sum\\limits_i(b_i - a_i^Tx)^2 \\]","title":"Model Fitting"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#reasons-to-choose-mse","text":"Key Assumptions: MSE = MLE with Gaussian noise From Maximum Likelihood Estimation ( MLE ) , the data is assumed to be with Gaussian noise . \\[ b_i = a_i^Tx + n,\\ n \\sim G(0, \\sigma) \\] The likelihood of observing \\((a_i, b_i)\\) is \\[ P[(a_i, b_i)|x] = P[b_i - a_i^Tx] \\propto \\exp\\left(-\\frac{(b_i - a_i^Tx)^2}{2\\sigma^2}\\right) \\] Note If the data points are independent , \\[ \\begin{align} P[(a_1, b_1)(a_2, b_2)\\dots|x] & = \\prod\\limits_iP[(a_i, b_i)|x] = \\prod\\limits_iP[b_i - a_{i^{Tx]}} \\\\ & \\propto \\exp\\left(-\\frac{\\sum_i(b_i - a_i^Tx)^2}{2\\sigma^2}\\right) = \\exp\\left(-\\frac{||Ax-b||^2_2}{2\\sigma^2}\\right) \\end{align} \\] \\[ \\begin{align} \\hat x &= \\mathop{\\arg\\max}\\limits_x P[(a_1, b_1)(a_2, b_2)\\dots|x] \\\\ &= \\mathop{\\arg\\max}\\limits_x \\exp\\left(-\\frac{||Ax-b||^2_2}{2\\sigma^2}\\right) = \\mathop{\\arg\\min}\\limits_x||Ax - b||_2^2 \\end{align} \\]","title":"Reasons to choose MSE"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#numerical-methods","text":"","title":"Numerical Methods"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#analytical-solution","text":"The derivative of \\(||Ax-b||^2_2\\) is \\(2A^T(Ax - b)\\) , let it be 0. Then we get \\(\\hat x\\) satisfying \\[ A^TAx = A^Tb \\] But, if no analytical solution ...","title":"Analytical Solution \u89e3\u6790\u89e3"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#approximate-solution","text":"Method \\(x \\leftarrow x_0\\) \\(\\text{while not converge}\\) \\(p \\leftarrow \\text{descending_direction(x)}\\) \\(\\alpha \\leftarrow \\text{descending_step(x)}\\) \\(x \\leftarrow x + \\alpha p\\)","title":"Approximate Solution \u8fd1\u4f3c\u89e3"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#gradient-descent-gd","text":"","title":"Gradient Descent (GD)"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#steepest-descent-method","text":"\\[ F(x_0 + \\Delta x) \\approx F(x_0) + J_F\\Delta x \\] Direction \\(\\Delta x = -J^T_F\\) Step To minimize \\(\\phi(a)\\) Backtracking Algorithm Initialize \\(\\alpha\\) with a large value. Decrease \\(\\alpha\\) until \\(\\phi(\\alpha)\\le\\phi(0) + \\gamma\\phi'(0)\\alpha\\) . Advantage Easy to implement Perform well when far from the minimum Disadvantage Converge slowly when near the minimum, which wastes a lot of computation","title":"Steepest Descent Method"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#newton-method","text":"\\[ F(x_0 + \\Delta x) \\approx F(x_0) + J_F\\Delta x + \\frac12\\Delta x^TH_F\\Delta x \\] \\(\\Delta x = -H_F^{-1}J^T_F\\) Advantage Faster convergence Disadvantage Hessian matrix requires a lot of computation","title":"Newton Method"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#gauss-newton-method","text":"For \\(\\hat x = \\mathop{\\arg\\min}\\limits_x ||Ax-b||^2_2 \\overset{\\Delta}{=\\!=}\\mathop{\\arg\\min}\\limits_x||R(x)||^2_2\\) , expand \\(R(x)\\) . \\[ \\begin{align} ||R(x_k+\\Delta x)||^2_2 &\\approx ||R(x_k) + J_R\\Delta x||^2_2 \\newline &= ||R(x_k)||^2_2 + 2R(x_k)^TJ_R\\Delta x + \\Delta x^TJ^T_RJ_R\\Delta x \\end{align} \\] \\(\\Delta x = -(J_R^TJ_R)^{-1}J_R^TR(x_k) = -(J_R^TJ_R)^{-1}J_F^T\\) Compared to Newton Method, \\(J_R^TJ_R\\) is used to approximate \\(H_F\\) . Advantage No need to compute Hessian matrix Faster to converge Disadvantage \\(J^T_RJ_R\\) may be singular.","title":"Gauss-Newton Method"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#levenberg-marquardt-lm","text":"\\(\\Delta x = -(J_R^TJ_R + \\lambda I)^{-1}J_R^TR(x_k)\\) \\(\\lambda \\rightarrow \\infty\\) Steepest Descent \\(\\lambda \\rightarrow 0\\) Gauss-Newton Advantage Start and converge quickly Local Minimum and Global Minimum","title":"Levenberg-Marquardt (LM)"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#convex-optimization","text":"Question","title":"Convex Optimization"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#robust-estimation","text":"Inlier obeys the model assumption. Outlier differs significantly rom the assumption. Outlier makes MSE fail. To reduce its effect, we can use other loss functions, called robust functions.","title":"Robust Estimation"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#ransac-random-sample-concensus","text":"The most powerful method to handle outliers. Key ideas The distribution of inliers is similar while outliers differ a lot. Use data point pairs to vote","title":"RANSAC (Random Sample Concensus)"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#ill-posed-problem","text":"The solution is not unique. To make it unique: L2 regularization Suppress redundant variables \\(\\min_x||Ax-b||^2_2,\\ s.t. ||x||_2 \\le 1\\) (the same as \\(\\min_x||Ax-b||^2_2 + \\lambda||x||_2\\) ) L1 regularization Make \\(x\\) sparse \\(\\min_x||Ax-b||^2_2,\\ s.t. ||x||_1 \\le 1\\) (the same as \\(\\min_x||Ax-b||^2_2 + \\lambda||x||_1\\) )","title":"ill-posed Problem \u75c5\u6001\u95ee\u9898 / \u591a\u89e3\u95ee\u9898"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#graphcut-and-mrf","text":"Note A key idea: Neighboring pixels tend to take the same label. Images as Graphs A vertex for each pixel An edge between each pair, weighted by the affinity or similarity between its two vertices Pixel Dissimilarity \\(S(\\textbf{f}_i, \\textbf{f}_j) = \\sqrt{\\sum_k(f_{ik} - f_{jk})^2}\\) Pixel Affinity \\(w(i, j) = A(\\textbf{f}_i, \\textbf{f}_j) = \\exp\\left(\\frac{-1}{2\\sigma^2}S(\\textbf{f}_i, \\textbf{f}_j)\\right)\\) Graph Notation \\(G = (V, E)\\) \\(V\\) : a set of vertices \\(E\\) : a set of edges","title":"Graphcut and MRF"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#graph-cut","text":"Cut \\(C=(V_A, V_B)\\) is a parition of vertices \\(V\\) of a graph \\(G\\) into two disjoint subsets \\(V_A\\) and \\(V_B\\) . Cost of Cut \\(\\text{cut}(V_A, V_B) = \\sum_{u\\in V_A, v\\in V_B} w(u, v)\\)","title":"Graph Cut"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#problem-with-min-cut","text":"Bias to cut small, isolated segments Solution: Normalized cut Compute how strongly verices \\(V_A\\) are associated with vertices \\(V\\) \\[ \\text{assoc}(V_A, V) = \\sum_{u\\in V_A, v\\in V} w(u, v) \\] \\[ \\text{NCut}(V_A, V_B) = \\frac{\\text{cut}(V_A, V_B)}{\\text{assoc}(V_A, V)} + \\frac{\\text{cut}(V_A, V_B)}{\\text{assoc}(V_B, V)} \\]","title":"Problem with min-cut"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#markow-random-field-mrf","text":"Graphcut is an exception of MRF . Question","title":"Markow Random Field (MRF)"}]}