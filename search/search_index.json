{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u3053\u3053\u306f \u2014\u2014 \u306a\u3093\u3068! \u00b6 A note book by @sakuratsuyu. \u5076\u5c14\u5199\u70b9\u4e1c\u897f\uff0c\u5e0c\u671b\u4f60\u559c\u6b22\u3002 \\[ \\tt{Transfering\\ content\\ from\\ other\\ places\\ these\\ days\\ } \\cdots\\ \\&\\$@\\%\\&\\$@\\%\\&\\$@\\% \\] About me All ids I used: sakuratsuyu , \u685c\u3064\u3086 , \u66e6\u9732 and Mslease . Maybe you can find me somewhere else! Sophomore as a ZJU CS Undergraduate. ZJUSCT Freshman. (\u6478\u5927\u9c7c) MADer. (MAD \u5728\u505a\u4e86\u5728\u505a\u4e86.jpg) Hope you can find anything useful and interesting here! \u2728\u30ad\u30e9\u30ad\u30e9\u76ee\u2728","title":"\u3053\u3053\u306f \u2014\u2014 \u306a\u3093\u3068!"},{"location":"#_1","text":"A note book by @sakuratsuyu. \u5076\u5c14\u5199\u70b9\u4e1c\u897f\uff0c\u5e0c\u671b\u4f60\u559c\u6b22\u3002 \\[ \\tt{Transfering\\ content\\ from\\ other\\ places\\ these\\ days\\ } \\cdots\\ \\&\\$@\\%\\&\\$@\\%\\&\\$@\\% \\] About me All ids I used: sakuratsuyu , \u685c\u3064\u3086 , \u66e6\u9732 and Mslease . Maybe you can find me somewhere else! Sophomore as a ZJU CS Undergraduate. ZJUSCT Freshman. (\u6478\u5927\u9c7c) MADer. (MAD \u5728\u505a\u4e86\u5728\u505a\u4e86.jpg) Hope you can find anything useful and interesting here! \u2728\u30ad\u30e9\u30ad\u30e9\u76ee\u2728","title":"\u3053\u3053\u306f \u2014\u2014 \u306a\u3093\u3068!"},{"location":"comming_soon/","text":"Waiting... \u00b6 Failure Things there are comming soon.","title":"Waiting..."},{"location":"comming_soon/#waiting","text":"Failure Things there are comming soon.","title":"Waiting..."},{"location":"discussion/","text":"Discussion \u00b6 Free talk here about anything! Maybe I can put some blogrolls there ... Blogrolls Wants more friends! Isshiki\u4fee's Notebook \u545c\u545c\u4fee\u4f6c\u6559\u6559. Azily's Blog \u88ab\u4f6c\u8584\u7eb1.","title":"Blogroll"},{"location":"discussion/#discussion","text":"Free talk here about anything! Maybe I can put some blogrolls there ... Blogrolls Wants more friends! Isshiki\u4fee's Notebook \u545c\u545c\u4fee\u4f6c\u6559\u6559. Azily's Blog \u88ab\u4f6c\u8584\u7eb1.","title":"Discussion"},{"location":"tasklist/","text":"Task List \u00b6 Make a logo! Mathematics Basis Discrete Mathematics | DM Numerical Analysis | NA Abstract Algrebra Computer Science Courses The Missing Semester of your CS education \u2192 Cheat Sheets / Tools Introduction to Computer Systems | ICS HPC 101 Introduction to Computer Vision | ICV Fundemental of Data Structure | FDS Digital Logic Design Cheat Sheets / Tools Git Markdown Latex Vim / Neovim Shell Data Wrangling Profiling Pot-pourri Simple Cryptography Guidance to Configure Manjaro + i3wm","title":"Plan"},{"location":"tasklist/#task-list","text":"Make a logo! Mathematics Basis Discrete Mathematics | DM Numerical Analysis | NA Abstract Algrebra Computer Science Courses The Missing Semester of your CS education \u2192 Cheat Sheets / Tools Introduction to Computer Systems | ICS HPC 101 Introduction to Computer Vision | ICV Fundemental of Data Structure | FDS Digital Logic Design Cheat Sheets / Tools Git Markdown Latex Vim / Neovim Shell Data Wrangling Profiling Pot-pourri Simple Cryptography Guidance to Configure Manjaro + i3wm","title":"Task List"},{"location":"Computer_Science_Courses/","text":"Title Page \u00b6 Abstract This section stores the notes of the courses that I've learned about computer scicence (CS) from ZJU or other platforms like MIT and Standford. It helps me to build my knowledge system and hope it helps you too. Welcome to point out anything wrong and careless mistakes!","title":"Title Page"},{"location":"Computer_Science_Courses/#title-page","text":"Abstract This section stores the notes of the courses that I've learned about computer scicence (CS) from ZJU or other platforms like MIT and Standford. It helps me to build my knowledge system and hope it helps you too. Welcome to point out anything wrong and careless mistakes!","title":"Title Page"},{"location":"Computer_Science_Courses/FDS/Algorithm_Analysis/","text":"Algorithm Analysis \u00b6 Definition An algorithm is a finite set of instructions that, if followed, accomplishes a particular task. In addtion, all algorithms must satisfy the following criteria: Input (zero or more) and Output (at least one). Definiteness: clear and unambiguous. Finiteness: termination after finite number of steps. Effectiveness. NOTE: A program does not have to be finite. e.g. an operating system. Running Time \u00b6 The most important resource to analyze is the running time. It consists of two parts Machine and Compiler-Dependent run times. Time and Space Complexities (Machine and Complier-Independent). In FDS, we mainly consider the average time complexity \\(T_\\text{avg}(N)\\) and the worst time complexity \\(T_\\text{worst}(N)\\) as functions of input size \\(N\\) . Genernal Rules FOR LOOPS the statements inside the loop times the number of iterations . NESTED FOR LOOPS the statements multiplied by the product of the size . CONSECUTIVE STATEMENTS just add . IF/ELSE running time of the test plus the larger of the running time of S1 and S2. if (Condition) { S1; } else { S2; } Asymptotic Notation \u00b6 Definition \\(T(N) = O(f(N))\\) if there are positive constants \\(c\\) and \\(n_0\\) s.t. \\[ \\forall\\ N \\ge n_0,\\ \\ T(N) \\le c \\cdot f(N). \\] \\(T(N) = \\Omega(g(N))\\) if there are positive constants \\(c\\) and \\(n_0\\) s.t. \\[ \\forall\\ N \\ge n_0,\\ \\ T(N) \\ge c \\cdot g(N). \\] \\(T(N) = \\Theta(h(N))\\) iff \\[ T(N) = O(h(N)) = \\Omega(h(N)). \\] \\(T(N) = o(p(N))\\) if \\[ T(N) = O(p(N)),\\ \\ T(N) \\ne \\Theta(p(N)). \\] Rules If \\(T_1(N) = O(f(N))\\) and \\(T_2(N) = O(g(N))\\) , then \\[ \\begin{aligned} T_1(N) + T_2(N) &= \\max(O(f(N)), O(g(N))), \\\\ T_1(N) \\cdot T_2(N) &= O(f(N) \\cdot g(N)). \\\\ \\end{aligned} \\] If \\(T(N)\\) is a polynomial of degree \\(k\\) , then \\(T(N) = \\Theta(N^k)\\) . Example The recurrent equations for the time complexities of programs \\(P1\\) and \\(P2\\) are: \\[ \\begin{aligned} & P1: T(1)=1,T(N)=T(N/3)+1, \\\\ & P2: T(1)=1,T(N)=3T(N/3)+1. \\end{aligned} \\] Find \\(T(N)\\) for \\(P1\\) and \\(P2\\) respectively. Solution. For \\(P1\\) , \\[ \\begin{aligned} T(N) &= T(N / 3) + 1 = T(N / 3^k) + k \\overset{k = \\log N}{=\\!=\\!=\\!=\\!=} T(1) + \\log N = O(\\log N). \\end{aligned} \\] For \\(P2\\) , \\[ \\begin{aligned} T(N) &= 3T(N / 3) + 1 = 3^k T(N / 3^k) + 1 + k + \\cdots + 3^{k - 1} \\\\ & = 3^kT(N / 3^k) + O(3^k) \\overset{k = \\log N}{=\\!=\\!=\\!=\\!=} N \\cdot T(1) + O(N) = O(N). \\end{aligned} \\]","title":"Algorithm Analysis"},{"location":"Computer_Science_Courses/FDS/Algorithm_Analysis/#algorithm-analysis","text":"Definition An algorithm is a finite set of instructions that, if followed, accomplishes a particular task. In addtion, all algorithms must satisfy the following criteria: Input (zero or more) and Output (at least one). Definiteness: clear and unambiguous. Finiteness: termination after finite number of steps. Effectiveness. NOTE: A program does not have to be finite. e.g. an operating system.","title":"Algorithm Analysis"},{"location":"Computer_Science_Courses/FDS/Algorithm_Analysis/#running-time","text":"The most important resource to analyze is the running time. It consists of two parts Machine and Compiler-Dependent run times. Time and Space Complexities (Machine and Complier-Independent). In FDS, we mainly consider the average time complexity \\(T_\\text{avg}(N)\\) and the worst time complexity \\(T_\\text{worst}(N)\\) as functions of input size \\(N\\) . Genernal Rules FOR LOOPS the statements inside the loop times the number of iterations . NESTED FOR LOOPS the statements multiplied by the product of the size . CONSECUTIVE STATEMENTS just add . IF/ELSE running time of the test plus the larger of the running time of S1 and S2. if (Condition) { S1; } else { S2; }","title":"Running Time"},{"location":"Computer_Science_Courses/FDS/Algorithm_Analysis/#asymptotic-notation","text":"Definition \\(T(N) = O(f(N))\\) if there are positive constants \\(c\\) and \\(n_0\\) s.t. \\[ \\forall\\ N \\ge n_0,\\ \\ T(N) \\le c \\cdot f(N). \\] \\(T(N) = \\Omega(g(N))\\) if there are positive constants \\(c\\) and \\(n_0\\) s.t. \\[ \\forall\\ N \\ge n_0,\\ \\ T(N) \\ge c \\cdot g(N). \\] \\(T(N) = \\Theta(h(N))\\) iff \\[ T(N) = O(h(N)) = \\Omega(h(N)). \\] \\(T(N) = o(p(N))\\) if \\[ T(N) = O(p(N)),\\ \\ T(N) \\ne \\Theta(p(N)). \\] Rules If \\(T_1(N) = O(f(N))\\) and \\(T_2(N) = O(g(N))\\) , then \\[ \\begin{aligned} T_1(N) + T_2(N) &= \\max(O(f(N)), O(g(N))), \\\\ T_1(N) \\cdot T_2(N) &= O(f(N) \\cdot g(N)). \\\\ \\end{aligned} \\] If \\(T(N)\\) is a polynomial of degree \\(k\\) , then \\(T(N) = \\Theta(N^k)\\) . Example The recurrent equations for the time complexities of programs \\(P1\\) and \\(P2\\) are: \\[ \\begin{aligned} & P1: T(1)=1,T(N)=T(N/3)+1, \\\\ & P2: T(1)=1,T(N)=3T(N/3)+1. \\end{aligned} \\] Find \\(T(N)\\) for \\(P1\\) and \\(P2\\) respectively. Solution. For \\(P1\\) , \\[ \\begin{aligned} T(N) &= T(N / 3) + 1 = T(N / 3^k) + k \\overset{k = \\log N}{=\\!=\\!=\\!=\\!=} T(1) + \\log N = O(\\log N). \\end{aligned} \\] For \\(P2\\) , \\[ \\begin{aligned} T(N) &= 3T(N / 3) + 1 = 3^k T(N / 3^k) + 1 + k + \\cdots + 3^{k - 1} \\\\ & = 3^kT(N / 3^k) + O(3^k) \\overset{k = \\log N}{=\\!=\\!=\\!=\\!=} N \\cdot T(1) + O(N) = O(N). \\end{aligned} \\]","title":"Asymptotic Notation"},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/","text":"Disjoint Set \u00b6 Definition (Recap) A relation \\(R\\) is defined on a set \\(S\\) if for every pair of elements \\((a, b)\\) , \\(a, b \\in S\\) , \\(aRb\\) is either true of false. If \\(aRb\\) is true, then \\(a\\) is related to \\(b\\) . A relation \\(\\sim\\) over a set \\(S\\) is said to be an equivalence relation over \\(S\\) iff it's symmetric , reflexive and transitive over \\(S\\) . Two members \\(x\\) and \\(y\\) of a set \\(S\\) are said to be in the same equivalence class iff \\(x \\sim y\\) . ADT Objects: Elements of the sets: \\(1, 2, 3, \\dots, N\\) . Disjoint sets: \\(S_1, S_2, \\dots\\) . Operations: Union two sets \\(S_i\\) and \\(S_j\\) to \\(S\\) , i.e. \\(S = S_i \\cup S_j\\) . Find the set \\(S_k\\) which contains the element \\(i\\) . Implementation \u00b6 Method 1 | Tree \u00b6 The sets is represented by a forest. Each set \\(S_i\\) is represented by a tree, and the root of the tree is the representation element. And a list name stores the roots of the trees. For Union operation, it sets the parent of the root of one tree to the other root. The following picture shows \\[ S_2 \\cup S1 \\Rightarrow S_2. \\] For Find operation, it finds the root of \\(i\\) and its corresponding name , which is 'S' in the following picture. Method 2 | Array \u00b6 Another method is to maintain an array \\(S\\) of size \\(N\\) to reprsent the trees in Method 1. \\[ S[\\text{element}] = \\left\\{ \\begin{aligned} & 0, && \\text{if the element is root}, \\\\ & \\text{the element's parent}, && \\text{if the element isn't root}. \\end{aligned} \\right. \\] And the name of each set is the index of the root. For Union operation, we just set S [ root_2 ] = root_1 . For Find operation, we iteratively or recursively find its parent until S[x] = 0 . while ( S [ x ] > 0 ) { x = s [ x ]; } return x ; Improvement \u00b6 Motivation: There may be the case that a sequence of special operations that makes the tree degenerate to a list. Heuristic Union Stategy \u00b6 Union-by-Size \u00b6 When union two sets \\(S_1\\) and \\(S_2\\) (represented by trees), if the size of \\(S_1\\) is smaller , then we union \\(S_1\\) to \\(S_2\\) . To indicate the size of a set, suppose we only consider the positive indices, we can let S[root] = -size (initialized with -1 ) instead of S[root] = 0 . Union by Size void Union ( int * S , const int root_a , const int root_b ) { if ( root_a < root_b ) { S [ root_a ] += S [ root_b ]; S [ root_b ] = root_a ; } else { S [ root_b ] += S [ root_a ]; S [ root_a ] = root_b ; } } Proposition Let \\(T\\) be a tree created by union-by-size with \\(N\\) nodes, then \\[ \\text{height}(T) \\le \\lfloor \\log_2N \\rfloor. \\] Time complexity of \\(N\\) Union and \\(M\\) Find operations is now \\(O(N + M\\log_2N)\\) . Union-by-Height / Rank \u00b6 When union two sets \\(S_1\\) and \\(S_2\\) (represented by trees), if the height of tree \\(S_1\\) is smaller , then we union \\(S_1\\) to \\(S_2\\) . The height of a tree increases only when two equally deep trees are joined (and then the height goes up by one). Union by Height void Union ( int * S , const int root_a , const int root_b ) { if ( S [ root_a ] > S [ root_b ]) { S [ root_a ] = root_b ; return ; } if ( S [ root_a ] < S [ root_b ]) { S [ root_b ] = root_a ; return ; } S [ root_a ] -- ; S [ root_b ] = root_a ; } Path Compression \u00b6 When we are finding an element, we can simultaneously change the parents of the nodes along the finding path, including itself, to the root, which makes the tree shallower. When finding the root of the purple node Union-Find / Disjoint Set int * Initialize ( const int n ) { int * S = ( int * ) malloc ( n * sizeof ( int )); for ( int i = 0 ; i < n ; ++ i ) { S [ i ] = -1 ; } return S ; } int Find ( int * S , const int a ) { if ( S [ a ] < 0 ) { return a ; } return S [ a ] = Find ( S , S [ a ]); } void Union ( int * S , const int root_a , const int root_b ) { if ( root_a < root_b ) { S [ root_a ] += S [ root_b ]; S [ root_b ] = root_a ; } else { S [ root_b ] += S [ root_a ]; S [ root_a ] = root_b ; } } Time Complexity \u00b6 After heuristic union (union-by-rank) and path compression, the average time complexity of each operation is \\[ O(\\alpha(n)),\\ \\ \\text{ where $\\alpha$ is the inverse function of Ackermann function}. \\] Its growth is very slow, which can be regarded as a constant. Ackermann function $A(m,n) $ is defined by \\[ A(i, j) = \\left\\{ \\begin{aligned} & 2^j, && i = 1 \\text{ and } j \\ge 1 \\\\ & A(i - 1, 2), && i \\ge 2 \\text{ and } j = 1 \\\\ & A(i - 1, A(i, j - 1)), && i \\ge 2 \\text{ and } j \\ge 2 \\\\ \\end{aligned} \\right. \\] and \\(\\alpha(n)\\) is defined by the maximum \\(m\\) s.t. \\(A(m, m) \\le n\\) . \\(\\alpha(n) = O(log^* n)\\) (iterative logarithm). Iterative Logarithm \\[ \\log^* n = \\left\\{ \\begin{aligned} & 0, && \\text{if } n \\le 1 \\\\ & 1 + \\log^*(\\log_2 n). && \\text{if } n > 1 \\\\ \\end{aligned} \\right. \\] Thus we have \\[ \\begin{aligned} \\log^* 2 &= 1 + \\log^*(\\log_2 2) = 1 + \\log^*1 = 1, \\\\ \\log^* 4 &= 1 + \\log^*(\\log_2 4) = 1 + \\log^*2 = 2, \\\\ \\log^* 16 &= 1 + \\log^*(\\log_2 16) = 1 + \\log^*4 = 3, \\\\ \\log^* 65536 &= 1 + \\log^*(\\log_2 65536) = 1 + \\log^*16 = 4, \\\\ \\log^* 2^{65536} &= 1 + \\log^*(\\log_2 2^{65536}) = 1 + \\log^*65536 = 5. \\\\ \\end{aligned} \\] Improvement Average Time Complexity Worst Time Complexity No improvement \\(O(\\log n)\\) \\(O(n)\\) Path Compression \\(O(\\alpha (n))\\) \\(O(\\log n)\\) Union by Rank \\(O(\\log n)\\) \\(O(\\log n)\\) Both \\(O(\\alpha(n))\\) \\(O(\\alpha(n))\\)","title":"Disjoint Set"},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/#disjoint-set","text":"Definition (Recap) A relation \\(R\\) is defined on a set \\(S\\) if for every pair of elements \\((a, b)\\) , \\(a, b \\in S\\) , \\(aRb\\) is either true of false. If \\(aRb\\) is true, then \\(a\\) is related to \\(b\\) . A relation \\(\\sim\\) over a set \\(S\\) is said to be an equivalence relation over \\(S\\) iff it's symmetric , reflexive and transitive over \\(S\\) . Two members \\(x\\) and \\(y\\) of a set \\(S\\) are said to be in the same equivalence class iff \\(x \\sim y\\) . ADT Objects: Elements of the sets: \\(1, 2, 3, \\dots, N\\) . Disjoint sets: \\(S_1, S_2, \\dots\\) . Operations: Union two sets \\(S_i\\) and \\(S_j\\) to \\(S\\) , i.e. \\(S = S_i \\cup S_j\\) . Find the set \\(S_k\\) which contains the element \\(i\\) .","title":"Disjoint Set"},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/#implementation","text":"","title":"Implementation"},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/#method-1-tree","text":"The sets is represented by a forest. Each set \\(S_i\\) is represented by a tree, and the root of the tree is the representation element. And a list name stores the roots of the trees. For Union operation, it sets the parent of the root of one tree to the other root. The following picture shows \\[ S_2 \\cup S1 \\Rightarrow S_2. \\] For Find operation, it finds the root of \\(i\\) and its corresponding name , which is 'S' in the following picture.","title":"Method 1 | Tree"},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/#method-2-array","text":"Another method is to maintain an array \\(S\\) of size \\(N\\) to reprsent the trees in Method 1. \\[ S[\\text{element}] = \\left\\{ \\begin{aligned} & 0, && \\text{if the element is root}, \\\\ & \\text{the element's parent}, && \\text{if the element isn't root}. \\end{aligned} \\right. \\] And the name of each set is the index of the root. For Union operation, we just set S [ root_2 ] = root_1 . For Find operation, we iteratively or recursively find its parent until S[x] = 0 . while ( S [ x ] > 0 ) { x = s [ x ]; } return x ;","title":"Method 2 | Array"},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/#improvement","text":"Motivation: There may be the case that a sequence of special operations that makes the tree degenerate to a list.","title":"Improvement"},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/#heuristic-union-stategy","text":"","title":"Heuristic Union Stategy"},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/#union-by-size","text":"When union two sets \\(S_1\\) and \\(S_2\\) (represented by trees), if the size of \\(S_1\\) is smaller , then we union \\(S_1\\) to \\(S_2\\) . To indicate the size of a set, suppose we only consider the positive indices, we can let S[root] = -size (initialized with -1 ) instead of S[root] = 0 . Union by Size void Union ( int * S , const int root_a , const int root_b ) { if ( root_a < root_b ) { S [ root_a ] += S [ root_b ]; S [ root_b ] = root_a ; } else { S [ root_b ] += S [ root_a ]; S [ root_a ] = root_b ; } } Proposition Let \\(T\\) be a tree created by union-by-size with \\(N\\) nodes, then \\[ \\text{height}(T) \\le \\lfloor \\log_2N \\rfloor. \\] Time complexity of \\(N\\) Union and \\(M\\) Find operations is now \\(O(N + M\\log_2N)\\) .","title":"Union-by-Size"},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/#union-by-height-rank","text":"When union two sets \\(S_1\\) and \\(S_2\\) (represented by trees), if the height of tree \\(S_1\\) is smaller , then we union \\(S_1\\) to \\(S_2\\) . The height of a tree increases only when two equally deep trees are joined (and then the height goes up by one). Union by Height void Union ( int * S , const int root_a , const int root_b ) { if ( S [ root_a ] > S [ root_b ]) { S [ root_a ] = root_b ; return ; } if ( S [ root_a ] < S [ root_b ]) { S [ root_b ] = root_a ; return ; } S [ root_a ] -- ; S [ root_b ] = root_a ; }","title":"Union-by-Height / Rank"},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/#path-compression","text":"When we are finding an element, we can simultaneously change the parents of the nodes along the finding path, including itself, to the root, which makes the tree shallower. When finding the root of the purple node Union-Find / Disjoint Set int * Initialize ( const int n ) { int * S = ( int * ) malloc ( n * sizeof ( int )); for ( int i = 0 ; i < n ; ++ i ) { S [ i ] = -1 ; } return S ; } int Find ( int * S , const int a ) { if ( S [ a ] < 0 ) { return a ; } return S [ a ] = Find ( S , S [ a ]); } void Union ( int * S , const int root_a , const int root_b ) { if ( root_a < root_b ) { S [ root_a ] += S [ root_b ]; S [ root_b ] = root_a ; } else { S [ root_b ] += S [ root_a ]; S [ root_a ] = root_b ; } }","title":"Path Compression"},{"location":"Computer_Science_Courses/FDS/Disjoint_Set/#time-complexity","text":"After heuristic union (union-by-rank) and path compression, the average time complexity of each operation is \\[ O(\\alpha(n)),\\ \\ \\text{ where $\\alpha$ is the inverse function of Ackermann function}. \\] Its growth is very slow, which can be regarded as a constant. Ackermann function $A(m,n) $ is defined by \\[ A(i, j) = \\left\\{ \\begin{aligned} & 2^j, && i = 1 \\text{ and } j \\ge 1 \\\\ & A(i - 1, 2), && i \\ge 2 \\text{ and } j = 1 \\\\ & A(i - 1, A(i, j - 1)), && i \\ge 2 \\text{ and } j \\ge 2 \\\\ \\end{aligned} \\right. \\] and \\(\\alpha(n)\\) is defined by the maximum \\(m\\) s.t. \\(A(m, m) \\le n\\) . \\(\\alpha(n) = O(log^* n)\\) (iterative logarithm). Iterative Logarithm \\[ \\log^* n = \\left\\{ \\begin{aligned} & 0, && \\text{if } n \\le 1 \\\\ & 1 + \\log^*(\\log_2 n). && \\text{if } n > 1 \\\\ \\end{aligned} \\right. \\] Thus we have \\[ \\begin{aligned} \\log^* 2 &= 1 + \\log^*(\\log_2 2) = 1 + \\log^*1 = 1, \\\\ \\log^* 4 &= 1 + \\log^*(\\log_2 4) = 1 + \\log^*2 = 2, \\\\ \\log^* 16 &= 1 + \\log^*(\\log_2 16) = 1 + \\log^*4 = 3, \\\\ \\log^* 65536 &= 1 + \\log^*(\\log_2 65536) = 1 + \\log^*16 = 4, \\\\ \\log^* 2^{65536} &= 1 + \\log^*(\\log_2 2^{65536}) = 1 + \\log^*65536 = 5. \\\\ \\end{aligned} \\] Improvement Average Time Complexity Worst Time Complexity No improvement \\(O(\\log n)\\) \\(O(n)\\) Path Compression \\(O(\\alpha (n))\\) \\(O(\\log n)\\) Union by Rank \\(O(\\log n)\\) \\(O(\\log n)\\) Both \\(O(\\alpha(n))\\) \\(O(\\alpha(n))\\)","title":"Time Complexity"},{"location":"Computer_Science_Courses/FDS/Graph/","text":"Graph \u00b6 Definition A Graph consists of vertices and edges . It can be represented by the mark \\(G(V, E)\\) , where \\(G\\) denotes the graph . \\(V = V(G) = \\{v_1, \\dots, v_n\\}\\) denotes a finite nonempty set of vertices . \\(E = E(G) = \\{e_1, \\dots, e_m\\}\\) denotes a finite set of edges . When we discuss graph in this course FDS, we have the following restrictions Self loop is illegal. Multigraph is not considered. Definition Undirected graph (\u65e0\u5411\u56fe): the edge from \\(v_i\\) to \\(v_j\\) is the same as that from \\(v_j\\) to \\(v_i\\) , denoted by \\((v_i, v_j) = (v_j, v_i)\\) . Directed graph (digraph, \u6709\u5411\u56fe): the edge from \\(v_i\\) to \\(v_j\\) is the different from that from \\(v_j\\) to \\(v_i\\) , denoted by \\(<v_i, v_j> = <v_j, v_i>\\) . Complete graph (\u5b8c\u5168\u56fe): A graph that has the maximum number of edges. For a complete undirected graph with \\(n\\) vertices, there are \\(\\dfrac{n(n-1)}{2}\\) edges. For a complete directed grach with \\(n\\) vertices, there are \\(n(n-1)\\) edges. Adjacency (\u76f8\u90bb): For an undirected graph, if there is an edge \\((v_i, v_j)\\) , then \\(v_i\\) and \\(v_j\\) are adjacent and \\((v_i, v_j)\\) is incident* on \\(v_i\\) and \\(v_j\\) . For a directed graph, if there is an edge \\(<v_i, v_j>\\) , then \\(v_i\\) is adjacent to \\(v_j\\) , \\(v_j\\) is adjacent from \\(v_i\\) and \\(<v_i, v_j>\\) is incident on \\(v_i\\) and \\(v_j\\) . Subgrach \\(G' \\subset G\\) (\u5b50\u56fe): \\(V(G') \\subseteq V(G)\\) and \\(E(G')\\subseteq E(G)\\) . Path from \\(v_p\\) to \\(v_q\\) (\u8def\u5f84): \\(\\{v_p, v_{i_1}, v_{i_2}, \\cdots, v_{i_n}, v_q \\}\\) such that \\((v_p,v_{i1}), (v_{i1}, v_{i2}), \\dots, (v_{in}, v_q)\\) or \\(<v_p,v_{i1}>, <v_{i1}, v_{i2}>, \\dots, <v_{in}, v_q>\\) are in \\(E(G)\\) . Length of a path (\u8def\u5f84\u957f\u5ea6): number of edges on the path. Simple path (\u7b80\u5355\u8def\u5f84): \\(v_{i_1}, v_{i_2}, \\dots, v_{i_n}\\) are distinct. Cycle (\u73af): a simple path with \\(v_p=v_q\\) . Connection (\u8fde\u901a\u6027) For an undirected graph, Connected vertices (\u8fde\u901a\u70b9): there is a path from \\(v_i\\) to \\(v_j\\) . Connected graph (\u8fde\u901a\u56fe): all pairs of distinct vertices are connected. (Connected) Component (\u8fde\u901a\u5206\u91cf): the maximum connected subgraph. For a directed graph, Strongly Connected (\u5f3a\u8fde\u901a): all pairs of distinct vertices are connected. Weakly Connected (\u5f31\u8fde\u901a): the graph is connected without direction to the edges. Strongly Connected Component (\u5f3a\u8fde\u901a\u5206\u91cf): the maximum subgraph that is strongly connected. Biconnection (\u53cc\u8fde\u901a\u6027) for an undirected graph \\(G\\) \\(v\\) is an articulation point / cut point (\u5272\u70b9) if \\(G'\\) , which is \\(G\\) with vertex \\(v\\) deleted has at least 2 connected components. \\((v_i, v_j)\\) is an bridge / cut edge (\u6865 / \u5272\u8fb9) if \\(G'\\) , which is \\(G\\) with edge \\((v_i, v_j)\\) deleted has at least 2 connected components. \\(G\\) is a biconnected graph (\u53cc\u8fde\u901a\u56fe) if \\(G\\) is connected and has no articulation points. A biconnected component (\u53cc\u8fde\u901a\u5206\u91cf) is a maximal biconnected subgraph. A Tree: a graph that is connected and acyclic . A DAG (\u6709\u5411\u65e0\u73af\u56fe): a directed acyclic graph. Degree (\u5ea6): number of edges incident to \\(v\\) . For a directed \\(G\\) , we have indegree (\u5165\u5ea6) as the edges to \\(v\\) and outdegree (\u51fa\u5ea6) as the edges from \\(v\\) If there is a graph \\(G\\) with \\(n\\) vertices and \\(e\\) edges, then \\[ e=\\frac12\\sum\\limits_{i=0}^{n-1}degree(v_i). \\] Representation \u00b6 NOTE that all vertices of a graph with \\(n\\) vertices are numbered from \\(0\\) to \\(n - 1\\) . Weighted Edges In some cases, we want each edge has a weight \\(w\\) . In particular, when weight isn't considered, it's same to treat all weight be \\(1\\) . Data Type typedef struct { int u ; // start vector int v ; // end vector int w ; // weight } Edge ; Adjacency Matrix \u90bb\u63a5\u77e9\u9635 \u00b6 Use a matrix to represent the edges of a graph. \\[ \\text{AdjacencyMatrix}[i][j] = \\left\\{ \\begin{aligned} & 1 \\text{ or } \\text{weight}, && \\text{if } (v_i,v_j) \\text{ or } <v_i,v_j>\\ \\in E(G), \\\\ & 0, && \\text{otherwise}. \\end{aligned} \\right. \\] Data Type typedef struct { int n ; // number of vertices int m ; // number of edges int ** adjMat ; } Graph ; Adjacency Matrix of Digraph Graph * CreateGraph ( const int n ) { Graph * G = ( Graph * ) malloc ( sizeof ( Graph )); G -> n = n ; G -> m = 0 ; G -> adjMat = ( int ** ) malloc ( G -> n * sizeof ( int * )); G -> adjMat [ 0 ] = ( int * ) malloc ( G -> n * G -> n * sizeof ( int )); for ( int i = 1 ; i < G -> n ; i ++ ) { G -> adjMat [ i ] = G -> adjMat [ 0 ] + i * G -> n ; } return G ; } void AddEdge ( Graph * G , const Edge e ) { G -> m ++ ; G -> adjMat [ e . u ][ e . v ] = e . w ; } void FreeGraph ( Graph * G ) { free ( G -> adjMat [ 0 ]); free ( G -> adjMat ); free ( G ); } Pros Easy for implementation. Fast to reach. Cons Space complexity \\(O(n^2)\\) , which is a waste when representing a sparse graph. Adjacency Lists \u90bb\u63a5\u94fe\u8868 \u00b6 Replace each row of the adjacency matrix by a linked list. The order of vertices in each list doesn't matter. Data Type typedef struct _node { int to ; int w ; struct _node * next ; } Node ; typedef struct { int size ; Node * head ; } List ; typedef struct { int n ; int m ; List ** adjList ; } Graph ; Adjacency Lists of Digraph void Insert ( List * list , const int to , const int w ) { Node * p = ( Node * ) malloc ( sizeof ( Node )); p -> to = to ; p -> w = w ; p -> next = list -> head -> next ; list -> head -> next = p ; } Graph * CreateGraph ( const int n ) { Graph * G = ( Graph * ) malloc ( sizeof ( Graph )); G -> n = n ; G -> m = 0 ; G -> adjList = ( List ** ) malloc ( G -> n * sizeof ( List * )); for ( int i = 0 ; i < G -> n ; i ++ ) { G -> adjList [ i ] = ( List * ) malloc ( sizeof ( List )); G -> adjList [ i ] -> size = 0 ; G -> adjList [ i ] -> head = ( Node * ) malloc ( sizeof ( Node )); G -> adjList [ i ] -> head -> next = NULL ; } return G ; } void AddEdge ( Graph * G , const Edge e ) { G -> m ++ ; Insert ( G -> adjList [ e . u ], e . v , e . w ); } void FreeGraph ( Graph * G ) { for ( int i = 0 ; i < G -> n ; i ++ ) { for ( Node * p = G -> adjList [ i ] -> head , * temp = NULL ; p != NULL ;) { temp = p ; p = p -> next ; free ( temp ); } free ( G -> adjList [ i ]); } free ( G -> adjList ); free ( G ); } Pros Save space. Cons Complicated for implementation. Adjacency Multilists \u94fe\u5f0f\u524d\u5411\u661f \u00b6 Use the edges as nodes of multilists, and represent it by something similar with a static list . Data Type typedef struct { int to ; int w ; int next ; } EdgeNode ; typedef struct { int n ; int m ; EdgeNode * edge ; int * head ; } Graph ; Adjacency Multilists Graph * CreateGraph ( const int n , const int m ) { // (1)! Graph * G = ( Graph * ) malloc ( sizeof ( Graph )); G -> n = n ; G -> m = m ; G -> edge = ( EdgeNode * ) malloc ( G -> m * sizeof ( EdgeNode )); G -> head = ( int * ) malloc ( G -> n * sizeof ( int )); memset ( G -> edge , 0 , G -> m * sizeof ( EdgeNode )); for ( int i = 0 ; i < G -> n ; ++ i ) { G -> head [ i ] = -1 ; } G -> m = 0 ; return G ; } void AddEdge ( Graph * G , const Edge e ) { G -> edge [ G -> m ]. to = e . v ; G -> edge [ G -> m ]. w = e . w ; G -> edge [ G -> m ]. next = G -> head [ e . u ]; G -> head [ e . u ] = G -> m ; G -> m ++ ; } void FreeGraph ( Graph * G ) { free ( G -> edge ); free ( G -> head ); free ( G ); } Note that for undirected graph , m should be pass by 2 * m . Example: How Adjacency Multilists Build for the left given graph, the corresponding adjancency multilists is the right one Topological Sort \u00b6 Definition AOV Network (Activity On Vertex Network) is a digraph \\(G\\) in which \\(V(G)\\) represents activities and \\(E(G)\\) represents precedence relations between activities (vertices). \\(i\\) is a predecessor of \\(j\\) if there is a path from \\(i\\) to \\(j\\) , \\(i\\) is an immediate predecessor of j if \\(<i,j> \\in E(G)\\) , and \\(j\\) is called a successor ( immediate sucessor ) of \\(i\\) . If we can finish all activities in AOV Network in any order without finishing one before its predecessor finished, the AOV Network is feasible , else it is unfeasible Patial order is a precedence relation which is both transitive and irrelative . Proposition A project is feasible if it's irreflexive . Otherwise \\(i\\) is a predecessor of \\(i\\) , which means that \\(i\\) must be done before \\(i\\) starts. Feasible AOV network must be a DAG . Definition | Topological Order A topological order is a linear ordering of the vertices of a graph such that, for any two vertices \\(i\\) and \\(j\\) , if \\(i\\) is a predecessor of \\(j\\) in the network then \\(i\\) precedes \\(j\\) in the linear ordering. Topological Sort void FindIndegree ( const Graph * G , int indegree []) { memset ( indegree , 0 , G -> n * sizeof ( int )); for ( int i = 0 ; i < G -> n ; i ++ ) { for ( int j = G -> head [ i ]; j != -1 ; j = G -> edge [ j ]. next ) { indegree [ G -> edge [ j ]. to ] ++ ; } } } void TopologicalSort ( const Graph * G , int topNum []) { // (1)! Queue * queue = CreateQueue ( G -> m ); int indegree [ G -> n ]; FindIndegree ( G , indegree ); for ( int i = 0 ; i < G -> n ; i ++ ) { if ( indegree [ i ] == 0 ) { Enqueue ( queue , i ); } } int cnt = 0 ; while ( ! IsEmptyQueue ( queue )) { int now = Dequeue ( queue ); topNum [ cnt ++ ] = now ; for ( int idx = G -> head [ now ]; idx != -1 ; idx = G -> edge [ idx ]. next ) { //(2)! int to = G -> edge [ idx ]. to ; indegree [ to ] -- ; if ( indegree [ to ] == 0 ) { Enqueue ( queue , to ); } } } if ( cnt != G -> n ) { puts ( \"Graph has a cycle\" ); } } 1. topNum[] stores the result of topological sort. 2. The graph is represented by adjacent multilists . Other representations are also available. Complexity Suppose \\(E\\) is the number of edges and \\(V\\) is the number of vertices, then Time complexity \\(O(E + V)\\) . Space complexity \\(O(V)\\) . Note Topological sort can be used to detect whether the graph has a cycle , and it can also be used to detect whether the graph is a chain . Topological sort is not unique . Shortest Paths \u00b6 Definition Given a digraph \\(G=(V,E)\\) , and a cost function \\(c(e)\\) for \\(e\\in E(G)\\) , the length of a path \\(P\\) (also called weighted path length ) from source to destination is \\(\\sum\\limits_{e_i\\subset P}\\ w(e_i)\\) . The result of the following algorithms are stored in the following data type. Data Type typedef struct { int dist ; bool vis ; int prev ; } Path ; dist is the distance from source to v_i . vis is set to false if v_i is visited or true if not. prev record the previous vertex along the path. We can trace back until prev = -1 to obtain the path from source to v_i . Trace Back of Vertex v int now = v ; while ( now != -1 ) { printf ( \"%d \" , now ); now = path [ now ]. prev ; } Unweight Shortest Paths \u00b6 It is implemented by a breadth-first search (BFS) algorithm, which used to find how many vertices from source to destination . Unweighted Shortest Paths #define INF 0x3f3f3f void BFS ( const Graph * G , const int source , Path path []) { for ( int i = 0 ; i < G -> n ; i ++ ) { path [ i ]. dist = INF ; path [ i ]. vis = false ; path [ i ]. prev = -1 ; } path [ source ]. dist = 0 ; Queue * queue = CreateQueue ( G -> n ); Enqueue ( queue , source ); while ( ! IsEmptyQueue ( queue )) { int now = Dequeue ( queue ); path [ now ]. vis = true ; for ( int idx = G -> head [ now ]; idx != -1 ; idx = G -> edge [ idx ]. next ) { //(1)! int to = G -> edge [ idx ]. to ; if ( ! path [ to ]. vis ) { path [ to ]. dist = path [ now ]. dist + 1 ; path [ to ]. prev = now ; Enqueue ( queue , to ); } } } FreeQueue ( queue ); return path ; } The graph is represented by adjacent multilists . Other representations are also available. Reason to Choose INF to be 0x3f3f3f We can use memset(a, 0x3f, n * sizeof(int)) to initialze. Add two infinities will still result in an infinity. Complexity Time complexity \\(O(E + V)\\) . Space comlexity \\(O(E)\\) . Weighted Shortest Paths \u00b6 Floyd \u00b6 Remains Dijkstra \u00b6 Description of Dijkstra Let \\(S = \\{ s \\text{ and } v_i \\text{'s whose shortest paths have been found}\\}\\) . For any \\(u \\notin S\\) , define \\[ \\text{distance}[u] = \\text{minimal length of path} \\{ s \\rightarrow (v_i\\in S)\\rightarrow u \\}. \\] If the paths are generated in non-decreasing order, then The shortest path must go through ONLY \\(v_i \\in S\\) ( Greedy Method ) \\(u\\) is chosen so that \\(\\text{distance}[u] = \\min\\{w \\notin S | \\text{distance}[w]\\}\\) (If \\(u\\) is not unique, then we can choose any of them). If \\(\\text{distance} [u_1] < \\text{distance} [u_2]\\) and we add \\(u_1\\) into \\(S\\) , then distance \\([u_2]\\) may change. If so, a shorter path from \\(s\\) to \\(u_2\\) must go through \\(u_1\\) and \\[ \\text{distance}'[u_2] = \\text{distance} [u_1] + \\text{length}(<u_1,u_2>). \\] Note For finding \\(u\\) , we can maintain a priority queue , which can decrease time complexity of finding \\(u\\) from \\(O(V)\\) to \\(O(\\log V)\\) . Dijkstra's Algorithm Simple Improved by Priority Queue void Dijkstra ( const Graph * G , const int source , Path path []) { for ( int i = 0 ; i < G -> n ; i ++ ) { path [ i ]. dist = INF ; path [ i ]. vis = false ; path [ i ]. prev = -1 ; } path [ source ]. dist = 0 ; int now = source ; while ( ! path [ now ]. vis ) { path [ now ]. vis = true ; for ( int idx = G -> head [ now ]; idx != -1 ; idx = G -> edge [ idx ]. next ) { int to = G -> edge [ idx ]. to ; int cost = G -> edge [ idx ]. dis ; if ( path [ to ]. dist > path [ now ]. dist + cost ) { path [ to ]. dist = path [ now ]. dist + cost ; path [ to ]. prev = now ; } } int distMin = INF ; for ( int i = 0 ; i < G -> n ; i ++ ) { if ( ! path [ i ]. vis && path [ i ]. dist < distMin ) { distMin = path [ i ]. dist ; now = i ; } } } } typedef struct { int num ; int dis ; } Node ; #define MinData -1 typedef Node ElementType ; typedef struct { ElementType * element ; int capacity ; int size ; } PriorityQueue ; ... void Dijkstra ( const Graph * G , const int source , Path path []) { for ( int i = 0 ; i < G -> n ; i ++ ) { path [ i ]. dist = INF ; path [ i ]. vis = false ; path [ i ]. prev = -1 ; } path [ source ]. dist = 0 ; PriorityQueue * Q = Initialize ( G -> m ); Insert ( Q , ( Node ){ source , 0 }); while ( ! IsEmptyQueue ( Q )) { Node cur = DeleteMin ( Q ); int now = cur . num ; if ( path [ now ]. vis ) { continue ; } path [ now ]. vis = true ; for ( int idx = G -> head [ now ]; idx != -1 ; idx = G -> edge [ idx ]. next ) { int to = G -> edge [ idx ]. to ; int cost = G -> edge [ idx ]. dis ; if ( path [ to ]. dist > path [ now ]. dist + cost ) { path [ to ]. dist = path [ now ]. dist + cost ; path [ to ]. prev = now ; if ( ! path [ to ]. vis ) { Insert ( Q , ( Node ){ to , path [ to ]. dist }); } } } } } Complexity Time complexity Simple \\(O(E + V^2)\\) . Improved \\(O((E + V) \\log V)\\) . Space comlexity Simple \\(O(1)\\) . Improved \\(O(E)\\) . Warning By allowing the used vertices being pushed into the set \\(S\\) again, we can get a algorithm that deals with negative edges well with time complexity \\(T = O(E \\cdot V)\\) . However, if the graph has a negative-weight cycle, it will cause infinite loop . SPFA \u00b6 Remains Acyclic Graph \u00b6 If the graph is acyclic, vertices may be selected in topological order since when a vertex is selected, its distance can no longer be lowered without any incoming edges from unknown nodes \\(T=O(|E|+|V|)\\) and no priority queue is needed All-Pairs Shortest Path Problem \u00b6 Use single-source algorithm for \\(V\\) times with \\(T=O(V^3)\\) - works fast on sparse graph. Remains Another \\(O(|V|^3)\\) algorithm in book's Chapter 10 - works faster on dense graphs Network Flow Problems \u00b6 Definition | Network A network is a digraph \\(G(V, E)\\) . Each edge \\((u, v) \\in E(G)\\) has a weight \\(c(u, v)\\) called capacity . In particular, \\(\\forall\\ (u, v) \\notin E(G)\\) , \\(c(u, v) = 0\\) . Moreover, there are two special points, source (\u6e90\u70b9) \\(s\\) and sink (\u6c47\u70b9) \\(t\\) ( \\(s \\ne t\\) ). Definition | Flow If \\(f(u, v)\\) is a function that satisfies Capacity Restriction: \\(f(u, v) \\le c(u, v)\\) . Skew Symmetry: \\(f(u, v) = -f(v, u)\\) . Conservation of Flow: sum of outflows of source equals to sum of inflows of sink. \\[ \\forall\\ x \\in V - {s, t},\\ \\ \\sum\\limits_{(u, x) \\in E} f(u, x) = \\sum\\limits_{(x, v) \\in E} f(x, v). \\] Maximum Flow \u00b6 Maximum flow probelm want to determine the maximum amount of flow that can pass from source \\(s\\) to sink \\(t\\) . We introduce Ford-Fulkerson Augmenting Path Algorithm here. Definition Residual capacity \\(c_f(u, v)\\) is defined by the subtraction between the capacity of the edge and the flow, namely \\(c_f(u, v) = c(u, v) - f(u, v)\\) . A residual network \\(G_f\\) is the graph consists of all vertices of \\(G\\) and all edges that residual capacity is larger than \\(0\\) , namely \\(G_f = (V_f = V, E_f = \\{(u, v) \\in E | c_f(u, v) > 0\\})\\) . An augmenting Path (\u589e\u5e7f\u8def) is a path from \\(s\\) to \\(t\\) , with each edge using the minimum residual capacity of edges in the path. Proposition If the edge capabilities are rational numbers , this algorithm always terminate with a maximum flow. Note The algorithm works for \\(G\\) with cycles as well. Basic Idea Find an augmenting path from \\(s\\) to \\(t\\) . Augment it by subtracting minimum residual capacity along the augmenting path. adding minimum residual capacity along the inverse augmenting path. Example Description \\(G_r\\) is initialized to be the same as \\(G\\) to represent the selected flows. \\(G_f\\) is just a assistant graph of selected augmenting path. First, we find the blue augmenting path from \\(s\\) to \\(t\\) in \\(G_f\\) . The minimum residual capacity along tha path is \\(3\\) . Then we subtract \\(3\\) along the path, and add \\(3\\) along the inverse path. Second, we find the purple augmenting path from \\(s\\) to \\(t\\) in \\(G_f\\) . The minimum residual capacity along tha path is \\(2\\) . Then we subtract \\(2\\) along the path, and add \\(2\\) along the inverse path. Since there is no outflows of \\(s\\) in \\(G_f\\) , we can't find any augementing path any more. Thus it's the result case. Ford-Fulkerson Augmenting Path Algorithm DFS Edomnds-Karp / EK (BFS) int dfs ( const int now , const int prevCapacity , Graph * G , bool vis [], const int s , const int t ) { if ( now == t ) { return prevCapacity ; } vis [ now ] = true ; for ( int idx = G -> head [ now ]; idx ; idx = G -> edge [ idx ]. next ) { int to = G -> edge [ idx ]. to ; int capacity = G -> edge [ idx ]. capacity ; if ( capacity == 0 || vis [ to ] == true ) { continue ; } int cost = dfs ( to , min ( capacity , prevCapacity ), vis , s , t , G ); if ( cost != -1 ) { G -> edge [ idx ]. capacity -= cost ; G -> edge [ idx ^ 1 ]. capacity += cost ; return cost ; } } return -1 ; } int FF ( Graph * G , const int s , const int t ) { int ret = 0 , cost ; bool vis [ G -> n ]; memset ( vis , false , G -> n * sizeof ( bool )); while (( cost = dfs ( s , 0x3f3f3f , vis , s , t , G )) != -1 ) { memset ( vis , false , G -> n * sizeof ( bool )); ret += cost ; } return ret ; } Remains Complexity Suppose all capacity is integers, we have serval methods to find an augmenting path. Choose the augmenting path found by an unweighted shortest path algorithm. Time complexity \\[ \\begin{aligned} T &= T_{\\text{augmentation}} \\cdot T_{\\text{find a path}} \\\\ &= O(f) \\cdot O(E) \\\\ &= O(f\\cdot E),\\ \\ \\text{ where $f$ is the maximum flow.} \\end{aligned} \\] Choose the augmenting path that allows the largest increase in flow found by Dijkstra's algorithm. Time complextiy \\[ \\begin{aligned} T &= T_{\\text{augmentation}} \\cdot T_{\\text{find a path}} \\\\ &= O(E\\log cap_{\\text{max}}) \\cdot O(E\\log V) \\\\ &= O(E^2\\log V),\\ \\ \\text{if $cap_{\\text{max}} = \\max\\{c(u, v)\\}$ is small}. \\end{aligned} \\] Choose the augmenting path that has the least number of edges found by unweighted shortest path algorithm. Time complexity \\[ \\begin{aligned} T &= T_{augmentation} \\cdot T_{find a path} \\\\ &= O(E) \\cdot O(E \\cdot V) \\\\ &= O(E^2V). \\end{aligned} \\] Proposition If every \\(v\\notin {s,t}\\) has either a single incoming edge of capacity \\(1\\) oe a single outgoing edge of capacity \\(1\\) , then time bound is reduced to \\(O(E V^{1/2})\\) . Min Cost Flow \u00b6 The min-cost flow problem want to find among all maximum flows, the one flow of minimum cost provided that each edge has a cost per unit of flow. Remains Minimum Spanning Tree (MST) \u00b6 Definition A spanning tree of a graph \\(G\\) is a tree which consists of \\(V(G)\\) and a subset of \\(E(G)\\) . A minimum spanning tree (MST) is a spanning tree that the total cost of its edges is minimized . Note MST is a tree since it is acyclic , and thus the number of edges is \\(V-1\\) . It is spanning because it covers every vertex. A minimum spanning tree exists iff \\(G\\) is connected (in most case it's undirected ). Adding a non-tree edge to a spanning tree, it becomes a cycle . Kruskal's Algorithm \u00b6 It maintains a forest and is implemented by Disjoint Set . Basic idea is adding one edge at one time in an non-decreasing order and it's a Greedy Method . Results of Kruskal are stored in the following data type. Data Type typedef struct { int n ; // number of vertices in MST. int * vertex ; // vertices in MST. int edgeSum ; // sum of the weight of edges in MST. } Point ; Kruskal's Algorithm int cmp ( const void * a , const void * b ) { return (( Edge * ) a ) -> w - (( Edge * ) b ) -> w ; } int Find ( int * S , const int a ) { if ( S [ a ] < 0 ) { return a ; } return S [ a ] = Find ( S , S [ a ]); } void Union ( int * S , const int root_a , const int root_b ) { if ( root_a < root_b ) { S [ root_a ] += S [ root_b ]; S [ root_b ] = root_a ; } else { S [ root_b ] += S [ root_a ]; S [ root_a ] = root_b ; } } void Kruskal ( Graph * G , Point * point ) { Edge edge [ G -> m ]; for ( int i = 0 , cnt = 0 ; i < G -> n ; i ++ ) { for ( int idx = G -> head [ i ]; idx != -1 ; idx = G -> edge [ idx ]. next ) { edge [ cnt ++ ] = ( Edge ){ i , G -> edge [ idx ]. to , G -> edge [ idx ]. w }; } } qsort ( edge , G -> m , sizeof ( Edge ), cmp ); int * S = ( int * ) malloc ( G -> n * sizeof ( int )); for ( int i = 0 ; i < G -> n ; ++ i ) { S [ i ] = -1 ; } int cnt = 0 , vertex [ G -> n ], sum = 0 ; for ( int i = 0 ; i < G -> m ; ++ i ) { int root_a = Find ( S , edge [ i ]. u ); int root_b = Find ( S , edge [ i ]. v ); if ( root_a == root_b ) { continue ; } Union ( S , root_a , root_b ); vertex [ cnt ++ ] = i ; sum += edge [ i ]. w ; } point -> n = cnt ; point -> vertex = ( int * ) malloc ( point -> n * sizeof ( int )); memcpy ( point -> vertex , vertex , cnt * sizeof ( int )); point -> edgeSum = sum ; free ( S ); } Complexity Time complexity \\(O(E\\log E)\\) (sorting algorithm of \\(O(E \\log E)\\) , and union-find of \\(O(E \\alpha(V))\\) or \\(O(E \\log V)\\) ). Space complexity \\(O(V)\\) . Prim's Algorithm \u00b6 Basic idea is adding one vertex at one time to grow a tree, very similar to Dijkstra's algorithm. \\(T=O(|E|\\log|V|)\\) . Prim's Algorithm Simple Improve by Priority Queue void Prim ( Graph * G , Point * point ) { int dist [ G -> n ], prev [ G -> n ]; bool vis [ G -> n ]; memset ( vis , false , sizeof ( vis )); memset ( dist , 0x3f , sizeof ( dist )); int cnt = 0 , vertex [ G -> n ], sum = 0 ; int now = 0 ; dist [ now ] = 0 ; for ( int r = 0 ; r < G -> n ; r ++ ) { vis [ now ] = true ; if ( r != 0 ) { sum += G -> adjMat [ prev [ now ]][ now ]; } vertex [ cnt ++ ] = now ; for ( int i = 0 ; i < G -> n ; i ++ ) { if ( ! vis [ i ] && G -> adjMat [ now ][ i ] < dist [ i ]) { dist [ i ] = G -> adjMat [ now ][ i ]; prev [ i ] = now ; } } int minDist = 0x3f3f3f ; for ( int i = 0 ; i < G -> n ; i ++ ) { if ( ! vis [ i ] && dist [ i ] < minDist ) { now = i ; minDist = dist [ i ]; } } } point -> n = cnt ; point -> vertex = ( int * ) malloc ( cnt * sizeof ( int )); memcpy ( point -> vertex , vertex , cnt * sizeof ( int )); point -> edgeSum = sum ; } Complexity Time complexity Simple \\(O(E + V^2)\\) . Improved \\(O((E + V) \\log V)\\) . Space complexity \\(O(V)\\) . Depth-First Search \u00b6 DFS can be regarded as a generalization of preorder traversal. DFS of a graph can generate a spanning tree, called DFS spanning tree . void DFS ( Vertex V ) { vis [ V ] = true ; /* mark this vertex to avoid cycles */ for ( each W adjacent to V ) { if ( ! vis [ W ]) { DFS ( W ); } } } Complexity Time complexity \\(O(E + V)\\) . Space complexity \\(O(1)\\) . Tarjan Algorithm \u00b6 Tarjan algorithm is used to find strongly connected components as well as cut point . Define \\(\\text{Low}(u)\\) by \\[ \\begin{aligned} \\text{Low}(u) = \\min\\{ & \\text{Index}(u), \\\\ & \\min\\{\\text{Low}(w) | w \\text{ is a child of } u\\}, \\\\ & \\min\\{\\text{Index}(w) | (u, w) \\text{ is a back edge}\\}. \\end{aligned} \\] Strongly Connected Components (SCC) \u00b6 For finding SCC, tarjan algorithm maintains a DFS spanning tree and a stack . Tarjan Algorithm for Strongly Connected Components void Tarjan ( const int now , const Graph * G , int dfn [], int low [], Stack * stack , bool inStack []) { static int idx = 0 ; dfn [ now ] = low [ now ] = ++ idx ; inStack [ now ] = true ; Push ( stack , now ); for ( int i = G -> head [ now ]; i != -1 ; i = G -> edge [ i ]. next ) { int to = G -> edge [ i ]. to ; if ( dfn [ to ] == 0 ) { Tarjan ( to , G , dfn , low , stack , inStack ); low [ now ] = min ( low [ now ], low [ to ]); } else { if ( inStack [ to ]) { low [ now ] = min ( low [ now ], dfn [ to ]); } } } if ( dfn [ now ] == low [ now ]) { int temp ; do { temp = Pop ( stack ); inStack [ temp ] = false ; printf ( \"%d\" , temp ) } while ( now != temp ); putchar ( '\\n' ); } } void FindStronglyConnectedComponents ( const Graph * G ) { int dfn [ G -> n ], low [ G -> n ]; memset ( dfn , 0 , G -> n * sizeof ( int )); memset ( low , 0 , G -> n * sizeof ( int )); Stack * stack = CreateStack ( G -> n ); bool inStack [ G -> n ]; memset ( inStack , false , G -> n * sizeof ( bool )); for ( int i = 0 ; i < G -> n ; i ++ ) { if ( dfn [ i ] == 0 ) { Tarjan ( i , G , dfn , low , stack , inStack ); } } } Cut Point \u00b6 \\(u\\) is an articulation point if \\(u\\) is the root and has at least 2 children. \\(u\\) is not the root , and has at least 1 child such that \\(\\text{Low}(child) \\ge \\text{Index}(u)\\) . Example Suppose DFS spanning tree starts from vertex \\(3\\) . Blue numbers are indices of searching order stored in dfn[] and red points are the cut points. Tarjan for Cut Point Tarjan Algorithm for Articulation Point / Cut Point void Tarjan ( const int now , const Graph * G , const int root , int dfn [], int low [], bool isCutPoint []) { static int idx = 0 ; int child = 0 ; dfn [ now ] = low [ now ] = ++ idx ; for ( int i = G -> head [ now ]; i != -1 ; i = G -> edge [ i ]. next ) { int to = G -> edge [ i ]. to ; if ( dfn [ to ] == 0 ) { child ++ ; Tarjan ( to , G , root , dfn , low , isCutPoint ); low [ now ] = min ( low [ now ], low [ to ]); if ( dfn [ now ] <= low [ to ] && now != root ) { isCutPoint [ now ] = true ; } } else { low [ now ] = min ( low [ now ], dfn [ to ]); } } if ( child > 1 && now == root ) { isCutPoint [ now ] = true ; } } void FindCutPoint ( const Graph * G , bool isCutPoint []) { int dfn [ G -> n ], low [ G -> n ]; memset ( dfn , 0 , G -> n * sizeof ( int )); memset ( low , 0 , G -> n * sizeof ( int )); memset ( isCutPoint , false , G -> n * sizeof ( bool )); for ( int i = 0 ; i < G -> n ; i ++ ) { if ( dfn [ i ] == 0 ) { Tarjan ( i , G , i , dfn , low , isCutPoint ); } } } Euler Paths and Circuits \u00b6 Definition Euler tour / Path is a path go through all edges. Euler circuit is a path go through all edges and finish at the starting point. Theorem An Euler circuit is possible only if the graph is connected and each vertex has an even degree An Euler tour is possible if there are exactly two vertices having odd degree. One must start at one of the odd-degree vertices. Note The path should be maintained as a linked list For each adjacency list, maintain a pointer to the last edge scanned \\(T=O(|E|+|V|)\\) Hamilton Paths and Circuits \u00b6 Definition Hamilton tour / Path is a path go through all vertices. Hamilton circuit is a path go through all vertices and finish at the starting point.","title":"Graph"},{"location":"Computer_Science_Courses/FDS/Graph/#graph","text":"Definition A Graph consists of vertices and edges . It can be represented by the mark \\(G(V, E)\\) , where \\(G\\) denotes the graph . \\(V = V(G) = \\{v_1, \\dots, v_n\\}\\) denotes a finite nonempty set of vertices . \\(E = E(G) = \\{e_1, \\dots, e_m\\}\\) denotes a finite set of edges . When we discuss graph in this course FDS, we have the following restrictions Self loop is illegal. Multigraph is not considered. Definition Undirected graph (\u65e0\u5411\u56fe): the edge from \\(v_i\\) to \\(v_j\\) is the same as that from \\(v_j\\) to \\(v_i\\) , denoted by \\((v_i, v_j) = (v_j, v_i)\\) . Directed graph (digraph, \u6709\u5411\u56fe): the edge from \\(v_i\\) to \\(v_j\\) is the different from that from \\(v_j\\) to \\(v_i\\) , denoted by \\(<v_i, v_j> = <v_j, v_i>\\) . Complete graph (\u5b8c\u5168\u56fe): A graph that has the maximum number of edges. For a complete undirected graph with \\(n\\) vertices, there are \\(\\dfrac{n(n-1)}{2}\\) edges. For a complete directed grach with \\(n\\) vertices, there are \\(n(n-1)\\) edges. Adjacency (\u76f8\u90bb): For an undirected graph, if there is an edge \\((v_i, v_j)\\) , then \\(v_i\\) and \\(v_j\\) are adjacent and \\((v_i, v_j)\\) is incident* on \\(v_i\\) and \\(v_j\\) . For a directed graph, if there is an edge \\(<v_i, v_j>\\) , then \\(v_i\\) is adjacent to \\(v_j\\) , \\(v_j\\) is adjacent from \\(v_i\\) and \\(<v_i, v_j>\\) is incident on \\(v_i\\) and \\(v_j\\) . Subgrach \\(G' \\subset G\\) (\u5b50\u56fe): \\(V(G') \\subseteq V(G)\\) and \\(E(G')\\subseteq E(G)\\) . Path from \\(v_p\\) to \\(v_q\\) (\u8def\u5f84): \\(\\{v_p, v_{i_1}, v_{i_2}, \\cdots, v_{i_n}, v_q \\}\\) such that \\((v_p,v_{i1}), (v_{i1}, v_{i2}), \\dots, (v_{in}, v_q)\\) or \\(<v_p,v_{i1}>, <v_{i1}, v_{i2}>, \\dots, <v_{in}, v_q>\\) are in \\(E(G)\\) . Length of a path (\u8def\u5f84\u957f\u5ea6): number of edges on the path. Simple path (\u7b80\u5355\u8def\u5f84): \\(v_{i_1}, v_{i_2}, \\dots, v_{i_n}\\) are distinct. Cycle (\u73af): a simple path with \\(v_p=v_q\\) . Connection (\u8fde\u901a\u6027) For an undirected graph, Connected vertices (\u8fde\u901a\u70b9): there is a path from \\(v_i\\) to \\(v_j\\) . Connected graph (\u8fde\u901a\u56fe): all pairs of distinct vertices are connected. (Connected) Component (\u8fde\u901a\u5206\u91cf): the maximum connected subgraph. For a directed graph, Strongly Connected (\u5f3a\u8fde\u901a): all pairs of distinct vertices are connected. Weakly Connected (\u5f31\u8fde\u901a): the graph is connected without direction to the edges. Strongly Connected Component (\u5f3a\u8fde\u901a\u5206\u91cf): the maximum subgraph that is strongly connected. Biconnection (\u53cc\u8fde\u901a\u6027) for an undirected graph \\(G\\) \\(v\\) is an articulation point / cut point (\u5272\u70b9) if \\(G'\\) , which is \\(G\\) with vertex \\(v\\) deleted has at least 2 connected components. \\((v_i, v_j)\\) is an bridge / cut edge (\u6865 / \u5272\u8fb9) if \\(G'\\) , which is \\(G\\) with edge \\((v_i, v_j)\\) deleted has at least 2 connected components. \\(G\\) is a biconnected graph (\u53cc\u8fde\u901a\u56fe) if \\(G\\) is connected and has no articulation points. A biconnected component (\u53cc\u8fde\u901a\u5206\u91cf) is a maximal biconnected subgraph. A Tree: a graph that is connected and acyclic . A DAG (\u6709\u5411\u65e0\u73af\u56fe): a directed acyclic graph. Degree (\u5ea6): number of edges incident to \\(v\\) . For a directed \\(G\\) , we have indegree (\u5165\u5ea6) as the edges to \\(v\\) and outdegree (\u51fa\u5ea6) as the edges from \\(v\\) If there is a graph \\(G\\) with \\(n\\) vertices and \\(e\\) edges, then \\[ e=\\frac12\\sum\\limits_{i=0}^{n-1}degree(v_i). \\]","title":"Graph"},{"location":"Computer_Science_Courses/FDS/Graph/#representation","text":"NOTE that all vertices of a graph with \\(n\\) vertices are numbered from \\(0\\) to \\(n - 1\\) . Weighted Edges In some cases, we want each edge has a weight \\(w\\) . In particular, when weight isn't considered, it's same to treat all weight be \\(1\\) . Data Type typedef struct { int u ; // start vector int v ; // end vector int w ; // weight } Edge ;","title":"Representation"},{"location":"Computer_Science_Courses/FDS/Graph/#adjacency-matrix","text":"Use a matrix to represent the edges of a graph. \\[ \\text{AdjacencyMatrix}[i][j] = \\left\\{ \\begin{aligned} & 1 \\text{ or } \\text{weight}, && \\text{if } (v_i,v_j) \\text{ or } <v_i,v_j>\\ \\in E(G), \\\\ & 0, && \\text{otherwise}. \\end{aligned} \\right. \\] Data Type typedef struct { int n ; // number of vertices int m ; // number of edges int ** adjMat ; } Graph ; Adjacency Matrix of Digraph Graph * CreateGraph ( const int n ) { Graph * G = ( Graph * ) malloc ( sizeof ( Graph )); G -> n = n ; G -> m = 0 ; G -> adjMat = ( int ** ) malloc ( G -> n * sizeof ( int * )); G -> adjMat [ 0 ] = ( int * ) malloc ( G -> n * G -> n * sizeof ( int )); for ( int i = 1 ; i < G -> n ; i ++ ) { G -> adjMat [ i ] = G -> adjMat [ 0 ] + i * G -> n ; } return G ; } void AddEdge ( Graph * G , const Edge e ) { G -> m ++ ; G -> adjMat [ e . u ][ e . v ] = e . w ; } void FreeGraph ( Graph * G ) { free ( G -> adjMat [ 0 ]); free ( G -> adjMat ); free ( G ); } Pros Easy for implementation. Fast to reach. Cons Space complexity \\(O(n^2)\\) , which is a waste when representing a sparse graph.","title":"Adjacency Matrix \u90bb\u63a5\u77e9\u9635"},{"location":"Computer_Science_Courses/FDS/Graph/#adjacency-lists","text":"Replace each row of the adjacency matrix by a linked list. The order of vertices in each list doesn't matter. Data Type typedef struct _node { int to ; int w ; struct _node * next ; } Node ; typedef struct { int size ; Node * head ; } List ; typedef struct { int n ; int m ; List ** adjList ; } Graph ; Adjacency Lists of Digraph void Insert ( List * list , const int to , const int w ) { Node * p = ( Node * ) malloc ( sizeof ( Node )); p -> to = to ; p -> w = w ; p -> next = list -> head -> next ; list -> head -> next = p ; } Graph * CreateGraph ( const int n ) { Graph * G = ( Graph * ) malloc ( sizeof ( Graph )); G -> n = n ; G -> m = 0 ; G -> adjList = ( List ** ) malloc ( G -> n * sizeof ( List * )); for ( int i = 0 ; i < G -> n ; i ++ ) { G -> adjList [ i ] = ( List * ) malloc ( sizeof ( List )); G -> adjList [ i ] -> size = 0 ; G -> adjList [ i ] -> head = ( Node * ) malloc ( sizeof ( Node )); G -> adjList [ i ] -> head -> next = NULL ; } return G ; } void AddEdge ( Graph * G , const Edge e ) { G -> m ++ ; Insert ( G -> adjList [ e . u ], e . v , e . w ); } void FreeGraph ( Graph * G ) { for ( int i = 0 ; i < G -> n ; i ++ ) { for ( Node * p = G -> adjList [ i ] -> head , * temp = NULL ; p != NULL ;) { temp = p ; p = p -> next ; free ( temp ); } free ( G -> adjList [ i ]); } free ( G -> adjList ); free ( G ); } Pros Save space. Cons Complicated for implementation.","title":"Adjacency Lists \u90bb\u63a5\u94fe\u8868"},{"location":"Computer_Science_Courses/FDS/Graph/#adjacency-multilists","text":"Use the edges as nodes of multilists, and represent it by something similar with a static list . Data Type typedef struct { int to ; int w ; int next ; } EdgeNode ; typedef struct { int n ; int m ; EdgeNode * edge ; int * head ; } Graph ; Adjacency Multilists Graph * CreateGraph ( const int n , const int m ) { // (1)! Graph * G = ( Graph * ) malloc ( sizeof ( Graph )); G -> n = n ; G -> m = m ; G -> edge = ( EdgeNode * ) malloc ( G -> m * sizeof ( EdgeNode )); G -> head = ( int * ) malloc ( G -> n * sizeof ( int )); memset ( G -> edge , 0 , G -> m * sizeof ( EdgeNode )); for ( int i = 0 ; i < G -> n ; ++ i ) { G -> head [ i ] = -1 ; } G -> m = 0 ; return G ; } void AddEdge ( Graph * G , const Edge e ) { G -> edge [ G -> m ]. to = e . v ; G -> edge [ G -> m ]. w = e . w ; G -> edge [ G -> m ]. next = G -> head [ e . u ]; G -> head [ e . u ] = G -> m ; G -> m ++ ; } void FreeGraph ( Graph * G ) { free ( G -> edge ); free ( G -> head ); free ( G ); } Note that for undirected graph , m should be pass by 2 * m . Example: How Adjacency Multilists Build for the left given graph, the corresponding adjancency multilists is the right one","title":"Adjacency Multilists \u94fe\u5f0f\u524d\u5411\u661f"},{"location":"Computer_Science_Courses/FDS/Graph/#topological-sort","text":"Definition AOV Network (Activity On Vertex Network) is a digraph \\(G\\) in which \\(V(G)\\) represents activities and \\(E(G)\\) represents precedence relations between activities (vertices). \\(i\\) is a predecessor of \\(j\\) if there is a path from \\(i\\) to \\(j\\) , \\(i\\) is an immediate predecessor of j if \\(<i,j> \\in E(G)\\) , and \\(j\\) is called a successor ( immediate sucessor ) of \\(i\\) . If we can finish all activities in AOV Network in any order without finishing one before its predecessor finished, the AOV Network is feasible , else it is unfeasible Patial order is a precedence relation which is both transitive and irrelative . Proposition A project is feasible if it's irreflexive . Otherwise \\(i\\) is a predecessor of \\(i\\) , which means that \\(i\\) must be done before \\(i\\) starts. Feasible AOV network must be a DAG . Definition | Topological Order A topological order is a linear ordering of the vertices of a graph such that, for any two vertices \\(i\\) and \\(j\\) , if \\(i\\) is a predecessor of \\(j\\) in the network then \\(i\\) precedes \\(j\\) in the linear ordering. Topological Sort void FindIndegree ( const Graph * G , int indegree []) { memset ( indegree , 0 , G -> n * sizeof ( int )); for ( int i = 0 ; i < G -> n ; i ++ ) { for ( int j = G -> head [ i ]; j != -1 ; j = G -> edge [ j ]. next ) { indegree [ G -> edge [ j ]. to ] ++ ; } } } void TopologicalSort ( const Graph * G , int topNum []) { // (1)! Queue * queue = CreateQueue ( G -> m ); int indegree [ G -> n ]; FindIndegree ( G , indegree ); for ( int i = 0 ; i < G -> n ; i ++ ) { if ( indegree [ i ] == 0 ) { Enqueue ( queue , i ); } } int cnt = 0 ; while ( ! IsEmptyQueue ( queue )) { int now = Dequeue ( queue ); topNum [ cnt ++ ] = now ; for ( int idx = G -> head [ now ]; idx != -1 ; idx = G -> edge [ idx ]. next ) { //(2)! int to = G -> edge [ idx ]. to ; indegree [ to ] -- ; if ( indegree [ to ] == 0 ) { Enqueue ( queue , to ); } } } if ( cnt != G -> n ) { puts ( \"Graph has a cycle\" ); } } 1. topNum[] stores the result of topological sort. 2. The graph is represented by adjacent multilists . Other representations are also available. Complexity Suppose \\(E\\) is the number of edges and \\(V\\) is the number of vertices, then Time complexity \\(O(E + V)\\) . Space complexity \\(O(V)\\) . Note Topological sort can be used to detect whether the graph has a cycle , and it can also be used to detect whether the graph is a chain . Topological sort is not unique .","title":"Topological Sort"},{"location":"Computer_Science_Courses/FDS/Graph/#shortest-paths","text":"Definition Given a digraph \\(G=(V,E)\\) , and a cost function \\(c(e)\\) for \\(e\\in E(G)\\) , the length of a path \\(P\\) (also called weighted path length ) from source to destination is \\(\\sum\\limits_{e_i\\subset P}\\ w(e_i)\\) . The result of the following algorithms are stored in the following data type. Data Type typedef struct { int dist ; bool vis ; int prev ; } Path ; dist is the distance from source to v_i . vis is set to false if v_i is visited or true if not. prev record the previous vertex along the path. We can trace back until prev = -1 to obtain the path from source to v_i . Trace Back of Vertex v int now = v ; while ( now != -1 ) { printf ( \"%d \" , now ); now = path [ now ]. prev ; }","title":"Shortest Paths"},{"location":"Computer_Science_Courses/FDS/Graph/#unweight-shortest-paths","text":"It is implemented by a breadth-first search (BFS) algorithm, which used to find how many vertices from source to destination . Unweighted Shortest Paths #define INF 0x3f3f3f void BFS ( const Graph * G , const int source , Path path []) { for ( int i = 0 ; i < G -> n ; i ++ ) { path [ i ]. dist = INF ; path [ i ]. vis = false ; path [ i ]. prev = -1 ; } path [ source ]. dist = 0 ; Queue * queue = CreateQueue ( G -> n ); Enqueue ( queue , source ); while ( ! IsEmptyQueue ( queue )) { int now = Dequeue ( queue ); path [ now ]. vis = true ; for ( int idx = G -> head [ now ]; idx != -1 ; idx = G -> edge [ idx ]. next ) { //(1)! int to = G -> edge [ idx ]. to ; if ( ! path [ to ]. vis ) { path [ to ]. dist = path [ now ]. dist + 1 ; path [ to ]. prev = now ; Enqueue ( queue , to ); } } } FreeQueue ( queue ); return path ; } The graph is represented by adjacent multilists . Other representations are also available. Reason to Choose INF to be 0x3f3f3f We can use memset(a, 0x3f, n * sizeof(int)) to initialze. Add two infinities will still result in an infinity. Complexity Time complexity \\(O(E + V)\\) . Space comlexity \\(O(E)\\) .","title":"Unweight Shortest Paths"},{"location":"Computer_Science_Courses/FDS/Graph/#weighted-shortest-paths","text":"","title":"Weighted Shortest Paths"},{"location":"Computer_Science_Courses/FDS/Graph/#floyd","text":"Remains","title":"Floyd"},{"location":"Computer_Science_Courses/FDS/Graph/#dijkstra","text":"Description of Dijkstra Let \\(S = \\{ s \\text{ and } v_i \\text{'s whose shortest paths have been found}\\}\\) . For any \\(u \\notin S\\) , define \\[ \\text{distance}[u] = \\text{minimal length of path} \\{ s \\rightarrow (v_i\\in S)\\rightarrow u \\}. \\] If the paths are generated in non-decreasing order, then The shortest path must go through ONLY \\(v_i \\in S\\) ( Greedy Method ) \\(u\\) is chosen so that \\(\\text{distance}[u] = \\min\\{w \\notin S | \\text{distance}[w]\\}\\) (If \\(u\\) is not unique, then we can choose any of them). If \\(\\text{distance} [u_1] < \\text{distance} [u_2]\\) and we add \\(u_1\\) into \\(S\\) , then distance \\([u_2]\\) may change. If so, a shorter path from \\(s\\) to \\(u_2\\) must go through \\(u_1\\) and \\[ \\text{distance}'[u_2] = \\text{distance} [u_1] + \\text{length}(<u_1,u_2>). \\] Note For finding \\(u\\) , we can maintain a priority queue , which can decrease time complexity of finding \\(u\\) from \\(O(V)\\) to \\(O(\\log V)\\) . Dijkstra's Algorithm Simple Improved by Priority Queue void Dijkstra ( const Graph * G , const int source , Path path []) { for ( int i = 0 ; i < G -> n ; i ++ ) { path [ i ]. dist = INF ; path [ i ]. vis = false ; path [ i ]. prev = -1 ; } path [ source ]. dist = 0 ; int now = source ; while ( ! path [ now ]. vis ) { path [ now ]. vis = true ; for ( int idx = G -> head [ now ]; idx != -1 ; idx = G -> edge [ idx ]. next ) { int to = G -> edge [ idx ]. to ; int cost = G -> edge [ idx ]. dis ; if ( path [ to ]. dist > path [ now ]. dist + cost ) { path [ to ]. dist = path [ now ]. dist + cost ; path [ to ]. prev = now ; } } int distMin = INF ; for ( int i = 0 ; i < G -> n ; i ++ ) { if ( ! path [ i ]. vis && path [ i ]. dist < distMin ) { distMin = path [ i ]. dist ; now = i ; } } } } typedef struct { int num ; int dis ; } Node ; #define MinData -1 typedef Node ElementType ; typedef struct { ElementType * element ; int capacity ; int size ; } PriorityQueue ; ... void Dijkstra ( const Graph * G , const int source , Path path []) { for ( int i = 0 ; i < G -> n ; i ++ ) { path [ i ]. dist = INF ; path [ i ]. vis = false ; path [ i ]. prev = -1 ; } path [ source ]. dist = 0 ; PriorityQueue * Q = Initialize ( G -> m ); Insert ( Q , ( Node ){ source , 0 }); while ( ! IsEmptyQueue ( Q )) { Node cur = DeleteMin ( Q ); int now = cur . num ; if ( path [ now ]. vis ) { continue ; } path [ now ]. vis = true ; for ( int idx = G -> head [ now ]; idx != -1 ; idx = G -> edge [ idx ]. next ) { int to = G -> edge [ idx ]. to ; int cost = G -> edge [ idx ]. dis ; if ( path [ to ]. dist > path [ now ]. dist + cost ) { path [ to ]. dist = path [ now ]. dist + cost ; path [ to ]. prev = now ; if ( ! path [ to ]. vis ) { Insert ( Q , ( Node ){ to , path [ to ]. dist }); } } } } } Complexity Time complexity Simple \\(O(E + V^2)\\) . Improved \\(O((E + V) \\log V)\\) . Space comlexity Simple \\(O(1)\\) . Improved \\(O(E)\\) . Warning By allowing the used vertices being pushed into the set \\(S\\) again, we can get a algorithm that deals with negative edges well with time complexity \\(T = O(E \\cdot V)\\) . However, if the graph has a negative-weight cycle, it will cause infinite loop .","title":"Dijkstra"},{"location":"Computer_Science_Courses/FDS/Graph/#spfa","text":"Remains","title":"SPFA"},{"location":"Computer_Science_Courses/FDS/Graph/#acyclic-graph","text":"If the graph is acyclic, vertices may be selected in topological order since when a vertex is selected, its distance can no longer be lowered without any incoming edges from unknown nodes \\(T=O(|E|+|V|)\\) and no priority queue is needed","title":"Acyclic Graph"},{"location":"Computer_Science_Courses/FDS/Graph/#all-pairs-shortest-path-problem","text":"Use single-source algorithm for \\(V\\) times with \\(T=O(V^3)\\) - works fast on sparse graph. Remains Another \\(O(|V|^3)\\) algorithm in book's Chapter 10 - works faster on dense graphs","title":"All-Pairs Shortest Path Problem"},{"location":"Computer_Science_Courses/FDS/Graph/#network-flow-problems","text":"Definition | Network A network is a digraph \\(G(V, E)\\) . Each edge \\((u, v) \\in E(G)\\) has a weight \\(c(u, v)\\) called capacity . In particular, \\(\\forall\\ (u, v) \\notin E(G)\\) , \\(c(u, v) = 0\\) . Moreover, there are two special points, source (\u6e90\u70b9) \\(s\\) and sink (\u6c47\u70b9) \\(t\\) ( \\(s \\ne t\\) ). Definition | Flow If \\(f(u, v)\\) is a function that satisfies Capacity Restriction: \\(f(u, v) \\le c(u, v)\\) . Skew Symmetry: \\(f(u, v) = -f(v, u)\\) . Conservation of Flow: sum of outflows of source equals to sum of inflows of sink. \\[ \\forall\\ x \\in V - {s, t},\\ \\ \\sum\\limits_{(u, x) \\in E} f(u, x) = \\sum\\limits_{(x, v) \\in E} f(x, v). \\]","title":"Network Flow Problems"},{"location":"Computer_Science_Courses/FDS/Graph/#maximum-flow","text":"Maximum flow probelm want to determine the maximum amount of flow that can pass from source \\(s\\) to sink \\(t\\) . We introduce Ford-Fulkerson Augmenting Path Algorithm here. Definition Residual capacity \\(c_f(u, v)\\) is defined by the subtraction between the capacity of the edge and the flow, namely \\(c_f(u, v) = c(u, v) - f(u, v)\\) . A residual network \\(G_f\\) is the graph consists of all vertices of \\(G\\) and all edges that residual capacity is larger than \\(0\\) , namely \\(G_f = (V_f = V, E_f = \\{(u, v) \\in E | c_f(u, v) > 0\\})\\) . An augmenting Path (\u589e\u5e7f\u8def) is a path from \\(s\\) to \\(t\\) , with each edge using the minimum residual capacity of edges in the path. Proposition If the edge capabilities are rational numbers , this algorithm always terminate with a maximum flow. Note The algorithm works for \\(G\\) with cycles as well. Basic Idea Find an augmenting path from \\(s\\) to \\(t\\) . Augment it by subtracting minimum residual capacity along the augmenting path. adding minimum residual capacity along the inverse augmenting path. Example Description \\(G_r\\) is initialized to be the same as \\(G\\) to represent the selected flows. \\(G_f\\) is just a assistant graph of selected augmenting path. First, we find the blue augmenting path from \\(s\\) to \\(t\\) in \\(G_f\\) . The minimum residual capacity along tha path is \\(3\\) . Then we subtract \\(3\\) along the path, and add \\(3\\) along the inverse path. Second, we find the purple augmenting path from \\(s\\) to \\(t\\) in \\(G_f\\) . The minimum residual capacity along tha path is \\(2\\) . Then we subtract \\(2\\) along the path, and add \\(2\\) along the inverse path. Since there is no outflows of \\(s\\) in \\(G_f\\) , we can't find any augementing path any more. Thus it's the result case. Ford-Fulkerson Augmenting Path Algorithm DFS Edomnds-Karp / EK (BFS) int dfs ( const int now , const int prevCapacity , Graph * G , bool vis [], const int s , const int t ) { if ( now == t ) { return prevCapacity ; } vis [ now ] = true ; for ( int idx = G -> head [ now ]; idx ; idx = G -> edge [ idx ]. next ) { int to = G -> edge [ idx ]. to ; int capacity = G -> edge [ idx ]. capacity ; if ( capacity == 0 || vis [ to ] == true ) { continue ; } int cost = dfs ( to , min ( capacity , prevCapacity ), vis , s , t , G ); if ( cost != -1 ) { G -> edge [ idx ]. capacity -= cost ; G -> edge [ idx ^ 1 ]. capacity += cost ; return cost ; } } return -1 ; } int FF ( Graph * G , const int s , const int t ) { int ret = 0 , cost ; bool vis [ G -> n ]; memset ( vis , false , G -> n * sizeof ( bool )); while (( cost = dfs ( s , 0x3f3f3f , vis , s , t , G )) != -1 ) { memset ( vis , false , G -> n * sizeof ( bool )); ret += cost ; } return ret ; } Remains Complexity Suppose all capacity is integers, we have serval methods to find an augmenting path. Choose the augmenting path found by an unweighted shortest path algorithm. Time complexity \\[ \\begin{aligned} T &= T_{\\text{augmentation}} \\cdot T_{\\text{find a path}} \\\\ &= O(f) \\cdot O(E) \\\\ &= O(f\\cdot E),\\ \\ \\text{ where $f$ is the maximum flow.} \\end{aligned} \\] Choose the augmenting path that allows the largest increase in flow found by Dijkstra's algorithm. Time complextiy \\[ \\begin{aligned} T &= T_{\\text{augmentation}} \\cdot T_{\\text{find a path}} \\\\ &= O(E\\log cap_{\\text{max}}) \\cdot O(E\\log V) \\\\ &= O(E^2\\log V),\\ \\ \\text{if $cap_{\\text{max}} = \\max\\{c(u, v)\\}$ is small}. \\end{aligned} \\] Choose the augmenting path that has the least number of edges found by unweighted shortest path algorithm. Time complexity \\[ \\begin{aligned} T &= T_{augmentation} \\cdot T_{find a path} \\\\ &= O(E) \\cdot O(E \\cdot V) \\\\ &= O(E^2V). \\end{aligned} \\] Proposition If every \\(v\\notin {s,t}\\) has either a single incoming edge of capacity \\(1\\) oe a single outgoing edge of capacity \\(1\\) , then time bound is reduced to \\(O(E V^{1/2})\\) .","title":"Maximum Flow"},{"location":"Computer_Science_Courses/FDS/Graph/#min-cost-flow","text":"The min-cost flow problem want to find among all maximum flows, the one flow of minimum cost provided that each edge has a cost per unit of flow. Remains","title":"Min Cost Flow"},{"location":"Computer_Science_Courses/FDS/Graph/#minimum-spanning-tree-mst","text":"Definition A spanning tree of a graph \\(G\\) is a tree which consists of \\(V(G)\\) and a subset of \\(E(G)\\) . A minimum spanning tree (MST) is a spanning tree that the total cost of its edges is minimized . Note MST is a tree since it is acyclic , and thus the number of edges is \\(V-1\\) . It is spanning because it covers every vertex. A minimum spanning tree exists iff \\(G\\) is connected (in most case it's undirected ). Adding a non-tree edge to a spanning tree, it becomes a cycle .","title":"Minimum Spanning Tree (MST)"},{"location":"Computer_Science_Courses/FDS/Graph/#kruskals-algorithm","text":"It maintains a forest and is implemented by Disjoint Set . Basic idea is adding one edge at one time in an non-decreasing order and it's a Greedy Method . Results of Kruskal are stored in the following data type. Data Type typedef struct { int n ; // number of vertices in MST. int * vertex ; // vertices in MST. int edgeSum ; // sum of the weight of edges in MST. } Point ; Kruskal's Algorithm int cmp ( const void * a , const void * b ) { return (( Edge * ) a ) -> w - (( Edge * ) b ) -> w ; } int Find ( int * S , const int a ) { if ( S [ a ] < 0 ) { return a ; } return S [ a ] = Find ( S , S [ a ]); } void Union ( int * S , const int root_a , const int root_b ) { if ( root_a < root_b ) { S [ root_a ] += S [ root_b ]; S [ root_b ] = root_a ; } else { S [ root_b ] += S [ root_a ]; S [ root_a ] = root_b ; } } void Kruskal ( Graph * G , Point * point ) { Edge edge [ G -> m ]; for ( int i = 0 , cnt = 0 ; i < G -> n ; i ++ ) { for ( int idx = G -> head [ i ]; idx != -1 ; idx = G -> edge [ idx ]. next ) { edge [ cnt ++ ] = ( Edge ){ i , G -> edge [ idx ]. to , G -> edge [ idx ]. w }; } } qsort ( edge , G -> m , sizeof ( Edge ), cmp ); int * S = ( int * ) malloc ( G -> n * sizeof ( int )); for ( int i = 0 ; i < G -> n ; ++ i ) { S [ i ] = -1 ; } int cnt = 0 , vertex [ G -> n ], sum = 0 ; for ( int i = 0 ; i < G -> m ; ++ i ) { int root_a = Find ( S , edge [ i ]. u ); int root_b = Find ( S , edge [ i ]. v ); if ( root_a == root_b ) { continue ; } Union ( S , root_a , root_b ); vertex [ cnt ++ ] = i ; sum += edge [ i ]. w ; } point -> n = cnt ; point -> vertex = ( int * ) malloc ( point -> n * sizeof ( int )); memcpy ( point -> vertex , vertex , cnt * sizeof ( int )); point -> edgeSum = sum ; free ( S ); } Complexity Time complexity \\(O(E\\log E)\\) (sorting algorithm of \\(O(E \\log E)\\) , and union-find of \\(O(E \\alpha(V))\\) or \\(O(E \\log V)\\) ). Space complexity \\(O(V)\\) .","title":"Kruskal's Algorithm"},{"location":"Computer_Science_Courses/FDS/Graph/#prims-algorithm","text":"Basic idea is adding one vertex at one time to grow a tree, very similar to Dijkstra's algorithm. \\(T=O(|E|\\log|V|)\\) . Prim's Algorithm Simple Improve by Priority Queue void Prim ( Graph * G , Point * point ) { int dist [ G -> n ], prev [ G -> n ]; bool vis [ G -> n ]; memset ( vis , false , sizeof ( vis )); memset ( dist , 0x3f , sizeof ( dist )); int cnt = 0 , vertex [ G -> n ], sum = 0 ; int now = 0 ; dist [ now ] = 0 ; for ( int r = 0 ; r < G -> n ; r ++ ) { vis [ now ] = true ; if ( r != 0 ) { sum += G -> adjMat [ prev [ now ]][ now ]; } vertex [ cnt ++ ] = now ; for ( int i = 0 ; i < G -> n ; i ++ ) { if ( ! vis [ i ] && G -> adjMat [ now ][ i ] < dist [ i ]) { dist [ i ] = G -> adjMat [ now ][ i ]; prev [ i ] = now ; } } int minDist = 0x3f3f3f ; for ( int i = 0 ; i < G -> n ; i ++ ) { if ( ! vis [ i ] && dist [ i ] < minDist ) { now = i ; minDist = dist [ i ]; } } } point -> n = cnt ; point -> vertex = ( int * ) malloc ( cnt * sizeof ( int )); memcpy ( point -> vertex , vertex , cnt * sizeof ( int )); point -> edgeSum = sum ; } Complexity Time complexity Simple \\(O(E + V^2)\\) . Improved \\(O((E + V) \\log V)\\) . Space complexity \\(O(V)\\) .","title":"Prim's Algorithm"},{"location":"Computer_Science_Courses/FDS/Graph/#depth-first-search","text":"DFS can be regarded as a generalization of preorder traversal. DFS of a graph can generate a spanning tree, called DFS spanning tree . void DFS ( Vertex V ) { vis [ V ] = true ; /* mark this vertex to avoid cycles */ for ( each W adjacent to V ) { if ( ! vis [ W ]) { DFS ( W ); } } } Complexity Time complexity \\(O(E + V)\\) . Space complexity \\(O(1)\\) .","title":"Depth-First Search"},{"location":"Computer_Science_Courses/FDS/Graph/#tarjan-algorithm","text":"Tarjan algorithm is used to find strongly connected components as well as cut point . Define \\(\\text{Low}(u)\\) by \\[ \\begin{aligned} \\text{Low}(u) = \\min\\{ & \\text{Index}(u), \\\\ & \\min\\{\\text{Low}(w) | w \\text{ is a child of } u\\}, \\\\ & \\min\\{\\text{Index}(w) | (u, w) \\text{ is a back edge}\\}. \\end{aligned} \\]","title":"Tarjan Algorithm"},{"location":"Computer_Science_Courses/FDS/Graph/#strongly-connected-components-scc","text":"For finding SCC, tarjan algorithm maintains a DFS spanning tree and a stack . Tarjan Algorithm for Strongly Connected Components void Tarjan ( const int now , const Graph * G , int dfn [], int low [], Stack * stack , bool inStack []) { static int idx = 0 ; dfn [ now ] = low [ now ] = ++ idx ; inStack [ now ] = true ; Push ( stack , now ); for ( int i = G -> head [ now ]; i != -1 ; i = G -> edge [ i ]. next ) { int to = G -> edge [ i ]. to ; if ( dfn [ to ] == 0 ) { Tarjan ( to , G , dfn , low , stack , inStack ); low [ now ] = min ( low [ now ], low [ to ]); } else { if ( inStack [ to ]) { low [ now ] = min ( low [ now ], dfn [ to ]); } } } if ( dfn [ now ] == low [ now ]) { int temp ; do { temp = Pop ( stack ); inStack [ temp ] = false ; printf ( \"%d\" , temp ) } while ( now != temp ); putchar ( '\\n' ); } } void FindStronglyConnectedComponents ( const Graph * G ) { int dfn [ G -> n ], low [ G -> n ]; memset ( dfn , 0 , G -> n * sizeof ( int )); memset ( low , 0 , G -> n * sizeof ( int )); Stack * stack = CreateStack ( G -> n ); bool inStack [ G -> n ]; memset ( inStack , false , G -> n * sizeof ( bool )); for ( int i = 0 ; i < G -> n ; i ++ ) { if ( dfn [ i ] == 0 ) { Tarjan ( i , G , dfn , low , stack , inStack ); } } }","title":"Strongly Connected Components (SCC)"},{"location":"Computer_Science_Courses/FDS/Graph/#cut-point","text":"\\(u\\) is an articulation point if \\(u\\) is the root and has at least 2 children. \\(u\\) is not the root , and has at least 1 child such that \\(\\text{Low}(child) \\ge \\text{Index}(u)\\) . Example Suppose DFS spanning tree starts from vertex \\(3\\) . Blue numbers are indices of searching order stored in dfn[] and red points are the cut points. Tarjan for Cut Point Tarjan Algorithm for Articulation Point / Cut Point void Tarjan ( const int now , const Graph * G , const int root , int dfn [], int low [], bool isCutPoint []) { static int idx = 0 ; int child = 0 ; dfn [ now ] = low [ now ] = ++ idx ; for ( int i = G -> head [ now ]; i != -1 ; i = G -> edge [ i ]. next ) { int to = G -> edge [ i ]. to ; if ( dfn [ to ] == 0 ) { child ++ ; Tarjan ( to , G , root , dfn , low , isCutPoint ); low [ now ] = min ( low [ now ], low [ to ]); if ( dfn [ now ] <= low [ to ] && now != root ) { isCutPoint [ now ] = true ; } } else { low [ now ] = min ( low [ now ], dfn [ to ]); } } if ( child > 1 && now == root ) { isCutPoint [ now ] = true ; } } void FindCutPoint ( const Graph * G , bool isCutPoint []) { int dfn [ G -> n ], low [ G -> n ]; memset ( dfn , 0 , G -> n * sizeof ( int )); memset ( low , 0 , G -> n * sizeof ( int )); memset ( isCutPoint , false , G -> n * sizeof ( bool )); for ( int i = 0 ; i < G -> n ; i ++ ) { if ( dfn [ i ] == 0 ) { Tarjan ( i , G , i , dfn , low , isCutPoint ); } } }","title":"Cut Point"},{"location":"Computer_Science_Courses/FDS/Graph/#euler-paths-and-circuits","text":"Definition Euler tour / Path is a path go through all edges. Euler circuit is a path go through all edges and finish at the starting point. Theorem An Euler circuit is possible only if the graph is connected and each vertex has an even degree An Euler tour is possible if there are exactly two vertices having odd degree. One must start at one of the odd-degree vertices. Note The path should be maintained as a linked list For each adjacency list, maintain a pointer to the last edge scanned \\(T=O(|E|+|V|)\\)","title":"Euler Paths and Circuits"},{"location":"Computer_Science_Courses/FDS/Graph/#hamilton-paths-and-circuits","text":"Definition Hamilton tour / Path is a path go through all vertices. Hamilton circuit is a path go through all vertices and finish at the starting point.","title":"Hamilton Paths and Circuits"},{"location":"Computer_Science_Courses/FDS/Hashing/","text":"Hashing \u00b6 Hash table (\u6563\u5217\u8868, \u54c8\u5e0c\u8868) , or say Symbol table (Dictionary) is an ADT that supports insertions , deletions and finds in constant average time. ADT Objects: A set of key-value pairs, where keys are unique . Operations: Create a hash table. Check whether a key is already in a hash table. Find the corresponding value of a key in a hash table. Insert a new key-value pair into a hash table. Delete a key and its corresponding value in a hash table. Hash Function \u00b6 A hash function \\(f\\) is a function used for building hash table by \\[ hash(key) = value. \\] But sometimes we have \\(key_1 \\ne key_2\\) , but \\(hash(key_1) = hash(key_2)\\) , we called this a collision . Property \\(hash(x)\\) is easy to compute and minimize collisions. \\(hash(x)\\) is supposed to be unbiased , or say uniform . That is for any \\(x\\) and \\(i\\) , we want that \\[ P\\{hash(x) = i\\} = \\frac1b,\\ \\ \\text{ where $b$ is some number}. \\] Such kind of a hash function is called a uniform hash function . Common Choice of Hash Function Remainder . \\(N\\) is the size of hash table, often a prime number. \\[ hash(x) = x \\text{ mod } N. \\] Middle Square . Take middle some digits of the square of the key as the address of hash function. Random . Hash Table \u00b6 Definition \\(T\\) is the total number of distinct possible value for \\(x\\) . \\(n\\) is the total number of identifers in hash table. Identifier Density \\(d = \\dfrac{n}{T}\\) . Loading Density \\(\\lambda = \\dfrac{n}{sb}\\) . An overflow occur when a new identifier are hashed into a full bucket. Collision Resolution \u00b6 Although we try many methods to make a hash function to avoid collision. But it's unfortunately unavoidable. The most important issue of hash table is to deal with collision. We have some strategy for it. Suppose the hash function we use below are all \\[ hash(x) = x \\text{ mod } N,\\ \\ N \\in \\mathbb{P}. \\] Separate Chaining \u62c9\u94fe\u6cd5 \u00b6 Separate chaining keeps a list of all keys that hash to the same value. Data Type typedef struct { int key ; int value ; } Pair ; typedef Pair ElementType ; typedef struct _node { ElementType * element ; struct _node * next ; } Node ; typedef struct { int size ; Node * head ; } List ; typedef struct { int tableSize ; List ** list ; int ( * Hash )( const int key , const int N ); } HashTable ; Separate Chaining static bool IsPrime ( const int x ) { bool ret = true ; for ( int i = 2 , temp = sqrt ( x ); i <= temp ; i ++ ) { if ( x % i == 0 ) { ret = false ; break ; } } return ret ; } static int NextPrime ( int x ) { while ( ! IsPrime ( x )) { x ++ ; } return x ; } static int Hash ( const int key , const int N ) { return key % N ; } HashTable * CreateHashTable ( const int tableSize ) { HashTable * H = ( HashTable * ) malloc ( sizeof ( HashTable )); H -> tableSize = NextPrime ( tableSize ); // (1)! H -> list = ( List ** ) malloc ( H -> tableSize * sizeof ( List * )); for ( int i = 0 ; i < H -> tableSize ; i ++ ) { H -> list [ i ] = ( List * ) malloc ( sizeof ( List )); H -> list [ i ] -> head = ( Node * ) malloc ( sizeof ( Node )); // (2)! H -> list [ i ] -> head -> next = NULL ; } H -> Hash = Hash ; return H ; } void FreeHashTable ( HashTable * H ) { for ( int i = 0 ; i < H -> tableSize ; i ++ ) { for ( Node * p = H -> list [ i ] -> head , * temp = NULL ; p != NULL ; ) { temp = p ; p = p -> next ; free ( temp ); } free ( H -> list [ i ]); } free ( H -> list ); free ( H ); } Pair * Find ( HashTable * H , const int key ) { Pair * ret = NULL ; List * L = H -> list [ H -> Hash ( key , H -> tableSize )]; for ( Node * p = L -> head -> next ; p != NULL ; p = p -> next ) { if ( p -> element -> key == key ) { ret = p -> element ; break ; } } return ret ; } void Insert ( HashTable * H , const ElementType pair ) { if ( Find ( H , pair . key ) != NULL ) { // WARNING(\"The key is already in the hash table\"); return ; } List * L = H -> list [ H -> Hash ( pair . key , H -> tableSize )]; L -> size ++ ; Node * p = ( Node * ) malloc ( sizeof ( Node )); p -> element = ( Pair * ) malloc ( sizeof ( Pair )); p -> element -> key = pair . key ; p -> element -> value = pair . value ; p -> next = L -> head -> next ; L -> head -> next = p ; } void Delete ( HashTable * H , const int key ) { bool flag = false ; List * L = H -> list [ H -> Hash ( key , H -> tableSize )]; for ( Node * p = L -> head -> next , * q = L -> head ; p != NULL ; p = p -> next , q = p ) { if ( p -> element -> key == key ) { L -> size -- ; q -> next = p -> next ; free ( p ); flag = true ; break ; } } if ( flag == false ) { // WARNING(\"There is no key in the hash table\"); return ; } } Ensure the tableSize is prime . Sentinel. Open Addressing \u5f00\u653e\u5bfb\u5740\u6cd5 / Closed Hashing \u95ed\u6563\u5217\u6cd5 \u00b6 Instead of using pointer and linked lists, if we do not want to expand the hash table, an alternative way is Open Addressing . The basic idea is that if there is a collision where \\(hash(x)\\) is already occupied, we make some offset , which means to take the following function as the address of hash function with \\(i\\) increasing until the address is not occupied. \\[ (hash(key) + f(i)) \\text{ mod } N. \\] Linear Probing \u7ebf\u6027\u63a2\u6d4b \u00b6 Linear probing defines \\(f(i)\\) by \\[ f(i) = i. \\] Analysis of linear probing show that the expected number of probes \\(p\\) satisfies \\[ \\begin{aligned} p = &\\left\\{ \\begin{aligned} & \\frac12\\left(1 + \\frac{1}{(1 - \\lambda)^2}\\right), && \\text{for insertions and unsuccessful searches,} \\\\ & \\frac12\\left(1 + \\frac{1}{(1 - \\lambda)}\\right), && \\text{for successful searches,} \\end{aligned} \\right. ,\\\\ & \\text{where $\\lambda$ is the loading density}. \\end{aligned} \\] Quadratic Probing \u5e73\u65b9\u63a2\u6d4b \u00b6 Quadratic probing defines \\(f(i)\\) by \\[ f(i) = i^2. \\] Different from linear probing, there may be the cases that all addresses of \\(hash(key) + f(i)\\) are occupied even though the hash table is not full. To avoid this case, an important property for quadratic probing is described in the following theroem. Theorem If quadratic probing is used, and the table size is prime , then a new element can always be inserted if the table is at least half empty . Theorem If the table size is a prime of the form \\(4k + 3\\) , then the quadratic probing can probe the entire table. Cluster \u805a\u96c6 / \u5806\u79ef Cluster means the ununiformity occupation of the hash table, which forms clusters or blocks. For linear probing , there is primary clustering and for quadratic probing , there is secondary clustering . Since cluster will result in increased search time, we should make efforts to avoid it. Double Hashing \u53cc\u6563\u5217 \u00b6 Double hashing defines \\(f(i)\\) by \\[ f(i) = i \\cdot hash_2(key), \\] which means that the total hash address is \\[ hash(key) = hash_1(key) + i \\cdot hash_2(key). \\] The second hash function \\(hash_2(x)\\) must satisfy \\(hash_2(x) \\ne 0\\) . Make sure that all addresses can be probed. A choice of \\(hash_2(x)\\) that relatively works well is \\[ hash_2(x) = R - (x \\text{ mod } R),\\ \\ \\text{ where } R \\in \\mathbb{P} \\text{ and } R < N. \\] NOTE If double hashing is correctly implemented, simulations imply that the expected number of probes is almost the same as for a random collision resolution strategy. Data Type typedef struct { int key ; int value ; } Pair ; typedef Pair ElementType ; typedef struct { int tableSize ; bool * occupied ; ElementType ** table ; int ( * Hash )( const int key , const int N ); int ( * f )( const int i ); } HashTable ; Open Addressing static bool IsPrime ( const int x ) { bool ret = true ; for ( int i = 2 , temp = sqrt ( x ); i <= temp ; i ++ ) { if ( x % i == 0 ) { ret = false ; break ; } } return ret ; } static int NextPrime ( int x ) { while ( ! IsPrime ( x )) { x ++ ; } return x ; } static int Hash ( const int key , const int N ) { return key % N ; } static int f ( const int i ) { return i * i ; // (1)! } HashTable * CreateHashTable ( const int tableSize ) { HashTable * H = ( HashTable * ) malloc ( sizeof ( HashTable )); H -> tableSize = NextPrime ( tableSize ); H -> occupied = ( bool * ) malloc ( H -> tableSize * sizeof ( bool )); memset ( H -> occupied , false , H -> tableSize * sizeof ( bool )); H -> table = ( ElementType ** ) malloc ( H -> tableSize * sizeof ( ElementType * )); for ( int i = 0 ; i < H -> tableSize ; i ++ ) { H -> table [ i ] = ( ElementType * ) malloc ( sizeof ( ElementType )); } H -> Hash = Hash ; H -> f = f ; return H ; } void FreeHashTable ( HashTable * H ) { free ( H -> occupied ); for ( int i = 0 ; i < H -> tableSize ; i ++ ) { free ( H -> table [ i ]); } free ( H -> table ); free ( H ); } void PrintHashTable ( const HashTable * H ) { for ( int i = 0 ; i < H -> tableSize ; i ++ ) { if ( H -> occupied [ i ] == false ) { printf ( \"0 \" ); } else { printf ( \"%d \" , H -> table [ i ] -> key ); } } putchar ( '\\n' ); for ( int i = 0 ; i < H -> tableSize ; i ++ ) { if ( H -> occupied [ i ] == false ) { printf ( \"0 \" ); } else { printf ( \"%d \" , H -> table [ i ] -> value ); } } putchar ( '\\n' ); } Pair * Find ( HashTable * H , const int key ) { Pair * ret = NULL ; int pos = H -> Hash ( key , H -> tableSize ); int pivot = pos , i = 1 ; while ( H -> occupied [ pos ] == true ) { if ( H -> table [ pos ] -> key == key ) { ret = H -> table [ pos ]; break ; } pos = ( pivot + H -> f ( i )) % H -> tableSize ; i ++ ; } return ret ; } void Insert ( HashTable * H , const ElementType pair ) { bool flag = false ; int pos = H -> Hash ( pair . key , H -> tableSize ); int pivot = pos , i = 1 ; while ( H -> occupied [ pos ] == true ) { if ( H -> table [ pos ] -> key == pair . key ) { flag = true ; break ; } pos = ( pivot + H -> f ( i )) % H -> tableSize ; i ++ ; } if ( flag == true ) { // WARNING(\"The key is already in the hash table\"); return ; } H -> occupied [ pos ] = true ; H -> table [ pos ] -> key = pair . key ; H -> table [ pos ] -> value = pair . value ; } void Delete ( HashTable * H , const int key ) { bool flag = false ; int pos = H -> Hash ( key , H -> tableSize ); int pivot = pos , i = 1 ; while ( H -> occupied [ pos ] == true ) { if ( H -> table [ pos ] -> key == key ) { H -> occupied [ pos ] = false ; flag = true ; break ; } pos = ( pivot + H -> f ( i )) % H -> tableSize ; i ++ ; } if ( flag == false ) { // WARNING(\"There is no key in the hash table\"); return ; } } Here is the quadratic probing function. We can replace it by return i for linear probing . Or we can use double hashing by the following snippet. typedef struct { ... int R ; int ( * f )( const int i , const int key , const int R ); } HashTable ; HashTable * CreateHashTable ( const int tableSize ) { ... H -> R = /* Some prime smaller than `tableSize` */ ; H -> f = f ; return H ; } static int f ( const int i , const int key , const int R ) { int h = R - ( key % R ); return i * h ; } Tip For the probing statement pos = ( pivot + H -> f ( i )) % H -> tableSize ; We can replace it by the following more efficient snippet. Linear Probing Quadratic Probing Double Hashing pos = ( pos + 1 ) % H -> tableSize ; pos = ( pos + 2 * i - 1 ) % H -> tableSize ; int h = H -> R - ( key % H -> R ); while (...) { ... pos = ( pos + h ) % H -> tableSize ; ... } Rehashing \u518d\u6563\u5217 \u00b6 Quadratic probing does not require the use of a second hash function and is thus likely to be simpler and faster in practice. But when the hash table gets too full, quadratic probing may fail. Thus we need rehashing . When to Rehash? The hash table is half full. An insertion fails. The hash table reaches a certain load factor. Process of Rehashing Step.1 Build another table that is about twice as big (Remember it had better be prime ). Step.2 Scan down the entire original hash table for non-deleted elements. Step.3 Use a new function to hash those elements into the new table. Rehashing HashTable * Rehash ( HashTable * H ) { HashTable newH = CreateHashTable ( 2 * H -> tableSize ); for ( int i = 0 ; i < H -> tableSize ; i ++ ) { if ( H -> flag [ i ] == false ) { Insert ( newH , H -> table [ i ]); } } free ( H -> flag ); free ( H -> table ); return newH ; }","title":"Hashing"},{"location":"Computer_Science_Courses/FDS/Hashing/#hashing","text":"Hash table (\u6563\u5217\u8868, \u54c8\u5e0c\u8868) , or say Symbol table (Dictionary) is an ADT that supports insertions , deletions and finds in constant average time. ADT Objects: A set of key-value pairs, where keys are unique . Operations: Create a hash table. Check whether a key is already in a hash table. Find the corresponding value of a key in a hash table. Insert a new key-value pair into a hash table. Delete a key and its corresponding value in a hash table.","title":"Hashing"},{"location":"Computer_Science_Courses/FDS/Hashing/#hash-function","text":"A hash function \\(f\\) is a function used for building hash table by \\[ hash(key) = value. \\] But sometimes we have \\(key_1 \\ne key_2\\) , but \\(hash(key_1) = hash(key_2)\\) , we called this a collision . Property \\(hash(x)\\) is easy to compute and minimize collisions. \\(hash(x)\\) is supposed to be unbiased , or say uniform . That is for any \\(x\\) and \\(i\\) , we want that \\[ P\\{hash(x) = i\\} = \\frac1b,\\ \\ \\text{ where $b$ is some number}. \\] Such kind of a hash function is called a uniform hash function . Common Choice of Hash Function Remainder . \\(N\\) is the size of hash table, often a prime number. \\[ hash(x) = x \\text{ mod } N. \\] Middle Square . Take middle some digits of the square of the key as the address of hash function. Random .","title":"Hash Function"},{"location":"Computer_Science_Courses/FDS/Hashing/#hash-table","text":"Definition \\(T\\) is the total number of distinct possible value for \\(x\\) . \\(n\\) is the total number of identifers in hash table. Identifier Density \\(d = \\dfrac{n}{T}\\) . Loading Density \\(\\lambda = \\dfrac{n}{sb}\\) . An overflow occur when a new identifier are hashed into a full bucket.","title":"Hash Table"},{"location":"Computer_Science_Courses/FDS/Hashing/#collision-resolution","text":"Although we try many methods to make a hash function to avoid collision. But it's unfortunately unavoidable. The most important issue of hash table is to deal with collision. We have some strategy for it. Suppose the hash function we use below are all \\[ hash(x) = x \\text{ mod } N,\\ \\ N \\in \\mathbb{P}. \\]","title":"Collision Resolution"},{"location":"Computer_Science_Courses/FDS/Hashing/#separate-chaining","text":"Separate chaining keeps a list of all keys that hash to the same value. Data Type typedef struct { int key ; int value ; } Pair ; typedef Pair ElementType ; typedef struct _node { ElementType * element ; struct _node * next ; } Node ; typedef struct { int size ; Node * head ; } List ; typedef struct { int tableSize ; List ** list ; int ( * Hash )( const int key , const int N ); } HashTable ; Separate Chaining static bool IsPrime ( const int x ) { bool ret = true ; for ( int i = 2 , temp = sqrt ( x ); i <= temp ; i ++ ) { if ( x % i == 0 ) { ret = false ; break ; } } return ret ; } static int NextPrime ( int x ) { while ( ! IsPrime ( x )) { x ++ ; } return x ; } static int Hash ( const int key , const int N ) { return key % N ; } HashTable * CreateHashTable ( const int tableSize ) { HashTable * H = ( HashTable * ) malloc ( sizeof ( HashTable )); H -> tableSize = NextPrime ( tableSize ); // (1)! H -> list = ( List ** ) malloc ( H -> tableSize * sizeof ( List * )); for ( int i = 0 ; i < H -> tableSize ; i ++ ) { H -> list [ i ] = ( List * ) malloc ( sizeof ( List )); H -> list [ i ] -> head = ( Node * ) malloc ( sizeof ( Node )); // (2)! H -> list [ i ] -> head -> next = NULL ; } H -> Hash = Hash ; return H ; } void FreeHashTable ( HashTable * H ) { for ( int i = 0 ; i < H -> tableSize ; i ++ ) { for ( Node * p = H -> list [ i ] -> head , * temp = NULL ; p != NULL ; ) { temp = p ; p = p -> next ; free ( temp ); } free ( H -> list [ i ]); } free ( H -> list ); free ( H ); } Pair * Find ( HashTable * H , const int key ) { Pair * ret = NULL ; List * L = H -> list [ H -> Hash ( key , H -> tableSize )]; for ( Node * p = L -> head -> next ; p != NULL ; p = p -> next ) { if ( p -> element -> key == key ) { ret = p -> element ; break ; } } return ret ; } void Insert ( HashTable * H , const ElementType pair ) { if ( Find ( H , pair . key ) != NULL ) { // WARNING(\"The key is already in the hash table\"); return ; } List * L = H -> list [ H -> Hash ( pair . key , H -> tableSize )]; L -> size ++ ; Node * p = ( Node * ) malloc ( sizeof ( Node )); p -> element = ( Pair * ) malloc ( sizeof ( Pair )); p -> element -> key = pair . key ; p -> element -> value = pair . value ; p -> next = L -> head -> next ; L -> head -> next = p ; } void Delete ( HashTable * H , const int key ) { bool flag = false ; List * L = H -> list [ H -> Hash ( key , H -> tableSize )]; for ( Node * p = L -> head -> next , * q = L -> head ; p != NULL ; p = p -> next , q = p ) { if ( p -> element -> key == key ) { L -> size -- ; q -> next = p -> next ; free ( p ); flag = true ; break ; } } if ( flag == false ) { // WARNING(\"There is no key in the hash table\"); return ; } } Ensure the tableSize is prime . Sentinel.","title":"Separate Chaining \u62c9\u94fe\u6cd5"},{"location":"Computer_Science_Courses/FDS/Hashing/#open-addressing-closed-hashing","text":"Instead of using pointer and linked lists, if we do not want to expand the hash table, an alternative way is Open Addressing . The basic idea is that if there is a collision where \\(hash(x)\\) is already occupied, we make some offset , which means to take the following function as the address of hash function with \\(i\\) increasing until the address is not occupied. \\[ (hash(key) + f(i)) \\text{ mod } N. \\]","title":"Open Addressing \u5f00\u653e\u5bfb\u5740\u6cd5 / Closed Hashing \u95ed\u6563\u5217\u6cd5"},{"location":"Computer_Science_Courses/FDS/Hashing/#linear-probing","text":"Linear probing defines \\(f(i)\\) by \\[ f(i) = i. \\] Analysis of linear probing show that the expected number of probes \\(p\\) satisfies \\[ \\begin{aligned} p = &\\left\\{ \\begin{aligned} & \\frac12\\left(1 + \\frac{1}{(1 - \\lambda)^2}\\right), && \\text{for insertions and unsuccessful searches,} \\\\ & \\frac12\\left(1 + \\frac{1}{(1 - \\lambda)}\\right), && \\text{for successful searches,} \\end{aligned} \\right. ,\\\\ & \\text{where $\\lambda$ is the loading density}. \\end{aligned} \\]","title":"Linear Probing \u7ebf\u6027\u63a2\u6d4b"},{"location":"Computer_Science_Courses/FDS/Hashing/#quadratic-probing","text":"Quadratic probing defines \\(f(i)\\) by \\[ f(i) = i^2. \\] Different from linear probing, there may be the cases that all addresses of \\(hash(key) + f(i)\\) are occupied even though the hash table is not full. To avoid this case, an important property for quadratic probing is described in the following theroem. Theorem If quadratic probing is used, and the table size is prime , then a new element can always be inserted if the table is at least half empty . Theorem If the table size is a prime of the form \\(4k + 3\\) , then the quadratic probing can probe the entire table. Cluster \u805a\u96c6 / \u5806\u79ef Cluster means the ununiformity occupation of the hash table, which forms clusters or blocks. For linear probing , there is primary clustering and for quadratic probing , there is secondary clustering . Since cluster will result in increased search time, we should make efforts to avoid it.","title":"Quadratic Probing \u5e73\u65b9\u63a2\u6d4b"},{"location":"Computer_Science_Courses/FDS/Hashing/#double-hashing","text":"Double hashing defines \\(f(i)\\) by \\[ f(i) = i \\cdot hash_2(key), \\] which means that the total hash address is \\[ hash(key) = hash_1(key) + i \\cdot hash_2(key). \\] The second hash function \\(hash_2(x)\\) must satisfy \\(hash_2(x) \\ne 0\\) . Make sure that all addresses can be probed. A choice of \\(hash_2(x)\\) that relatively works well is \\[ hash_2(x) = R - (x \\text{ mod } R),\\ \\ \\text{ where } R \\in \\mathbb{P} \\text{ and } R < N. \\] NOTE If double hashing is correctly implemented, simulations imply that the expected number of probes is almost the same as for a random collision resolution strategy. Data Type typedef struct { int key ; int value ; } Pair ; typedef Pair ElementType ; typedef struct { int tableSize ; bool * occupied ; ElementType ** table ; int ( * Hash )( const int key , const int N ); int ( * f )( const int i ); } HashTable ; Open Addressing static bool IsPrime ( const int x ) { bool ret = true ; for ( int i = 2 , temp = sqrt ( x ); i <= temp ; i ++ ) { if ( x % i == 0 ) { ret = false ; break ; } } return ret ; } static int NextPrime ( int x ) { while ( ! IsPrime ( x )) { x ++ ; } return x ; } static int Hash ( const int key , const int N ) { return key % N ; } static int f ( const int i ) { return i * i ; // (1)! } HashTable * CreateHashTable ( const int tableSize ) { HashTable * H = ( HashTable * ) malloc ( sizeof ( HashTable )); H -> tableSize = NextPrime ( tableSize ); H -> occupied = ( bool * ) malloc ( H -> tableSize * sizeof ( bool )); memset ( H -> occupied , false , H -> tableSize * sizeof ( bool )); H -> table = ( ElementType ** ) malloc ( H -> tableSize * sizeof ( ElementType * )); for ( int i = 0 ; i < H -> tableSize ; i ++ ) { H -> table [ i ] = ( ElementType * ) malloc ( sizeof ( ElementType )); } H -> Hash = Hash ; H -> f = f ; return H ; } void FreeHashTable ( HashTable * H ) { free ( H -> occupied ); for ( int i = 0 ; i < H -> tableSize ; i ++ ) { free ( H -> table [ i ]); } free ( H -> table ); free ( H ); } void PrintHashTable ( const HashTable * H ) { for ( int i = 0 ; i < H -> tableSize ; i ++ ) { if ( H -> occupied [ i ] == false ) { printf ( \"0 \" ); } else { printf ( \"%d \" , H -> table [ i ] -> key ); } } putchar ( '\\n' ); for ( int i = 0 ; i < H -> tableSize ; i ++ ) { if ( H -> occupied [ i ] == false ) { printf ( \"0 \" ); } else { printf ( \"%d \" , H -> table [ i ] -> value ); } } putchar ( '\\n' ); } Pair * Find ( HashTable * H , const int key ) { Pair * ret = NULL ; int pos = H -> Hash ( key , H -> tableSize ); int pivot = pos , i = 1 ; while ( H -> occupied [ pos ] == true ) { if ( H -> table [ pos ] -> key == key ) { ret = H -> table [ pos ]; break ; } pos = ( pivot + H -> f ( i )) % H -> tableSize ; i ++ ; } return ret ; } void Insert ( HashTable * H , const ElementType pair ) { bool flag = false ; int pos = H -> Hash ( pair . key , H -> tableSize ); int pivot = pos , i = 1 ; while ( H -> occupied [ pos ] == true ) { if ( H -> table [ pos ] -> key == pair . key ) { flag = true ; break ; } pos = ( pivot + H -> f ( i )) % H -> tableSize ; i ++ ; } if ( flag == true ) { // WARNING(\"The key is already in the hash table\"); return ; } H -> occupied [ pos ] = true ; H -> table [ pos ] -> key = pair . key ; H -> table [ pos ] -> value = pair . value ; } void Delete ( HashTable * H , const int key ) { bool flag = false ; int pos = H -> Hash ( key , H -> tableSize ); int pivot = pos , i = 1 ; while ( H -> occupied [ pos ] == true ) { if ( H -> table [ pos ] -> key == key ) { H -> occupied [ pos ] = false ; flag = true ; break ; } pos = ( pivot + H -> f ( i )) % H -> tableSize ; i ++ ; } if ( flag == false ) { // WARNING(\"There is no key in the hash table\"); return ; } } Here is the quadratic probing function. We can replace it by return i for linear probing . Or we can use double hashing by the following snippet. typedef struct { ... int R ; int ( * f )( const int i , const int key , const int R ); } HashTable ; HashTable * CreateHashTable ( const int tableSize ) { ... H -> R = /* Some prime smaller than `tableSize` */ ; H -> f = f ; return H ; } static int f ( const int i , const int key , const int R ) { int h = R - ( key % R ); return i * h ; } Tip For the probing statement pos = ( pivot + H -> f ( i )) % H -> tableSize ; We can replace it by the following more efficient snippet. Linear Probing Quadratic Probing Double Hashing pos = ( pos + 1 ) % H -> tableSize ; pos = ( pos + 2 * i - 1 ) % H -> tableSize ; int h = H -> R - ( key % H -> R ); while (...) { ... pos = ( pos + h ) % H -> tableSize ; ... }","title":"Double Hashing \u53cc\u6563\u5217"},{"location":"Computer_Science_Courses/FDS/Hashing/#rehashing","text":"Quadratic probing does not require the use of a second hash function and is thus likely to be simpler and faster in practice. But when the hash table gets too full, quadratic probing may fail. Thus we need rehashing . When to Rehash? The hash table is half full. An insertion fails. The hash table reaches a certain load factor. Process of Rehashing Step.1 Build another table that is about twice as big (Remember it had better be prime ). Step.2 Scan down the entire original hash table for non-deleted elements. Step.3 Use a new function to hash those elements into the new table. Rehashing HashTable * Rehash ( HashTable * H ) { HashTable newH = CreateHashTable ( 2 * H -> tableSize ); for ( int i = 0 ; i < H -> tableSize ; i ++ ) { if ( H -> flag [ i ] == false ) { Insert ( newH , H -> table [ i ]); } } free ( H -> flag ); free ( H -> table ); return newH ; }","title":"Rehashing \u518d\u6563\u5217"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/","text":"Lists, Stacks and Queues \u00b6 Definition Data type consists of objects and operations . An Abstract Data Type (ADT) is a data type that is organized in such a way that the specification on the objects are separated from the representation of the objects. the specification of the operations on the objects are separated from the implementation on the operations List (Linear List) \u00b6 ADT Objects : (item 0 , item 1 , ..., item N - 1 ) Operations Make an empty list. Check whether it's an empty list. Print all the items in a list. Get the length of a list. Find the k -th item from a list. Insert a new item at position k of a list. Delete an item at position k from a list. Find the next item of the current item from a list. Find the previous item of the current item from a list. Following are some implementations of list. Array / Sequential List \u00b6 We say it a sequential mapping or sequantial storage structure . Data Type typedef int ElementType ; typedef struct { ElementType * array ; int capacity ; int size ; } List ; Array / Sequential List List * CreateList ( const int capacity ) { List * list = ( List * ) malloc ( sizeof ( List )); list -> capacity = capacity ; list -> size = 0 ; list -> array = ( ElementType * ) malloc ( list -> capacity * sizeof ( ElementType )); return list ; } void FreeList ( List * list ) { free ( list -> array ); free ( list ); } void PrintList ( const List * list ) { for ( int i = 0 ; i < list -> size ; i ++ ) { printf ( \"%d \" , list -> array [ i ]); } putchar ( '\\n' ); } int GetListLength ( const List * list ) { return list -> size ; } bool IsEmptyList ( const List * list ) { return list -> size == 0 ; } ElementType Find ( const List * list , const int position ) { return list -> array [ position ]; } void Insert ( List * list , const int position , const ElementType element ) { assert ( 0 < position && position <= list -> size && ( list -> capacity == -1 || list -> size < list -> capacity )); list -> size ++ ; for ( int i = list -> size ; i > position ; i -- ) { list -> array [ i ] = list -> array [ i - 1 ]; } list -> array [ position ] = element ; } void Delete ( List * list , const int position ) { assert ( 0 <= position && postion < list -> size && list -> size > 0 ); list -> size -- ; for ( int i = position ; i < size ; i ++ ) { list -> array [ i ] = list -> array [ i + 1 ]; } } Pros \u00b6 Finding takes \\(O(1)\\) time. Cons \u00b6 Maximum size is limited, or overestimated. Insertion and deletion take \\(O(N)\\) time with plenty of data movements. Linked List \u00b6 We say it a linked storage structure . Data Type typedef int ElementType ; typedef struct _node { ElementType element ; struct _node * next ; } Node ; typedef struct { Node * head ; int capacity ; int size ; } List ; Linked List without Sentinel List * CreateList ( const int capacity ) { List * list = ( List * ) malloc ( sizeof ( List )); list -> capacity = capacity ; list -> size = 0 ; list -> head = NULL ; return list ; } void FreeList ( List * list ) { for ( Node * p = list -> head , * temp = NULL ; p != NULL ;) { temp = p ; p = p -> next ; free ( temp ); } free ( list ); } void PrintList ( const List * list ) { for ( Node * p = list -> head ; p != NULL ; p = p -> next ) { printf ( \"%d \" , p -> element ); } putchar ( '\\n' ); } int GetListLength ( const List * list ) { return list -> size ; } bool IsEmptyList ( const List * list ) { return list -> size == 0 ; } ElementType Find ( const List * list , const int position ) { Node * p = list -> head ; for ( int i = 0 ; i < position ; i ++ , p = p -> next ); return p -> element ; } void Insert ( List * list , const int position , const ElementType element ) { assert ( 0 < position && position <= list -> size && ( list -> capacity == -1 || list -> size < list -> capacity )); Node * p = ( Node * ) malloc ( sizeof ( Node )); p -> element = element ; p -> next = NULL ; list -> size ++ ; if ( position == 0 ) { p -> next = list -> head ; list -> head = p ; } else { Node * q = list -> head ; for ( int i = 0 ; i < position - 1 ; i ++ , q = q -> next ); p -> next = q -> next ; q -> next = p ; } } void Delete ( List * list , const int position ) { assert ( 0 <= position && postion < list -> size && list -> size > 0 ); list -> size -- ; Node * temp = NULL ; if ( position == 0 ) { temp = list -> head ; list -> head = list -> head -> next ; free ( temp ); } else { Node * q = list -> head ; for ( int i = 0 ; i < position - 1 ; i ++ , q = q -> next ); temp = q -> next ; q -> next = temp -> next ; free ( temp ); } } From the implementation, we find it complicated for the special judge of position = 0 . Thus, we add a dummy node , or say sentinel (\u54e8\u5175) to simplify the implementation. Linked List with Sentinel List * CreateList ( const int capacity ) { List * list = ( List * ) malloc ( sizeof ( List )); list -> capacity = capacity ; list -> size = 0 ; list -> head = ( Node * ) malloc ( sizeof ( Node )); // (1)! list -> head -> element = 0 ; list -> head -> next = NULL ; return list ; } void FreeList ( List * list ) { for ( Node * p = list -> head -> next , * temp = NULL ; p != NULL ;) { temp = p ; p = p -> next ; free ( temp ); } free ( list -> head ); free ( list ); } void PrintList ( const List * list ) { for ( Node * p = list -> head -> next ; p != NULL ; p = p -> next ) { printf ( \"%d \" , p -> element ); } putchar ( '\\n' ); } int GetListLength ( const List * list ) { return list -> size ; } bool IsEmptyList ( const List * list ) { return list -> size == 0 ; } ElementType Find ( const List * list , const int position ) { Node * p = list -> head -> next ; for ( int i = 0 ; i < position ; i ++ , p = p -> next ); return p -> element ; } void Insert ( List * list , const int position , const ElementType element ) { assert ( 0 < position && position <= list -> size && ( list -> capacity == -1 || list -> size < list -> capacity )); Node * p = ( Node * ) malloc ( sizeof ( Node )); p -> element = element ; p -> next = NULL ; list -> size ++ ; Node * q = list -> head ; for ( int i = 0 ; i < position ; i ++ , q = q -> next ); p -> next = q -> next ; q -> next = p ; } void Delete ( List * list , const int position ) { assert ( 0 <= position && postion < list -> size && list -> size > 0 ); list -> size -- ; Node * temp = NULL ; Node * q = list -> head ; for ( int i = 0 ; i < position ; i ++ , q = q -> next ); temp = q -> next ; q -> next = temp -> next ; free ( temp ); } Dummy Node, or say Sentinel \u54e8\u5175 Pros \u00b6 No limited size (In the implementation above, capacity = -1 means unlimited capacity). Insertion and deletion take \\(O(1)\\) time. Cons \u00b6 Finding takes \\(O(N)\\) time. Doubly Linked List \u00b6 Data Type typedef int ElementType ; typedef struct _node { ElementType element ; struct _node * next ; struct _node * prev ; } Node ; typedef struct { Node * head ; int capacity ; int size ; } List ; Also, it's reasonable to add a field Node *tail to List and maintain tail for some convenient usage. Doubly Linked List with Sentinel List * CreateList ( const int capacity ) { List * list = ( List * ) malloc ( sizeof ( List )); list -> capacity = capacity ; list -> size = 0 ; list -> head = ( Node * ) malloc ( sizeof ( Node )); list -> head -> element = 0 ; list -> head -> next = NULL ; list -> head -> prev = NULL ; return list ; } void FreeList ( List * list ) { for ( Node * p = list -> head -> next , * temp = NULL ; p != NULL ;) { temp = p ; p = p -> next ; free ( temp ); } free ( list -> head ); free ( list ); } void PrintList ( const List * list ) { for ( Node * p = list -> head -> next ; p != NULL ; p = p -> next ) { printf ( \"%d \" , p -> element ); } putchar ( '\\n' ); } int GetListLength ( const List * list ) { return list -> size ; } bool IsEmptyList ( const List * list ) { return list -> size == 0 ; } ElementType Find ( const List * list , const int position ) { Node * p = list -> head -> next ; for ( int i = 0 ; i < position ; i ++ , p = p -> next ); return p -> element ; } void Insert ( List * list , const int position , const ElementType element ) { assert ( 0 < position && position <= list -> size && ( list -> capacity == -1 || list -> size < list -> capacity )); Node * p = ( Node * ) malloc ( sizeof ( Node )); p -> element = element ; p -> next = p -> prev = NULL ; list -> size ++ ; Node * q = list -> head ; for ( int i = 0 ; i < position ; i ++ , q = q -> next ); p -> next = q -> next ; q -> next = p ; q -> next -> prev = q ; p -> next -> prev = p ; } void Delete ( List * list , const int position ) { assert ( 0 <= position && postion < list -> size && list -> size > 0 ); list -> size -- ; Node * temp = NULL ; Node * q = list -> head ; for ( int i = 0 ; i < position ; i ++ , q = q -> next ); temp = q -> next ; q -> next = temp -> next ; temp -> next -> prev = q ; free ( temp ); } Circularly Linked List \u00b6 If there is a sentinel, then the next node of the last node is the sentinel. And Since it's circular, we can define position iteratively and take the mod of position as its actual pos . int pos = (( position % list -> size ) + list -> size ) % list -> size ; Circularly Doubly Linked List with Sentinel List * CreateList ( const int capacity ) { List * list = ( List * ) malloc ( sizeof ( List )); list -> capacity = capacity ; list -> size = 0 ; list -> head = ( Node * ) malloc ( sizeof ( Node )); list -> head -> element = 0 ; list -> head -> next = list -> head ; list -> head -> prev = list -> head ; return list ; } void FreeList ( List * list ) { for ( Node * p = list -> head -> next , * temp = NULL ; p != list -> head ;) { temp = p ; p = p -> next ; free ( temp ); } free ( list -> head ); free ( list ); } void PrintList ( const List * list ) { for ( Node * p = list -> head -> next ; p != list -> head ; p = p -> next ) { printf ( \"%d \" , p -> element ); } putchar ( '\\n' ); } int GetListLength ( const List * list ) { return list -> size ; } bool IsEmptyList ( const List * list ) { return list -> size == 0 ; } ElementType Find ( const List * list , const int position ) { Node * p = list -> head -> next ; int pos = (( position % list -> size ) + list -> size ) % list -> size ; for ( int i = 0 ; i < pos ; i ++ , p = p -> next ); return p -> element ; } void Insert ( List * list , const int position , const ElementType element ) { assert ( 0 < position && position <= list -> size && ( list -> capacity == -1 || list -> size < list -> capacity )); Node * p = ( Node * ) malloc ( sizeof ( Node )); p -> element = element ; p -> next = p -> prev = NULL ; list -> size ++ ; int pos = (( position % list -> size ) + list -> size ) % list -> size ; Node * q = list -> head ; for ( int i = 0 ; i < pos ; i ++ , q = q -> next ); p -> next = q -> next ; q -> next = p ; q -> next -> prev = q ; p -> next -> prev = p ; } void Delete ( List * list , const int position ) { assert ( list -> size > 0 ); int pos = (( position % list -> size ) + list -> size ) % list -> size ; list -> size -- ; Node * temp = NULL ; Node * q = list -> head ; for ( int i = 0 ; i < pos ; i ++ , q = q -> next ); temp = q -> next ; q -> next = temp -> next ; temp -> next -> prev = q ; free ( temp ); } Note Considering whether there is the field tail , whether there is a sentinel , or two sentinels (one for head and one for tail ), whether it's doubly linked, whether it's circularly linked, we have numerous implementations of various linked lists. Example: Polynomial ADT Objects: \\(P(x) = a_1x^{e_1} + \\dots + a_nx^{e_n}\\) Operations: Find degree. Addition, Subtraction, Multiplication, Differentiation. Example: Multilists / Sparse Matrix Representation Suppose a university with 40000 students and 2500 courses. We want to print the students' name list for each course and the registered classes list for each student. If we want the students' name of course \\(C3\\) , we start from node \\(C3\\) and move to the right . For each node, we sub-move another pointer circularly up and down to find \\(S1\\) , which is the student's name of this node. Static List \u00b6 Or called Cursor Implementation of Linked Lists without pointer . It's an alternative method to implement linked list in the cases that some languages don't support pointers . Reference: \u9759\u6001\u94fe\u8868\u53ca\u5b9e\u73b0\uff08C\u8bed\u8a00\uff09\u8be6\u89e3 . Simply put, we need an array to maintain TWO linked lists, one for data, the other for the unused space. The head of the unused space list is commonly at position 0 . Data Type #define MAXN 1000 typedef int ElementType ; typedef struct { ElementType element ; int next ; } Node ; typedef struct { Node MEM_SPACE [ MAXN ]; int head ; int size ; } List ; Also, we can consider the sentinel, the tail, and the double and circular property with different data type defined. The following is an implementation of static list with sentinel. Static List with Sentinel First, we should define our own malloc and free function. int MemAlloc ( List * list ) { int pos = list -> MEM_SPACE [ 0 ]. next ; list -> MEM_SPACE [ 0 ]. next = list -> MEM_SPACE [ pos ]. next ; return pos ; } void MemFree ( List * list , const int pos ) { list -> MEM_SPACE [ pos ]. next = list -> MEM_SPACE [ 0 ]. next ; list -> MEM_SPACE [ 0 ]. next = pos ; } The rest are quite similar, we mainly make the following modification. From x->element to list->MEM_SPACE[x].element ; From x->next to list->MEM_SPACE[x].next ; List * CreateList () { List * list = ( List * ) malloc ( sizeof ( List )); // (1)! for ( int i = 0 ; i < MAXN - 1 ; i ++ ) { list -> MEM_SPACE [ i ]. next = i + 1 ; } list -> MEM_SPACE [ MAXN - 1 ]. next = 0 ; list -> size = 0 ; list -> head = MemAlloc ( list ); list -> MEM_SPACE [ list -> head ]. element = 0 ; list -> MEM_SPACE [ list -> head ]. next = 0 ; return list ; } void FreeList ( List * list ) { free ( list ); } void PrintList ( const List * list ) { for ( int p = list -> MEM_SPACE [ list -> head ]. next ; p != 0 ; p = list -> MEM_SPACE [ p ]. next ) { printf ( \"%d \" , list -> MEM_SPACE [ p ]. element ); } putchar ( '\\n' ); } int GetListLength ( const List * list ) { return list -> size ; } bool IsEmptyList ( const List * list ) { return list -> size == 0 ; } ElementType Find ( const List * list , const int position ) { int p = list -> MEM_SPACE [ list -> head ]. next ; for ( int i = 0 ; i < position ; i ++ , p = list -> MEM_SPACE [ p ]. next ); return list -> MEM_SPACE [ p ]. element ; } void Insert ( List * list , const int position , const ElementType element ) { assert ( 0 < position && position <= list -> size && ( list -> capacity == -1 || list -> size < list -> capacity )); int p = MemAlloc ( list ); list -> MEM_SPACE [ p ]. element = element ; list -> MEM_SPACE [ p ]. next = 0 ; list -> size ++ ; int q = list -> head ; for ( int i = 0 ; i < position ; i ++ , q = list -> MEM_SPACE [ q ]. next ); list -> MEM_SPACE [ p ]. next = list -> MEM_SPACE [ q ]. next ; list -> MEM_SPACE [ q ]. next = p ; } void Delete ( List * list , const int position ) { assert ( 0 <= postion && position < list -> size && list -> size > 0 ); list -> size -- ; int temp = 0 ; int q = list -> head ; for ( int i = 0 ; i < position ; i ++ , q = list -> MEM_SPACE [ q ]. next ); temp = list -> MEM_SPACE [ q ]. next ; list -> MEM_SPACE [ q ]. next = list -> MEM_SPACE [ temp ]. next ; MemFree ( list , temp ); } For tidiness and simplicty, I still use malloc here, but in actual cases, all fields of list are often used as a global variable and list will not be a pointer. Stack \u00b6 A stack is a Last-In-First-Out (LIFO) list. ADT Objects : (item 0 , item 1 , ..., item N - 1 ) Operations Make an empty stack. Print all the items in a stack. Get the depth of a stack. Find the top item of a stack. Push a new item onto a stack. Pop an item from a stack. Data Type typedef int ElementType ; typedef struct _node { ElementType element ; struct _node * next ; } Node ; typedef struct { Node * top ; int capacity ; int size ; } Stack ; Similarly, we can use array and linked list to implement a stack. The following is the linked list implementation. NOTE: There is convention of Pop function that Pop will not only delete the element on the top, but return the deleted element. Stack Stack * CreateStack ( const int capacity ) { Stack * stack = ( Stack * ) malloc ( sizeof ( Stack )); stack -> capacity = capacity ; stack -> size = 0 ; stack -> top = NULL ; return stack ; } void FreeStack ( Stack * stack ) { for ( Node * p = stack -> top , * temp = NULL ; p != NULL ;) { temp = p ; p = p -> next ; free ( p ); } free ( stack ); } void PrintStack ( const Stack * stack ) { for ( Node * p = stack -> top ; p != NULL ; p = p -> next ) { printf ( \"%d \" , p -> element ); } putchar ( '\\n' ); } int GetStackDepth ( const Stack * stack ) { return stack -> size ; } bool IsEmptyStack ( const Stack * stack ) { return stack -> size == 0 ; } ElementType Top ( const Stack * stack ) { return stack -> top -> element ; } void Push ( Stack * stack , ElementType element ) { if ( stack -> capacity != -1 && stack -> size == stack -> capacity ) { // Warning return ; } stack -> size ++ ; Node * p = ( Node * ) malloc ( sizeof ( Node )); p -> element = element ; p -> next = stack -> top ; stack -> top = p ; } ElementType Pop ( Stack * stack ) { if ( stack -> size == 0 ) { // Warning return -1 ; } stack -> size -- ; Node * temp = stack -> top ; stack -> top = temp -> next ; ElementType element = temp -> element ; free ( temp ); return element ; } Queue \u00b6 A queue is a First-In-First-Out (FIFO) list. ADT Objects : (item 0 , item 1 , ..., item N - 1 ) Operations Make an empty queue. Print all the items in a queue. Get the length of a queue. Find the front item of a queue. Enqueue a new item to a queue. Dequeue an item from a queue. Data Type typedef int ElementType ; typedef struct _node { ElementType element ; struct _node * next ; } Node ; typedef struct { Node * front ; Node * rear ; int capacity ; int size ; } Queue ; Similarly, we can use array and linked list to implement a queue. The following is the linked list implementation. NOTE: There is convention of Dequeue function that Dequeue will not only delete the element in the front of the queue, but return the deleted element. Queue Queue * CreateQueue ( const int capacity ) { Queue * queue = ( Queue * ) malloc ( sizeof ( Queue )); queue -> capacity = capacity ; queue -> size = 0 ; queue -> front = ( Node * ) malloc ( sizeof ( Node )); queue -> front -> element = 0 ; queue -> front -> next = NULL ; queue -> rear = queue -> front ; return queue ; } void FreeQueue ( Queue * queue ) { for ( Node * p = queue -> front -> next , * temp = NULL ; p != NULL ;) { temp = p ; p = p -> next ; free ( p ); } free ( queue ); } void PrintQueue ( const Queue * queue ) { for ( Node * p = queue -> front -> next ; p != NULL ; p = p -> next ) { printf ( \"%d \" , p -> element ); } putchar ( '\\n' ); } int GetQueueLength ( const Queue * queue ) { return queue -> size ; } bool IsEmptyQueue ( const Queue * queue ) { return queue -> size == 0 ; } ElementType Front ( const Queue * queue ) { return queue -> front -> next -> element ; } void Enqueue ( Queue * queue , ElementType element ) { if ( queue -> capacity != -1 && queue -> size == queue -> capacity ) { // Warning return ; } queue -> size ++ ; Node * p = ( Node * ) malloc ( sizeof ( Node )); p -> element = element ; p -> next = NULL ; queue -> rear -> next = p ; queue -> rear = p ; } ElementType Dequeue ( Queue * queue ) { if ( queue -> size == 0 ) { // Warning return -1 ; } queue -> size -- ; if ( queue -> size == 0 ) { queue -> rear = queue -> front ; } Node * temp = queue -> front -> next ; queue -> front -> next = temp -> next ; ElementType element = temp -> element ; free ( temp ); return element ; } Other Queues \u00b6 Deque \u00b6 A deque is a queue that we can enqueue and dequeue from both sides. The interfaces are like below. ElementType Front ( const Deque dequue ); ElementType Back ( const Deque dequue ); void PushFront ( Deque dequue , const ElementType element ); void PushBack ( Deque dequue , const ElementType element ); ElementType PopFront ( Deque dequue ); ElementType PopBack ( Deque dequue ); Circular Queue \u00b6 A circular queue is a queue that store elements circularly. It can be seen in the array implementation of queue. To distinguish whether is empty or full, we stipulate that when it's empty, it has \\(N - 1\\) elements. Then If a queue is empty, then front = rear . If a queue is full, then front = (rear + 1) % N .","title":"Lists, Stacks and Queues"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#lists-stacks-and-queues","text":"Definition Data type consists of objects and operations . An Abstract Data Type (ADT) is a data type that is organized in such a way that the specification on the objects are separated from the representation of the objects. the specification of the operations on the objects are separated from the implementation on the operations","title":"Lists, Stacks and Queues"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#list-linear-list","text":"ADT Objects : (item 0 , item 1 , ..., item N - 1 ) Operations Make an empty list. Check whether it's an empty list. Print all the items in a list. Get the length of a list. Find the k -th item from a list. Insert a new item at position k of a list. Delete an item at position k from a list. Find the next item of the current item from a list. Find the previous item of the current item from a list. Following are some implementations of list.","title":"List (Linear List)"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#array-sequential-list","text":"We say it a sequential mapping or sequantial storage structure . Data Type typedef int ElementType ; typedef struct { ElementType * array ; int capacity ; int size ; } List ; Array / Sequential List List * CreateList ( const int capacity ) { List * list = ( List * ) malloc ( sizeof ( List )); list -> capacity = capacity ; list -> size = 0 ; list -> array = ( ElementType * ) malloc ( list -> capacity * sizeof ( ElementType )); return list ; } void FreeList ( List * list ) { free ( list -> array ); free ( list ); } void PrintList ( const List * list ) { for ( int i = 0 ; i < list -> size ; i ++ ) { printf ( \"%d \" , list -> array [ i ]); } putchar ( '\\n' ); } int GetListLength ( const List * list ) { return list -> size ; } bool IsEmptyList ( const List * list ) { return list -> size == 0 ; } ElementType Find ( const List * list , const int position ) { return list -> array [ position ]; } void Insert ( List * list , const int position , const ElementType element ) { assert ( 0 < position && position <= list -> size && ( list -> capacity == -1 || list -> size < list -> capacity )); list -> size ++ ; for ( int i = list -> size ; i > position ; i -- ) { list -> array [ i ] = list -> array [ i - 1 ]; } list -> array [ position ] = element ; } void Delete ( List * list , const int position ) { assert ( 0 <= position && postion < list -> size && list -> size > 0 ); list -> size -- ; for ( int i = position ; i < size ; i ++ ) { list -> array [ i ] = list -> array [ i + 1 ]; } }","title":"Array / Sequential List"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#pros","text":"Finding takes \\(O(1)\\) time.","title":"Pros"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#cons","text":"Maximum size is limited, or overestimated. Insertion and deletion take \\(O(N)\\) time with plenty of data movements.","title":"Cons"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#linked-list","text":"We say it a linked storage structure . Data Type typedef int ElementType ; typedef struct _node { ElementType element ; struct _node * next ; } Node ; typedef struct { Node * head ; int capacity ; int size ; } List ; Linked List without Sentinel List * CreateList ( const int capacity ) { List * list = ( List * ) malloc ( sizeof ( List )); list -> capacity = capacity ; list -> size = 0 ; list -> head = NULL ; return list ; } void FreeList ( List * list ) { for ( Node * p = list -> head , * temp = NULL ; p != NULL ;) { temp = p ; p = p -> next ; free ( temp ); } free ( list ); } void PrintList ( const List * list ) { for ( Node * p = list -> head ; p != NULL ; p = p -> next ) { printf ( \"%d \" , p -> element ); } putchar ( '\\n' ); } int GetListLength ( const List * list ) { return list -> size ; } bool IsEmptyList ( const List * list ) { return list -> size == 0 ; } ElementType Find ( const List * list , const int position ) { Node * p = list -> head ; for ( int i = 0 ; i < position ; i ++ , p = p -> next ); return p -> element ; } void Insert ( List * list , const int position , const ElementType element ) { assert ( 0 < position && position <= list -> size && ( list -> capacity == -1 || list -> size < list -> capacity )); Node * p = ( Node * ) malloc ( sizeof ( Node )); p -> element = element ; p -> next = NULL ; list -> size ++ ; if ( position == 0 ) { p -> next = list -> head ; list -> head = p ; } else { Node * q = list -> head ; for ( int i = 0 ; i < position - 1 ; i ++ , q = q -> next ); p -> next = q -> next ; q -> next = p ; } } void Delete ( List * list , const int position ) { assert ( 0 <= position && postion < list -> size && list -> size > 0 ); list -> size -- ; Node * temp = NULL ; if ( position == 0 ) { temp = list -> head ; list -> head = list -> head -> next ; free ( temp ); } else { Node * q = list -> head ; for ( int i = 0 ; i < position - 1 ; i ++ , q = q -> next ); temp = q -> next ; q -> next = temp -> next ; free ( temp ); } } From the implementation, we find it complicated for the special judge of position = 0 . Thus, we add a dummy node , or say sentinel (\u54e8\u5175) to simplify the implementation. Linked List with Sentinel List * CreateList ( const int capacity ) { List * list = ( List * ) malloc ( sizeof ( List )); list -> capacity = capacity ; list -> size = 0 ; list -> head = ( Node * ) malloc ( sizeof ( Node )); // (1)! list -> head -> element = 0 ; list -> head -> next = NULL ; return list ; } void FreeList ( List * list ) { for ( Node * p = list -> head -> next , * temp = NULL ; p != NULL ;) { temp = p ; p = p -> next ; free ( temp ); } free ( list -> head ); free ( list ); } void PrintList ( const List * list ) { for ( Node * p = list -> head -> next ; p != NULL ; p = p -> next ) { printf ( \"%d \" , p -> element ); } putchar ( '\\n' ); } int GetListLength ( const List * list ) { return list -> size ; } bool IsEmptyList ( const List * list ) { return list -> size == 0 ; } ElementType Find ( const List * list , const int position ) { Node * p = list -> head -> next ; for ( int i = 0 ; i < position ; i ++ , p = p -> next ); return p -> element ; } void Insert ( List * list , const int position , const ElementType element ) { assert ( 0 < position && position <= list -> size && ( list -> capacity == -1 || list -> size < list -> capacity )); Node * p = ( Node * ) malloc ( sizeof ( Node )); p -> element = element ; p -> next = NULL ; list -> size ++ ; Node * q = list -> head ; for ( int i = 0 ; i < position ; i ++ , q = q -> next ); p -> next = q -> next ; q -> next = p ; } void Delete ( List * list , const int position ) { assert ( 0 <= position && postion < list -> size && list -> size > 0 ); list -> size -- ; Node * temp = NULL ; Node * q = list -> head ; for ( int i = 0 ; i < position ; i ++ , q = q -> next ); temp = q -> next ; q -> next = temp -> next ; free ( temp ); } Dummy Node, or say Sentinel \u54e8\u5175","title":"Linked List"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#pros_1","text":"No limited size (In the implementation above, capacity = -1 means unlimited capacity). Insertion and deletion take \\(O(1)\\) time.","title":"Pros"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#cons_1","text":"Finding takes \\(O(N)\\) time.","title":"Cons"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#doubly-linked-list","text":"Data Type typedef int ElementType ; typedef struct _node { ElementType element ; struct _node * next ; struct _node * prev ; } Node ; typedef struct { Node * head ; int capacity ; int size ; } List ; Also, it's reasonable to add a field Node *tail to List and maintain tail for some convenient usage. Doubly Linked List with Sentinel List * CreateList ( const int capacity ) { List * list = ( List * ) malloc ( sizeof ( List )); list -> capacity = capacity ; list -> size = 0 ; list -> head = ( Node * ) malloc ( sizeof ( Node )); list -> head -> element = 0 ; list -> head -> next = NULL ; list -> head -> prev = NULL ; return list ; } void FreeList ( List * list ) { for ( Node * p = list -> head -> next , * temp = NULL ; p != NULL ;) { temp = p ; p = p -> next ; free ( temp ); } free ( list -> head ); free ( list ); } void PrintList ( const List * list ) { for ( Node * p = list -> head -> next ; p != NULL ; p = p -> next ) { printf ( \"%d \" , p -> element ); } putchar ( '\\n' ); } int GetListLength ( const List * list ) { return list -> size ; } bool IsEmptyList ( const List * list ) { return list -> size == 0 ; } ElementType Find ( const List * list , const int position ) { Node * p = list -> head -> next ; for ( int i = 0 ; i < position ; i ++ , p = p -> next ); return p -> element ; } void Insert ( List * list , const int position , const ElementType element ) { assert ( 0 < position && position <= list -> size && ( list -> capacity == -1 || list -> size < list -> capacity )); Node * p = ( Node * ) malloc ( sizeof ( Node )); p -> element = element ; p -> next = p -> prev = NULL ; list -> size ++ ; Node * q = list -> head ; for ( int i = 0 ; i < position ; i ++ , q = q -> next ); p -> next = q -> next ; q -> next = p ; q -> next -> prev = q ; p -> next -> prev = p ; } void Delete ( List * list , const int position ) { assert ( 0 <= position && postion < list -> size && list -> size > 0 ); list -> size -- ; Node * temp = NULL ; Node * q = list -> head ; for ( int i = 0 ; i < position ; i ++ , q = q -> next ); temp = q -> next ; q -> next = temp -> next ; temp -> next -> prev = q ; free ( temp ); }","title":"Doubly Linked List"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#circularly-linked-list","text":"If there is a sentinel, then the next node of the last node is the sentinel. And Since it's circular, we can define position iteratively and take the mod of position as its actual pos . int pos = (( position % list -> size ) + list -> size ) % list -> size ; Circularly Doubly Linked List with Sentinel List * CreateList ( const int capacity ) { List * list = ( List * ) malloc ( sizeof ( List )); list -> capacity = capacity ; list -> size = 0 ; list -> head = ( Node * ) malloc ( sizeof ( Node )); list -> head -> element = 0 ; list -> head -> next = list -> head ; list -> head -> prev = list -> head ; return list ; } void FreeList ( List * list ) { for ( Node * p = list -> head -> next , * temp = NULL ; p != list -> head ;) { temp = p ; p = p -> next ; free ( temp ); } free ( list -> head ); free ( list ); } void PrintList ( const List * list ) { for ( Node * p = list -> head -> next ; p != list -> head ; p = p -> next ) { printf ( \"%d \" , p -> element ); } putchar ( '\\n' ); } int GetListLength ( const List * list ) { return list -> size ; } bool IsEmptyList ( const List * list ) { return list -> size == 0 ; } ElementType Find ( const List * list , const int position ) { Node * p = list -> head -> next ; int pos = (( position % list -> size ) + list -> size ) % list -> size ; for ( int i = 0 ; i < pos ; i ++ , p = p -> next ); return p -> element ; } void Insert ( List * list , const int position , const ElementType element ) { assert ( 0 < position && position <= list -> size && ( list -> capacity == -1 || list -> size < list -> capacity )); Node * p = ( Node * ) malloc ( sizeof ( Node )); p -> element = element ; p -> next = p -> prev = NULL ; list -> size ++ ; int pos = (( position % list -> size ) + list -> size ) % list -> size ; Node * q = list -> head ; for ( int i = 0 ; i < pos ; i ++ , q = q -> next ); p -> next = q -> next ; q -> next = p ; q -> next -> prev = q ; p -> next -> prev = p ; } void Delete ( List * list , const int position ) { assert ( list -> size > 0 ); int pos = (( position % list -> size ) + list -> size ) % list -> size ; list -> size -- ; Node * temp = NULL ; Node * q = list -> head ; for ( int i = 0 ; i < pos ; i ++ , q = q -> next ); temp = q -> next ; q -> next = temp -> next ; temp -> next -> prev = q ; free ( temp ); } Note Considering whether there is the field tail , whether there is a sentinel , or two sentinels (one for head and one for tail ), whether it's doubly linked, whether it's circularly linked, we have numerous implementations of various linked lists. Example: Polynomial ADT Objects: \\(P(x) = a_1x^{e_1} + \\dots + a_nx^{e_n}\\) Operations: Find degree. Addition, Subtraction, Multiplication, Differentiation. Example: Multilists / Sparse Matrix Representation Suppose a university with 40000 students and 2500 courses. We want to print the students' name list for each course and the registered classes list for each student. If we want the students' name of course \\(C3\\) , we start from node \\(C3\\) and move to the right . For each node, we sub-move another pointer circularly up and down to find \\(S1\\) , which is the student's name of this node.","title":"Circularly Linked List"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#static-list","text":"Or called Cursor Implementation of Linked Lists without pointer . It's an alternative method to implement linked list in the cases that some languages don't support pointers . Reference: \u9759\u6001\u94fe\u8868\u53ca\u5b9e\u73b0\uff08C\u8bed\u8a00\uff09\u8be6\u89e3 . Simply put, we need an array to maintain TWO linked lists, one for data, the other for the unused space. The head of the unused space list is commonly at position 0 . Data Type #define MAXN 1000 typedef int ElementType ; typedef struct { ElementType element ; int next ; } Node ; typedef struct { Node MEM_SPACE [ MAXN ]; int head ; int size ; } List ; Also, we can consider the sentinel, the tail, and the double and circular property with different data type defined. The following is an implementation of static list with sentinel. Static List with Sentinel First, we should define our own malloc and free function. int MemAlloc ( List * list ) { int pos = list -> MEM_SPACE [ 0 ]. next ; list -> MEM_SPACE [ 0 ]. next = list -> MEM_SPACE [ pos ]. next ; return pos ; } void MemFree ( List * list , const int pos ) { list -> MEM_SPACE [ pos ]. next = list -> MEM_SPACE [ 0 ]. next ; list -> MEM_SPACE [ 0 ]. next = pos ; } The rest are quite similar, we mainly make the following modification. From x->element to list->MEM_SPACE[x].element ; From x->next to list->MEM_SPACE[x].next ; List * CreateList () { List * list = ( List * ) malloc ( sizeof ( List )); // (1)! for ( int i = 0 ; i < MAXN - 1 ; i ++ ) { list -> MEM_SPACE [ i ]. next = i + 1 ; } list -> MEM_SPACE [ MAXN - 1 ]. next = 0 ; list -> size = 0 ; list -> head = MemAlloc ( list ); list -> MEM_SPACE [ list -> head ]. element = 0 ; list -> MEM_SPACE [ list -> head ]. next = 0 ; return list ; } void FreeList ( List * list ) { free ( list ); } void PrintList ( const List * list ) { for ( int p = list -> MEM_SPACE [ list -> head ]. next ; p != 0 ; p = list -> MEM_SPACE [ p ]. next ) { printf ( \"%d \" , list -> MEM_SPACE [ p ]. element ); } putchar ( '\\n' ); } int GetListLength ( const List * list ) { return list -> size ; } bool IsEmptyList ( const List * list ) { return list -> size == 0 ; } ElementType Find ( const List * list , const int position ) { int p = list -> MEM_SPACE [ list -> head ]. next ; for ( int i = 0 ; i < position ; i ++ , p = list -> MEM_SPACE [ p ]. next ); return list -> MEM_SPACE [ p ]. element ; } void Insert ( List * list , const int position , const ElementType element ) { assert ( 0 < position && position <= list -> size && ( list -> capacity == -1 || list -> size < list -> capacity )); int p = MemAlloc ( list ); list -> MEM_SPACE [ p ]. element = element ; list -> MEM_SPACE [ p ]. next = 0 ; list -> size ++ ; int q = list -> head ; for ( int i = 0 ; i < position ; i ++ , q = list -> MEM_SPACE [ q ]. next ); list -> MEM_SPACE [ p ]. next = list -> MEM_SPACE [ q ]. next ; list -> MEM_SPACE [ q ]. next = p ; } void Delete ( List * list , const int position ) { assert ( 0 <= postion && position < list -> size && list -> size > 0 ); list -> size -- ; int temp = 0 ; int q = list -> head ; for ( int i = 0 ; i < position ; i ++ , q = list -> MEM_SPACE [ q ]. next ); temp = list -> MEM_SPACE [ q ]. next ; list -> MEM_SPACE [ q ]. next = list -> MEM_SPACE [ temp ]. next ; MemFree ( list , temp ); } For tidiness and simplicty, I still use malloc here, but in actual cases, all fields of list are often used as a global variable and list will not be a pointer.","title":"Static List"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#stack","text":"A stack is a Last-In-First-Out (LIFO) list. ADT Objects : (item 0 , item 1 , ..., item N - 1 ) Operations Make an empty stack. Print all the items in a stack. Get the depth of a stack. Find the top item of a stack. Push a new item onto a stack. Pop an item from a stack. Data Type typedef int ElementType ; typedef struct _node { ElementType element ; struct _node * next ; } Node ; typedef struct { Node * top ; int capacity ; int size ; } Stack ; Similarly, we can use array and linked list to implement a stack. The following is the linked list implementation. NOTE: There is convention of Pop function that Pop will not only delete the element on the top, but return the deleted element. Stack Stack * CreateStack ( const int capacity ) { Stack * stack = ( Stack * ) malloc ( sizeof ( Stack )); stack -> capacity = capacity ; stack -> size = 0 ; stack -> top = NULL ; return stack ; } void FreeStack ( Stack * stack ) { for ( Node * p = stack -> top , * temp = NULL ; p != NULL ;) { temp = p ; p = p -> next ; free ( p ); } free ( stack ); } void PrintStack ( const Stack * stack ) { for ( Node * p = stack -> top ; p != NULL ; p = p -> next ) { printf ( \"%d \" , p -> element ); } putchar ( '\\n' ); } int GetStackDepth ( const Stack * stack ) { return stack -> size ; } bool IsEmptyStack ( const Stack * stack ) { return stack -> size == 0 ; } ElementType Top ( const Stack * stack ) { return stack -> top -> element ; } void Push ( Stack * stack , ElementType element ) { if ( stack -> capacity != -1 && stack -> size == stack -> capacity ) { // Warning return ; } stack -> size ++ ; Node * p = ( Node * ) malloc ( sizeof ( Node )); p -> element = element ; p -> next = stack -> top ; stack -> top = p ; } ElementType Pop ( Stack * stack ) { if ( stack -> size == 0 ) { // Warning return -1 ; } stack -> size -- ; Node * temp = stack -> top ; stack -> top = temp -> next ; ElementType element = temp -> element ; free ( temp ); return element ; }","title":"Stack"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#queue","text":"A queue is a First-In-First-Out (FIFO) list. ADT Objects : (item 0 , item 1 , ..., item N - 1 ) Operations Make an empty queue. Print all the items in a queue. Get the length of a queue. Find the front item of a queue. Enqueue a new item to a queue. Dequeue an item from a queue. Data Type typedef int ElementType ; typedef struct _node { ElementType element ; struct _node * next ; } Node ; typedef struct { Node * front ; Node * rear ; int capacity ; int size ; } Queue ; Similarly, we can use array and linked list to implement a queue. The following is the linked list implementation. NOTE: There is convention of Dequeue function that Dequeue will not only delete the element in the front of the queue, but return the deleted element. Queue Queue * CreateQueue ( const int capacity ) { Queue * queue = ( Queue * ) malloc ( sizeof ( Queue )); queue -> capacity = capacity ; queue -> size = 0 ; queue -> front = ( Node * ) malloc ( sizeof ( Node )); queue -> front -> element = 0 ; queue -> front -> next = NULL ; queue -> rear = queue -> front ; return queue ; } void FreeQueue ( Queue * queue ) { for ( Node * p = queue -> front -> next , * temp = NULL ; p != NULL ;) { temp = p ; p = p -> next ; free ( p ); } free ( queue ); } void PrintQueue ( const Queue * queue ) { for ( Node * p = queue -> front -> next ; p != NULL ; p = p -> next ) { printf ( \"%d \" , p -> element ); } putchar ( '\\n' ); } int GetQueueLength ( const Queue * queue ) { return queue -> size ; } bool IsEmptyQueue ( const Queue * queue ) { return queue -> size == 0 ; } ElementType Front ( const Queue * queue ) { return queue -> front -> next -> element ; } void Enqueue ( Queue * queue , ElementType element ) { if ( queue -> capacity != -1 && queue -> size == queue -> capacity ) { // Warning return ; } queue -> size ++ ; Node * p = ( Node * ) malloc ( sizeof ( Node )); p -> element = element ; p -> next = NULL ; queue -> rear -> next = p ; queue -> rear = p ; } ElementType Dequeue ( Queue * queue ) { if ( queue -> size == 0 ) { // Warning return -1 ; } queue -> size -- ; if ( queue -> size == 0 ) { queue -> rear = queue -> front ; } Node * temp = queue -> front -> next ; queue -> front -> next = temp -> next ; ElementType element = temp -> element ; free ( temp ); return element ; }","title":"Queue"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#other-queues","text":"","title":"Other Queues"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#deque","text":"A deque is a queue that we can enqueue and dequeue from both sides. The interfaces are like below. ElementType Front ( const Deque dequue ); ElementType Back ( const Deque dequue ); void PushFront ( Deque dequue , const ElementType element ); void PushBack ( Deque dequue , const ElementType element ); ElementType PopFront ( Deque dequue ); ElementType PopBack ( Deque dequue );","title":"Deque"},{"location":"Computer_Science_Courses/FDS/Lists_Stacks_and_Queues/#circular-queue","text":"A circular queue is a queue that store elements circularly. It can be seen in the array implementation of queue. To distinguish whether is empty or full, we stipulate that when it's empty, it has \\(N - 1\\) elements. Then If a queue is empty, then front = rear . If a queue is full, then front = (rear + 1) % N .","title":"Circular Queue"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/","text":"Priority Queues \u00b6 Priority queue (PQ, pq) is a First-In-Largest-Out queue. The elements must be comparable , which enables priority. Each element is added to the rear of the queue, and the prior element is deleted in the front of the queue. The implementation of priority queue is Heap . ADT Objects: A finite ordered list with zero or more elements. Operations: Initialize a priority queue with a set of elements. Find the prior element of a priority queue. Insert a new element into a priority queue. Delete the prior element of a priority queue. Comparison Let's first consider some data structure to implement the operations we need above. Array Insertion (Add one element at the end) - \\(\\Theta(1)\\) . Deletion (Find the prior key, remove and shift array) - \\(\\Theta(n) + O(n)\\) Linked List Insertion (Add to the front of the list) - \\(\\Theta(1)\\) Deletion (Find the prior key and remove it) - \\(\\Theta(n) + \\Theta(1)\\) . Ordered Array Insertion (Find the proper position, shift array and add the element) - \\(O(n) + O(n)\\) Deletion (Remove the first element) - \\(\\Theta(1)\\) Ordered Linked List Insertion (Find the proper position and add the element) - \\(O(n) + \\Theta(1)\\) Deletion (Remove the first element) - \\(\\Theta(1)\\) BST / AVL tree Both Insertion and Deletion are \\(O(\\log n)\\) Many operations that we don't need Thus we modify it to a binary heap Binary Heap \u4e8c\u53c9\u5806 \u00b6 Definition A binary tree with \\(n\\) nodes and height \\(h\\) is called a complete binary tree iff its notes correspond to the nodes numbers from \\(1\\) to \\(n\\) (up to down and left to right) in a perfect binary tree of height \\(h\\) . Property A complete binary tree of height \\(h\\) has between \\(2^h\\) and \\(2^{h + 1} - 1\\) nodes, and thus \\[ h = \\lfloor \\log n \\rfloor. \\] A complete binary tree with \\(n\\) nodes can be represented by an array with \\(n + 1\\) items. (The zeroth is not used). The array is built by the levelorder of the tree. For any node with index \\(i\\) , we have \\[ \\begin{aligned} \\text{Parent}(i) &= \\left\\{ \\begin{aligned} & \\lfloor i / 2 \\rfloor, & i \\ne 1 \\\\ & \\text{None}, & i = 1. \\end{aligned} \\right. \\\\ \\text{LeftChild}(i) &= \\left\\{ \\begin{aligned} & 2i, & 2i \\le n \\\\ & \\text{None}, & 2i > n. \\end{aligned} \\right. \\\\ \\text{RightChild}(i) &= \\left\\{ \\begin{aligned} & 2i + 1, & 2i + 1 \\le n \\\\ & \\text{None}, & 2i + 1 > n. \\end{aligned} \\right. \\end{aligned} \\] Definition A min tree is a tree in which the key value in each node is no larger than the key values in its children (if any). A min heap is a complete binary tree that is also a min tree. A max tree is a tree in which the key value in each node is no smaller than the key values in its children (if any). A max heap is a complete binary tree that is also a max tree. Thus Heap consists of max heap and min heap . And a heap is a complete binary tree and a max / min tree. Implementation \u00b6 Data Type #define MinData -1 typedef int ElementType ; typedef struct { ElementType * element ; int capacity ; int size ; } PriorityQueue ; NOTE: The heap is stored in an array since it's a complete binary tree. And element[0] is a sentinel . If it's a min heap, then element[0] must store the smallest value MinData . Priority Queue static void SwapElement ( ElementType * a , ElementType * b ) { ElementType temp = * a ; * a = * b ; * b = temp ; } PriorityQueue * CreateQueue ( const int capacity ) { PriorityQueue * Q = ( PriorityQueue * ) malloc ( sizeof ( PriorityQueue )); Q -> capacity = capacity ; Q -> size = 0 ; Q -> element = ( ElementType * ) malloc (( Q -> capacity + 1 ) * sizeof ( ElementType )); Q -> element [ 0 ] = MinData ; // Sentinel return Q ; } bool IsEmptyQueue ( PriorityQueue * Q ) { return Q -> size == 0 ; } static void PercolateUp ( PriorityQueue * Q , int p ) { while ( p >> 1 > 0 ) { int parent = p >> 1 ; if ( Q -> element [ p ] < Q -> element [ parent ]) { SwapElement ( Q -> element + p , Q -> element + parent ); } else { break ; } p = parent ; } } static void PercolateDown ( PriorityQueue * Q , int p ) { while ( p << 1 <= Q -> size ) { int child = p << 1 ; if ( child != Q -> size && Q -> element [ child + 1 ] < Q -> element [ child ]) { child ++ ; } if ( Q -> element [ p ] > Q -> element [ child ]) { SwapElement ( Q -> element + p , Q -> element + child ); } else { break ; } p = child ; } } void Insert ( PriorityQueue * Q , ElementType x ) { int p = ++ Q -> size ; Q -> element [ p ] = x ; PercolateUp ( Q , p ); } ElementType DeleteMin ( PriorityQueue * Q ) { ElementType minElement = Q -> element [ 1 ]; Q -> element [ 1 ] = Q -> element [ Q -> size -- ]; PercolateDown ( Q , 1 ); return minElement ; } Time Complexity Time complexities of both Insert and DeleteMin are \\(O(\\log n)\\) . Other Heap Operations \u00b6 Build Heap \u00b6 Suppose now there is an array of data and we want to build a heap based on the data. Instead of creating an empty heap and inserting one by one, we have the following faster implementation. PriorityQueue * BuildHeap ( const int n , const ElementType a []) { PriorityQueue * Q = ( PriorityQueue * ) malloc ( sizeof ( PriorityQueue )); Q -> size = Q -> capacity = n ; Q -> element = ( ElementType * ) malloc (( Q -> size + 1 ) * sizeof ( ElementType )); memcpy ( Q -> element + 1 , a , Q -> size * sizeof ( ElementType )); int pos = 1 ; while ( pos << 1 < Q -> size ) { pos << = 1 ; } for ( int i = pos ; i >= 1 ; i -- ) { PercolateDown ( Q , i ); } return Q ; } Time Complexity Theorem For the perfect binary tree of heigh \\(h\\) containing \\(2^{h + 1} - 1\\) nodes, the sum of the heights of the node is \\(2^{h + 1} - 1 - (h + 1)\\) . Thus the time complexity of building a heap is \\(O(n)\\) , which is linear. Increase / Decrease Value \u00b6 Increase or decrease some value delta ( delta > 0 ) at position pos of a heap. void IncreaseValue ( Priority Queue * Q , const int pos , const ElementType delta ) { assert ( delta > 0 ); Q -> element [ pos ] += delta ; PercolateUp ( Q , pos ); } void DecreaseValue ( Priority Queue * Q , const int pos , const ElementType delta ) { assert ( delta > 0 ); Q -> element [ pos ] -= delta ; PercolateDown ( Q , pos ); } Delete \u00b6 Delete the element at position pos of a heap. We just decrease it to the minimum and then delete it. #define inf 2147483647 void Delete ( PriorityQueue * Q , const int pos ) { DecreaseValue ( Q , pos , inf ); DeleteMin ( Q ); } d-Heap \u00b6 A d-Heap is a heap like binary heap except that all nodes have \\(d\\) children. 3-Heap Note DeleteMin will take \\(d - 1\\) comparisons to find the smallest child and thus totally take \\(O(d \\log_d n)\\) time. Insert will take \\(O(\\log_d n)\\) time. If a d-heap is stored as an array, for an entry of index \\(i\\) , the parent is at \\(\\lfloor (i + d - 2) / d \\rfloor\\) . the first child is at \\((i - 1)d + 2\\) . the last child is at \\(id + 1\\) . Leftist Tree / Heap \u5de6\u504f\u6811 \u00b6 Remains Skew Heap \u659c\u5806 \u00b6 Remains Binomial Queues \u4e8c\u9879\u961f\u5217 / \u4e8c\u9879\u5806 \u00b6 Remains","title":"Priority Queues"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#priority-queues","text":"Priority queue (PQ, pq) is a First-In-Largest-Out queue. The elements must be comparable , which enables priority. Each element is added to the rear of the queue, and the prior element is deleted in the front of the queue. The implementation of priority queue is Heap . ADT Objects: A finite ordered list with zero or more elements. Operations: Initialize a priority queue with a set of elements. Find the prior element of a priority queue. Insert a new element into a priority queue. Delete the prior element of a priority queue. Comparison Let's first consider some data structure to implement the operations we need above. Array Insertion (Add one element at the end) - \\(\\Theta(1)\\) . Deletion (Find the prior key, remove and shift array) - \\(\\Theta(n) + O(n)\\) Linked List Insertion (Add to the front of the list) - \\(\\Theta(1)\\) Deletion (Find the prior key and remove it) - \\(\\Theta(n) + \\Theta(1)\\) . Ordered Array Insertion (Find the proper position, shift array and add the element) - \\(O(n) + O(n)\\) Deletion (Remove the first element) - \\(\\Theta(1)\\) Ordered Linked List Insertion (Find the proper position and add the element) - \\(O(n) + \\Theta(1)\\) Deletion (Remove the first element) - \\(\\Theta(1)\\) BST / AVL tree Both Insertion and Deletion are \\(O(\\log n)\\) Many operations that we don't need Thus we modify it to a binary heap","title":"Priority Queues"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#binary-heap","text":"Definition A binary tree with \\(n\\) nodes and height \\(h\\) is called a complete binary tree iff its notes correspond to the nodes numbers from \\(1\\) to \\(n\\) (up to down and left to right) in a perfect binary tree of height \\(h\\) . Property A complete binary tree of height \\(h\\) has between \\(2^h\\) and \\(2^{h + 1} - 1\\) nodes, and thus \\[ h = \\lfloor \\log n \\rfloor. \\] A complete binary tree with \\(n\\) nodes can be represented by an array with \\(n + 1\\) items. (The zeroth is not used). The array is built by the levelorder of the tree. For any node with index \\(i\\) , we have \\[ \\begin{aligned} \\text{Parent}(i) &= \\left\\{ \\begin{aligned} & \\lfloor i / 2 \\rfloor, & i \\ne 1 \\\\ & \\text{None}, & i = 1. \\end{aligned} \\right. \\\\ \\text{LeftChild}(i) &= \\left\\{ \\begin{aligned} & 2i, & 2i \\le n \\\\ & \\text{None}, & 2i > n. \\end{aligned} \\right. \\\\ \\text{RightChild}(i) &= \\left\\{ \\begin{aligned} & 2i + 1, & 2i + 1 \\le n \\\\ & \\text{None}, & 2i + 1 > n. \\end{aligned} \\right. \\end{aligned} \\] Definition A min tree is a tree in which the key value in each node is no larger than the key values in its children (if any). A min heap is a complete binary tree that is also a min tree. A max tree is a tree in which the key value in each node is no smaller than the key values in its children (if any). A max heap is a complete binary tree that is also a max tree. Thus Heap consists of max heap and min heap . And a heap is a complete binary tree and a max / min tree.","title":"Binary Heap \u4e8c\u53c9\u5806"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#implementation","text":"Data Type #define MinData -1 typedef int ElementType ; typedef struct { ElementType * element ; int capacity ; int size ; } PriorityQueue ; NOTE: The heap is stored in an array since it's a complete binary tree. And element[0] is a sentinel . If it's a min heap, then element[0] must store the smallest value MinData . Priority Queue static void SwapElement ( ElementType * a , ElementType * b ) { ElementType temp = * a ; * a = * b ; * b = temp ; } PriorityQueue * CreateQueue ( const int capacity ) { PriorityQueue * Q = ( PriorityQueue * ) malloc ( sizeof ( PriorityQueue )); Q -> capacity = capacity ; Q -> size = 0 ; Q -> element = ( ElementType * ) malloc (( Q -> capacity + 1 ) * sizeof ( ElementType )); Q -> element [ 0 ] = MinData ; // Sentinel return Q ; } bool IsEmptyQueue ( PriorityQueue * Q ) { return Q -> size == 0 ; } static void PercolateUp ( PriorityQueue * Q , int p ) { while ( p >> 1 > 0 ) { int parent = p >> 1 ; if ( Q -> element [ p ] < Q -> element [ parent ]) { SwapElement ( Q -> element + p , Q -> element + parent ); } else { break ; } p = parent ; } } static void PercolateDown ( PriorityQueue * Q , int p ) { while ( p << 1 <= Q -> size ) { int child = p << 1 ; if ( child != Q -> size && Q -> element [ child + 1 ] < Q -> element [ child ]) { child ++ ; } if ( Q -> element [ p ] > Q -> element [ child ]) { SwapElement ( Q -> element + p , Q -> element + child ); } else { break ; } p = child ; } } void Insert ( PriorityQueue * Q , ElementType x ) { int p = ++ Q -> size ; Q -> element [ p ] = x ; PercolateUp ( Q , p ); } ElementType DeleteMin ( PriorityQueue * Q ) { ElementType minElement = Q -> element [ 1 ]; Q -> element [ 1 ] = Q -> element [ Q -> size -- ]; PercolateDown ( Q , 1 ); return minElement ; } Time Complexity Time complexities of both Insert and DeleteMin are \\(O(\\log n)\\) .","title":"Implementation"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#other-heap-operations","text":"","title":"Other Heap Operations"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#build-heap","text":"Suppose now there is an array of data and we want to build a heap based on the data. Instead of creating an empty heap and inserting one by one, we have the following faster implementation. PriorityQueue * BuildHeap ( const int n , const ElementType a []) { PriorityQueue * Q = ( PriorityQueue * ) malloc ( sizeof ( PriorityQueue )); Q -> size = Q -> capacity = n ; Q -> element = ( ElementType * ) malloc (( Q -> size + 1 ) * sizeof ( ElementType )); memcpy ( Q -> element + 1 , a , Q -> size * sizeof ( ElementType )); int pos = 1 ; while ( pos << 1 < Q -> size ) { pos << = 1 ; } for ( int i = pos ; i >= 1 ; i -- ) { PercolateDown ( Q , i ); } return Q ; } Time Complexity Theorem For the perfect binary tree of heigh \\(h\\) containing \\(2^{h + 1} - 1\\) nodes, the sum of the heights of the node is \\(2^{h + 1} - 1 - (h + 1)\\) . Thus the time complexity of building a heap is \\(O(n)\\) , which is linear.","title":"Build Heap"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#increase-decrease-value","text":"Increase or decrease some value delta ( delta > 0 ) at position pos of a heap. void IncreaseValue ( Priority Queue * Q , const int pos , const ElementType delta ) { assert ( delta > 0 ); Q -> element [ pos ] += delta ; PercolateUp ( Q , pos ); } void DecreaseValue ( Priority Queue * Q , const int pos , const ElementType delta ) { assert ( delta > 0 ); Q -> element [ pos ] -= delta ; PercolateDown ( Q , pos ); }","title":"Increase / Decrease Value"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#delete","text":"Delete the element at position pos of a heap. We just decrease it to the minimum and then delete it. #define inf 2147483647 void Delete ( PriorityQueue * Q , const int pos ) { DecreaseValue ( Q , pos , inf ); DeleteMin ( Q ); }","title":"Delete"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#d-heap","text":"A d-Heap is a heap like binary heap except that all nodes have \\(d\\) children. 3-Heap Note DeleteMin will take \\(d - 1\\) comparisons to find the smallest child and thus totally take \\(O(d \\log_d n)\\) time. Insert will take \\(O(\\log_d n)\\) time. If a d-heap is stored as an array, for an entry of index \\(i\\) , the parent is at \\(\\lfloor (i + d - 2) / d \\rfloor\\) . the first child is at \\((i - 1)d + 2\\) . the last child is at \\(id + 1\\) .","title":"d-Heap"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#leftist-tree-heap","text":"Remains","title":"Leftist Tree / Heap \u5de6\u504f\u6811"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#skew-heap","text":"Remains","title":"Skew Heap \u659c\u5806"},{"location":"Computer_Science_Courses/FDS/Priority_Queues/#binomial-queues","text":"Remains","title":"Binomial Queues \u4e8c\u9879\u961f\u5217 / \u4e8c\u9879\u5806"},{"location":"Computer_Science_Courses/FDS/Sort/","text":"Sort \u00b6 Suppose there is an array of comparable elements \\(x_0, \\dots x_{n - 1}\\) and sort the array. Property \u00b6 When discussing various sort algorithms, we concerns the following properties. Stability \u00b6 Stability measures whether the algorithm change the relative order of the same elements. Time Complexity \u00b6 We consider the best time complexity , worst time complexity and average complexity . Definition An inversion in an array of numbers is any ordered pair \\((i, j)\\) with the property that \\[ i < j,\\ \\ A[i] > A[j]. \\] Theorem The average number of inversions in an array of \\(N\\) distinct number is \\(\\dfrac{N(N - 1)}{4}\\) . Theorem Any algorithm that sorts by exchanging adjacent elements requires \\(\\Omega(N^2)\\) time on average. Any algorithm that sorts by comparison requires \\(\\Omega(N \\log N)\\) time on average. Also, there are some sort algorithms not based on exchange. Space Complexity \u00b6 Space complexity considers the extra space the algorithm need. Bubble Sort \u5192\u6ce1\u6392\u5e8f \u00b6 Iteration Iteration (Improved) Recursion void BubbleSort ( const int n , int a []) { for ( int i = n - 1 ; i >= 0 ; i -- ) { for ( int j = 0 ; j < i ; j ++ ) { if ( a [ j ] > a [ j + 1 ]) { Swap ( a + j , a + j + 1 ); } } } } void BubbleSort ( const int n , int a []) { for ( int i = n - 1 , end = 0 ; i > 0 ; i = end , end = 0 ) { for ( int j = 0 ; j < i ; j ++ ) { if ( a [ j ] > a [ j + 1 ]) { Swap ( a + j , a + j + 1 ); end = j ; } } } } void BubbleSort ( const int n , int a []) { if ( n == 0 ) { return ; } for ( int i = 0 ; i < n - 1 ; i ++ ) { if ( a [ i ] > a [ i + 1 ]) { Swap ( a + i , a + i + 1 ); } } BubbleSort ( n - 1 , a ); } Analysis Stable . Time complexity \\(O(n^2)\\) . Space complexity \\(O(1)\\) . Insertion Sort \u63d2\u5165\u6392\u5e8f \u00b6 Iteration Recursion void InsertionSort ( const int n , int a []) { int j ; for ( int i = 1 ; i < n ; i ++ ) { int x = a [ i ]; for ( j = i - 1 ; j >= 0 && a [ j ] > x ; j -- ) { a [ j + 1 ] = a [ j ]; } a [ j + 1 ] = x ; } } void InsertionSort ( const int n , int a []) { if ( n == 0 ) { return ; } InsertionSort ( n - 1 , a ); int x = a [ n - 1 ], j ; for ( j = n - 2 ; j >= 0 && a [ j ] > x ; j -- ) { a [ j + 1 ] = a [ j ]; } a [ j + 1 ] = x ; } Analysis Stable . Time complexity Avearge case \\(O(n^2)\\) . Best case \\(O(n)\\) ( High Efficiency when it's almost in order). When \\(n \\le 20\\) , insertion sort is the fastest . Space complexity \\(O(1)\\) . Selection Sort \u9009\u62e9\u6392\u5e8f \u00b6 Iteration Recursion void InsertionSort ( const int n , int a []) { for ( int i = 0 ; i < n - 1 ; i ++ ) { int idx = i ; for ( int j = i + 1 ; j < n ; j ++ ) { idx = a [ j ] < a [ idx ] ? j : idx ; } Swap ( a + i , a + idx ); } } int FindMin ( int a [], const int begin , const int end ) { int ret = begin ; for ( int i = begin + 1 ; i <= end ; i ++ ) { ret = a [ i ] < a [ ret ] ? i : ret ; } return ret ; } void Sort ( int a [], const int begin , const int end ) { if ( begin == end ) { return ; } Swap ( a + begin , a + FindMin ( a , begin , end )); Sort ( a , begin + 1 , end ); } void SelectionSort ( const int n , int a []) { Sort ( a , 0 , n - 1 ); } Analysis Unstable . Time complexity \\(O(n^2)\\) . Space complexity \\(O(1)\\) . Counting Sort \u8ba1\u6570\u6392\u5e8f \u00b6 #define m 10010 // (1)! void CountingSort ( const int n , int a []) { int cnt [ m ], b [ n ]; memset ( cnt , 0 , sizeof ( cnt )); for ( int i = 0 ; i < n ; i ++ ) { cnt [ a [ i ]] ++ ; } for ( int i = 0 ; i < m ; i ++ ) { cnt [ i ] += cnt [ i - 1 ]; } for ( int i = n - 1 ; i >= 0 ; i -- ) { b [ cnt [ a [ i ] -- ] = a [ i ]; } memcpy ( a , b , sizeof ( a )); } m is the upper bound of a[i] . Analysis Stable . Time complexity \\(O(n + m)\\) , where \\(m\\) is the upper bound of \\(a[i]\\) . Space complexity \\(O(n + m)\\) . Radix Sort \u57fa\u6570\u6392\u5e8f \u00b6 typedef struct { int element ; int key [ K ]; // (1)! } ElementType ; void CountingSort ( const int n , ElementType a [], int p , int cnt [], ElementType b []) { memset ( cnt , 0 , sizeof ( cnt )); for ( int i = 0 ; i < n ; i ++ ) { cnt [ a [ i ]. key [ p ]] ++ ; } for ( int i = 0 ; i < m [ p ]; i ++ ) { cnt [ i ] += cnt [ i - 1 ]; } for ( int i = n - 1 ; i >= 0 ; i -- ) { b [ cnt [ a [ i ]. key [ p ] -- ] = a [ i ]; } memcpy ( a , b , sizeof ( a )); } void RadixSort ( const int n , ElementType a []) { int cnt [ M ]; // (2)! ElementType b [ n ]; for ( int i = k - 1 ; i >= 0 ; i -- ) { CountingSort ( n , a , i , cnt , b ); } } K is the number of keys. M is the maximum of m[i] , where m[i] is the upper bound of each key. The most common keys for sorting integers are the digit number from high to low (left to right), which is called Most Significant Digit (MSD) sort. the digit number from low to high (right to left), which is called Least Significant Digit (LSD) sort. In general, LSD is better than MSD. Keys of LSD Keys of MSD for ( int i = 0 ; i < n ; i ++ ) { for ( int j = 0 ; j < K ; j ++ ) { a [ i ]. key [ j ] = ( int )( a [ i ]. element / pow ( 10 , j )) % 10 ; } } for ( int i = 0 ; i < n ; i ++ ) { for ( int j = 0 ; j < K ; j ++ ) { a [ i ]. key [ j ] = ( int )( a [ i ]. element / ( pow ( 10 , K - 1 ) / pow ( 10 , j ))) % 10 ; } } Analysis Stable unless the inner sort is unstable. Time complexity If \\(\\max\\{m_i\\}\\) is small, we can use Counting Sort as the inner sort, then the time complexity is \\(O\\left(nk + \\sum\\limits_{i = 1}^k m_i\\right)\\) , where \\(k\\) is the number of keys, \\(m_i\\) is the upper bound of each key. If \\(\\max\\{m_i\\}\\) is large, we use the sort based on comparison of \\(O(nk \\log n)\\) instead of Radix Sort. Space complexity \\(O(n + \\max\\{m_i\\})\\) . Quick Sort \u5feb\u901f\u6392\u5e8f \u00b6 void Sort ( int a [], const int L , const int R ) { if ( L >= R ) { return ; } int rnd = L + rand () % ( R - L ); // (1)! int pos = L , cnt = 0 ; Swap ( & a [ L ], & a [ rnd ]); for ( int i = L + 1 ; i <= R ; i ++ ) { // (2)! if ( a [ i ] < a [ L ]) { pos ++ ; if ( pos + cnt < i ) { Swap ( & a [ pos ], & a [ cnt + pos ]); } Swap ( & a [ i ], & a [ pos ]); continue ; } if ( a [ i ] == a [ L ]) { cnt ++ ; Swap ( & a [ i ], & a [ cnt + pos ]); } } Swap ( & a [ L ], & a [ pos ]); Sort ( a , L , pos - 1 ); Sort ( a , pos + 1 + cnt , R ); } void QuickSort ( const int n , int a []) { srand ( time ( NULL )); Sort ( a , 0 , n - 1 ); } Different pivot picking strategies are discussed below. This is a 3-way radix quicksort. Analysis Unstable . Time complexity Best and average cases \\(O(n \\log n)\\) . Worst cases \\(O(n^2)\\) . Each pivot a[pos] is the maxima / minima. Degenerate to list. Space complexity \\(O(1)\\) . Time Complexity \\[ T(N) = T(i) + T(N - i - 1) + cN. \\] Worst Case \\[ T(N) = T(N - 1) + cN \\Rightarrow T(N) = O(N^2), \\] Best Case \\[ T(N) = 2T\\left(\\frac{N}{2}\\right) + cN \\Rightarrow T(N) = O(N\\log N), \\] Average Case \\[ T(N) = \\frac2N\\sum\\limits_{j = 0}^{N - 1}T(j) + cN \\Rightarrow T(N) = O(N\\log N). \\] Pivot Picking Strategy A simple way is a[0] . A random way, which is used above, is a[rnd] , where rnd is a random number between L and R . Median-of-Three Paritioning a[mid] , where mid = median(left, center, right) . Code int MediamThree ( int a [], const int L , const int R ) { int mid = L + (( R - L ) >> 1 ); if ( a [ L ] > a [ mid ]) { Swap ( a + L , a + mid ); } if ( a [ L ] > a [ R ]) { Swap ( a + L , a + R ); } if ( a [ mid ] > a [ R ]) { Swap ( a + mid , a + R ); } assert ( a [ L ] <= a [ mid ] && a [ mid ] <= a [ R ]); Swap ( a [ mid ], a [ R - 1 ]); return a [ R - 1 ]; } Improvement Suppose the value of pivot is v . 2-way Radix Quicksort Scan from left and right respectively by two index (i, j) , make the elements <= v on the left of i and the elements >= v on the right of j . 3-way Radix Quicksort Partition the series into THREE parts: > m , < m and = m . It's efficient to deal with the cases of multiple identical values. Merge Sort \u5f52\u5e76\u6392\u5e8f \u00b6 Recursion Iteration void Merge ( int a [], const int L , const int mid , const int R , int temp []) { int i = L , j = mid + 1 ; for ( int k = L ; k <= R ; k ++ ) { if ( i <= mid && ( j > R || a [ i ] <= a [ j ])) { temp [ k ] = a [ i ]; i ++ ; } else { temp [ k ] = a [ j ]; j ++ ; } } memcpy ( a + L , temp + L , ( R - L + 1 ) * sizeof ( int )); } void Sort ( int a [], const int L , const int R , int temp []) { if ( L == R ) { return ; } int mid = L + (( R - L ) >> 1 ); Sort ( a , L , mid , temp ); Sort ( a , mid + 1 , R , temp ); Merge ( a , L , mid , R , temp ); } void MergeSort ( const int n , int a []) { int temp [ n ]; Sort ( a , 0 , n - 1 , temp ); } #define min(a, b) ((a) < (b) ? (a) : (b)) void Merge ( int a [], const int L , const int mid , const int R , int temp []) { int i = L , j = mid + 1 ; for ( int k = L ; k <= R ; k ++ ) { if ( i <= mid && ( j > R || a [ i ] <= a [ j ])) { temp [ k ] = a [ i ]; i ++ ; } else { temp [ k ] = a [ j ]; j ++ ; } } memcpy ( a + L , temp + L , ( R - L + 1 ) * sizeof ( int )); } void MergeSort ( const int n , int a []) { int temp [ n ]; for ( int width = 1 ; width < n ; width <<= 1 ) { for ( int i = 0 ; i < n ; i += width << 1 ) { Merge ( a , i , min ( n , i + width ) - 1 , min ( n , i + ( width << 1 ) - 1 ), temp ); } } } Analysis Stable . Time complexity \\(O(n \\log n)\\) . Space complexity \\(O(n)\\) . Better performance at parallel sort. Time Complexity \\[ \\begin{aligned} T(1) &= 1, \\\\ T(N) &= 2T\\left(\\frac{N}{2}\\right) + O(N) \\\\ &= 2^k T\\left(\\frac{N}{2^k}\\right) + k \\cdot O(N) \\\\ &= N \\cdot O(1) + \\log N \\cdot O(N) \\\\ &= O(N + N \\log N). \\end{aligned} \\] Bucket Sort \u6876\u6392\u5e8f \u00b6 #define m 10010 // (1)! typedef struct _node { int element ; struct _node * next ; } Node ; typedef struct { int size ; Node * head ; } List ; void InsertionSort ( List * list ) { // (2)! for ( Node * p = list -> head -> next , * q = list -> head ; p != NULL ;) { bool flag = false ; for ( Node * r = list -> head -> next , * s = list -> head ; r != p ; s = r , r = r -> next ) { if ( r -> element > p -> element ) { Node * temp = p ; p = p -> next ; q -> next = p ; flag = true ; s -> next = temp ; temp -> next = r ; break ; } } if ( flag == false ) { q = p ; p = p -> next ; } } } void BucketSort ( const int n , int a []) { int bucketNum = 6 ; // (3)! int bucketSize = ( m + bucketNum - 1 ) / bucketNum ; List ** bucket = ( List ** ) malloc ( bucketNum * sizeof ( List * )); for ( int i = 0 ; i < bucketNum ; i ++ ) { bucket [ i ] = ( List * ) malloc ( sizeof ( List )); bucket [ i ] -> size = 0 ; bucket [ i ] -> head = ( Node * ) malloc ( sizeof ( Node )); bucket [ i ] -> head -> next = NULL ; } for ( int i = 0 ; i < n ; i ++ ) { int pos = a [ i ] / bucketSize ; bucket [ pos ] -> size ++ ; Node * p = ( Node * ) malloc ( sizeof ( Node )); p -> element = a [ i ]; p -> next = bucket [ pos ] -> head -> next ; bucket [ pos ] -> head -> next = p ; } for ( int i = 0 , cnt = 0 ; i < bucketNum ; i ++ ) { InsertionSort ( bucket [ i ]); for ( Node * p = bucket [ i ] -> head -> next ; p != NULL ; p = p -> next ) { a [ cnt ++ ] = p -> element ; } } for ( int i = 0 ; i < bucketNum ; i ++ ) { for ( Node * p = bucket [ i ] -> head , * temp = NULL ; p != NULL ;) { temp = p ; p = p -> next ; free ( temp ); } free ( bucket [ i ]); } free ( bucket ); } m is the upper bound of a[i] . The inner sort is used insertion sort, and here is a list implementation of Insertion Sort . bucketNum is the number of the bucket. Analysis Similar to Radix Sort . Stable unless the inner sort is unstable. Time complexity Average case \\(O\\left(n + k \\cdot \\left(\\dfrac{n}{k}\\right)^2 + k\\right)\\) , where \\(k\\) is the number of the bucket. Worst case \\(O(n^2)\\) . Large constant of time complexity. Space complexity \\(O(n)\\) . Heap Sort \u5806\u6392\u5e8f \u00b6 void MaxHeapify ( int a [], int p , int size ) { // (1)! while ( p << 1 <= size ) { int child = p << 1 ; if ( child != size && a [ child ] < a [ child + 1 ]) { child ++ ; } if ( a [ p ] < a [ child ]) { Swap ( a + p , a + child ); } else { break ; } p = child ; } } void HeapSort ( const int n , int a []) { for ( int i = n / 2 - 1 ; i >= 0 ; i -- ) { // (2)! MaxHeapify ( a , i , n - 1 ); } for ( int i = n - 1 ; i > 0 ; i -- ) { Swap ( a , a + i ); MaxHeapify ( a , 0 , i - 1 ); } } The same as PercolateDown() of implementation of heap. The same as Build Heap . Analysis Improvement of Selection Sort . Unstable . Time complexity \\(O(n \\log n)\\) . Space complexity \\(O(1)\\) . Much cache miss and not based on partition, which means hard to parallel. Theorem The average number of comparisons used to heapsort a random permutation of N distinct items is \\(2N \\log N - O(N\\log \\log N)\\) . Shell Sort \u5e0c\u5c14\u6392\u5e8f \u00b6 Shell's Increment Shell's Increment (Simplified) Hibbard's Increment void ShellSort ( const int n , int a []) { for ( int step = n >> 1 ; step > 0 ; step >>= 1 ) { for ( int start = 0 ; start < step ; start ++ ) { for ( int i = start ; i < n ; i += step ) { int temp = a [ i ], j ; for ( j = i ; j >= step ; j -= step ) { if ( temp < a [ j - step ]) { a [ j ] = a [ j - step ]; } else { break ; } } a [ j ] = temp ; } } } } void ShellSort ( const int n , int a []) { for ( int step = n >> 1 ; step > 0 ; step >>= 1 ) { for ( int i = step ; i < n ; i += step ) { int temp = a [ i ], j ; for ( j = i ; j >= step ; j -= step ) { if ( temp < a [ j - step ]) { a [ j ] = a [ j - step ]; } else { break ; } } a [ j ] = temp ; } } } void ShellSort ( const int n , int a []) { int t = 1 , step ; while ( t << 1 < n ) { t <<= 1 ; } step = t - 1 ; while ( step > 0 ) { for ( int i = step ; i < n ; i += step ) { int temp = a [ i ], j ; for ( j = i ; j >= step ; j -= step ) { if ( temp < a [ j - step ]) { a [ j ] = a [ j - step ]; } else { break ; } } a [ j ] = temp ; } t >>= 1 ; step = t - 1 ; } } Analysis Improvement of Insertion Sort . Unstable . Time complexity Depend on the increment sequence (example are discussed below). Best case \\(O(n)\\) . Worst case \\(\\Theta(n^2)\\) . Best worst case that are known is \\(O(n \\log^2 n)\\) . Space complexity \\(O(1)\\) . Much cache miss and not based on partition, which means hard to parallel. Choice of Increment Sequence Shell's Increment Sequence \\[ h_t = \\left\\lfloor \\frac{N}{2} \\right\\rfloor,\\ \\ h_k = \\left\\lfloor \\frac{h_{k + 1}}{2} \\right\\rfloor. \\] Theorem The worst case running time of Shell Sort using Shell's increment sequence is \\(\\Theta(N^2)\\) . Worst Case of Shell's Increment Sequence Hibbard's Increment Sequence \\[ h_k = 2^k - 1. \\] Theorem The worst case running time of Shell Sort using Hibbard's increment sequence is \\(\\Theta\\left(N^{\\frac{3}{2}}\\right)\\) . Conjectures \\[ T_{avg-Hibbard}(N) = O\\left(N^{\\frac54}\\right). \\] Conjectures: Sedgewick's Best Sequence Sedgewick\u2019s best sequence is \\(\\{1, 5, 19, 41, 109, \\dots\\}\\) in which the terms are either of the form \\(9 \\times 4^i - 9 \\times 2^i + 1\\) or \\(4^i \u2013 3 \\times 2^i + 1\\) . \\[ T_{avg}(N) = O\\left(N^{\\frac76}\\right),\\ \\ T_{worst}(N) = O\\left(N^{\\frac43}\\right). \\]","title":"Sort"},{"location":"Computer_Science_Courses/FDS/Sort/#sort","text":"Suppose there is an array of comparable elements \\(x_0, \\dots x_{n - 1}\\) and sort the array.","title":"Sort"},{"location":"Computer_Science_Courses/FDS/Sort/#property","text":"When discussing various sort algorithms, we concerns the following properties.","title":"Property"},{"location":"Computer_Science_Courses/FDS/Sort/#stability","text":"Stability measures whether the algorithm change the relative order of the same elements.","title":"Stability"},{"location":"Computer_Science_Courses/FDS/Sort/#time-complexity","text":"We consider the best time complexity , worst time complexity and average complexity . Definition An inversion in an array of numbers is any ordered pair \\((i, j)\\) with the property that \\[ i < j,\\ \\ A[i] > A[j]. \\] Theorem The average number of inversions in an array of \\(N\\) distinct number is \\(\\dfrac{N(N - 1)}{4}\\) . Theorem Any algorithm that sorts by exchanging adjacent elements requires \\(\\Omega(N^2)\\) time on average. Any algorithm that sorts by comparison requires \\(\\Omega(N \\log N)\\) time on average. Also, there are some sort algorithms not based on exchange.","title":"Time Complexity"},{"location":"Computer_Science_Courses/FDS/Sort/#space-complexity","text":"Space complexity considers the extra space the algorithm need.","title":"Space Complexity"},{"location":"Computer_Science_Courses/FDS/Sort/#bubble-sort","text":"Iteration Iteration (Improved) Recursion void BubbleSort ( const int n , int a []) { for ( int i = n - 1 ; i >= 0 ; i -- ) { for ( int j = 0 ; j < i ; j ++ ) { if ( a [ j ] > a [ j + 1 ]) { Swap ( a + j , a + j + 1 ); } } } } void BubbleSort ( const int n , int a []) { for ( int i = n - 1 , end = 0 ; i > 0 ; i = end , end = 0 ) { for ( int j = 0 ; j < i ; j ++ ) { if ( a [ j ] > a [ j + 1 ]) { Swap ( a + j , a + j + 1 ); end = j ; } } } } void BubbleSort ( const int n , int a []) { if ( n == 0 ) { return ; } for ( int i = 0 ; i < n - 1 ; i ++ ) { if ( a [ i ] > a [ i + 1 ]) { Swap ( a + i , a + i + 1 ); } } BubbleSort ( n - 1 , a ); } Analysis Stable . Time complexity \\(O(n^2)\\) . Space complexity \\(O(1)\\) .","title":"Bubble Sort \u5192\u6ce1\u6392\u5e8f"},{"location":"Computer_Science_Courses/FDS/Sort/#insertion-sort","text":"Iteration Recursion void InsertionSort ( const int n , int a []) { int j ; for ( int i = 1 ; i < n ; i ++ ) { int x = a [ i ]; for ( j = i - 1 ; j >= 0 && a [ j ] > x ; j -- ) { a [ j + 1 ] = a [ j ]; } a [ j + 1 ] = x ; } } void InsertionSort ( const int n , int a []) { if ( n == 0 ) { return ; } InsertionSort ( n - 1 , a ); int x = a [ n - 1 ], j ; for ( j = n - 2 ; j >= 0 && a [ j ] > x ; j -- ) { a [ j + 1 ] = a [ j ]; } a [ j + 1 ] = x ; } Analysis Stable . Time complexity Avearge case \\(O(n^2)\\) . Best case \\(O(n)\\) ( High Efficiency when it's almost in order). When \\(n \\le 20\\) , insertion sort is the fastest . Space complexity \\(O(1)\\) .","title":"Insertion Sort \u63d2\u5165\u6392\u5e8f"},{"location":"Computer_Science_Courses/FDS/Sort/#selection-sort","text":"Iteration Recursion void InsertionSort ( const int n , int a []) { for ( int i = 0 ; i < n - 1 ; i ++ ) { int idx = i ; for ( int j = i + 1 ; j < n ; j ++ ) { idx = a [ j ] < a [ idx ] ? j : idx ; } Swap ( a + i , a + idx ); } } int FindMin ( int a [], const int begin , const int end ) { int ret = begin ; for ( int i = begin + 1 ; i <= end ; i ++ ) { ret = a [ i ] < a [ ret ] ? i : ret ; } return ret ; } void Sort ( int a [], const int begin , const int end ) { if ( begin == end ) { return ; } Swap ( a + begin , a + FindMin ( a , begin , end )); Sort ( a , begin + 1 , end ); } void SelectionSort ( const int n , int a []) { Sort ( a , 0 , n - 1 ); } Analysis Unstable . Time complexity \\(O(n^2)\\) . Space complexity \\(O(1)\\) .","title":"Selection Sort \u9009\u62e9\u6392\u5e8f"},{"location":"Computer_Science_Courses/FDS/Sort/#counting-sort","text":"#define m 10010 // (1)! void CountingSort ( const int n , int a []) { int cnt [ m ], b [ n ]; memset ( cnt , 0 , sizeof ( cnt )); for ( int i = 0 ; i < n ; i ++ ) { cnt [ a [ i ]] ++ ; } for ( int i = 0 ; i < m ; i ++ ) { cnt [ i ] += cnt [ i - 1 ]; } for ( int i = n - 1 ; i >= 0 ; i -- ) { b [ cnt [ a [ i ] -- ] = a [ i ]; } memcpy ( a , b , sizeof ( a )); } m is the upper bound of a[i] . Analysis Stable . Time complexity \\(O(n + m)\\) , where \\(m\\) is the upper bound of \\(a[i]\\) . Space complexity \\(O(n + m)\\) .","title":"Counting Sort \u8ba1\u6570\u6392\u5e8f"},{"location":"Computer_Science_Courses/FDS/Sort/#radix-sort","text":"typedef struct { int element ; int key [ K ]; // (1)! } ElementType ; void CountingSort ( const int n , ElementType a [], int p , int cnt [], ElementType b []) { memset ( cnt , 0 , sizeof ( cnt )); for ( int i = 0 ; i < n ; i ++ ) { cnt [ a [ i ]. key [ p ]] ++ ; } for ( int i = 0 ; i < m [ p ]; i ++ ) { cnt [ i ] += cnt [ i - 1 ]; } for ( int i = n - 1 ; i >= 0 ; i -- ) { b [ cnt [ a [ i ]. key [ p ] -- ] = a [ i ]; } memcpy ( a , b , sizeof ( a )); } void RadixSort ( const int n , ElementType a []) { int cnt [ M ]; // (2)! ElementType b [ n ]; for ( int i = k - 1 ; i >= 0 ; i -- ) { CountingSort ( n , a , i , cnt , b ); } } K is the number of keys. M is the maximum of m[i] , where m[i] is the upper bound of each key. The most common keys for sorting integers are the digit number from high to low (left to right), which is called Most Significant Digit (MSD) sort. the digit number from low to high (right to left), which is called Least Significant Digit (LSD) sort. In general, LSD is better than MSD. Keys of LSD Keys of MSD for ( int i = 0 ; i < n ; i ++ ) { for ( int j = 0 ; j < K ; j ++ ) { a [ i ]. key [ j ] = ( int )( a [ i ]. element / pow ( 10 , j )) % 10 ; } } for ( int i = 0 ; i < n ; i ++ ) { for ( int j = 0 ; j < K ; j ++ ) { a [ i ]. key [ j ] = ( int )( a [ i ]. element / ( pow ( 10 , K - 1 ) / pow ( 10 , j ))) % 10 ; } } Analysis Stable unless the inner sort is unstable. Time complexity If \\(\\max\\{m_i\\}\\) is small, we can use Counting Sort as the inner sort, then the time complexity is \\(O\\left(nk + \\sum\\limits_{i = 1}^k m_i\\right)\\) , where \\(k\\) is the number of keys, \\(m_i\\) is the upper bound of each key. If \\(\\max\\{m_i\\}\\) is large, we use the sort based on comparison of \\(O(nk \\log n)\\) instead of Radix Sort. Space complexity \\(O(n + \\max\\{m_i\\})\\) .","title":"Radix Sort \u57fa\u6570\u6392\u5e8f"},{"location":"Computer_Science_Courses/FDS/Sort/#quick-sort","text":"void Sort ( int a [], const int L , const int R ) { if ( L >= R ) { return ; } int rnd = L + rand () % ( R - L ); // (1)! int pos = L , cnt = 0 ; Swap ( & a [ L ], & a [ rnd ]); for ( int i = L + 1 ; i <= R ; i ++ ) { // (2)! if ( a [ i ] < a [ L ]) { pos ++ ; if ( pos + cnt < i ) { Swap ( & a [ pos ], & a [ cnt + pos ]); } Swap ( & a [ i ], & a [ pos ]); continue ; } if ( a [ i ] == a [ L ]) { cnt ++ ; Swap ( & a [ i ], & a [ cnt + pos ]); } } Swap ( & a [ L ], & a [ pos ]); Sort ( a , L , pos - 1 ); Sort ( a , pos + 1 + cnt , R ); } void QuickSort ( const int n , int a []) { srand ( time ( NULL )); Sort ( a , 0 , n - 1 ); } Different pivot picking strategies are discussed below. This is a 3-way radix quicksort. Analysis Unstable . Time complexity Best and average cases \\(O(n \\log n)\\) . Worst cases \\(O(n^2)\\) . Each pivot a[pos] is the maxima / minima. Degenerate to list. Space complexity \\(O(1)\\) . Time Complexity \\[ T(N) = T(i) + T(N - i - 1) + cN. \\] Worst Case \\[ T(N) = T(N - 1) + cN \\Rightarrow T(N) = O(N^2), \\] Best Case \\[ T(N) = 2T\\left(\\frac{N}{2}\\right) + cN \\Rightarrow T(N) = O(N\\log N), \\] Average Case \\[ T(N) = \\frac2N\\sum\\limits_{j = 0}^{N - 1}T(j) + cN \\Rightarrow T(N) = O(N\\log N). \\] Pivot Picking Strategy A simple way is a[0] . A random way, which is used above, is a[rnd] , where rnd is a random number between L and R . Median-of-Three Paritioning a[mid] , where mid = median(left, center, right) . Code int MediamThree ( int a [], const int L , const int R ) { int mid = L + (( R - L ) >> 1 ); if ( a [ L ] > a [ mid ]) { Swap ( a + L , a + mid ); } if ( a [ L ] > a [ R ]) { Swap ( a + L , a + R ); } if ( a [ mid ] > a [ R ]) { Swap ( a + mid , a + R ); } assert ( a [ L ] <= a [ mid ] && a [ mid ] <= a [ R ]); Swap ( a [ mid ], a [ R - 1 ]); return a [ R - 1 ]; } Improvement Suppose the value of pivot is v . 2-way Radix Quicksort Scan from left and right respectively by two index (i, j) , make the elements <= v on the left of i and the elements >= v on the right of j . 3-way Radix Quicksort Partition the series into THREE parts: > m , < m and = m . It's efficient to deal with the cases of multiple identical values.","title":"Quick Sort \u5feb\u901f\u6392\u5e8f"},{"location":"Computer_Science_Courses/FDS/Sort/#merge-sort","text":"Recursion Iteration void Merge ( int a [], const int L , const int mid , const int R , int temp []) { int i = L , j = mid + 1 ; for ( int k = L ; k <= R ; k ++ ) { if ( i <= mid && ( j > R || a [ i ] <= a [ j ])) { temp [ k ] = a [ i ]; i ++ ; } else { temp [ k ] = a [ j ]; j ++ ; } } memcpy ( a + L , temp + L , ( R - L + 1 ) * sizeof ( int )); } void Sort ( int a [], const int L , const int R , int temp []) { if ( L == R ) { return ; } int mid = L + (( R - L ) >> 1 ); Sort ( a , L , mid , temp ); Sort ( a , mid + 1 , R , temp ); Merge ( a , L , mid , R , temp ); } void MergeSort ( const int n , int a []) { int temp [ n ]; Sort ( a , 0 , n - 1 , temp ); } #define min(a, b) ((a) < (b) ? (a) : (b)) void Merge ( int a [], const int L , const int mid , const int R , int temp []) { int i = L , j = mid + 1 ; for ( int k = L ; k <= R ; k ++ ) { if ( i <= mid && ( j > R || a [ i ] <= a [ j ])) { temp [ k ] = a [ i ]; i ++ ; } else { temp [ k ] = a [ j ]; j ++ ; } } memcpy ( a + L , temp + L , ( R - L + 1 ) * sizeof ( int )); } void MergeSort ( const int n , int a []) { int temp [ n ]; for ( int width = 1 ; width < n ; width <<= 1 ) { for ( int i = 0 ; i < n ; i += width << 1 ) { Merge ( a , i , min ( n , i + width ) - 1 , min ( n , i + ( width << 1 ) - 1 ), temp ); } } } Analysis Stable . Time complexity \\(O(n \\log n)\\) . Space complexity \\(O(n)\\) . Better performance at parallel sort. Time Complexity \\[ \\begin{aligned} T(1) &= 1, \\\\ T(N) &= 2T\\left(\\frac{N}{2}\\right) + O(N) \\\\ &= 2^k T\\left(\\frac{N}{2^k}\\right) + k \\cdot O(N) \\\\ &= N \\cdot O(1) + \\log N \\cdot O(N) \\\\ &= O(N + N \\log N). \\end{aligned} \\]","title":"Merge Sort \u5f52\u5e76\u6392\u5e8f"},{"location":"Computer_Science_Courses/FDS/Sort/#bucket-sort","text":"#define m 10010 // (1)! typedef struct _node { int element ; struct _node * next ; } Node ; typedef struct { int size ; Node * head ; } List ; void InsertionSort ( List * list ) { // (2)! for ( Node * p = list -> head -> next , * q = list -> head ; p != NULL ;) { bool flag = false ; for ( Node * r = list -> head -> next , * s = list -> head ; r != p ; s = r , r = r -> next ) { if ( r -> element > p -> element ) { Node * temp = p ; p = p -> next ; q -> next = p ; flag = true ; s -> next = temp ; temp -> next = r ; break ; } } if ( flag == false ) { q = p ; p = p -> next ; } } } void BucketSort ( const int n , int a []) { int bucketNum = 6 ; // (3)! int bucketSize = ( m + bucketNum - 1 ) / bucketNum ; List ** bucket = ( List ** ) malloc ( bucketNum * sizeof ( List * )); for ( int i = 0 ; i < bucketNum ; i ++ ) { bucket [ i ] = ( List * ) malloc ( sizeof ( List )); bucket [ i ] -> size = 0 ; bucket [ i ] -> head = ( Node * ) malloc ( sizeof ( Node )); bucket [ i ] -> head -> next = NULL ; } for ( int i = 0 ; i < n ; i ++ ) { int pos = a [ i ] / bucketSize ; bucket [ pos ] -> size ++ ; Node * p = ( Node * ) malloc ( sizeof ( Node )); p -> element = a [ i ]; p -> next = bucket [ pos ] -> head -> next ; bucket [ pos ] -> head -> next = p ; } for ( int i = 0 , cnt = 0 ; i < bucketNum ; i ++ ) { InsertionSort ( bucket [ i ]); for ( Node * p = bucket [ i ] -> head -> next ; p != NULL ; p = p -> next ) { a [ cnt ++ ] = p -> element ; } } for ( int i = 0 ; i < bucketNum ; i ++ ) { for ( Node * p = bucket [ i ] -> head , * temp = NULL ; p != NULL ;) { temp = p ; p = p -> next ; free ( temp ); } free ( bucket [ i ]); } free ( bucket ); } m is the upper bound of a[i] . The inner sort is used insertion sort, and here is a list implementation of Insertion Sort . bucketNum is the number of the bucket. Analysis Similar to Radix Sort . Stable unless the inner sort is unstable. Time complexity Average case \\(O\\left(n + k \\cdot \\left(\\dfrac{n}{k}\\right)^2 + k\\right)\\) , where \\(k\\) is the number of the bucket. Worst case \\(O(n^2)\\) . Large constant of time complexity. Space complexity \\(O(n)\\) .","title":"Bucket Sort \u6876\u6392\u5e8f"},{"location":"Computer_Science_Courses/FDS/Sort/#heap-sort","text":"void MaxHeapify ( int a [], int p , int size ) { // (1)! while ( p << 1 <= size ) { int child = p << 1 ; if ( child != size && a [ child ] < a [ child + 1 ]) { child ++ ; } if ( a [ p ] < a [ child ]) { Swap ( a + p , a + child ); } else { break ; } p = child ; } } void HeapSort ( const int n , int a []) { for ( int i = n / 2 - 1 ; i >= 0 ; i -- ) { // (2)! MaxHeapify ( a , i , n - 1 ); } for ( int i = n - 1 ; i > 0 ; i -- ) { Swap ( a , a + i ); MaxHeapify ( a , 0 , i - 1 ); } } The same as PercolateDown() of implementation of heap. The same as Build Heap . Analysis Improvement of Selection Sort . Unstable . Time complexity \\(O(n \\log n)\\) . Space complexity \\(O(1)\\) . Much cache miss and not based on partition, which means hard to parallel. Theorem The average number of comparisons used to heapsort a random permutation of N distinct items is \\(2N \\log N - O(N\\log \\log N)\\) .","title":"Heap Sort \u5806\u6392\u5e8f"},{"location":"Computer_Science_Courses/FDS/Sort/#shell-sort","text":"Shell's Increment Shell's Increment (Simplified) Hibbard's Increment void ShellSort ( const int n , int a []) { for ( int step = n >> 1 ; step > 0 ; step >>= 1 ) { for ( int start = 0 ; start < step ; start ++ ) { for ( int i = start ; i < n ; i += step ) { int temp = a [ i ], j ; for ( j = i ; j >= step ; j -= step ) { if ( temp < a [ j - step ]) { a [ j ] = a [ j - step ]; } else { break ; } } a [ j ] = temp ; } } } } void ShellSort ( const int n , int a []) { for ( int step = n >> 1 ; step > 0 ; step >>= 1 ) { for ( int i = step ; i < n ; i += step ) { int temp = a [ i ], j ; for ( j = i ; j >= step ; j -= step ) { if ( temp < a [ j - step ]) { a [ j ] = a [ j - step ]; } else { break ; } } a [ j ] = temp ; } } } void ShellSort ( const int n , int a []) { int t = 1 , step ; while ( t << 1 < n ) { t <<= 1 ; } step = t - 1 ; while ( step > 0 ) { for ( int i = step ; i < n ; i += step ) { int temp = a [ i ], j ; for ( j = i ; j >= step ; j -= step ) { if ( temp < a [ j - step ]) { a [ j ] = a [ j - step ]; } else { break ; } } a [ j ] = temp ; } t >>= 1 ; step = t - 1 ; } } Analysis Improvement of Insertion Sort . Unstable . Time complexity Depend on the increment sequence (example are discussed below). Best case \\(O(n)\\) . Worst case \\(\\Theta(n^2)\\) . Best worst case that are known is \\(O(n \\log^2 n)\\) . Space complexity \\(O(1)\\) . Much cache miss and not based on partition, which means hard to parallel. Choice of Increment Sequence Shell's Increment Sequence \\[ h_t = \\left\\lfloor \\frac{N}{2} \\right\\rfloor,\\ \\ h_k = \\left\\lfloor \\frac{h_{k + 1}}{2} \\right\\rfloor. \\] Theorem The worst case running time of Shell Sort using Shell's increment sequence is \\(\\Theta(N^2)\\) . Worst Case of Shell's Increment Sequence Hibbard's Increment Sequence \\[ h_k = 2^k - 1. \\] Theorem The worst case running time of Shell Sort using Hibbard's increment sequence is \\(\\Theta\\left(N^{\\frac{3}{2}}\\right)\\) . Conjectures \\[ T_{avg-Hibbard}(N) = O\\left(N^{\\frac54}\\right). \\] Conjectures: Sedgewick's Best Sequence Sedgewick\u2019s best sequence is \\(\\{1, 5, 19, 41, 109, \\dots\\}\\) in which the terms are either of the form \\(9 \\times 4^i - 9 \\times 2^i + 1\\) or \\(4^i \u2013 3 \\times 2^i + 1\\) . \\[ T_{avg}(N) = O\\left(N^{\\frac76}\\right),\\ \\ T_{worst}(N) = O\\left(N^{\\frac43}\\right). \\]","title":"Shell Sort \u5e0c\u5c14\u6392\u5e8f"},{"location":"Computer_Science_Courses/FDS/Trees/","text":"Trees \u00b6 Definition A tree is a collection of nodes. The collection can be empty; otherwise, a tree consists of a distinguished node \\(r\\) , called the root . zero or more nonempty subtrees \\(T_1, \\dots, T_k\\) , each of whose roots are connected by a directed edge from \\(r\\) . Definition Degree of a node: number of subtrees of the node. Degree of a tree: maximum among the degrees of all nodes. Parent: a node that has subtrees. Children: the roots of the subtrees of parent. Siblings: children of the same parent. Leaf (terminal node): a node with degree \\(0\\) (no children). Path from \\(n_1\\) to \\(n_k\\) : a unique sequence of nodes \\(n_1,n_2,\\dots,n_k\\) such that \\(n_i\\) is the parent of \\(n_{i+1}\\) . Length of path: number of edges on the path. Ancestor of a node: all the nodes along the path from the node up to the root. Descendants of a node: all the nodes in its subtrees. Depth of \\(n_k\\) : length of the unique path from the root to \\(n_i\\) . Height of \\(n_k\\) : length of the longest path from \\(n_i\\) to a leaf. Height / Depth of a tree: the height of the root or the maximum depth among all leaves. Representation \u00b6 Consider the following tree. List Representation \u00b6 Array Representation Linked List Representation FirstChild-NextSibling Representation \u00b6 This representation is not unique with different FirstChild chosen. Binary Tree \u00b6 Definition A binary tree is a tree in which no node can have more than two children. Since it has the same structure of FirstChild-NextSibling Representation, thus all trees can be represented by binary trees . Property In a binary tree, left child and right child are different. The maximum number of nodes of level \\(i\\) is \\(2^{i - 1}\\) . The maximum number of nodes in a binary tree of depth \\(k\\) is \\(2^k - 1\\) . \\(n_0 = n_2 + 1\\) , where \\(n_0\\) is the number of leaf nodes and \\(n_2\\) is the number of nodes of degreee 2. Data Type typedef int ElementType ; typedef struct TreeNode { ElementType element ; struct TreeNode * left ; struct TreeNode * right ; } Tree ; Traversals \u00b6 There are four traversals for a binary tree, preorder traversal , inorder traversal , postorder traversal and levelorder traversal (or in graph, breadth first search, BFS ). Pre-, in-, and postorder traversals are implemented by recursion, or alternatively by a stack with iteration. Levelorder traversal is implemented by a queue. Tip For a general tree, it also has Preorder Traversal: first itself, then sons. Postorder Traversal: first sons, then itself. Levelorder Traversal. Preorder Traversal \u524d\u5e8f\u904d\u5386 Inorder Traversal \u4e2d\u5e8f\u904d\u5386 Postorder Traversal \u540e\u5e8f\u904d\u5386 void Preorder ( Tree * tree ) { if ( tree == NULL ) { return ; } printf ( \"%d \" , tree -> element ); Preorder ( tree -> left ); Preorder ( tree -> right ); } void Inorder ( Tree * tree ) { if ( tree == NULL ) { return ; } Inorder ( tree -> left ); printf ( \"%d \" , tree -> element ); Inorder ( tree -> right ); } void Postorder ( Tree * tree ) { if ( tree == NULL ) { return ; } Postorder ( tree -> left ); Postorder ( tree -> right ); printf ( \"%d \" , tree -> element ); } Levelorder Traversal \u5c42\u5e8f\u904d\u5386 / \u5bbd\u5ea6\u4f18\u5148\u904d\u5386 void Levelorder ( Tree * tree ) { Enqueue ( queue , tree ); while ( ! IsEmptyQueue ( queue )) { Tree * cur = Dequeue ( queue ); printf ( \"%d \" , cur -> element ); Enqueue ( queue , cur -> left ); Enqueue ( queue , cur -> right ); } } Threaded Binary Trees \u00b6 There are \\(n + 1\\) NULL children of the leaf nodes of a binarty tree with \\(n\\) nodes. We want to make these NULL pointers more useful. And here comes the Threaded Binary Trees . Data Type typedef int ElementType ; typedef struct ThreadedTreeNode { ElementType element ; struct ThreadedTreeNode * left ; struct ThreadedTreeNode * right ; bool leftThread ; // `true` to indicate `left` is a thread instead of a child pointer. bool rightThread ; // `true` to indicate `right` is a thread instead of a child pointer. } ThreadedTree ; Rule for Threaded Binary Trees NOTE: the following \" inorder \"\" can also be replaced by any traversals . Rule 1: If tree->left == NULL , then replace it with a pointer to its inorder predecessor . Rule 2: If tree->right == NULL , then replace it with a pointer to its inorder sucessor . Rule 3: A threaded binary tree must have a head node of which the left child points to the first node. Example The following is the infix syntax tree of \\[ A + B * C / D. \\] Binary Search Tree (BST) \u00b6 Definition A binary search tree is a binary tree . It may be empty. If it is not empty, it satisfies the following properties: Every node has a key which is an integer, and the keys are distinct . The keys in a nonempty left subtree must be smaller than the key in the root of the subtree. The keys in a nonempty right subtree must be larger than the key in the root of the subtree. The left and right subtrees are also binary search trees. ADT Objects: A finite ordered list with zero or more elements. Operations: Make an empty BST. Find a key in a BST. Find the minimum key in a BST. Find the maximum key in a BST. Insert a key to a BST. Delete a key in a BST. BST Tree * Find ( Tree * tree , const ElementType element ) { if ( tree == NULL ) { return NULL ; } if ( element == tree -> element ) { return tree ; } return Find ( tree , elemet < tree -> element ? tree -> left : tree -> right ); } Tree * FindMin ( Tree * tree ) { if ( tree == NULL ) { return NULL ; } return tree -> left == NULL ? tree : FindMin ( tree -> left ); } Tree * FindMax ( Tree * tree ) { if ( tree == NULL ) { return NULL ; } return tree -> right == NULL ? tree : FindMax ( tree -> right ); } Tree * Insert ( Tree * tree , const ElementType element ) { if ( tree == NULL ) { tree = ( TreeNode * ) malloc ( sizeof ( TreeNode )); tree -> element = element ; tree -> left = tree -> right = NULL ; } else { if ( element < tree -> element ) { tree -> left = Insert ( tree -> left , element ); } else { tree -> right = Insert ( tree -> right , element ); } } return tree ; } Tree * Delete ( Tree * tree , const ElementType element ) { if ( tree == NULL ) { // Warning: Element not found. return NULL ; } if ( element == tree -> element ) { if ( tree -> left != NULL && tree -> right != NULL ) { TreeNode * temp = FindMin ( tree -> right ); // (1)! tree -> element = temp -> element ; tree -> right = Delete ( tree -> right , temp -> element ); } else { TreeNode * temp = tree ; tree = tree -> left != NULL ? tree -> left : tree -> right ; free ( temp ); } } if ( element < tree -> element ) { tree -> left = Delete ( tree -> left , element ); } else { tree -> right = Delete ( tree -> right , element ); } } Here it's replaced with the smallest node in the right subtree. Or alternatively it can be replaced with the largest node in the left subtree. Time Complexity \u00b6 The average complexities of all operations are \\(O(d)\\) , where \\(d\\) is the depth of the target node. The average depth over all nodes in a tree is \\(O(\\log N)\\) . But for the worst cases, when building a tree by inserting a set of oredered elements, it will degenerate to linear list with \\(O(N)\\) complexity of each operation. AVL Tree \u00b6 Remains Splay Tree \u00b6 Remains B-Tree \u00b6 Remains","title":"Trees"},{"location":"Computer_Science_Courses/FDS/Trees/#trees","text":"Definition A tree is a collection of nodes. The collection can be empty; otherwise, a tree consists of a distinguished node \\(r\\) , called the root . zero or more nonempty subtrees \\(T_1, \\dots, T_k\\) , each of whose roots are connected by a directed edge from \\(r\\) . Definition Degree of a node: number of subtrees of the node. Degree of a tree: maximum among the degrees of all nodes. Parent: a node that has subtrees. Children: the roots of the subtrees of parent. Siblings: children of the same parent. Leaf (terminal node): a node with degree \\(0\\) (no children). Path from \\(n_1\\) to \\(n_k\\) : a unique sequence of nodes \\(n_1,n_2,\\dots,n_k\\) such that \\(n_i\\) is the parent of \\(n_{i+1}\\) . Length of path: number of edges on the path. Ancestor of a node: all the nodes along the path from the node up to the root. Descendants of a node: all the nodes in its subtrees. Depth of \\(n_k\\) : length of the unique path from the root to \\(n_i\\) . Height of \\(n_k\\) : length of the longest path from \\(n_i\\) to a leaf. Height / Depth of a tree: the height of the root or the maximum depth among all leaves.","title":"Trees"},{"location":"Computer_Science_Courses/FDS/Trees/#representation","text":"Consider the following tree.","title":"Representation"},{"location":"Computer_Science_Courses/FDS/Trees/#list-representation","text":"Array Representation Linked List Representation","title":"List Representation"},{"location":"Computer_Science_Courses/FDS/Trees/#firstchild-nextsibling-representation","text":"This representation is not unique with different FirstChild chosen.","title":"FirstChild-NextSibling Representation"},{"location":"Computer_Science_Courses/FDS/Trees/#binary-tree","text":"Definition A binary tree is a tree in which no node can have more than two children. Since it has the same structure of FirstChild-NextSibling Representation, thus all trees can be represented by binary trees . Property In a binary tree, left child and right child are different. The maximum number of nodes of level \\(i\\) is \\(2^{i - 1}\\) . The maximum number of nodes in a binary tree of depth \\(k\\) is \\(2^k - 1\\) . \\(n_0 = n_2 + 1\\) , where \\(n_0\\) is the number of leaf nodes and \\(n_2\\) is the number of nodes of degreee 2. Data Type typedef int ElementType ; typedef struct TreeNode { ElementType element ; struct TreeNode * left ; struct TreeNode * right ; } Tree ;","title":"Binary Tree"},{"location":"Computer_Science_Courses/FDS/Trees/#traversals","text":"There are four traversals for a binary tree, preorder traversal , inorder traversal , postorder traversal and levelorder traversal (or in graph, breadth first search, BFS ). Pre-, in-, and postorder traversals are implemented by recursion, or alternatively by a stack with iteration. Levelorder traversal is implemented by a queue. Tip For a general tree, it also has Preorder Traversal: first itself, then sons. Postorder Traversal: first sons, then itself. Levelorder Traversal. Preorder Traversal \u524d\u5e8f\u904d\u5386 Inorder Traversal \u4e2d\u5e8f\u904d\u5386 Postorder Traversal \u540e\u5e8f\u904d\u5386 void Preorder ( Tree * tree ) { if ( tree == NULL ) { return ; } printf ( \"%d \" , tree -> element ); Preorder ( tree -> left ); Preorder ( tree -> right ); } void Inorder ( Tree * tree ) { if ( tree == NULL ) { return ; } Inorder ( tree -> left ); printf ( \"%d \" , tree -> element ); Inorder ( tree -> right ); } void Postorder ( Tree * tree ) { if ( tree == NULL ) { return ; } Postorder ( tree -> left ); Postorder ( tree -> right ); printf ( \"%d \" , tree -> element ); } Levelorder Traversal \u5c42\u5e8f\u904d\u5386 / \u5bbd\u5ea6\u4f18\u5148\u904d\u5386 void Levelorder ( Tree * tree ) { Enqueue ( queue , tree ); while ( ! IsEmptyQueue ( queue )) { Tree * cur = Dequeue ( queue ); printf ( \"%d \" , cur -> element ); Enqueue ( queue , cur -> left ); Enqueue ( queue , cur -> right ); } }","title":"Traversals"},{"location":"Computer_Science_Courses/FDS/Trees/#threaded-binary-trees","text":"There are \\(n + 1\\) NULL children of the leaf nodes of a binarty tree with \\(n\\) nodes. We want to make these NULL pointers more useful. And here comes the Threaded Binary Trees . Data Type typedef int ElementType ; typedef struct ThreadedTreeNode { ElementType element ; struct ThreadedTreeNode * left ; struct ThreadedTreeNode * right ; bool leftThread ; // `true` to indicate `left` is a thread instead of a child pointer. bool rightThread ; // `true` to indicate `right` is a thread instead of a child pointer. } ThreadedTree ; Rule for Threaded Binary Trees NOTE: the following \" inorder \"\" can also be replaced by any traversals . Rule 1: If tree->left == NULL , then replace it with a pointer to its inorder predecessor . Rule 2: If tree->right == NULL , then replace it with a pointer to its inorder sucessor . Rule 3: A threaded binary tree must have a head node of which the left child points to the first node. Example The following is the infix syntax tree of \\[ A + B * C / D. \\]","title":"Threaded Binary Trees"},{"location":"Computer_Science_Courses/FDS/Trees/#binary-search-tree-bst","text":"Definition A binary search tree is a binary tree . It may be empty. If it is not empty, it satisfies the following properties: Every node has a key which is an integer, and the keys are distinct . The keys in a nonempty left subtree must be smaller than the key in the root of the subtree. The keys in a nonempty right subtree must be larger than the key in the root of the subtree. The left and right subtrees are also binary search trees. ADT Objects: A finite ordered list with zero or more elements. Operations: Make an empty BST. Find a key in a BST. Find the minimum key in a BST. Find the maximum key in a BST. Insert a key to a BST. Delete a key in a BST. BST Tree * Find ( Tree * tree , const ElementType element ) { if ( tree == NULL ) { return NULL ; } if ( element == tree -> element ) { return tree ; } return Find ( tree , elemet < tree -> element ? tree -> left : tree -> right ); } Tree * FindMin ( Tree * tree ) { if ( tree == NULL ) { return NULL ; } return tree -> left == NULL ? tree : FindMin ( tree -> left ); } Tree * FindMax ( Tree * tree ) { if ( tree == NULL ) { return NULL ; } return tree -> right == NULL ? tree : FindMax ( tree -> right ); } Tree * Insert ( Tree * tree , const ElementType element ) { if ( tree == NULL ) { tree = ( TreeNode * ) malloc ( sizeof ( TreeNode )); tree -> element = element ; tree -> left = tree -> right = NULL ; } else { if ( element < tree -> element ) { tree -> left = Insert ( tree -> left , element ); } else { tree -> right = Insert ( tree -> right , element ); } } return tree ; } Tree * Delete ( Tree * tree , const ElementType element ) { if ( tree == NULL ) { // Warning: Element not found. return NULL ; } if ( element == tree -> element ) { if ( tree -> left != NULL && tree -> right != NULL ) { TreeNode * temp = FindMin ( tree -> right ); // (1)! tree -> element = temp -> element ; tree -> right = Delete ( tree -> right , temp -> element ); } else { TreeNode * temp = tree ; tree = tree -> left != NULL ? tree -> left : tree -> right ; free ( temp ); } } if ( element < tree -> element ) { tree -> left = Delete ( tree -> left , element ); } else { tree -> right = Delete ( tree -> right , element ); } } Here it's replaced with the smallest node in the right subtree. Or alternatively it can be replaced with the largest node in the left subtree.","title":"Binary Search Tree (BST)"},{"location":"Computer_Science_Courses/FDS/Trees/#time-complexity","text":"The average complexities of all operations are \\(O(d)\\) , where \\(d\\) is the depth of the target node. The average depth over all nodes in a tree is \\(O(\\log N)\\) . But for the worst cases, when building a tree by inserting a set of oredered elements, it will degenerate to linear list with \\(O(N)\\) complexity of each operation.","title":"Time Complexity"},{"location":"Computer_Science_Courses/FDS/Trees/#avl-tree","text":"Remains","title":"AVL Tree"},{"location":"Computer_Science_Courses/FDS/Trees/#splay-tree","text":"Remains","title":"Splay Tree"},{"location":"Computer_Science_Courses/FDS/Trees/#b-tree","text":"Remains","title":"B-Tree"},{"location":"Computer_Science_Courses/FDS/class_notes/","text":"Class Notes \u00b6 Lecturer \u5e72\u7ea2\u534e Lecture Grade (100 %) \u00b6 Laboratory Projects (25/30%) 3 labs. Take Programming Ability Test in the first week. Top 30% can choose to take the HARD MODE. Peer Review (PR) Quizzes (10%) problems chosen from HW Homework (10%) Mid-Term Exam (15%) Final exam (40%) Code of Academic Honesty \u00b6 To attend the final exam, it's neccessary to pass a test called Code of Academic Honesty. \u5982\u4f55\u770b\u5f85\u6d59\u6c5f\u5927\u5b66\u6570\u636e\u7ed3\u6784\u8bfe\u4e0a\u5de5\u9ad8\u73ed\u4e32\u901a\u821e\u5f0a\u7684\u884c\u4e3a\uff1f","title":"Class Notes"},{"location":"Computer_Science_Courses/FDS/class_notes/#class-notes","text":"Lecturer \u5e72\u7ea2\u534e","title":"Class Notes"},{"location":"Computer_Science_Courses/FDS/class_notes/#lecture-grade-100","text":"Laboratory Projects (25/30%) 3 labs. Take Programming Ability Test in the first week. Top 30% can choose to take the HARD MODE. Peer Review (PR) Quizzes (10%) problems chosen from HW Homework (10%) Mid-Term Exam (15%) Final exam (40%)","title":"Lecture Grade (100 %)"},{"location":"Computer_Science_Courses/FDS/class_notes/#code-of-academic-honesty","text":"To attend the final exam, it's neccessary to pass a test called Code of Academic Honesty. \u5982\u4f55\u770b\u5f85\u6d59\u6c5f\u5927\u5b66\u6570\u636e\u7ed3\u6784\u8bfe\u4e0a\u5de5\u9ad8\u73ed\u4e32\u901a\u821e\u5f0a\u7684\u884c\u4e3a\uff1f","title":"Code of Academic Honesty"},{"location":"Computer_Science_Courses/ICV/10_Recognition/","text":"","title":"Lecture 10"},{"location":"Computer_Science_Courses/ICV/11_3D_Deep_Learning/","text":"","title":"Lecture 11"},{"location":"Computer_Science_Courses/ICV/12_13_Computational_Photography/","text":"","title":"Lecture 12/13"},{"location":"Computer_Science_Courses/ICV/1_Introduction/","text":"Lecture 1 Introduction \u00b6 Review of Linear Algrebra \u00b6 Affine Transformations \u4eff\u5c04\u53d8\u6362 \u00b6 Affine map = linear map + translation \\[ \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix} \\] Using homogenous coordinates \uff08\u9f50\u6b21\u5750\u6807\uff09 \\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a & b & t_x \\\\ c & d & t_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\] Eigenvectors and Eigenvalues \u00b6 The eigenvalues of symmetric matrices are real numbers. The eigenvalues of positive definite matrices are positive numbers.","title":"Lecture 1"},{"location":"Computer_Science_Courses/ICV/1_Introduction/#lecture-1-introduction","text":"","title":"Lecture 1 Introduction"},{"location":"Computer_Science_Courses/ICV/1_Introduction/#review-of-linear-algrebra","text":"","title":"Review of Linear Algrebra"},{"location":"Computer_Science_Courses/ICV/1_Introduction/#affine-transformations","text":"Affine map = linear map + translation \\[ \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix} \\] Using homogenous coordinates \uff08\u9f50\u6b21\u5750\u6807\uff09 \\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a & b & t_x \\\\ c & d & t_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\]","title":"Affine Transformations \u4eff\u5c04\u53d8\u6362"},{"location":"Computer_Science_Courses/ICV/1_Introduction/#eigenvectors-and-eigenvalues","text":"The eigenvalues of symmetric matrices are real numbers. The eigenvalues of positive definite matrices are positive numbers.","title":"Eigenvectors and Eigenvalues"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/","text":"Lecture 2 Image Formation \u00b6 Camera and Lens \u00b6 Pinhole Camera \u00b6 A barrier to block off most of the rays. The opening is the aperture \u5149\u5708 . Flaws Less light gets through. Diffraction effects (\u884d\u5c04). Lens \u900f\u955c \u00b6 \\[ \\frac{1}{i} + \\frac{1}{o} = \\frac{1}{f}. \\] Focal length \\(f\\) \u7126\u8ddd \u00b6 If \\(o = \\infty\\) , then \\(f = i\\) . Magnification \\(m\\) \u653e\u5927\u7387 \u00b6 \\[ m = \\frac{h_i}{h_o}. \\] Field of View (FOV) \u89c6\u91ce \u00b6 Factor Focal Length. Longer focal length, Narrower angle of view. Vice versa. Note 50mm / 46\u00b0 (and full frame) is the most similar FOV with human eyes. Thus 50mm lens is called standard lens (\u6807\u51c6\u955c\u5934). telephoto lens (\u957f\u7126\u955c\u5934\uff0c\u671b\u8fdc\u955c\u5934): \u89c6\u91ce\u5c0f, \u653e\u5927\u7387\u5927. short focal lens (\u77ed\u7126\u955c\u5934\uff0c\u5e7f\u89d2\u955c\u5934); \u89c6\u91ce\u5927, \u653e\u5927\u7387\u5c0f. Sensor Size Bigger sensor size, Wider angle of view. Vice versa. Aperture \u5149\u5708 \u00b6 The representation of aperture is its Diameter \\(D\\) . F-Number \\[ N = \\frac{f}{D} \\text{ (mostly greater than 1, around 1.8 \\sim 22)}. \\] Lens Defocus \u00b6 Blur Circle Diameter (\u5149\u6591\u534a\u5f84) \\[ b = \\frac{D}{i'}|i' -i|, b \\propto D \\propto \\frac{1}{N} \\] Focusing \u5bf9\u7126 \u00b6 Depth of Field (DoF) \u666f\u6df1 \u00b6 \\[ {\\tt DoF} = o_2 - o_1 = \\frac{2of^2cN(o-f)}{f^4-c^2N^2(o-f)^2}. \\] From the equation above, we can find that DoF is almost proportional to \\(N\\) , and thus the Larger aperture, the Smaller F-Number and the Smaller DoF. How to blur the background \u00b6 Large aperture. Long focal length. Near foreground. Far background. Geometric Image Formation\uff08\u5b9a\u4f4d\uff09 \u00b6 Camera model \\[ \\begin{bmatrix} u \\\\ v \\end{bmatrix} = \\begin{bmatrix} f \\frac{x}{z} \\\\ f \\frac{y}{z} \\end{bmatrix}. \\] Homogeneous Coordinates / Projective Coordinates \u00b6 Suppose that \\(\\begin{bmatrix} x \\\\ y \\\\ w \\end{bmatrix}\\) is the same as \\(\\begin{bmatrix} x/w \\\\ y/w \\\\ 1 \\end{bmatrix}\\) , then we get \\[ \\begin{bmatrix} f & 0 & 0 & 0 \\\\ 0 & f & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} fx \\\\ fy \\\\ z \\\\ \\end{bmatrix} \\cong \\begin{bmatrix} f\\frac{x}{z} \\\\ f\\frac{y}{z} \\\\ 1 \\end{bmatrix}. \\] We can also put the image plane in front of the camera (opposite to the previous picture). Perspective Projection \u00b6 Preserevd - Straight lines are still straight Lost - Length and Angle Vanishing Points \u00b6 Properties Any wo parallel lines have the same vanishing point v. v can be outside the image frame or at infinity. Line from C to v is parallel to the lines. Vanishing Lines \u00b6 Multiple vanishing points The direction of the vanishing line tells us the orientation of the plane. Distortion \u00b6 Converging verticals Problem and Solution (View Camera \u79fb\u8f74\u76f8\u673a) Exterior columns appear bigger Due to lens flaws Radial distortion Due to imperfect lens \\[ \\begin{aligned} r^2 &= {x'}_n^{2} + {y'}_n^{2} \\\\ {x'}_d &= {x'}_n(1 + \\kappa_1r^2 + \\kappa_2r^4) \\\\ {y'}_d &= {y'}_n(1 + \\kappa_1r^2 + \\kappa_2r^4) \\\\ \\end{aligned} \\] Solution Take a photo of a grid at the same point and then use the mathmatics to calculate and correct radial distortion. Orthographic Projection \u00b6 \\[ \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\\\ 1 \\\\ \\end{bmatrix} \\Rightarrow (x, y) \\] Photometric Image Formation\uff08\u5b9a\u989c\u8272/\u4eae\u5ea6\uff09 \u00b6 Image Sensor \u00b6 CMOS CCD (Charge Coupled Device) Shutter \u00b6 Shutter speed controls exposure time. Color Sensing \u00b6 Color Spaces RGB HSV Practical Color Sensing: Bayer Filter Shading \u7740\u8272 \u00b6 BRDF (Bidirectional Reflectance Distribution Function) \u00b6 \\[ L_r(\\hat{\\textbf{v}_r};\\lambda) = \\int L_i(\\hat{\\textbf{i}_r};\\lambda)f_r(\\hat{\\textbf{v}_r}, \\hat{\\textbf{v}_i}, \\hat{\\textbf{n}}; \\lambda)\\cos^+\\theta_i\\ d\\hat{\\textbf{v}_i} \\] Diffuse (Lambertian) Reflection \u00b6 Shading independent of view direction Specular Term \u00b6 Intensity depends on view direction Blinn-Phong Reflection Model \u00b6 \\[ L = L_a + L_d + L_s = k_aI_a + k_d(I/r^2)\\max(0, \\textbf{n}\\cdot\\textbf{l}) + k_s(I/r^2)\\max(0, \\textbf{n}\\cdot\\textbf{h})^p \\]","title":"Lecture 2"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#lecture-2-image-formation","text":"","title":"Lecture 2 Image Formation"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#camera-and-lens","text":"","title":"Camera and Lens"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#pinhole-camera","text":"A barrier to block off most of the rays. The opening is the aperture \u5149\u5708 . Flaws Less light gets through. Diffraction effects (\u884d\u5c04).","title":"Pinhole Camera"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#lens","text":"\\[ \\frac{1}{i} + \\frac{1}{o} = \\frac{1}{f}. \\]","title":"Lens \u900f\u955c"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#focal-length-f","text":"If \\(o = \\infty\\) , then \\(f = i\\) .","title":"Focal length \\(f\\)  \u7126\u8ddd"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#magnification-m","text":"\\[ m = \\frac{h_i}{h_o}. \\]","title":"Magnification \\(m\\) \u653e\u5927\u7387"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#field-of-view-fov","text":"Factor Focal Length. Longer focal length, Narrower angle of view. Vice versa. Note 50mm / 46\u00b0 (and full frame) is the most similar FOV with human eyes. Thus 50mm lens is called standard lens (\u6807\u51c6\u955c\u5934). telephoto lens (\u957f\u7126\u955c\u5934\uff0c\u671b\u8fdc\u955c\u5934): \u89c6\u91ce\u5c0f, \u653e\u5927\u7387\u5927. short focal lens (\u77ed\u7126\u955c\u5934\uff0c\u5e7f\u89d2\u955c\u5934); \u89c6\u91ce\u5927, \u653e\u5927\u7387\u5c0f. Sensor Size Bigger sensor size, Wider angle of view. Vice versa.","title":"Field of View (FOV) \u89c6\u91ce"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#aperture","text":"The representation of aperture is its Diameter \\(D\\) . F-Number \\[ N = \\frac{f}{D} \\text{ (mostly greater than 1, around 1.8 \\sim 22)}. \\]","title":"Aperture \u5149\u5708"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#lens-defocus","text":"Blur Circle Diameter (\u5149\u6591\u534a\u5f84) \\[ b = \\frac{D}{i'}|i' -i|, b \\propto D \\propto \\frac{1}{N} \\]","title":"Lens Defocus"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#focusing","text":"","title":"Focusing \u5bf9\u7126"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#depth-of-field-dof","text":"\\[ {\\tt DoF} = o_2 - o_1 = \\frac{2of^2cN(o-f)}{f^4-c^2N^2(o-f)^2}. \\] From the equation above, we can find that DoF is almost proportional to \\(N\\) , and thus the Larger aperture, the Smaller F-Number and the Smaller DoF.","title":"Depth of Field (DoF) \u666f\u6df1"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#how-to-blur-the-background","text":"Large aperture. Long focal length. Near foreground. Far background.","title":"How to blur the background"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#geometric-image-formation","text":"Camera model \\[ \\begin{bmatrix} u \\\\ v \\end{bmatrix} = \\begin{bmatrix} f \\frac{x}{z} \\\\ f \\frac{y}{z} \\end{bmatrix}. \\]","title":"Geometric Image Formation\uff08\u5b9a\u4f4d\uff09"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#homogeneous-coordinates-projective-coordinates","text":"Suppose that \\(\\begin{bmatrix} x \\\\ y \\\\ w \\end{bmatrix}\\) is the same as \\(\\begin{bmatrix} x/w \\\\ y/w \\\\ 1 \\end{bmatrix}\\) , then we get \\[ \\begin{bmatrix} f & 0 & 0 & 0 \\\\ 0 & f & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} fx \\\\ fy \\\\ z \\\\ \\end{bmatrix} \\cong \\begin{bmatrix} f\\frac{x}{z} \\\\ f\\frac{y}{z} \\\\ 1 \\end{bmatrix}. \\] We can also put the image plane in front of the camera (opposite to the previous picture).","title":"Homogeneous Coordinates / Projective Coordinates"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#perspective-projection","text":"Preserevd - Straight lines are still straight Lost - Length and Angle","title":"Perspective Projection"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#vanishing-points","text":"Properties Any wo parallel lines have the same vanishing point v. v can be outside the image frame or at infinity. Line from C to v is parallel to the lines.","title":"Vanishing Points"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#vanishing-lines","text":"Multiple vanishing points The direction of the vanishing line tells us the orientation of the plane.","title":"Vanishing Lines"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#distortion","text":"Converging verticals Problem and Solution (View Camera \u79fb\u8f74\u76f8\u673a) Exterior columns appear bigger Due to lens flaws Radial distortion Due to imperfect lens \\[ \\begin{aligned} r^2 &= {x'}_n^{2} + {y'}_n^{2} \\\\ {x'}_d &= {x'}_n(1 + \\kappa_1r^2 + \\kappa_2r^4) \\\\ {y'}_d &= {y'}_n(1 + \\kappa_1r^2 + \\kappa_2r^4) \\\\ \\end{aligned} \\] Solution Take a photo of a grid at the same point and then use the mathmatics to calculate and correct radial distortion.","title":"Distortion"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#orthographic-projection","text":"\\[ \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\\\ 1 \\\\ \\end{bmatrix} \\Rightarrow (x, y) \\]","title":"Orthographic Projection"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#photometric-image-formation","text":"","title":"Photometric Image Formation\uff08\u5b9a\u989c\u8272/\u4eae\u5ea6\uff09"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#image-sensor","text":"CMOS CCD (Charge Coupled Device)","title":"Image Sensor"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#shutter","text":"Shutter speed controls exposure time.","title":"Shutter"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#color-sensing","text":"Color Spaces RGB HSV Practical Color Sensing: Bayer Filter","title":"Color Sensing"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#shading","text":"","title":"Shading \u7740\u8272"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#brdf-bidirectional-reflectance-distribution-function","text":"\\[ L_r(\\hat{\\textbf{v}_r};\\lambda) = \\int L_i(\\hat{\\textbf{i}_r};\\lambda)f_r(\\hat{\\textbf{v}_r}, \\hat{\\textbf{v}_i}, \\hat{\\textbf{n}}; \\lambda)\\cos^+\\theta_i\\ d\\hat{\\textbf{v}_i} \\]","title":"BRDF (Bidirectional Reflectance Distribution Function)"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#diffuse-lambertian-reflection","text":"Shading independent of view direction","title":"Diffuse (Lambertian) Reflection"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#specular-term","text":"Intensity depends on view direction","title":"Specular Term"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#blinn-phong-reflection-model","text":"\\[ L = L_a + L_d + L_s = k_aI_a + k_d(I/r^2)\\max(0, \\textbf{n}\\cdot\\textbf{l}) + k_s(I/r^2)\\max(0, \\textbf{n}\\cdot\\textbf{h})^p \\]","title":"Blinn-Phong Reflection Model"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/","text":"Lecture 3 Imae Processing \u00b6 Image Processing Basis \u00b6 Operations: Incresing contrast, Invert, Blur, Sharpen, Edge Detection Convolution \u00b6 \\[ (f * g)(x) = \\int_{-\\infty}^\\infty f(y)g(x-y)dy \\] discrete 2D form \\[ (f * g)(x) = \\sum_{i, j =-\\infty}^\\infty f(i, j)g(x - i, y - j) \\] Gaussian Blur \u00b6 2D Gaussian function \\[ \\large f(i, j) = \\frac{1}{2\\pi\\sigma^2}e^{-\\frac{i^2 + j^2}{2\\sigma^2}} \\] Usage Define a window size (commmly a square, say \\(n \\times n\\) , and let \\(r = \\lfloor n / 2 \\rfloor\\) ). Select a point, say \\((x, y)\\) and then a window around it. Apply the Gaussian function at each point in the window, and sum them up, namely \\(G(x, y) = \\sum\\limits_{i = x - r}^{x + r}\\sum\\limits_{j = y - r}^{y + r}f(i,j)\\) . Then the \"blurred\" value of point \\((x, y)\\) is \\(G(x, y)\\) . Sharpen \u00b6 An example of kernel matrix \\[ f = \\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 5 & -1 \\\\ 0 & -1 & 0 \\end{bmatrix} \\] An insight Let \\(I\\) be the original image, then the sharpen image \\(I' = I + (I - \\text{blur}(I))\\) , where \\(I - \\text{blur}(I)\\) can be regarded as the high frequency content. Extract Gradients \u00b6 Examples of kernel matrix \\(f = \\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1\\end{bmatrix}\\) extracts horizontal gradients. \\(f = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\) extracts vertical gradients. Bilateral Filters \u00b6 Kernel depends on image content. Better performance but lower efficiency. A trick: Separable Filter A filter is separable if it can be wriiten as the outer product of two other filters. Example \\[ \\frac19 \\begin{bmatrix} 1 & 1 & 1\\\\ 1 & 1 & 1\\\\ 1 & 1 & 1 \\end{bmatrix} = \\frac13 \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\times \\frac13 \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix} \\] Purpose / Advantages : speed up the calculation Image Sampling \u00b6 Abstract Down-sampling \u2192 Reducing image size Aliasing \u00b6 Aliasing is the artifacts due to sampling Higher frequencies need faster sampling. Undersampling creates frequency aliases. Fourier Transform \u00b6 Simly put, fourier transform represents a function as a weighted sum of sines and cosines , where the sines and consines are in various frequencies . Convolution Theorem From the view of Fourier Transform ... \u00b6 Sampling and Aliasing \u00b6 Sampling is repeating frequency contents. Aliasing is mixed frequency contents. Method to reduce aliasing Increase sampling rate Nyquist-Shannon theorem the signal can be perfectly reconstructed if sampled with a frequency larger than \\(2f_0\\) . Anti-aliasing Filtering, then sampling Example Image Magnification \u00b6 Abstract Up-sampling - Inverse of down-sampling An important method: Interpolation . 1D Interpolation \u00b6 Nearest-neighbour Interpolation Not continuous; Not smooth Linear Interpolation Continuous; Not smooth Cubic Interpolation Continuous; Smooth 2D Interpolation \u00b6 (similar to 1D cases) Nearest-neighbour Interpolation Bilinear Interpolation define \\(\\text{lerp}(x, v_0,v_1) = v_0 + x(v_1 - v_0)\\) . Suppose the point in the rectangle surrounded by four points \\(u_{00},u_{01},u_{10},u_{11}\\) . then \\(f(x, y) = \\text{lerp}(t, u_0, u_1)\\) , where \\(u_0 = \\text{lerp}(s, u_{00}, u_{10})\\) and \\(u_1 = \\text{lerp}(s, u_{01}, u_{11})\\) . Bicubic Interpolation Super-Resolution \u00b6 Remains Change Aspect Ratio \u00b6 Remains","title":"Lecture 3"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#lecture-3-imae-processing","text":"","title":"Lecture 3 Imae Processing"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#image-processing-basis","text":"Operations: Incresing contrast, Invert, Blur, Sharpen, Edge Detection","title":"Image Processing Basis"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#convolution","text":"\\[ (f * g)(x) = \\int_{-\\infty}^\\infty f(y)g(x-y)dy \\] discrete 2D form \\[ (f * g)(x) = \\sum_{i, j =-\\infty}^\\infty f(i, j)g(x - i, y - j) \\]","title":"Convolution"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#gaussian-blur","text":"2D Gaussian function \\[ \\large f(i, j) = \\frac{1}{2\\pi\\sigma^2}e^{-\\frac{i^2 + j^2}{2\\sigma^2}} \\] Usage Define a window size (commmly a square, say \\(n \\times n\\) , and let \\(r = \\lfloor n / 2 \\rfloor\\) ). Select a point, say \\((x, y)\\) and then a window around it. Apply the Gaussian function at each point in the window, and sum them up, namely \\(G(x, y) = \\sum\\limits_{i = x - r}^{x + r}\\sum\\limits_{j = y - r}^{y + r}f(i,j)\\) . Then the \"blurred\" value of point \\((x, y)\\) is \\(G(x, y)\\) .","title":"Gaussian Blur"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#sharpen","text":"An example of kernel matrix \\[ f = \\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 5 & -1 \\\\ 0 & -1 & 0 \\end{bmatrix} \\] An insight Let \\(I\\) be the original image, then the sharpen image \\(I' = I + (I - \\text{blur}(I))\\) , where \\(I - \\text{blur}(I)\\) can be regarded as the high frequency content.","title":"Sharpen"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#extract-gradients","text":"Examples of kernel matrix \\(f = \\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1\\end{bmatrix}\\) extracts horizontal gradients. \\(f = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\) extracts vertical gradients.","title":"Extract Gradients"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#bilateral-filters","text":"Kernel depends on image content. Better performance but lower efficiency. A trick: Separable Filter A filter is separable if it can be wriiten as the outer product of two other filters. Example \\[ \\frac19 \\begin{bmatrix} 1 & 1 & 1\\\\ 1 & 1 & 1\\\\ 1 & 1 & 1 \\end{bmatrix} = \\frac13 \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\times \\frac13 \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix} \\] Purpose / Advantages : speed up the calculation","title":"Bilateral Filters"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#image-sampling","text":"Abstract Down-sampling \u2192 Reducing image size","title":"Image Sampling"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#aliasing","text":"Aliasing is the artifacts due to sampling Higher frequencies need faster sampling. Undersampling creates frequency aliases.","title":"Aliasing"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#fourier-transform","text":"Simly put, fourier transform represents a function as a weighted sum of sines and cosines , where the sines and consines are in various frequencies . Convolution Theorem","title":"Fourier Transform"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#from-the-view-of-fourier-transform","text":"","title":"From the view of Fourier Transform ..."},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#sampling-and-aliasing","text":"Sampling is repeating frequency contents. Aliasing is mixed frequency contents. Method to reduce aliasing Increase sampling rate Nyquist-Shannon theorem the signal can be perfectly reconstructed if sampled with a frequency larger than \\(2f_0\\) . Anti-aliasing Filtering, then sampling Example","title":"Sampling and Aliasing"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#image-magnification","text":"Abstract Up-sampling - Inverse of down-sampling An important method: Interpolation .","title":"Image Magnification"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#1d-interpolation","text":"Nearest-neighbour Interpolation Not continuous; Not smooth Linear Interpolation Continuous; Not smooth Cubic Interpolation Continuous; Smooth","title":"1D Interpolation"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#2d-interpolation","text":"(similar to 1D cases) Nearest-neighbour Interpolation Bilinear Interpolation define \\(\\text{lerp}(x, v_0,v_1) = v_0 + x(v_1 - v_0)\\) . Suppose the point in the rectangle surrounded by four points \\(u_{00},u_{01},u_{10},u_{11}\\) . then \\(f(x, y) = \\text{lerp}(t, u_0, u_1)\\) , where \\(u_0 = \\text{lerp}(s, u_{00}, u_{10})\\) and \\(u_1 = \\text{lerp}(s, u_{01}, u_{11})\\) . Bicubic Interpolation","title":"2D Interpolation"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#super-resolution","text":"Remains","title":"Super-Resolution"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#change-aspect-ratio","text":"Remains","title":"Change Aspect Ratio"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/","text":"Lecture 4 Model Fitting and Optimization \u00b6 Optimization \u00b6 \\[ \\begin{aligned} & \\text{Minimize } & f_0(x) \\\\ & \\text{Subject to } &f_i(x) & \\le 0,\\ i = 1, \\dots, m \\\\ & & g_i(x) & = 0,\\ i = 1,\\dots, p \\end{aligned} \\] \\(x \\in \\mathbb{R}^n\\) is a vector variable to be chosen. \\(f_0\\) is the objective function to be minimized. \\(f_1, \\dots, f_m\\) are the inequality constraint functions. \\(g_1, \\dots, g_p\\) are the equality constraint functions. Model Fitting \u00b6 A typical approach: Minimize the Mean Square Error ( MSE ) \\[ \\hat x = \\mathop{\\arg\\min}\\limits_x \\sum\\limits_i(b_i - a_i^Tx)^2 \\] Reasons to choose MSE \u00b6 Key Assumptions: MSE = MLE with Gaussian noise From Maximum Likelihood Estimation ( MLE ) , the data is assumed to be with Gaussian noise . \\[ b_i = a_i^Tx + n,\\ n \\sim G(0, \\sigma) \\] The likelihood of observing \\((a_i, b_i)\\) is \\[ P[(a_i, b_i)|x] = P[b_i - a_i^Tx] \\propto \\exp\\left(-\\frac{(b_i - a_i^Tx)^2}{2\\sigma^2}\\right) \\] Note If the data points are independent , \\[ \\begin{aligned} P[(a_1, b_1)(a_2, b_2)\\dots|x] & = \\prod\\limits_iP[(a_i, b_i)|x] = \\prod\\limits_iP[b_i - a_{i^{Tx]}} \\\\ & \\propto \\exp\\left(-\\frac{\\sum_i(b_i - a_i^Tx)^2}{2\\sigma^2}\\right) = \\exp\\left(-\\frac{||Ax-b||^2_2}{2\\sigma^2}\\right) \\end{aligned} \\] \\[ \\begin{aligned} \\hat x &= \\mathop{\\arg\\max}\\limits_x P[(a_1, b_1)(a_2, b_2)\\dots|x] \\\\ &= \\mathop{\\arg\\max}\\limits_x \\exp\\left(-\\frac{||Ax-b||^2_2}{2\\sigma^2}\\right) = \\mathop{\\arg\\min}\\limits_x||Ax - b||_2^2 \\end{aligned} \\] Numerical Methods \u00b6 Analytical Solution \u89e3\u6790\u89e3 \u00b6 The derivative of \\(||Ax-b||^2_2\\) is \\(2A^T(Ax - b)\\) , let it be 0. Then we get \\(\\hat x\\) satisfying \\[ A^TAx = A^Tb \\] But, if no analytical solution ... Approximate Solution \u8fd1\u4f3c\u89e3 \u00b6 Method \\(x \\leftarrow x_0\\) \\(\\text{while not converge}\\) \\(p \\leftarrow \\text{descending_direction(x)}\\) \\(\\alpha \\leftarrow \\text{descending_step(x)}\\) \\(x \\leftarrow x + \\alpha p\\) Gradient Descent (GD) \u00b6 Steepest Descent Method \u00b6 \\[ F(x_0 + \\Delta x) \\approx F(x_0) + J_F\\Delta x \\] Direction \\(\\Delta x = -J^T_F\\) Step To minimize \\(\\phi(a)\\) Backtracking Algorithm Initialize \\(\\alpha\\) with a large value. Decrease \\(\\alpha\\) until \\(\\phi(\\alpha)\\le\\phi(0) + \\gamma\\phi'(0)\\alpha\\) . Advantage Easy to implement Perform well when far from the minimum Disadvantage Converge slowly when near the minimum, which wastes a lot of computation Newton Method \u00b6 \\[ F(x_0 + \\Delta x) \\approx F(x_0) + J_F\\Delta x + \\frac12\\Delta x^TH_F\\Delta x \\] \\(\\Delta x = -H_F^{-1}J^T_F\\) Advantage Faster convergence Disadvantage Hessian matrix requires a lot of computation Gauss-Newton Method \u00b6 For \\(\\hat x = \\mathop{\\arg\\min}\\limits_x ||Ax-b||^2_2 \\overset{\\Delta}{=\\!=}\\mathop{\\arg\\min}\\limits_x||R(x)||^2_2\\) , expand \\(R(x)\\) . \\[ \\begin{aligned} ||R(x_k+\\Delta x)||^2_2 &\\approx ||R(x_k) + J_R\\Delta x||^2_2 \\newline &= ||R(x_k)||^2_2 + 2R(x_k)^TJ_R\\Delta x + \\Delta x^TJ^T_RJ_R\\Delta x \\end{aligned} \\] \\(\\Delta x = -(J_R^TJ_R)^{-1}J_R^TR(x_k) = -(J_R^TJ_R)^{-1}J_F^T\\) Compared to Newton Method, \\(J_R^TJ_R\\) is used to approximate \\(H_F\\) . Advantage No need to compute Hessian matrix Faster to converge Disadvantage \\(J^T_RJ_R\\) may be singular. Levenberg-Marquardt (LM) \u00b6 \\(\\Delta x = -(J_R^TJ_R + \\lambda I)^{-1}J_R^TR(x_k)\\) \\(\\lambda \\rightarrow \\infty\\) Steepest Descent \\(\\lambda \\rightarrow 0\\) Gauss-Newton Advantage Start and converge quickly Local Minimum and Global Minimum Convex Optimization \u00b6 Remains Robust Estimation \u00b6 Inlier obeys the model assumption. Outlier differs significantly rom the assumption. Outlier makes MSE fail. To reduce its effect, we can use other loss functions, called robust functions. RANSAC (Random Sample Concensus) \u00b6 The most powerful method to handle outliers. Key ideas The distribution of inliers is similar while outliers differ a lot. Use data point pairs to vote ill-posed Problem \u75c5\u6001\u95ee\u9898 / \u591a\u89e3\u95ee\u9898 \u00b6 The solution is not unique. To make it unique: L2 regularization Suppress redundant variables \\(\\min_x||Ax-b||^2_2,\\ s.t. ||x||_2 \\le 1\\) (the same as \\(\\min_x||Ax-b||^2_2 + \\lambda||x||_2\\) ) L1 regularization Make \\(x\\) sparse \\(\\min_x||Ax-b||^2_2,\\ s.t. ||x||_1 \\le 1\\) (the same as \\(\\min_x||Ax-b||^2_2 + \\lambda||x||_1\\) ) Graphcut and MRF \u00b6 Note A key idea: Neighboring pixels tend to take the same label. Images as Graphs A vertex for each pixel An edge between each pair, weighted by the affinity or similarity between its two vertices Pixel Dissimilarity \\(S(\\textbf{f}_i, \\textbf{f}_j) = \\sqrt{\\sum_k(f_{ik} - f_{jk})^2}\\) Pixel Affinity \\(w(i, j) = A(\\textbf{f}_i, \\textbf{f}_j) = \\exp\\left(\\frac{-1}{2\\sigma^2}S(\\textbf{f}_i, \\textbf{f}_j)\\right)\\) Graph Notation \\(G = (V, E)\\) \\(V\\) : a set of vertices \\(E\\) : a set of edges Graph Cut \u00b6 Cut \\(C=(V_A, V_B)\\) is a parition of vertices \\(V\\) of a graph \\(G\\) into two disjoint subsets \\(V_A\\) and \\(V_B\\) . Cost of Cut \\(\\text{cut}(V_A, V_B) = \\sum_{u\\in V_A, v\\in V_B} w(u, v)\\) Problem with min-cut \u00b6 Bias to cut small, isolated segments Solution: Normalized cut Compute how strongly verices \\(V_A\\) are associated with vertices \\(V\\) \\[ \\text{assoc}(V_A, V) = \\sum_{u\\in V_A, v\\in V} w(u, v) \\] \\[ \\text{NCut}(V_A, V_B) = \\frac{\\text{cut}(V_A, V_B)}{\\text{assoc}(V_A, V)} + \\frac{\\text{cut}(V_A, V_B)}{\\text{assoc}(V_B, V)} \\] Markow Random Field ( MRF ) \u00b6 Graphcut is an exception of MRF . Question","title":"Lecture 4"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#lecture-4-model-fitting-and-optimization","text":"","title":"Lecture 4 Model Fitting and Optimization"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#optimization","text":"\\[ \\begin{aligned} & \\text{Minimize } & f_0(x) \\\\ & \\text{Subject to } &f_i(x) & \\le 0,\\ i = 1, \\dots, m \\\\ & & g_i(x) & = 0,\\ i = 1,\\dots, p \\end{aligned} \\] \\(x \\in \\mathbb{R}^n\\) is a vector variable to be chosen. \\(f_0\\) is the objective function to be minimized. \\(f_1, \\dots, f_m\\) are the inequality constraint functions. \\(g_1, \\dots, g_p\\) are the equality constraint functions.","title":"Optimization"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#model-fitting","text":"A typical approach: Minimize the Mean Square Error ( MSE ) \\[ \\hat x = \\mathop{\\arg\\min}\\limits_x \\sum\\limits_i(b_i - a_i^Tx)^2 \\]","title":"Model Fitting"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#reasons-to-choose-mse","text":"Key Assumptions: MSE = MLE with Gaussian noise From Maximum Likelihood Estimation ( MLE ) , the data is assumed to be with Gaussian noise . \\[ b_i = a_i^Tx + n,\\ n \\sim G(0, \\sigma) \\] The likelihood of observing \\((a_i, b_i)\\) is \\[ P[(a_i, b_i)|x] = P[b_i - a_i^Tx] \\propto \\exp\\left(-\\frac{(b_i - a_i^Tx)^2}{2\\sigma^2}\\right) \\] Note If the data points are independent , \\[ \\begin{aligned} P[(a_1, b_1)(a_2, b_2)\\dots|x] & = \\prod\\limits_iP[(a_i, b_i)|x] = \\prod\\limits_iP[b_i - a_{i^{Tx]}} \\\\ & \\propto \\exp\\left(-\\frac{\\sum_i(b_i - a_i^Tx)^2}{2\\sigma^2}\\right) = \\exp\\left(-\\frac{||Ax-b||^2_2}{2\\sigma^2}\\right) \\end{aligned} \\] \\[ \\begin{aligned} \\hat x &= \\mathop{\\arg\\max}\\limits_x P[(a_1, b_1)(a_2, b_2)\\dots|x] \\\\ &= \\mathop{\\arg\\max}\\limits_x \\exp\\left(-\\frac{||Ax-b||^2_2}{2\\sigma^2}\\right) = \\mathop{\\arg\\min}\\limits_x||Ax - b||_2^2 \\end{aligned} \\]","title":"Reasons to choose MSE"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#numerical-methods","text":"","title":"Numerical Methods"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#analytical-solution","text":"The derivative of \\(||Ax-b||^2_2\\) is \\(2A^T(Ax - b)\\) , let it be 0. Then we get \\(\\hat x\\) satisfying \\[ A^TAx = A^Tb \\] But, if no analytical solution ...","title":"Analytical Solution \u89e3\u6790\u89e3"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#approximate-solution","text":"Method \\(x \\leftarrow x_0\\) \\(\\text{while not converge}\\) \\(p \\leftarrow \\text{descending_direction(x)}\\) \\(\\alpha \\leftarrow \\text{descending_step(x)}\\) \\(x \\leftarrow x + \\alpha p\\)","title":"Approximate Solution \u8fd1\u4f3c\u89e3"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#gradient-descent-gd","text":"","title":"Gradient Descent (GD)"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#steepest-descent-method","text":"\\[ F(x_0 + \\Delta x) \\approx F(x_0) + J_F\\Delta x \\] Direction \\(\\Delta x = -J^T_F\\) Step To minimize \\(\\phi(a)\\) Backtracking Algorithm Initialize \\(\\alpha\\) with a large value. Decrease \\(\\alpha\\) until \\(\\phi(\\alpha)\\le\\phi(0) + \\gamma\\phi'(0)\\alpha\\) . Advantage Easy to implement Perform well when far from the minimum Disadvantage Converge slowly when near the minimum, which wastes a lot of computation","title":"Steepest Descent Method"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#newton-method","text":"\\[ F(x_0 + \\Delta x) \\approx F(x_0) + J_F\\Delta x + \\frac12\\Delta x^TH_F\\Delta x \\] \\(\\Delta x = -H_F^{-1}J^T_F\\) Advantage Faster convergence Disadvantage Hessian matrix requires a lot of computation","title":"Newton Method"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#gauss-newton-method","text":"For \\(\\hat x = \\mathop{\\arg\\min}\\limits_x ||Ax-b||^2_2 \\overset{\\Delta}{=\\!=}\\mathop{\\arg\\min}\\limits_x||R(x)||^2_2\\) , expand \\(R(x)\\) . \\[ \\begin{aligned} ||R(x_k+\\Delta x)||^2_2 &\\approx ||R(x_k) + J_R\\Delta x||^2_2 \\newline &= ||R(x_k)||^2_2 + 2R(x_k)^TJ_R\\Delta x + \\Delta x^TJ^T_RJ_R\\Delta x \\end{aligned} \\] \\(\\Delta x = -(J_R^TJ_R)^{-1}J_R^TR(x_k) = -(J_R^TJ_R)^{-1}J_F^T\\) Compared to Newton Method, \\(J_R^TJ_R\\) is used to approximate \\(H_F\\) . Advantage No need to compute Hessian matrix Faster to converge Disadvantage \\(J^T_RJ_R\\) may be singular.","title":"Gauss-Newton Method"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#levenberg-marquardt-lm","text":"\\(\\Delta x = -(J_R^TJ_R + \\lambda I)^{-1}J_R^TR(x_k)\\) \\(\\lambda \\rightarrow \\infty\\) Steepest Descent \\(\\lambda \\rightarrow 0\\) Gauss-Newton Advantage Start and converge quickly Local Minimum and Global Minimum","title":"Levenberg-Marquardt (LM)"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#convex-optimization","text":"Remains","title":"Convex Optimization"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#robust-estimation","text":"Inlier obeys the model assumption. Outlier differs significantly rom the assumption. Outlier makes MSE fail. To reduce its effect, we can use other loss functions, called robust functions.","title":"Robust Estimation"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#ransac-random-sample-concensus","text":"The most powerful method to handle outliers. Key ideas The distribution of inliers is similar while outliers differ a lot. Use data point pairs to vote","title":"RANSAC (Random Sample Concensus)"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#ill-posed-problem","text":"The solution is not unique. To make it unique: L2 regularization Suppress redundant variables \\(\\min_x||Ax-b||^2_2,\\ s.t. ||x||_2 \\le 1\\) (the same as \\(\\min_x||Ax-b||^2_2 + \\lambda||x||_2\\) ) L1 regularization Make \\(x\\) sparse \\(\\min_x||Ax-b||^2_2,\\ s.t. ||x||_1 \\le 1\\) (the same as \\(\\min_x||Ax-b||^2_2 + \\lambda||x||_1\\) )","title":"ill-posed Problem \u75c5\u6001\u95ee\u9898 / \u591a\u89e3\u95ee\u9898"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#graphcut-and-mrf","text":"Note A key idea: Neighboring pixels tend to take the same label. Images as Graphs A vertex for each pixel An edge between each pair, weighted by the affinity or similarity between its two vertices Pixel Dissimilarity \\(S(\\textbf{f}_i, \\textbf{f}_j) = \\sqrt{\\sum_k(f_{ik} - f_{jk})^2}\\) Pixel Affinity \\(w(i, j) = A(\\textbf{f}_i, \\textbf{f}_j) = \\exp\\left(\\frac{-1}{2\\sigma^2}S(\\textbf{f}_i, \\textbf{f}_j)\\right)\\) Graph Notation \\(G = (V, E)\\) \\(V\\) : a set of vertices \\(E\\) : a set of edges","title":"Graphcut and MRF"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#graph-cut","text":"Cut \\(C=(V_A, V_B)\\) is a parition of vertices \\(V\\) of a graph \\(G\\) into two disjoint subsets \\(V_A\\) and \\(V_B\\) . Cost of Cut \\(\\text{cut}(V_A, V_B) = \\sum_{u\\in V_A, v\\in V_B} w(u, v)\\)","title":"Graph Cut"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#problem-with-min-cut","text":"Bias to cut small, isolated segments Solution: Normalized cut Compute how strongly verices \\(V_A\\) are associated with vertices \\(V\\) \\[ \\text{assoc}(V_A, V) = \\sum_{u\\in V_A, v\\in V} w(u, v) \\] \\[ \\text{NCut}(V_A, V_B) = \\frac{\\text{cut}(V_A, V_B)}{\\text{assoc}(V_A, V)} + \\frac{\\text{cut}(V_A, V_B)}{\\text{assoc}(V_B, V)} \\]","title":"Problem with min-cut"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#markow-random-field-mrf","text":"Graphcut is an exception of MRF . Question","title":"Markow Random Field (MRF)"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/","text":"Lecture 5 Image Matching and Motion Estimation \u00b6 Image Matching \u00b6 Abstract Dectection Description Matching Learned based matching 1. Detection \u00b6 We want uniqueness. Corner Detection \u00b6 Local measures of uniqueness \u00b6 shifting the window in any direction causes a big change distribution of gradients Principal Component Analysis (PCA) \u4e3b\u6210\u5206\u5206\u6790 To describe the distribution of gradients by two eigenvectors. Method Compute the covariance matrix \\(H\\) at each point and compute its eigenvalues \\(\\lambda_1\\) , \\(\\lambda_2\\) . Classify Flat - \\(\\lambda_1\\) and \\(lambda_2\\) are both small. Edge - \\(\\lambda_1 \\gg \\lambda_2\\) or \\(\\lambda_1 \\ll \\lambda_2\\) . Corner - \\(\\lambda_1\\) and $lambda_2are both large. Harri Corner Detector However, computing eigenvalues are expensive. Instead, we use Harris corner detector to indicate it. Compute derivatives at each pixel. Compute covariance matrix \\(H\\) in a Gaussian window around each pixel. Compute corner response function \\(f\\) , given by \\[ f = \\frac{\\lambda_1\\lambda_2}{\\lambda_1 + \\lambda_2} = \\frac{\\det(H)}{\\text{tr}(H)} \\] Threshold \\(f\\) . Find local maxima of response function. Invariance Properties Invariant to intensity shift, translation, rotation Not invariant to intensity scaling, scaling How to find the correct scale? Try each scale and find the scale of maximum of \\(f\\) . Implementation \u2014\u2014 image pyramid with a fixed window size. Blob Detection \u00b6 convolution! Laplacian of Gaussian (LoG) Filter \u00b6 Example \\[ f = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & -4 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix} \\] Laplacian is sensitive to noise. To solve this flaw, we often Smooth image with a Gaussian filter Compute Laplacian \\[ \\nabla^2(f*g) = f*\\nabla^2g, \\text{ where $f$ is the Laplacian and $g$ is the Gaussian} \\] Scale Selection (the same problem as corner detection) \u00b6 The same solution as corner detection - Try and find maximum. Implementation: Difference of Gaussian (DoG) \u00b6 A way to acclerate computation, since LoG can be approximated by DoG. \\[ \\nabla^2G_\\sigma \\approx G_{\\sigma_1} - G_{\\sigma_2} \\] 2. Description \u00b6 We mainly focus on the SIFT (Scale Invariant Feature Transform) descriptor. Orientation Normalization \u00b6 Compute orientation histogram Select dominant orientation Normalize: rotate to fixed orientation Properites of SIFT \u00b6 Extraordinaraily robust matching technique handle changes in viewpoint Theoretically invariant to scale and rotation handle significant changes in illumination Fast Other dectectores and descriptors HOG SURF FAST ORB Fast Retina Key-point 3. Matching \u00b6 Feature matching \u00b6 Define distance function to compare two descriptors. Test all to find the minimum distance. Problem: repeated elements To find that the problem happens: Ratio Test Ratio score = \\(\\frac{||f_1 - f_2||}{||f_1 - f_2'||}\\) best match \\(f_2\\) , second best match \\(f'_2\\) . Another strategy: Mutual Nearest Neighbor \\(f_2\\) is the nearest neighbour of \\(f_1\\) in \\(I_2\\) . \\(f_1\\) is the nearest neighbour of \\(f_2\\) in \\(I_1\\) . 4. Learning based matching \u00b6 Question Motion Estimation \u00b6 Problems Feature-tracking Optical flow Method: Lucas-Kanade Method \u00b6 Assumptions \u00b6 Small motion Brightness constancy Spatial coherence Brightness constancy \u00b6 \\[ \\begin{aligned} I(x, y, t) &= I(x + u, y + v, t + 1) \\\\ I(x + u, y + v, t + 1) &\\approx I(x, y, t) + I_x u + I_y v + I_t \\\\ I_x u + I_y v + I_t &\\approx 0 \\\\ \\nabla I \\cdot [u, v]^T + I_t &= 0 \\end{aligned} \\] Aperture Problem \u00b6 Idea: To get more equations \u2192 Spatial coherence constraint Assume the pixel's neighbors have the same motion \\([u, v]\\) . If we use an \\(n\\times n\\) window, \\[ \\begin{bmatrix} I_x(\\textbf{p}_\\textbf{1}) & I_y(\\textbf{p}_\\textbf{1}) \\\\ \\vdots & \\vdots\\\\ I_x(\\textbf{p}_\\textbf{1}) & I_y(\\textbf{p}_{\\textbf{n}^\\textbf{2}}) \\\\ \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = - \\begin{bmatrix} I_t(\\textbf{p}_\\textbf{1}) \\\\ \\vdots \\\\ I_t(\\textbf{p}_{\\textbf{n}^\\textbf{2}}) \\end{bmatrix} \\Rightarrow Ad=b \\] More equations than variables. Thus we solve \\(\\min_d||Ad-b||^2\\) . Solution is given by \\((A^TA)d = A^Tb\\) , when solvable, which is \\(A^TA\\) should be invertible and well-conditioned (eigenvalues should not be too small, similarly with criteria for Harris corner detector). Thus it's easier to estimate the motion of a corner, then edge, then low texture region (flat). Small Motion Assumption \u00b6 For taylor expansion, we want the motion as small as possbile. But probably not - at least larger than one pixel. Solution \u00b6 Reduce the resolution Use the image pyramid","title":"Lecture 5"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#lecture-5-image-matching-and-motion-estimation","text":"","title":"Lecture 5 Image Matching and Motion Estimation"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#image-matching","text":"Abstract Dectection Description Matching Learned based matching","title":"Image Matching"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#1-detection","text":"We want uniqueness.","title":"1. Detection"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#corner-detection","text":"","title":"Corner Detection"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#local-measures-of-uniqueness","text":"shifting the window in any direction causes a big change distribution of gradients Principal Component Analysis (PCA) \u4e3b\u6210\u5206\u5206\u6790 To describe the distribution of gradients by two eigenvectors. Method Compute the covariance matrix \\(H\\) at each point and compute its eigenvalues \\(\\lambda_1\\) , \\(\\lambda_2\\) . Classify Flat - \\(\\lambda_1\\) and \\(lambda_2\\) are both small. Edge - \\(\\lambda_1 \\gg \\lambda_2\\) or \\(\\lambda_1 \\ll \\lambda_2\\) . Corner - \\(\\lambda_1\\) and $lambda_2are both large. Harri Corner Detector However, computing eigenvalues are expensive. Instead, we use Harris corner detector to indicate it. Compute derivatives at each pixel. Compute covariance matrix \\(H\\) in a Gaussian window around each pixel. Compute corner response function \\(f\\) , given by \\[ f = \\frac{\\lambda_1\\lambda_2}{\\lambda_1 + \\lambda_2} = \\frac{\\det(H)}{\\text{tr}(H)} \\] Threshold \\(f\\) . Find local maxima of response function. Invariance Properties Invariant to intensity shift, translation, rotation Not invariant to intensity scaling, scaling How to find the correct scale? Try each scale and find the scale of maximum of \\(f\\) . Implementation \u2014\u2014 image pyramid with a fixed window size.","title":"Local measures of uniqueness"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#blob-detection","text":"convolution!","title":"Blob Detection"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#laplacian-of-gaussian-log-filter","text":"Example \\[ f = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & -4 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix} \\] Laplacian is sensitive to noise. To solve this flaw, we often Smooth image with a Gaussian filter Compute Laplacian \\[ \\nabla^2(f*g) = f*\\nabla^2g, \\text{ where $f$ is the Laplacian and $g$ is the Gaussian} \\]","title":"Laplacian of Gaussian (LoG) Filter"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#scale-selection-the-same-problem-as-corner-detection","text":"The same solution as corner detection - Try and find maximum.","title":"Scale Selection (the same problem as corner detection)"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#implementation-difference-of-gaussian-dog","text":"A way to acclerate computation, since LoG can be approximated by DoG. \\[ \\nabla^2G_\\sigma \\approx G_{\\sigma_1} - G_{\\sigma_2} \\]","title":"Implementation: Difference of Gaussian (DoG)"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#2-description","text":"We mainly focus on the SIFT (Scale Invariant Feature Transform) descriptor.","title":"2. Description"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#orientation-normalization","text":"Compute orientation histogram Select dominant orientation Normalize: rotate to fixed orientation","title":"Orientation Normalization"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#properites-of-sift","text":"Extraordinaraily robust matching technique handle changes in viewpoint Theoretically invariant to scale and rotation handle significant changes in illumination Fast Other dectectores and descriptors HOG SURF FAST ORB Fast Retina Key-point","title":"Properites of SIFT"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#3-matching","text":"","title":"3. Matching"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#feature-matching","text":"Define distance function to compare two descriptors. Test all to find the minimum distance. Problem: repeated elements To find that the problem happens: Ratio Test Ratio score = \\(\\frac{||f_1 - f_2||}{||f_1 - f_2'||}\\) best match \\(f_2\\) , second best match \\(f'_2\\) . Another strategy: Mutual Nearest Neighbor \\(f_2\\) is the nearest neighbour of \\(f_1\\) in \\(I_2\\) . \\(f_1\\) is the nearest neighbour of \\(f_2\\) in \\(I_1\\) .","title":"Feature matching"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#4-learning-based-matching","text":"Question","title":"4. Learning based matching"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#motion-estimation","text":"Problems Feature-tracking Optical flow","title":"Motion Estimation"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#method-lucas-kanade-method","text":"","title":"Method: Lucas-Kanade Method"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#assumptions","text":"Small motion Brightness constancy Spatial coherence","title":"Assumptions"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#brightness-constancy","text":"\\[ \\begin{aligned} I(x, y, t) &= I(x + u, y + v, t + 1) \\\\ I(x + u, y + v, t + 1) &\\approx I(x, y, t) + I_x u + I_y v + I_t \\\\ I_x u + I_y v + I_t &\\approx 0 \\\\ \\nabla I \\cdot [u, v]^T + I_t &= 0 \\end{aligned} \\]","title":"Brightness constancy"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#aperture-problem","text":"Idea: To get more equations \u2192 Spatial coherence constraint Assume the pixel's neighbors have the same motion \\([u, v]\\) . If we use an \\(n\\times n\\) window, \\[ \\begin{bmatrix} I_x(\\textbf{p}_\\textbf{1}) & I_y(\\textbf{p}_\\textbf{1}) \\\\ \\vdots & \\vdots\\\\ I_x(\\textbf{p}_\\textbf{1}) & I_y(\\textbf{p}_{\\textbf{n}^\\textbf{2}}) \\\\ \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = - \\begin{bmatrix} I_t(\\textbf{p}_\\textbf{1}) \\\\ \\vdots \\\\ I_t(\\textbf{p}_{\\textbf{n}^\\textbf{2}}) \\end{bmatrix} \\Rightarrow Ad=b \\] More equations than variables. Thus we solve \\(\\min_d||Ad-b||^2\\) . Solution is given by \\((A^TA)d = A^Tb\\) , when solvable, which is \\(A^TA\\) should be invertible and well-conditioned (eigenvalues should not be too small, similarly with criteria for Harris corner detector). Thus it's easier to estimate the motion of a corner, then edge, then low texture region (flat).","title":"Aperture Problem"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#small-motion-assumption","text":"For taylor expansion, we want the motion as small as possbile. But probably not - at least larger than one pixel.","title":"Small Motion Assumption"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#solution","text":"Reduce the resolution Use the image pyramid","title":"Solution"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/","text":"Lecture 6 Image Stitching \u00b6 Image Warping \u00b6 Info Image filtering changes intensity of image. Image warping (\u7fd8\u66f2) changes shape of image. 2D Transformations \u00b6 Affine Transformations \u4eff\u5c04\u53d8\u6362 \u00b6 \\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\] \\[ \\begin{aligned} x' &= ax + by + c \\\\ y' &= dx + ey + f \\end{aligned} \\] \u81ea\u7531\u5ea6 (degree of freedom) \u4e3a 6 \uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u81f3\u5c11 3 \u7ec4\u70b9\uff0c\u4e5f\u5c31\u662f 6 \u4e2a\u65b9\u7a0b\uff0c\u4ee5\u80fd\u591f\u6c42\u89e3\u51fa \\(a, b, c, d, e, f\\) . Projective Transformation (Homography) \u5355\u5e94\u53d8\u6362 \u00b6 \\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} \\cong \\begin{bmatrix} h_{00} & h_{01} & h_{02} \\\\ h_{10} & h_{11} & h_{12} \\\\ h_{20} & h_{21} & h_{22} \\\\ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\] \\[ \\begin{aligned} x' = \\frac{h_{00}x + h_{01}y + h_{02}}{h_{20}x + h_{11}y + h_{12}} y' = \\frac{h_{10}x + h_{11}y + h_{12}}{h_{20}x + h_{11}y + h_{12}} \\end{aligned} \\] Constraint: \u4ee4 \\(||(h_{00}, h_{01}, \\dots, h_{22})||\\) (\u5411\u91cf\u8303\u6570) \u89c4\u5b9a\u4e3a \\(1\\) \uff0c\u6216\u8005\u4ee4 \\(h_{22}\\) \u4e3a \\(1\\) . \u8003\u8651\u5230\u4ee5\u4e0a\u7684\u9650\u5236\u6761\u4ef6 (constraint)\uff0c\u81ea\u7531\u5ea6 (degree of freedom) \u4e3a 8 \uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u81f3\u5c11 4 \u7ec4\u70b9\uff0c\u4e5f\u5c31\u662f 8 \u4e2a\u65b9\u7a0b\uff0c\u4ee5\u80fd\u591f\u6c42\u89e3\u51fa \\(h_{ij}\\) . Summary \u00b6 Implementation \u00b6 \u51fa\u4e8e\u65b9\u4fbf\u548c\u53ef\u64cd\u4f5c\u6027\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u8868\u5f81\u6574\u4f53\u7fd8\u66f2\u7684\u51fd\u6570 \\(T\\) \u800c\u4e0d\u662f\u5bf9\u6bcf\u4e2a\u50cf\u7d20\u70b9\u91c7\u53d6\u4e0d\u540c\u7684\u7fd8\u66f2\u7b56\u7565. Forward Warping (Not so good) \u00b6 \u5047\u8bbe\u56fe\u50cf\u51fd\u6570 \\(f:(x, y) \\rightarrow (r, g, b)\\) \u7ecf\u8fc7\u4e86\u53d8\u6362 \\(T: (x, y) \\rightarrow (x', y')\\) \u5f97\u5230\u65b0\u7684\u56fe\u50cf\u51fd\u6570 \\(g:(x', y') \\rightarrow (r, g, b)\\) . If the transformed pixed lands inside the pixels - Quite complicated to solve it. Inverse Warping \u00b6 Why not consider inversely? If the transformed pixed lands inside the pixels Solution 2D Interpolation! Image Stitching \u00b6 \\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} \\cong T \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\] Image Matching \u00b6 \u901a\u8fc7\u56fe\u50cf\u5339\u914d\u5f97\u5230\u5339\u914d\u70b9\uff0c\u6bcf\u4e2a\u5339\u914d\u70b9\u53ef\u4ee5\u63d0\u4f9b\u4e00\u4e2a\u4ee5\u4e0a\u5f62\u5f0f\u7684\u77e9\u9635\u4e58\u6cd5\u7b49\u5f0f. Find \\(T\\) for Image Wraping \u00b6 Affine Transformations \u00b6 \u5bf9\u4e8e\u6bcf\u4e2a\u5339\u914d\uff0c \\[ \\begin{aligned} x' = ax + by + c \\\\ y' = dx + ey + f \\end{aligned} \\] \u77e9\u9635\u5f62\u5f0f\u4e3a \\[ \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} x & y & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x & y & 1 \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix} \\] \u5bf9\u4e8e \\(n\\) \u4e2a\u5339\u914d\uff0c\u5f97\u5230\u4ee5\u4e0b\u65b9\u7a0b\u7ec4 \\[ \\begin{bmatrix} x_1 & y_1 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_1 & y_1 & 1 \\\\ x_2 & y_2 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_2 & y_2 & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_n & y_n & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_n & y_n & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix} = \\begin{bmatrix} x_1' \\\\ y_1' \\\\ x_2' \\\\ y_2' \\\\ \\vdots \\\\ x_n' \\\\ y_n' \\end{bmatrix} \\] \u4e5f\u5373 \\[ \\mathbf{A}_{2n \\times 6} \\times \\mathbf{t}_{6 \\times 1} = \\mathbf{b}_{2n \\times 1} \\] Note \u867d\u7136\u9009\u62e9 3 \u7ec4\u70b9\u662f\u80fd\u591f\u89e3\u51fa \\(a, b, c, d, e, f\\) \u7684\u6700\u5c11\u6570\u91cf\uff0c\u4f46\u662f\u5f88\u53ef\u80fd\u5b58\u5728\u90e8\u5206\u884c\u662f\u7ebf\u6027\u76f8\u5173\u4ece\u800c\u5bfc\u81f4\u5947\u5f02\u77e9\u9635\uff0c\u9000\u5316\u4e3a\u6ca1\u6709\u552f\u4e00\u89e3. \u6240\u4ee5\u4e3a\u4e86\u80fd\u591f\u5f97\u5230 \\(a, b, c, d, e, f\\) \u7684\u503c\uff0c\u91c7\u7528\u7684\u65b9\u5f0f\u662f\u53d6\u8f83\u591a\u7684\u70b9\u7136\u540e\u901a\u8fc7\u6700\u5c0f\u4e8c\u4e58\u6cd5\u8ba1\u7b97\uff0c\u4e5f\u5373\u8ba9 \\(\\mathbf{t}\\) \u6700\u5c0f\u5316 \\(||\\mathbf{A} \\mathbf{t} - \\mathbf{b}||\\) . Method of Least Square \\[ \\begin{aligned} \\mathbf{A^{T}At} &= \\mathbf{A^{T}b} \\\\ t &= \\mathbf{A^{T}A^{-1}A^{T}b} \\end{aligned} \\] Projective Transformations \u00b6 \u7c7b\u4f3c\u5730\uff0c\u5bf9\u4e8e\u6bcf\u4e2a\u5339\u914d\uff0c \\[ \\begin{aligned} x_i'(h_{20}x_i + h_{21} y_i + h_{22}) = h_{00}x_i + h_{01}y_i + h_{02} \\\\ y_i'(h_{20}x_i + h_{21} y_i + h_{22}) = h_{10}x_i + h_{11}y_i + h_{12} \\\\ \\end{aligned} \\] \u77e9\u9635\u5f62\u5f0f\u4e3a \\[ \\begin{bmatrix} x_i & y_i & 1 & 0 & 0 & 0 & -x_i'x_i & -x_i'y_i & -x_i'\\\\ 0 & 0 & 0 & x_i & y_i & 1 & -y_i'x_i & -y_i'y_i & -y_i' \\end{bmatrix} \\begin{bmatrix} h_{00} \\\\ h_{01} \\\\ h_{02} \\\\ h_{10} \\\\ h_{11} \\\\ h_{12} \\\\ h_{20} \\\\ h_{21} \\\\ h_{22} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\] \u5bf9\u4e8e \\(n\\) \u4e2a\u5339\u914d\uff0c\u5f97\u5230\u4ee5\u4e0b\u65b9\u7a0b\u7ec4 \\[ \\begin{bmatrix} x_1 & y_1 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_1 & y_1 & 1 \\\\ x_2 & y_2 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_2 & y_2 & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_n & y_n & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_n & y_n & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix} = \\begin{bmatrix} x_1' \\\\ y_1' \\\\ x_2' \\\\ y_2' \\\\ \\vdots \\\\ x_n' \\\\ y_n' \\end{bmatrix} \\] \u4e5f\u5373 \\[ \\mathbf{A}_{2n \\times 9} \\times \\mathbf{t}_{9 \\times 1} = \\mathbf{b}_{2n \\times 1} \\] \u540c\u6837\u5730\uff0c\u6211\u4eec\u8981\u6700\u5c0f\u5316 \\(||\\mathbf{Ah - 0}|| = \\mathbf{||Ah||}\\) . Contraint: \\(||\\mathbf{h}|| = 1\\) \u5982\u679c\u4f7f\u7528\u7684\u662f\u5411\u91cf\u8303\u6570\u4e3a \\(1\\) \u7684\u9650\u5236\uff0c\u90a3\u4e48\u6211\u4eec\u53ea\u9700\u8981\u786e\u5b9a \\(\\mathbf{h}\\) \u7684\u65b9\u5411 \\(\\mathbf{\\hat h}\\) . \u6570\u5b66\u4e0a\u53ef\u4ee5\u8bc1\u660e\u89e3 \\(\\mathbf{\\hat h}\\) \u662f \\(\\mathbf{A^{T}A}\\) \u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf. Robustness \u00b6 Outliers \u56fe\u50cf\u5339\u914d\u4e5f\u4e0d\u662f\u5b8c\u7f8e\u7684\uff0c\u53ef\u80fd\u6709\u90e8\u5206\u9519\u8bef\u7684\u5339\u914d\u70b9\u6210\u4e3a\u5b64\u7acb\u70b9 (outliner). Solution: RANSAC \u6bcf\u6b21\u968f\u673a\u53d6 \\(s\\) \u7ec4\u5339\u914d\u70b9. \u901a\u8fc7\u9009\u53d6\u7684\u5339\u914d\u70b9\u8ba1\u7b97\u51fa\u53d8\u6362\u77e9\u9635 \\(T\\) . \u5728\u6240\u6709\u7684\u5339\u914d\u70b9\u4e2d\uff0c\u7edf\u8ba1\u5728\u4e00\u5b9a\u8bef\u5dee\u8303\u56f4\u5185\u7b26\u5408\u53d8\u6362 \\(T\\) \u7684\u70b9 (inlier) \u7684\u6570\u91cf\u4f5c\u4e3a\u5f97\u5206. \u91cd\u590d \\(N\\) \u6b21\uff0c\u53d6\u5f97\u5206\u6700\u9ad8\u7684\u53d8\u6362 \\(T_0\\) . (Better Step) \u5bf9\u6240\u6709\u5927\u81f4\u7b26\u5408\u53d8\u6362 \\(T_0\\) \u7684\u5339\u914d\u70b9\u518d\u6b21\u8ba1\u7b97\u4e00\u4e2a\u5e73\u5747\u7684\u53d8\u6362\u77e9\u9635 \\(T\\) . Summary \u00b6 How to make image stitching? Input Images. Feature Matching. Compute transformation matrices \\(T\\) with RANSAC. Warp image 2 to image 1. Extension: Panaroma and Cylindrical Projection Problem: Drift Solution Small vertical errors accumulate over time. Apply correction s.t. the sum of drift is 0. \u4e5f\u5c31\u662f\u62cd\u6444\u5168\u666f\u56fe\u65f6\u5c3d\u53ef\u80fd\u51cf\u5c11\u4e0a\u4e0b\u6296\u52a8.","title":"Lecture 6"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#lecture-6-image-stitching","text":"","title":"Lecture 6 Image Stitching"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#image-warping","text":"Info Image filtering changes intensity of image. Image warping (\u7fd8\u66f2) changes shape of image.","title":"Image Warping"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#2d-transformations","text":"","title":"2D Transformations"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#affine-transformations","text":"\\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\] \\[ \\begin{aligned} x' &= ax + by + c \\\\ y' &= dx + ey + f \\end{aligned} \\] \u81ea\u7531\u5ea6 (degree of freedom) \u4e3a 6 \uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u81f3\u5c11 3 \u7ec4\u70b9\uff0c\u4e5f\u5c31\u662f 6 \u4e2a\u65b9\u7a0b\uff0c\u4ee5\u80fd\u591f\u6c42\u89e3\u51fa \\(a, b, c, d, e, f\\) .","title":"Affine Transformations \u4eff\u5c04\u53d8\u6362"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#projective-transformation-homography","text":"\\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} \\cong \\begin{bmatrix} h_{00} & h_{01} & h_{02} \\\\ h_{10} & h_{11} & h_{12} \\\\ h_{20} & h_{21} & h_{22} \\\\ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\] \\[ \\begin{aligned} x' = \\frac{h_{00}x + h_{01}y + h_{02}}{h_{20}x + h_{11}y + h_{12}} y' = \\frac{h_{10}x + h_{11}y + h_{12}}{h_{20}x + h_{11}y + h_{12}} \\end{aligned} \\] Constraint: \u4ee4 \\(||(h_{00}, h_{01}, \\dots, h_{22})||\\) (\u5411\u91cf\u8303\u6570) \u89c4\u5b9a\u4e3a \\(1\\) \uff0c\u6216\u8005\u4ee4 \\(h_{22}\\) \u4e3a \\(1\\) . \u8003\u8651\u5230\u4ee5\u4e0a\u7684\u9650\u5236\u6761\u4ef6 (constraint)\uff0c\u81ea\u7531\u5ea6 (degree of freedom) \u4e3a 8 \uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u81f3\u5c11 4 \u7ec4\u70b9\uff0c\u4e5f\u5c31\u662f 8 \u4e2a\u65b9\u7a0b\uff0c\u4ee5\u80fd\u591f\u6c42\u89e3\u51fa \\(h_{ij}\\) .","title":"Projective Transformation (Homography) \u5355\u5e94\u53d8\u6362"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#summary","text":"","title":"Summary"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#implementation","text":"\u51fa\u4e8e\u65b9\u4fbf\u548c\u53ef\u64cd\u4f5c\u6027\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u8868\u5f81\u6574\u4f53\u7fd8\u66f2\u7684\u51fd\u6570 \\(T\\) \u800c\u4e0d\u662f\u5bf9\u6bcf\u4e2a\u50cf\u7d20\u70b9\u91c7\u53d6\u4e0d\u540c\u7684\u7fd8\u66f2\u7b56\u7565.","title":"Implementation"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#forward-warping-not-so-good","text":"\u5047\u8bbe\u56fe\u50cf\u51fd\u6570 \\(f:(x, y) \\rightarrow (r, g, b)\\) \u7ecf\u8fc7\u4e86\u53d8\u6362 \\(T: (x, y) \\rightarrow (x', y')\\) \u5f97\u5230\u65b0\u7684\u56fe\u50cf\u51fd\u6570 \\(g:(x', y') \\rightarrow (r, g, b)\\) . If the transformed pixed lands inside the pixels - Quite complicated to solve it.","title":"Forward Warping (Not so good)"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#inverse-warping","text":"Why not consider inversely? If the transformed pixed lands inside the pixels Solution 2D Interpolation!","title":"Inverse Warping"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#image-stitching","text":"\\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} \\cong T \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\]","title":"Image Stitching"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#image-matching","text":"\u901a\u8fc7\u56fe\u50cf\u5339\u914d\u5f97\u5230\u5339\u914d\u70b9\uff0c\u6bcf\u4e2a\u5339\u914d\u70b9\u53ef\u4ee5\u63d0\u4f9b\u4e00\u4e2a\u4ee5\u4e0a\u5f62\u5f0f\u7684\u77e9\u9635\u4e58\u6cd5\u7b49\u5f0f.","title":"Image Matching"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#find-t-for-image-wraping","text":"","title":"Find \\(T\\) for Image Wraping"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#affine-transformations_1","text":"\u5bf9\u4e8e\u6bcf\u4e2a\u5339\u914d\uff0c \\[ \\begin{aligned} x' = ax + by + c \\\\ y' = dx + ey + f \\end{aligned} \\] \u77e9\u9635\u5f62\u5f0f\u4e3a \\[ \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} x & y & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x & y & 1 \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix} \\] \u5bf9\u4e8e \\(n\\) \u4e2a\u5339\u914d\uff0c\u5f97\u5230\u4ee5\u4e0b\u65b9\u7a0b\u7ec4 \\[ \\begin{bmatrix} x_1 & y_1 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_1 & y_1 & 1 \\\\ x_2 & y_2 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_2 & y_2 & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_n & y_n & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_n & y_n & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix} = \\begin{bmatrix} x_1' \\\\ y_1' \\\\ x_2' \\\\ y_2' \\\\ \\vdots \\\\ x_n' \\\\ y_n' \\end{bmatrix} \\] \u4e5f\u5373 \\[ \\mathbf{A}_{2n \\times 6} \\times \\mathbf{t}_{6 \\times 1} = \\mathbf{b}_{2n \\times 1} \\] Note \u867d\u7136\u9009\u62e9 3 \u7ec4\u70b9\u662f\u80fd\u591f\u89e3\u51fa \\(a, b, c, d, e, f\\) \u7684\u6700\u5c11\u6570\u91cf\uff0c\u4f46\u662f\u5f88\u53ef\u80fd\u5b58\u5728\u90e8\u5206\u884c\u662f\u7ebf\u6027\u76f8\u5173\u4ece\u800c\u5bfc\u81f4\u5947\u5f02\u77e9\u9635\uff0c\u9000\u5316\u4e3a\u6ca1\u6709\u552f\u4e00\u89e3. \u6240\u4ee5\u4e3a\u4e86\u80fd\u591f\u5f97\u5230 \\(a, b, c, d, e, f\\) \u7684\u503c\uff0c\u91c7\u7528\u7684\u65b9\u5f0f\u662f\u53d6\u8f83\u591a\u7684\u70b9\u7136\u540e\u901a\u8fc7\u6700\u5c0f\u4e8c\u4e58\u6cd5\u8ba1\u7b97\uff0c\u4e5f\u5373\u8ba9 \\(\\mathbf{t}\\) \u6700\u5c0f\u5316 \\(||\\mathbf{A} \\mathbf{t} - \\mathbf{b}||\\) . Method of Least Square \\[ \\begin{aligned} \\mathbf{A^{T}At} &= \\mathbf{A^{T}b} \\\\ t &= \\mathbf{A^{T}A^{-1}A^{T}b} \\end{aligned} \\]","title":"Affine Transformations"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#projective-transformations","text":"\u7c7b\u4f3c\u5730\uff0c\u5bf9\u4e8e\u6bcf\u4e2a\u5339\u914d\uff0c \\[ \\begin{aligned} x_i'(h_{20}x_i + h_{21} y_i + h_{22}) = h_{00}x_i + h_{01}y_i + h_{02} \\\\ y_i'(h_{20}x_i + h_{21} y_i + h_{22}) = h_{10}x_i + h_{11}y_i + h_{12} \\\\ \\end{aligned} \\] \u77e9\u9635\u5f62\u5f0f\u4e3a \\[ \\begin{bmatrix} x_i & y_i & 1 & 0 & 0 & 0 & -x_i'x_i & -x_i'y_i & -x_i'\\\\ 0 & 0 & 0 & x_i & y_i & 1 & -y_i'x_i & -y_i'y_i & -y_i' \\end{bmatrix} \\begin{bmatrix} h_{00} \\\\ h_{01} \\\\ h_{02} \\\\ h_{10} \\\\ h_{11} \\\\ h_{12} \\\\ h_{20} \\\\ h_{21} \\\\ h_{22} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\] \u5bf9\u4e8e \\(n\\) \u4e2a\u5339\u914d\uff0c\u5f97\u5230\u4ee5\u4e0b\u65b9\u7a0b\u7ec4 \\[ \\begin{bmatrix} x_1 & y_1 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_1 & y_1 & 1 \\\\ x_2 & y_2 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_2 & y_2 & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_n & y_n & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_n & y_n & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix} = \\begin{bmatrix} x_1' \\\\ y_1' \\\\ x_2' \\\\ y_2' \\\\ \\vdots \\\\ x_n' \\\\ y_n' \\end{bmatrix} \\] \u4e5f\u5373 \\[ \\mathbf{A}_{2n \\times 9} \\times \\mathbf{t}_{9 \\times 1} = \\mathbf{b}_{2n \\times 1} \\] \u540c\u6837\u5730\uff0c\u6211\u4eec\u8981\u6700\u5c0f\u5316 \\(||\\mathbf{Ah - 0}|| = \\mathbf{||Ah||}\\) . Contraint: \\(||\\mathbf{h}|| = 1\\) \u5982\u679c\u4f7f\u7528\u7684\u662f\u5411\u91cf\u8303\u6570\u4e3a \\(1\\) \u7684\u9650\u5236\uff0c\u90a3\u4e48\u6211\u4eec\u53ea\u9700\u8981\u786e\u5b9a \\(\\mathbf{h}\\) \u7684\u65b9\u5411 \\(\\mathbf{\\hat h}\\) . \u6570\u5b66\u4e0a\u53ef\u4ee5\u8bc1\u660e\u89e3 \\(\\mathbf{\\hat h}\\) \u662f \\(\\mathbf{A^{T}A}\\) \u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf.","title":"Projective Transformations"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#robustness","text":"Outliers \u56fe\u50cf\u5339\u914d\u4e5f\u4e0d\u662f\u5b8c\u7f8e\u7684\uff0c\u53ef\u80fd\u6709\u90e8\u5206\u9519\u8bef\u7684\u5339\u914d\u70b9\u6210\u4e3a\u5b64\u7acb\u70b9 (outliner). Solution: RANSAC \u6bcf\u6b21\u968f\u673a\u53d6 \\(s\\) \u7ec4\u5339\u914d\u70b9. \u901a\u8fc7\u9009\u53d6\u7684\u5339\u914d\u70b9\u8ba1\u7b97\u51fa\u53d8\u6362\u77e9\u9635 \\(T\\) . \u5728\u6240\u6709\u7684\u5339\u914d\u70b9\u4e2d\uff0c\u7edf\u8ba1\u5728\u4e00\u5b9a\u8bef\u5dee\u8303\u56f4\u5185\u7b26\u5408\u53d8\u6362 \\(T\\) \u7684\u70b9 (inlier) \u7684\u6570\u91cf\u4f5c\u4e3a\u5f97\u5206. \u91cd\u590d \\(N\\) \u6b21\uff0c\u53d6\u5f97\u5206\u6700\u9ad8\u7684\u53d8\u6362 \\(T_0\\) . (Better Step) \u5bf9\u6240\u6709\u5927\u81f4\u7b26\u5408\u53d8\u6362 \\(T_0\\) \u7684\u5339\u914d\u70b9\u518d\u6b21\u8ba1\u7b97\u4e00\u4e2a\u5e73\u5747\u7684\u53d8\u6362\u77e9\u9635 \\(T\\) .","title":"Robustness"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#summary_1","text":"How to make image stitching? Input Images. Feature Matching. Compute transformation matrices \\(T\\) with RANSAC. Warp image 2 to image 1. Extension: Panaroma and Cylindrical Projection Problem: Drift Solution Small vertical errors accumulate over time. Apply correction s.t. the sum of drift is 0. \u4e5f\u5c31\u662f\u62cd\u6444\u5168\u666f\u56fe\u65f6\u5c3d\u53ef\u80fd\u51cf\u5c11\u4e0a\u4e0b\u6296\u52a8.","title":"Summary"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/","text":"Lecture 7 Structure from Motion (SfM) \u00b6 Camera Calibration \u76f8\u673a\u6807\u5b9a \u00b6 \u76f8\u673a\u6807\u5b9a\u7684\u8fc7\u7a0b\uff0c\u5c31\u662f\u4ece\u4e16\u754c\u5750\u6807\u7cfb\u5411\u50cf\u7d20\u5750\u6807\u7cfb\u8f6c\u6362\u7684\u8fc7\u7a0b. \u9700\u8981\u6ce8\u610f\u7684\u56db\u4e2a\u5750\u6807\u7cfb\uff1a \u4e16\u754c\u5750\u6807\u7cfb (World Coordinates) \u76f8\u673a\u5750\u6807\u7cfb (Camera Coordinates) \u56fe\u50cf\u5750\u6807\u7cfb (Image Plane) \u50cf\u7d20\u5750\u6807\u7cfb (Image Sensor) \u5176\u4e2d\uff0c\u524d\u4e24\u4e2a\u5750\u6807\u7cfb\u7684\u8f6c\u6362\u77e9\u9635\u53c2\u6570\u88ab\u79f0\u4e3a \u76f8\u673a\u5916\u53c2 \uff1b\u540e\u4e09\u4e2a\u5750\u6807\u7cfb\u7684\u8f6c\u6362\u77e9\u9635\u53c2\u6570\u88ab\u79f0\u4e3a \u76f8\u673a\u5185\u53c2 . Principle Analysis \u539f\u7406\u5206\u6790 \u00b6 Coordinate Transformation \u00b6 \u4ece\u4e16\u754c\u5750\u6807\u7cfb\u5230\u76f8\u673a\u5750\u6807\u7cfb\u7684\u53d8\u6362. \u7279\u70b9 \u00b6 \u521a\u4f53\u53d8\u6362\uff0c\u53ea\u6709\u65cb\u8f6c\u548c\u5e73\u79fb\uff0c\u5bf9\u5e94\u65cb\u8f6c\u77e9\u9635 \\(R\\) \uff08 \\(R\\) \u662f \u5355\u4f4d\u6b63\u4ea4\u77e9\u9635 (Orthonormal) ) \u548c\u5e73\u79fb\u5411\u91cf \\(c_w\\) . \\[ \\mathbf{x}_c = R(\\mathbf{x}_w - \\mathbf{c}_w) = R \\mathbf{x}_w - R \\mathbf{c}_w \\overset{\\Delta}{=\\!=} R \\mathbf{x}_w + \\mathbf{t}, \\text{ where } \\mathbf{t } = -R \\mathbf{c}_w. \\] \u6545\u8fd9\u4e2a\u53d8\u6362\u4e5f\u53ef\u4ee5\u7b49\u4ef7\u5730\u7528 \\(R\\) \u548c \\(\\mathbf{t}\\) \u6765\u8868\u5f81. \\[ \\mathbf{x}_c = \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\\\ \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} \\] \u540c\u6837\u8003\u8651\u9f50\u6b21\u5750\u6807\u7cfb \\[ \\mathbf{\\tilde x}_c = \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1\\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix} \\] Extrinsic Matrix (\u5916\u53c2\u77e9\u9635) \\[ M_{ext} = \\begin{bmatrix} R_{3\\times 3} & \\mathbf{t} \\\\ \\mathbf{0}_{1\\times 3} & 1 \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\] \u4e5f\u53ef\u4ee5\u5199\u4f5c \\[ M_{ext} = \\begin{bmatrix} R_{3\\times 3} & \\mathbf{t} \\\\ \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ \\end{bmatrix} \\] Perspective Projection \u00b6 \u4ece\u76f8\u673a\u5750\u6807\u7cfb\u5230\u56fe\u50cf\u5750\u6807\u7cfb\u7684\u53d8\u6362. \u524d\u9762\u51e0\u4e2a Lecture \u4e2d\u5df2\u7ecf\u8ba8\u8bba\u8fc7\u8fd9\u4e00\u90e8\u5206 \u5185\u5bb9 . \u6b64\u5904\u4e0d\u518d\u8d58\u8ff0. \\[ \\begin{bmatrix} x_i \\\\ y_i \\\\ 1 \\end{bmatrix} \\cong \\begin{bmatrix} f & 0 & 0 & 0 \\\\ 0 & f & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1 \\end{bmatrix} \\] Image Plane to Image Sensor Mapping \u00b6 \u4ece\u56fe\u50cf\u5750\u6807\u7cfb\u5230\u50cf\u7d20\u5750\u6807\u7cfb\u7684\u53d8\u6362. \u50cf\u7d20\u70b9 pixel \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u7531\u4e8e\u76f8\u673a\u81ea\u8eab\u7684\u5236\u4f5c\u5de5\u827a\u548c\u8981\u6c42\uff0c\u50cf\u7d20\u70b9\u5e76\u4e0d\u4e00\u5b9a\u662f\u6b63\u65b9\u5f62\uff0c\u800c\u662f\u77e9\u5f62\uff0c\u4e0e\u539f\u56fe\u50cf\u76f8\u6bd4\u5177\u6709\u4e00\u5b9a\u7684 \u7f29\u653e . \u8fd9\u4e00\u70b9\u5728\u89c6\u9891\u7684\u5236\u4f5c\u4e2d\u4e5f\u6709\u6240\u4f53\u73b0. \u6b64\u5916\uff0c\u7531\u4e8e\u56fe\u50cf\u5750\u6807\u7cfb\u662f\u4ece\u4e2d\u5fc3\u4e3a\u539f\u70b9\uff0c\u800c\u50cf\u7d20\u5750\u6807\u7cfb\u662f\u4ee5\u5de6\u4e0a\u89d2\u4e3a\u539f\u70b9\u7684\uff0c\u5750\u6807\u53d8\u6362\u8fd8\u9700\u8981\u4e00\u5b9a\u7684* \u5e73\u79fb . \\[ \\begin{aligned} u &= m_x f \\frac{x_c}{z_c} + c_x \\\\ v &= m_y f \\frac{y_c}{z_c} + c_y \\end{aligned} \\] \u7ed3\u5408\u4ee5\u4e0a\u4e24\u4e2a\u53d8\u6362\uff0c\u8bb0 \\(f_x = m_x f\\) , \\(f_y = m_y f\\) . \u5219\u4ece\u76f8\u673a\u5750\u6807\u7cfb\u5230\u50cf\u7d20\u5750\u6807\u7cfb\u7684\u53d8\u6362\u4e3a \\[ \\mathbf{\\tilde{u}} = \\begin{bmatrix} u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1 \\end{bmatrix} \\] Intrinsic Matrix (\u5185\u53c2\u77e9\u9635) \\[ M_{int} = \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\] \u4e5f\u53ef\u4ee5\u5199\u4f5c \\[ K = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\] Projection Matrix \\(P\\) World to Camera \\[ \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1\\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix} \\] \\[ \\mathbf{\\tilde u} = M_{int}\\mathbf{\\tilde x}_c \\] Camera to Pixel \\[ \\begin{bmatrix} u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1 \\end{bmatrix} \\] \\[ \\mathbf{\\tilde x}_c = M_{ext}\\mathbf{\\tilde x}_w \\] \u6545 \\[ \\mathbf{\\tilde u} = M_{int}M_{ext} \\mathbf{\\tilde x}_w = P \\mathbf{\\tilde x} \\] \u5176\u4e2d\uff0c \\(P\\) \u88ab\u79f0\u4e3a\u6295\u5f71\u77e9\u9635 (Projection Matrix). \\[ \\begin{bmatrix} u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\\\ p_{21} & p_{22} & p_{23} & p_{24} \\\\ p_{31} & p_{32} & p_{33} & p_{34} \\\\ p_{41} & p_{42} & p_{43} & p_{44} \\\\ \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix} \\] Implemetation \u00b6 Step 1. Capture an image of an object with known geometry. \u00b6 Step 2. Identify correspondences (Image Matching \u56fe\u50cf\u5339\u914d) between 3D scene points and image points. \u00b6 Step 3. For each corresponding point \\(i\\) , we get \u00b6 \\[ \\begin{bmatrix} u^{(i)} \\\\ v^{(i)} \\\\ w^{(i)} \\end{bmatrix} \\cong \\begin{bmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\\\ p_{21} & p_{22} & p_{23} & p_{24} \\\\ p_{31} & p_{32} & p_{33} & p_{34} \\\\ p_{41} & p_{42} & p_{43} & p_{44} \\\\ \\end{bmatrix} \\begin{bmatrix} x_w^{(i)} \\\\ y_w^{(i)} \\\\ z_w^{(i)} \\\\ 1 \\end{bmatrix} \\] \\[ \\begin{aligned} u^{(i)} &= \\frac{p_{11}x_w^{(i)} + p_{12}y_w^{(i)} + p_{13}z_w^{(i)} + p_{14}}{p_{31}x_w^{(i)} + p_{32}y_w^{(i)} + p_{33}z_w^{(i)} + p_{34}} \\\\ v^{(i)} &= \\frac{p_{21}x_w^{(i)} + p_{22}y_w^{(i)} + p_{23}z_w^{(i)} + p_{24}}{p_{31}x_w^{(i)} + p_{32}y_w^{(i)} + p_{33}z_w^{(i)} + p_{34}} \\\\ \\end{aligned} \\] Step 4. Rearrange. \u00b6 \\[ \\scriptsize \\begin{bmatrix} x_w^{(1)} & y_w^{(1)} & z_w^{(1)} & 1 & 0 & 0 & 0 & 0 & -u_1x_w^{(1)} & -u_1y_w^{(1)} & -u_1z_w^{(1)} & -u_1 \\\\ 0 & 0 & 0 & 0 & x_w^{(1)} & y_w^{(1)} & z_w^{(1)} & 1 & -v_1x_w^{(1)} & -v_1y_w^{(1)} & -v_1z_w^{(1)} & -v_1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_w^{(i)} & y_w^{(i)} & z_w^{(i)} & 1 & 0 & 0 & 0 & 0 & -u_ix_w^{(i)} & -u_iy_w^{(i)} & -u_iz_w^{(i)} & -u_i \\\\ 0 & 0 & 0 & 0 & x_w^{(i)} & y_w^{(i)} & z_w^{(i)} & 1 & -v_ix_w^{(i)} & -v_iy_w^{(i)} & -v_iz_w^{(i)} & -v_i \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_w^{(n)} & y_w^{(n)} & z_w^{(n)} & 1 & 0 & 0 & 0 & 0 & -u_nx_w^{(n)} & -u_ny_w^{(n)} & -u_nz_w^{(n)} & -u_n \\\\ 0 & 0 & 0 & 0 & x_w^{(n)} & y_w^{(n)} & z_w^{(n)} & 1 & -v_nx_w^{(n)} & -v_ny_w^{(n)} & -v_nz_w^{(n)} & -v_n \\\\ \\end{bmatrix} \\begin{bmatrix} p_{11} \\\\ p_{12} \\\\ p_{13} \\\\ p_{14} \\\\ p_{21} \\\\ p_{22} \\\\ p_{23} \\\\ p_{24} \\\\ p_{31} \\\\ p_{32} \\\\ p_{33} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\] \\[ i.e.\\ A \\mathbf{p} = 0 \\] Step 5 Solve for \\(P\\) . \u00b6 \u7c7b\u4f3c\u5730\uff0c\u6211\u4eec\u53ef\u4ee5\u7ed9\u51fa\u4e00\u5b9a\u7684\u9650\u5236 constraint. Option 1. \u4ee4 \\(p_{34} = 1\\) . Option 2. \u4ee4 \\(||p|| = 1\\) . \u5f97\u5230\u81ea\u7531\u5ea6\u4e3a \\(11\\) \uff0c\u81f3\u5c11\u9700\u8981 6 \u7ec4\u70b9. \u5e76\u540c\u6837\u8981\u6700\u5c0f\u5316 \\(||\\mathbf{Ap - 0}|| = \\mathbf{||Ap||}\\) . \u540c\u6837\u5982\u679c\u4f7f\u7528\u7684\u662f\u5411\u91cf\u8303\u6570\u4e3a \\(1\\) \u7684\u9650\u5236\uff0c\u90a3\u4e48\u6211\u4eec\u53ea\u9700\u8981\u786e\u5b9a \\(\\mathbf{p}\\) \u7684\u65b9\u5411 \\(\\mathbf{\\hat p}\\) . \u4e14 \\(\\mathbf{\\hat p}\\) \u662f \\(\\mathbf{A^{T}A}\\) \u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf. Step 6 From \\(P\\) to Answers \u00b6 \u7531 \\[ \\begin{bmatrix} p_{11} \\\\ p_{12} \\\\ p_{13} \\\\ p_{14} \\\\ p_{21} \\\\ p_{22} \\\\ p_{23} \\\\ p_{24} \\\\ p_{31} \\\\ p_{32} \\\\ p_{33} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\] \u6ce8\u610f\u5230 \\[ \\begin{bmatrix} p_{11} \\\\ p_{12} \\\\ p_{13} \\\\ p_{21} \\\\ p_{22} \\\\ p_{23} \\\\ p_{31} \\\\ p_{32} \\\\ p_{33} \\end{bmatrix} = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\\\ \\end{bmatrix} = KR \\] \u5176\u4e2d \\(K\\) \u662f\u4e0a\u4e09\u89d2\u77e9\u9635 (Upper Right Triangular Matrix)\uff0c \\(R\\) \u662f\u5355\u4f4d\u6b63\u4ea4\u77e9\u9635 (Orthonormal Matrix). \u7531 QR \u5206\u89e3 (QR Factorization) \u53ef\u77e5\uff0c \\(K\\) \u4e0e \\(R\\) \u7684\u89e3\u662f\u552f\u4e00\u7684. \u518d\u7531 \\[ \\begin{bmatrix} p_{14} \\\\ p_{24} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} = K \\mathbf{t} \\] \u6545\u53ef\u89e3\u5f97 \\[ \\mathbf{t} = K^{-1} \\begin{bmatrix} p_{14} \\\\ p_{24} \\\\ p_{34} \\end{bmatrix} \\] Problem: Distortion \u5b9e\u9645\u4e0a\u76f8\u673a\u81ea\u8eab\u4f1a\u5177\u6709\u4e00\u5b9a\u7684\u626d\u66f2\u7cfb\u6570\uff0c\u6211\u4eec\u5728\u6b64 \u5ffd\u7565 \u5b83\u7684\u5f71\u54cd. Perspective-n-Point Problem (PnP \u95ee\u9898) \u5f31\u5316\u4e00\u4e0b\u6761\u4ef6\uff0c\u5047\u8bbe\u5df2\u77e5\u76f8\u673a\u7684\u5185\u53c2\uff08\u5373\u77e9\u9635 \\(K\\) \uff09\uff0c\u6c42\u76f8\u673a\u7684\u5916\u53c2\uff08\u5373\u77e9\u9635 \\(R\\) \u548c \\(\\mathbf{t}\\) \uff09\uff0c\u4e5f\u5373\u76f8\u673a\u59ff\u6001 (camera pose). Direct Linear Transform (DLT) \u00b6 \u5373\u4ee5\u4e0a\u5728\u76f8\u673a\u6807\u5b9a\u4e2d\u63d0\u5230\u7684\u65b9\u6cd5\uff0c\u5148\u6c42\u89e3\u77e9\u9635 \\(P\\) \uff0c\u518d\u901a\u8fc7 \\(K\\) \u5f97\u5230 \\(R\\) \u548c \\(\\mathbf{t}\\) . P3P \u00b6 \u5373\u5df2\u77e5 \\(a, b, c, A, B, C\\) \u70b9\u5750\u6807\uff0c\u6c42 \\(O\\) \u5750\u6807. \u81f3\u5c11\u9700\u8981 \\(3\\) \u7ec4\u70b9 Intermediates. \u7531\u4f59\u5f26\u5b9a\u7406 \\[ \\begin{aligned} OA^2 + OB^2 - 2 OA \\cdot OB \\cos \\angle AOB &= AB^2 \\\\ OA^2 + OC^2 - 2 OB \\cdot OC \\cos \\angle BOC &= BC^2 \\\\ OA^2 + OC^2 - 2 OA \\cdot OC \\cos \\angle AOC &= AC^2 \\end{aligned} \\] \u4ee4 \\(x = \\frac{OA}{OC}, y = \\frac{OB}{OC}, v = \\frac{AB^2}{OC^2}, u = \\frac{BC^2}{AB^2}, w = \\frac{AC^2}{AB^2}\\) \uff0c\u5316\u7b80\u5f97 \\[ \\begin{aligned} (1 - u)y^2 - ux^2 - cos \\angle BOC y + 2uxy \\cos \\angle AOB + 1 &= 0 \\\\ (1 - w)x^2 - wy^2 - cos \\angle AOC y + 2wxy \\cos \\angle AOB + 1 &= 0 \\end{aligned} \\] \u4ee5\u4e0a\u65b9\u7a0b\u7ec4\u662f\u4e8c\u5143\u4e8c\u6b21\u65b9\u7a0b\u7ec4 (Binary Quadratic Equation)\uff0c\u6709\u56db\u7ec4\u89e3. \u5176\u4e2d\u6709\u4e24\u7ec4\u89e3\u7684 \\(O\\) \u5750\u6807\u5728 \u9762 \\(ABC\\) \u4e0e \u9762 \\(abc\\) \u4e4b\u95f4\uff0c\u820d\u53bb. \u8fd8\u9700\u8981\u4e00\u7ec4\u989d\u5916\u7684\u70b9\u6765\u786e\u5b9a\u552f\u4e00\u89e3\uff0c\u6545\u4e00\u5171\u9700\u8981 \\(4\\) \u7ec4\u70b9. PnP \u00b6 \u6700\u5c0f\u5316 \u91cd\u6295\u5f71\u8bef\u5dee (Reprojection Error) \\[ \\mathop{\\arg\\min\\limits_{R, \\mathbf{t}}} \\sum\\limits_i ||(p_i, K(RP_i + \\mathbf{t})||^2 \\] \u5176\u4e2d\uff0c \\(p_i\\) \u662f\u6295\u5f71\u9762\u4e0a\u7684\u4e8c\u7ef4\u5750\u6807\uff0c \\(P_i\\) \u662f\u7a7a\u95f4\u5185\u7684\u4e09\u7ef4\u5750\u6807. \u901a\u8fc7 P3P \u521d\u59cb\u5316\uff0cGauss-Newton \u6cd5\u4f18\u5316. EPnP \u00b6 one of the most popluar methods. time complexity \\(O(N)\\) . high accuracy. \u5927\u81f4\u65b9\u5f0f\u662f\u901a\u8fc7\u56db\u7ec4\u63a7\u5236\u70b9\u7684\u7ebf\u6027\u7ec4\u5408\u6c42\u89e3. Two-frame Structure from Motion \u00b6 Stereo vision \u7acb\u4f53\u89c6\u89c9 Compute 3D structure of the scene and camera poses from views (images). Two-frame SfM \u610f\u5728\u89e3\u51b3\u8fd9\u6837\u4e00\u4e2a\u95ee\u9898\uff1a\u5df2\u77e5\u4e24\u5f20\u56fe\u50cf\u548c\u76f8\u673a\u5185\u53c2\u77e9\u9635 ( \\(K_l(3 \\times 3)\\) \u4e0e \\(K_r(3 \\times 3)\\) )\uff0c\u6c42\u76f8\u673a\u7684\u4f4d\u7f6e\u4ee5\u53ca\u6240\u62cd\u6444\u4e3b\u4f53\u7684\u4e09\u7ef4\u4f4d\u7f6e\u5750\u6807. Principle Induction \u539f\u7406\u63a8\u5bfc \u00b6 Epipolar Geometry \u5bf9\u6781\u51e0\u4f55 \u5bf9\u6781\u51e0\u4f55\u63cf\u8ff0\u4e86\u4e00\u4e2a\u4e09\u7ef4\u70b9 \\(P\\) \u5728\u4e24\u4e2a\u89c6\u89d2\u7684\u4e8c\u7ef4\u6295\u5f71 \\(u_l, u_r\\) \u4e4b\u95f4\u7684\u51e0\u4f55\u5173\u7cfb. \u5e76\u901a\u8fc7\u8fd9\u4e2a\u51e0\u4f55\u5173\u7cfb\u5efa\u7acb\u4e24\u4e2a\u6444\u50cf\u673a\u4e4b\u95f4\u7684\u53d8\u6362\u5173\u7cfb\uff08\u5373\u7ed9\u51fa\u77e9\u9635 \\(R\\) \u548c \\(\\mathbf{t}\\) \uff09. Definition Epipole \u5bf9\u6781\u70b9 \uff1a\u4e00\u53f0\u6444\u50cf\u673a\u6295\u5f71\u5728\u53e6\u4e00\u53f0\u6444\u50cf\u673a\u4e2d\u7684\u70b9. \u5982\u56fe\u4e2d\u7684 \\(e_l\\) \u548c \\(e_r\\) . \u5bf9\u7ed9\u5b9a\u7684\u4e24\u53f0\u6444\u50cf\u673a\uff0c \\(e_l\\) \u548c \\(e_r\\) \u662f\u552f\u4e00\u7684. Epipolar Plane of Scene Point P \u5173\u4e8e P \u70b9\u7684\u5bf9\u6781\u9762 \uff1a\u7531 Scene Point \\(P\\) \u70b9\u548c\u4e24\u4e2a\u6444\u50cf\u673a \\(O_l\\) \u548c \\(O_r\\) \u5f62\u6210\u7684\u5e73\u9762. \u6bcf\u4e2a scene point \u6709\u552f\u4e00\u5bf9\u5e94\u7684\u5bf9\u6781\u9762. \u5bf9\u6781\u70b9\u5728\u5bf9\u6781\u9762\u4e0a. Essential Matrix \\(E\\) \u57fa\u672c\u77e9\u9635 \u00b6 \u5bf9\u4e8e\u4e00\u4e2a\u5bf9\u6781\u9762\uff0c\u4ee5\u4e0b\u7b49\u5f0f\u5173\u7cfb\u6210\u7acb\uff1a \\[ \\begin{aligned} \\mathbf{n} = \\mathbf{t} \\times \\mathbf{x}_l \\\\ \\\\ \\mathbf{x}_l \\cdot (\\mathbf{t} \\times \\mathbf{x}_l) = 0 \\end{aligned} \\] \u5176\u4e2d \\(\\mathbf{n}\\) \u88ab\u79f0\u4e3a Normal Vector. \u5047\u8bbe\u53f3\u8fb9\u7684\u6444\u50cf\u673a\u76f8\u5bf9\u5de6\u8fb9\u7684\u6444\u50cf\u673a\u8fdb\u884c\u521a\u4f53\u53d8\u6362 + \u5e73\u79fb\uff0c\u5373\uff1a \\[ \\begin{aligned} \\mathbf{x}_l &= R \\mathbf{x}_r + \\mathbf{t} \\\\ \\\\ \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\end{bmatrix} &= \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ y_r \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} \\end{aligned} \\] \u8fdb\u4e00\u6b65\u6f14\u7ece\u63a8\u7406\uff0c\u5f97\u5230\uff1a \\[ \\begin{aligned} \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\begin{bmatrix} t_yz_l - t_zy_l \\\\ t_zx_l - t_xz_l \\\\ t_xy_l - t_yx_l \\end{bmatrix} = 0 & \\text{, Cross-product definition} \\\\ \\\\ \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\underbrace{ \\begin{bmatrix} 0 & -t_z & t_y \\\\ t_z & 0 & -t_x \\\\ -t_y & t_x & 0 \\end{bmatrix} }_{T_\\times} \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\end{bmatrix} = 0 & \\text{, Matrix-vector form} \\end{aligned} \\] \u4ee3\u5165\u53d8\u6362\u77e9\u9635 \\[ \\small \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\left( \\underbrace{ \\begin{bmatrix} 0 & -t_z & t_y \\\\ t_z & 0 & -t_x \\\\ -t_y & t_x & 0 \\end{bmatrix} }_{T_\\times} \\underbrace{ \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\end{bmatrix} }_{R} \\begin{bmatrix} x_r \\\\ y_r \\\\ y_r \\end{bmatrix} + \\underbrace{ \\begin{bmatrix} 0 & -t_z & t_y \\\\ t_z & 0 & -t_x \\\\ -t_y & t_x & 0 \\end{bmatrix} \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} }_{\\mathbf{t} \\times \\mathbf{t} = \\mathbf{0}} \\right) = 0 \\] \\[ \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\underbrace{ \\begin{bmatrix} e_{11} & e_{12} & e_{13} \\\\ e_{21} & e_{22} & e_{23} \\\\ e_{31} & e_{32} & e_{33} \\end{bmatrix} }_{\\text{Essential Matrix } E} \\begin{bmatrix} x_r \\\\ y_r \\\\ y_r \\end{bmatrix} \\text{, where } E = T_\\times R \\] \u6ce8\u610f\u5230 \\(T_\\times\\) \u662f\u53cd\u5bf9\u79f0\u77e9\u9635 (skew-symmetric matrix) \u800c \\(R\\) \u662f\u5355\u4f4d\u6b63\u4ea4\u77e9\u9635 (orthonomal matrix)\uff0c\u7531 SVD \u5206\u89e3 (Singule Value Decomposition)\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece \\(E\\) \u552f\u4e00\u5206\u89e3\u5f97\u5230 \\(T_\\times\\) \u548c \\(R\\) . Fundemental Matrix \\(F\\) \u672c\u771f\u77e9\u9635 \u00b6 \u7531\u6295\u5f71\u5173\u7cfb\u4ee5\u53ca\u5185\u53c2\u77e9\u9635 \\(K\\) \uff0c\u5f97\u5230\uff1a \\[ \\small \\begin{aligned} \\text{Left Camera} && \\text{Right Camera} \\\\ z_l \\begin{bmatrix} u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &= \\underbrace{ \\begin{bmatrix} f_x^{(l)} & 0 & o_x^{(l)} \\\\ 0 & f_y^{(l)} & o_y^{(l)} \\\\ 0 & 0 & 1 \\end{bmatrix} }_{K_l} \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\end{bmatrix} & z_r \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} &= \\underbrace{ \\begin{bmatrix} f_x^{(r)} & 0 & o_x^{(r)} \\\\ 0 & f_y^{(r)} & o_y^{(r)} \\\\ 0 & 0 & 1 \\end{bmatrix} }_{K_r} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\end{bmatrix} \\\\ \\mathbf{x}_l^T &= \\begin{bmatrix} u_l & v_l & 1 \\end{bmatrix} z_l (K_l^{-1})^T & \\mathbf{x}_r &= K_r^{-1} z_r \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} \\end{aligned} \\] \u4ee3\u5165\u4ee5\u4e0a\u5f97\u5230\u7684\uff1a \\[ \\mathbf{x}^T_l E \\mathbf{x}_r = 0 \\] \u6709 \\[ \\begin{bmatrix} u_l & v_l & 1 \\end{bmatrix} z_l (K_l^{-1})^T E K_r^{-1} z_r \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = 0 \\] \u7ea6\u53bb \\(z_l\\) , \\(z_r\\) \uff0c\u5219 \\[ \\begin{bmatrix} u_l & v_l & 1 \\end{bmatrix} F \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = 0 \\text{, where } E = K_l^T F K_r \\] Implementation \u5b9e\u73b0 \u00b6 Step 1. Find a set ( \\(m\\) pairs) of corresponding features. \u627e\u5230\u4e00\u7ec4\uff08 \\(m\\) \u5bf9\uff09\u5339\u914d\u70b9. \u00b6 Least Number of Points At least 8 pairs: \\((u_l^{(i)}, v_l^{(i)})\\) \u2194 \\((u_r^{(i)}, v_r^{(i)})\\) NOTE that one pair only corresponds one equation. (not the same as before) Step 2. Build linear systems and solve for \\(F\\) . \u5efa\u7acb\u65b9\u7a0b\u7ec4\u5e76\u6c42\u89e3\u77e9\u9635 \\(F\\) . \u00b6 \u5bf9\u4e8e\u6bcf\u7ec4\u5339\u914d \\(i\\) \uff0c \\[ \\begin{bmatrix} u_l^{(i)} & v_l^{(i)} & 1 \\end{bmatrix} \\begin{bmatrix} f_{11} & f_{12} & f_{13} \\\\ f_{21} & f_{22} & f_{23} \\\\ f_{31} & f_{32} & f_{33} \\end{bmatrix} \\begin{bmatrix} u_r^{(i)} \\\\ v_r^{(i)} \\\\ 1 \\end{bmatrix} = 0 \\] \u5c55\u5f00\u5f97\uff0c \\[ \\small \\left( f_{11} u_r^{(i)} + f_{12} v_r^{(i)} + f_13 \\right)u_l^{(i)} + \\left( f_{21} u_r^{(i)} + f_{22} v_r^{(i)} + f_23 \\right)v_l^{(i)} + f_{31} u_r^{(i)} + f_{32} v_r^{(i)} + f_33 = 0 \\] \u5bf9\u6240\u6709\u9009\u62e9\u7684\u70b9\uff0c\u6709 \\[ \\small \\begin{bmatrix} u_l^{(1)}u_r^{(1)} & u_l^{(1)}v_r^{(1)} & u_l^{(1)} & v_l^{(1)}u_r^{(1)} & v_l^{(1)}v_r^{(1)} & v_l^{(1)} & u_r^{(1)} & v_r^{(1)} & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ u_l^{(i)}u_r^{(i)} & u_l^{(i)}v_r^{(i)} & u_l^{(i)} & v_l^{(i)}u_r^{(i)} & v_l^{(i)}v_r^{(i)} & v_l^{(i)} & u_r^{(i)} & v_r^{(i)} & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ u_l^{(m)}u_r^{(m)} & u_l^{(m)}v_r^{(m)} & u_l^{(m)} & v_l^{(m)}u_r^{(m)} & v_l^{(m)}v_r^{(m)} & v_l^{(m)} & u_r^{(m)} & v_r^{(m)} & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} f_{11} \\\\ f_{12} \\\\ f_{13} \\\\ f_{21} \\\\ f_{22} \\\\ f_{23} \\\\ f_{31} \\\\ f_{32} \\\\ f_{33} \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\] \\[ A \\mathbf{f} = \\mathbf{0} \\] \u540c\u6837\u5730 \uff0c\u6211\u4eec\u53ef\u4ee5\u7ed9\u51fa\u4e00\u5b9a\u7684\u9650\u5236 constraint. Option 1. \u4ee4 \\(f_{33} = 1\\) . Option 2. \u4ee4 \\(||f|| = 1\\) . \u5f97\u5230\u81ea\u7531\u5ea6\u4e3a \\(8\\) \uff0c\u81f3\u5c11\u9700\u8981 8 \u7ec4\u70b9. \u5e76\u540c\u6837\u8981\u6700\u5c0f\u5316 \\(||\\mathbf{Af - 0}|| = \\mathbf{||Af||}\\) . \u540c\u6837\u5982\u679c\u4f7f\u7528\u7684\u662f\u5411\u91cf\u8303\u6570\u4e3a \\(1\\) \u7684\u9650\u5236\uff0c\u90a3\u4e48\u89e3\u5c31\u662f \\(\\mathbf{A^{T}A}\\) \u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf. Step 3. Find \\(E\\) and Extract \\(R, \\mathbf{t}\\) \u00b6 \\[ E = K_l^T F K_r \\] \u901a\u8fc7 SVD \u5206\u89e3\uff0c \\(E = T_\\times R\\) \u5f97\u5230 \\(T_\\times\\) \u548c \\(R\\) . \\(R\\) \u5373\u4e3a\u65cb\u8f6c\u77e9\u9635\uff0c \\(\\mathbf{t}\\) \u53ef\u4ee5\u901a\u8fc7 \\(T_\\times\\) \u76f4\u63a5\u5f97\u5230. Step 4. Find 3D Position of Scene Points \u00b6 \u73b0\u5728\u5df2\u7ecf\u6709\u4ee5\u4e0b\u7b49\u5f0f \\[ \\small \\begin{aligned} \\text{Left Camera} \\\\ \\begin{bmatrix} u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} f_x^{(l)} & 0 & o_x^{(l)} & 0 \\\\ 0 & f_y^{(l)} & o_y^{(l)} & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\\\ 1 \\end{bmatrix} ,\\ \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y \\\\ r_{31} & r_{32} & r_{33} & t_z \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\\\ \\begin{bmatrix} u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} f_x^{(l)} & 0 & o_x^{(l)} & 0 \\\\ 0 & f_y^{(l)} & o_y^{(l)} & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y \\\\ r_{31} & r_{32} & r_{33} & t_z \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\\\ \\\\ \\normalsize \\mathbf{\\tilde u}_l &= \\normalsize P_l \\mathbf{\\tilde x}_r \\\\ \\\\ \\small \\text{Right Camera} \\\\ \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} f_x^{(r)} & 0 & o_x^{(r)} & 0 \\\\ 0 & f_y^{(r)} & o_y^{(r)} & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\\\ \\\\ \\normalsize \\mathbf{\\tilde u}_l &= \\normalsize M_{int_r} \\mathbf{\\tilde x}_r \\end{aligned} \\] \u7531 \\[ \\small \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} m_{11} & m_{12} & m_{13} & m_{14} \\\\ m_{21} & m_{22} & m_{23} & m_{24} \\\\ m_{31} & m_{32} & m_{33} & m_{34} \\\\ \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} ,\\ \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\\\ p_{21} & p_{22} & p_{23} & p_{24} \\\\ p_{31} & p_{32} & p_{33} & p_{34} \\\\ \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\] \u6574\u7406\u5f97 \\[ \\underbrace{ \\begin{bmatrix} u_r m_{31} - m_{11} & u_r m_{32} - m_{12} & u_r m_{33} - m_{13} \\\\ v_r m_{31} - m_{21} & v_r m_{32} - m_{22} & v_r m_{33} - m_{23} \\\\ u_l p_{31} - p_{11} & u_l p_{32} - p_{12} & u_l p_{33} - p_{13} \\\\ v_l p_{31} - p_{21} & v_l p_{32} - p_{22} & v_l p_{33} - p_{23} \\end{bmatrix} }_{A_{4\\times 3}} \\underbrace{ \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\end{bmatrix} }_{\\mathbf{x}_r} = \\underbrace{ \\begin{bmatrix} m_{14} - m_{34} \\\\ m_{24} - m_{34} \\\\ p_{14} - p_{34} \\\\ p_{24} - p_{34} \\end{bmatrix} }_{\\mathbf{b}} \\] \u7531\u5206\u6790\u4e0a\u89e3\u7684\u552f\u4e00\u6027\uff0c\u7406\u8bba\u4e0a\u6709\u4e24\u884c\u662f\u7ebf\u6027\u76f8\u5173\u7684. \u4f46\u662f\u5728\u5b9e\u9645\u53d6\u70b9\u4e2d\u53ef\u80fd\u4f1a\u5b58\u5728\u4e00\u5b9a\u7684\u8bef\u5dee\uff0c\u6240\u4ee5\u4e3a\u4e86\u6700\u5927\u5316\u5229\u7528\u6570\u636e\u91cf\uff0c\u8fd8\u662f\u91c7\u7528\u6700\u5c0f\u4e8c\u4e58\u6cd5. \u7531\u6700\u5c0f\u4e8c\u4e58\u6cd5\u5f97\uff0c \\[ \\mathbf{x}_r = (A^TA)^{-1}A^T \\mathbf{b} \\] Non-linear Solution \u4e0a\u9762\u7684\u662f\u901a\u8fc7\u89e3\u7ebf\u6027\u7cfb\u7edf\u7684\u65b9\u6cd5\uff0c\u53e6\u4e00\u79cd\u60f3\u6cd5\u662f\u6700\u5c0f\u5316 \u91cd\u6620\u5c04\u8bef\u5dee (reprojection error) . \\[ \\text{cost}(P) = \\text{dist}(\\mathbf{u}_l, \\mathbf{\\hat u}_l)^2 + \\text{dist}(\\mathbf{u}_r, \\mathbf{\\hat u}_r)^2 \\] \u53e6\u5916\uff0c\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\u6211\u4eec\u4e5f\u53ef\u4ee5\u4f18\u5316 \\(R\\) \u548c \\(\\mathbf{t}\\) . Multi-frame Structure from Motion \u00b6 \u4e0e Two-frame SfM \u7c7b\u4f3c\uff0c\u5df2\u77e5 \\(m\\) \u5f20\u56fe\u50cf\uff0c \\(n\\) \u4e2a\u4e09\u7ef4\u70b9\u4ee5\u53ca\u76f8\u673a\u5185\u53c2\u77e9\u9635\uff0c\u6c42\u76f8\u673a\u7684\u4f4d\u7f6e\u4ee5\u53ca\u6240\u62cd\u6444\u4e3b\u4f53\u7684\u4e09\u7ef4\u4f4d\u7f6e\u5750\u6807. \u4e5f\u5c31\u662f\u5bf9\u4e8e\u4ee5\u4e0b\u7b49\u5f0f\uff1a \\[ \\mathbf{u}_j^{(i)} = P_{proj}^{(i)} \\mathbf{P}_j \\text{, where } i = 1, \\dots, m, j = 1, \\dots, n \\] \u7531 \\(mn\\) \u4e2a\u6295\u5f71\u7684\u4e8c\u7ef4\u70b9 \\(\\mathbf{u}_j^{(i)}\\) \u6c42\u89e3 \\(m\\) \u4e2a\u6295\u5f71\u77e9\u9635 \\(P_{proj}^{(i)}\\) \u4e0e \\(n\\) \u4e2a\u4e09\u7ef4\u70b9\u5750\u6807 \\(P_j\\) . Solution: Sequential Structrue from Motion \u00b6 Step 1. Initialize camera motion and scene structure. \u521d\u59cb\u5316 \u00b6 \u9996\u5148\u9009\u4e24\u5f20\u56fe\u505a\u4e00\u6b21 Two-frame SfM. Step 2. Deal with an addition view. \u5904\u7406\u4e0b\u4e00\u5f20\u56fe \u00b6 \u5bf9\u4e8e\u6bcf\u65b0\u589e\u7684\u4e00\u5f20\u56fe\uff0c \u5bf9\u4e8e\u5df2\u7ecf\u5efa\u7acb\u4e09\u7ef4\u5750\u6807\u7684\u70b9\uff1a PnP \u95ee\u9898 . \u5bf9\u4e8e\u5c1a\u672a\u5efa\u7acb\u4e09\u7ef4\u5750\u6807\u7684\u70b9\uff1a\u505a Two-frame SfM. Step 3. Refine structure and motion: Bundle Adjustment. \u96c6\u675f\u8c03\u6574 \u00b6 \u5904\u7406\u5b8c\u6240\u6709\u7684\u56fe\u540e\uff0c\u518d\u901a\u8fc7\u4e00\u6b21 \u96c6\u675f\u8c03\u6574 \uff08\u5177\u4f53\u5b9e\u73b0\u662f LM \u7b97\u6cd5 \uff09\u5bf9\u4e09\u7ef4\u70b9\u5750\u6807\u548c\u76f8\u673a\u53c2\u6570\u505a\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u5373\u6700\u5c0f\u5316 \u91cd\u6620\u5c04\u8bef\u5dee \uff08\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\uff09\uff1a \\[ E(P_{proj}, \\mathbf{P}) = \\sum\\limits_{i=1}^m \\sum\\limits_{j=1}^n \\text{dist} \\left( u_j^{(i)}, P_{proj}^{(i)} \\mathbf{P}_j \\right)^2 \\] COLMAP \u00b6 COLMAP is a general-purpose Structure-from-Motion (SfM) and Multi-View Stereo (MVS) pipeline with a graphical and commandline interface. It offers a wide range of features for reconstruction of ordered and unordered image collections. Pipeline","title":"Lecture 7"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#lecture-7-structure-from-motion-sfm","text":"","title":"Lecture 7 Structure from Motion (SfM)"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#camera-calibration","text":"\u76f8\u673a\u6807\u5b9a\u7684\u8fc7\u7a0b\uff0c\u5c31\u662f\u4ece\u4e16\u754c\u5750\u6807\u7cfb\u5411\u50cf\u7d20\u5750\u6807\u7cfb\u8f6c\u6362\u7684\u8fc7\u7a0b. \u9700\u8981\u6ce8\u610f\u7684\u56db\u4e2a\u5750\u6807\u7cfb\uff1a \u4e16\u754c\u5750\u6807\u7cfb (World Coordinates) \u76f8\u673a\u5750\u6807\u7cfb (Camera Coordinates) \u56fe\u50cf\u5750\u6807\u7cfb (Image Plane) \u50cf\u7d20\u5750\u6807\u7cfb (Image Sensor) \u5176\u4e2d\uff0c\u524d\u4e24\u4e2a\u5750\u6807\u7cfb\u7684\u8f6c\u6362\u77e9\u9635\u53c2\u6570\u88ab\u79f0\u4e3a \u76f8\u673a\u5916\u53c2 \uff1b\u540e\u4e09\u4e2a\u5750\u6807\u7cfb\u7684\u8f6c\u6362\u77e9\u9635\u53c2\u6570\u88ab\u79f0\u4e3a \u76f8\u673a\u5185\u53c2 .","title":"Camera Calibration \u76f8\u673a\u6807\u5b9a"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#principle-analysis","text":"","title":"Principle Analysis \u539f\u7406\u5206\u6790"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#coordinate-transformation","text":"\u4ece\u4e16\u754c\u5750\u6807\u7cfb\u5230\u76f8\u673a\u5750\u6807\u7cfb\u7684\u53d8\u6362.","title":"Coordinate Transformation"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#_1","text":"\u521a\u4f53\u53d8\u6362\uff0c\u53ea\u6709\u65cb\u8f6c\u548c\u5e73\u79fb\uff0c\u5bf9\u5e94\u65cb\u8f6c\u77e9\u9635 \\(R\\) \uff08 \\(R\\) \u662f \u5355\u4f4d\u6b63\u4ea4\u77e9\u9635 (Orthonormal) ) \u548c\u5e73\u79fb\u5411\u91cf \\(c_w\\) . \\[ \\mathbf{x}_c = R(\\mathbf{x}_w - \\mathbf{c}_w) = R \\mathbf{x}_w - R \\mathbf{c}_w \\overset{\\Delta}{=\\!=} R \\mathbf{x}_w + \\mathbf{t}, \\text{ where } \\mathbf{t } = -R \\mathbf{c}_w. \\] \u6545\u8fd9\u4e2a\u53d8\u6362\u4e5f\u53ef\u4ee5\u7b49\u4ef7\u5730\u7528 \\(R\\) \u548c \\(\\mathbf{t}\\) \u6765\u8868\u5f81. \\[ \\mathbf{x}_c = \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\\\ \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} \\] \u540c\u6837\u8003\u8651\u9f50\u6b21\u5750\u6807\u7cfb \\[ \\mathbf{\\tilde x}_c = \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1\\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix} \\] Extrinsic Matrix (\u5916\u53c2\u77e9\u9635) \\[ M_{ext} = \\begin{bmatrix} R_{3\\times 3} & \\mathbf{t} \\\\ \\mathbf{0}_{1\\times 3} & 1 \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\] \u4e5f\u53ef\u4ee5\u5199\u4f5c \\[ M_{ext} = \\begin{bmatrix} R_{3\\times 3} & \\mathbf{t} \\\\ \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ \\end{bmatrix} \\]","title":"\u7279\u70b9"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#perspective-projection","text":"\u4ece\u76f8\u673a\u5750\u6807\u7cfb\u5230\u56fe\u50cf\u5750\u6807\u7cfb\u7684\u53d8\u6362. \u524d\u9762\u51e0\u4e2a Lecture \u4e2d\u5df2\u7ecf\u8ba8\u8bba\u8fc7\u8fd9\u4e00\u90e8\u5206 \u5185\u5bb9 . \u6b64\u5904\u4e0d\u518d\u8d58\u8ff0. \\[ \\begin{bmatrix} x_i \\\\ y_i \\\\ 1 \\end{bmatrix} \\cong \\begin{bmatrix} f & 0 & 0 & 0 \\\\ 0 & f & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1 \\end{bmatrix} \\]","title":"Perspective Projection"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#image-plane-to-image-sensor-mapping","text":"\u4ece\u56fe\u50cf\u5750\u6807\u7cfb\u5230\u50cf\u7d20\u5750\u6807\u7cfb\u7684\u53d8\u6362. \u50cf\u7d20\u70b9 pixel \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u7531\u4e8e\u76f8\u673a\u81ea\u8eab\u7684\u5236\u4f5c\u5de5\u827a\u548c\u8981\u6c42\uff0c\u50cf\u7d20\u70b9\u5e76\u4e0d\u4e00\u5b9a\u662f\u6b63\u65b9\u5f62\uff0c\u800c\u662f\u77e9\u5f62\uff0c\u4e0e\u539f\u56fe\u50cf\u76f8\u6bd4\u5177\u6709\u4e00\u5b9a\u7684 \u7f29\u653e . \u8fd9\u4e00\u70b9\u5728\u89c6\u9891\u7684\u5236\u4f5c\u4e2d\u4e5f\u6709\u6240\u4f53\u73b0. \u6b64\u5916\uff0c\u7531\u4e8e\u56fe\u50cf\u5750\u6807\u7cfb\u662f\u4ece\u4e2d\u5fc3\u4e3a\u539f\u70b9\uff0c\u800c\u50cf\u7d20\u5750\u6807\u7cfb\u662f\u4ee5\u5de6\u4e0a\u89d2\u4e3a\u539f\u70b9\u7684\uff0c\u5750\u6807\u53d8\u6362\u8fd8\u9700\u8981\u4e00\u5b9a\u7684* \u5e73\u79fb . \\[ \\begin{aligned} u &= m_x f \\frac{x_c}{z_c} + c_x \\\\ v &= m_y f \\frac{y_c}{z_c} + c_y \\end{aligned} \\] \u7ed3\u5408\u4ee5\u4e0a\u4e24\u4e2a\u53d8\u6362\uff0c\u8bb0 \\(f_x = m_x f\\) , \\(f_y = m_y f\\) . \u5219\u4ece\u76f8\u673a\u5750\u6807\u7cfb\u5230\u50cf\u7d20\u5750\u6807\u7cfb\u7684\u53d8\u6362\u4e3a \\[ \\mathbf{\\tilde{u}} = \\begin{bmatrix} u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1 \\end{bmatrix} \\] Intrinsic Matrix (\u5185\u53c2\u77e9\u9635) \\[ M_{int} = \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\] \u4e5f\u53ef\u4ee5\u5199\u4f5c \\[ K = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\] Projection Matrix \\(P\\) World to Camera \\[ \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1\\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix} \\] \\[ \\mathbf{\\tilde u} = M_{int}\\mathbf{\\tilde x}_c \\] Camera to Pixel \\[ \\begin{bmatrix} u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1 \\end{bmatrix} \\] \\[ \\mathbf{\\tilde x}_c = M_{ext}\\mathbf{\\tilde x}_w \\] \u6545 \\[ \\mathbf{\\tilde u} = M_{int}M_{ext} \\mathbf{\\tilde x}_w = P \\mathbf{\\tilde x} \\] \u5176\u4e2d\uff0c \\(P\\) \u88ab\u79f0\u4e3a\u6295\u5f71\u77e9\u9635 (Projection Matrix). \\[ \\begin{bmatrix} u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\\\ p_{21} & p_{22} & p_{23} & p_{24} \\\\ p_{31} & p_{32} & p_{33} & p_{34} \\\\ p_{41} & p_{42} & p_{43} & p_{44} \\\\ \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix} \\]","title":"Image Plane to Image Sensor Mapping"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#implemetation","text":"","title":"Implemetation"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-1-capture-an-image-of-an-object-with-known-geometry","text":"","title":"Step 1. Capture an image of an object with known geometry."},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-2-identify-correspondences-image-matching-between-3d-scene-points-and-image-points","text":"","title":"Step 2. Identify correspondences (Image Matching \u56fe\u50cf\u5339\u914d) between 3D scene points and image points."},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-3-for-each-corresponding-point-i-we-get","text":"\\[ \\begin{bmatrix} u^{(i)} \\\\ v^{(i)} \\\\ w^{(i)} \\end{bmatrix} \\cong \\begin{bmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\\\ p_{21} & p_{22} & p_{23} & p_{24} \\\\ p_{31} & p_{32} & p_{33} & p_{34} \\\\ p_{41} & p_{42} & p_{43} & p_{44} \\\\ \\end{bmatrix} \\begin{bmatrix} x_w^{(i)} \\\\ y_w^{(i)} \\\\ z_w^{(i)} \\\\ 1 \\end{bmatrix} \\] \\[ \\begin{aligned} u^{(i)} &= \\frac{p_{11}x_w^{(i)} + p_{12}y_w^{(i)} + p_{13}z_w^{(i)} + p_{14}}{p_{31}x_w^{(i)} + p_{32}y_w^{(i)} + p_{33}z_w^{(i)} + p_{34}} \\\\ v^{(i)} &= \\frac{p_{21}x_w^{(i)} + p_{22}y_w^{(i)} + p_{23}z_w^{(i)} + p_{24}}{p_{31}x_w^{(i)} + p_{32}y_w^{(i)} + p_{33}z_w^{(i)} + p_{34}} \\\\ \\end{aligned} \\]","title":"Step 3. For each corresponding point \\(i\\),  we get"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-4-rearrange","text":"\\[ \\scriptsize \\begin{bmatrix} x_w^{(1)} & y_w^{(1)} & z_w^{(1)} & 1 & 0 & 0 & 0 & 0 & -u_1x_w^{(1)} & -u_1y_w^{(1)} & -u_1z_w^{(1)} & -u_1 \\\\ 0 & 0 & 0 & 0 & x_w^{(1)} & y_w^{(1)} & z_w^{(1)} & 1 & -v_1x_w^{(1)} & -v_1y_w^{(1)} & -v_1z_w^{(1)} & -v_1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_w^{(i)} & y_w^{(i)} & z_w^{(i)} & 1 & 0 & 0 & 0 & 0 & -u_ix_w^{(i)} & -u_iy_w^{(i)} & -u_iz_w^{(i)} & -u_i \\\\ 0 & 0 & 0 & 0 & x_w^{(i)} & y_w^{(i)} & z_w^{(i)} & 1 & -v_ix_w^{(i)} & -v_iy_w^{(i)} & -v_iz_w^{(i)} & -v_i \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_w^{(n)} & y_w^{(n)} & z_w^{(n)} & 1 & 0 & 0 & 0 & 0 & -u_nx_w^{(n)} & -u_ny_w^{(n)} & -u_nz_w^{(n)} & -u_n \\\\ 0 & 0 & 0 & 0 & x_w^{(n)} & y_w^{(n)} & z_w^{(n)} & 1 & -v_nx_w^{(n)} & -v_ny_w^{(n)} & -v_nz_w^{(n)} & -v_n \\\\ \\end{bmatrix} \\begin{bmatrix} p_{11} \\\\ p_{12} \\\\ p_{13} \\\\ p_{14} \\\\ p_{21} \\\\ p_{22} \\\\ p_{23} \\\\ p_{24} \\\\ p_{31} \\\\ p_{32} \\\\ p_{33} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\] \\[ i.e.\\ A \\mathbf{p} = 0 \\]","title":"Step 4. Rearrange."},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-5-solve-for-p","text":"\u7c7b\u4f3c\u5730\uff0c\u6211\u4eec\u53ef\u4ee5\u7ed9\u51fa\u4e00\u5b9a\u7684\u9650\u5236 constraint. Option 1. \u4ee4 \\(p_{34} = 1\\) . Option 2. \u4ee4 \\(||p|| = 1\\) . \u5f97\u5230\u81ea\u7531\u5ea6\u4e3a \\(11\\) \uff0c\u81f3\u5c11\u9700\u8981 6 \u7ec4\u70b9. \u5e76\u540c\u6837\u8981\u6700\u5c0f\u5316 \\(||\\mathbf{Ap - 0}|| = \\mathbf{||Ap||}\\) . \u540c\u6837\u5982\u679c\u4f7f\u7528\u7684\u662f\u5411\u91cf\u8303\u6570\u4e3a \\(1\\) \u7684\u9650\u5236\uff0c\u90a3\u4e48\u6211\u4eec\u53ea\u9700\u8981\u786e\u5b9a \\(\\mathbf{p}\\) \u7684\u65b9\u5411 \\(\\mathbf{\\hat p}\\) . \u4e14 \\(\\mathbf{\\hat p}\\) \u662f \\(\\mathbf{A^{T}A}\\) \u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf.","title":"Step 5 Solve for \\(P\\)."},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-6-from-p-to-answers","text":"\u7531 \\[ \\begin{bmatrix} p_{11} \\\\ p_{12} \\\\ p_{13} \\\\ p_{14} \\\\ p_{21} \\\\ p_{22} \\\\ p_{23} \\\\ p_{24} \\\\ p_{31} \\\\ p_{32} \\\\ p_{33} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\] \u6ce8\u610f\u5230 \\[ \\begin{bmatrix} p_{11} \\\\ p_{12} \\\\ p_{13} \\\\ p_{21} \\\\ p_{22} \\\\ p_{23} \\\\ p_{31} \\\\ p_{32} \\\\ p_{33} \\end{bmatrix} = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\\\ \\end{bmatrix} = KR \\] \u5176\u4e2d \\(K\\) \u662f\u4e0a\u4e09\u89d2\u77e9\u9635 (Upper Right Triangular Matrix)\uff0c \\(R\\) \u662f\u5355\u4f4d\u6b63\u4ea4\u77e9\u9635 (Orthonormal Matrix). \u7531 QR \u5206\u89e3 (QR Factorization) \u53ef\u77e5\uff0c \\(K\\) \u4e0e \\(R\\) \u7684\u89e3\u662f\u552f\u4e00\u7684. \u518d\u7531 \\[ \\begin{bmatrix} p_{14} \\\\ p_{24} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} = K \\mathbf{t} \\] \u6545\u53ef\u89e3\u5f97 \\[ \\mathbf{t} = K^{-1} \\begin{bmatrix} p_{14} \\\\ p_{24} \\\\ p_{34} \\end{bmatrix} \\] Problem: Distortion \u5b9e\u9645\u4e0a\u76f8\u673a\u81ea\u8eab\u4f1a\u5177\u6709\u4e00\u5b9a\u7684\u626d\u66f2\u7cfb\u6570\uff0c\u6211\u4eec\u5728\u6b64 \u5ffd\u7565 \u5b83\u7684\u5f71\u54cd. Perspective-n-Point Problem (PnP \u95ee\u9898) \u5f31\u5316\u4e00\u4e0b\u6761\u4ef6\uff0c\u5047\u8bbe\u5df2\u77e5\u76f8\u673a\u7684\u5185\u53c2\uff08\u5373\u77e9\u9635 \\(K\\) \uff09\uff0c\u6c42\u76f8\u673a\u7684\u5916\u53c2\uff08\u5373\u77e9\u9635 \\(R\\) \u548c \\(\\mathbf{t}\\) \uff09\uff0c\u4e5f\u5373\u76f8\u673a\u59ff\u6001 (camera pose).","title":"Step 6 From \\(P\\) to Answers"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#direct-linear-transform-dlt","text":"\u5373\u4ee5\u4e0a\u5728\u76f8\u673a\u6807\u5b9a\u4e2d\u63d0\u5230\u7684\u65b9\u6cd5\uff0c\u5148\u6c42\u89e3\u77e9\u9635 \\(P\\) \uff0c\u518d\u901a\u8fc7 \\(K\\) \u5f97\u5230 \\(R\\) \u548c \\(\\mathbf{t}\\) .","title":"Direct Linear Transform (DLT)"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#p3p","text":"\u5373\u5df2\u77e5 \\(a, b, c, A, B, C\\) \u70b9\u5750\u6807\uff0c\u6c42 \\(O\\) \u5750\u6807. \u81f3\u5c11\u9700\u8981 \\(3\\) \u7ec4\u70b9 Intermediates. \u7531\u4f59\u5f26\u5b9a\u7406 \\[ \\begin{aligned} OA^2 + OB^2 - 2 OA \\cdot OB \\cos \\angle AOB &= AB^2 \\\\ OA^2 + OC^2 - 2 OB \\cdot OC \\cos \\angle BOC &= BC^2 \\\\ OA^2 + OC^2 - 2 OA \\cdot OC \\cos \\angle AOC &= AC^2 \\end{aligned} \\] \u4ee4 \\(x = \\frac{OA}{OC}, y = \\frac{OB}{OC}, v = \\frac{AB^2}{OC^2}, u = \\frac{BC^2}{AB^2}, w = \\frac{AC^2}{AB^2}\\) \uff0c\u5316\u7b80\u5f97 \\[ \\begin{aligned} (1 - u)y^2 - ux^2 - cos \\angle BOC y + 2uxy \\cos \\angle AOB + 1 &= 0 \\\\ (1 - w)x^2 - wy^2 - cos \\angle AOC y + 2wxy \\cos \\angle AOB + 1 &= 0 \\end{aligned} \\] \u4ee5\u4e0a\u65b9\u7a0b\u7ec4\u662f\u4e8c\u5143\u4e8c\u6b21\u65b9\u7a0b\u7ec4 (Binary Quadratic Equation)\uff0c\u6709\u56db\u7ec4\u89e3. \u5176\u4e2d\u6709\u4e24\u7ec4\u89e3\u7684 \\(O\\) \u5750\u6807\u5728 \u9762 \\(ABC\\) \u4e0e \u9762 \\(abc\\) \u4e4b\u95f4\uff0c\u820d\u53bb. \u8fd8\u9700\u8981\u4e00\u7ec4\u989d\u5916\u7684\u70b9\u6765\u786e\u5b9a\u552f\u4e00\u89e3\uff0c\u6545\u4e00\u5171\u9700\u8981 \\(4\\) \u7ec4\u70b9.","title":"P3P"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#pnp","text":"\u6700\u5c0f\u5316 \u91cd\u6295\u5f71\u8bef\u5dee (Reprojection Error) \\[ \\mathop{\\arg\\min\\limits_{R, \\mathbf{t}}} \\sum\\limits_i ||(p_i, K(RP_i + \\mathbf{t})||^2 \\] \u5176\u4e2d\uff0c \\(p_i\\) \u662f\u6295\u5f71\u9762\u4e0a\u7684\u4e8c\u7ef4\u5750\u6807\uff0c \\(P_i\\) \u662f\u7a7a\u95f4\u5185\u7684\u4e09\u7ef4\u5750\u6807. \u901a\u8fc7 P3P \u521d\u59cb\u5316\uff0cGauss-Newton \u6cd5\u4f18\u5316.","title":"PnP"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#epnp","text":"one of the most popluar methods. time complexity \\(O(N)\\) . high accuracy. \u5927\u81f4\u65b9\u5f0f\u662f\u901a\u8fc7\u56db\u7ec4\u63a7\u5236\u70b9\u7684\u7ebf\u6027\u7ec4\u5408\u6c42\u89e3.","title":"EPnP"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#two-frame-structure-from-motion","text":"Stereo vision \u7acb\u4f53\u89c6\u89c9 Compute 3D structure of the scene and camera poses from views (images). Two-frame SfM \u610f\u5728\u89e3\u51b3\u8fd9\u6837\u4e00\u4e2a\u95ee\u9898\uff1a\u5df2\u77e5\u4e24\u5f20\u56fe\u50cf\u548c\u76f8\u673a\u5185\u53c2\u77e9\u9635 ( \\(K_l(3 \\times 3)\\) \u4e0e \\(K_r(3 \\times 3)\\) )\uff0c\u6c42\u76f8\u673a\u7684\u4f4d\u7f6e\u4ee5\u53ca\u6240\u62cd\u6444\u4e3b\u4f53\u7684\u4e09\u7ef4\u4f4d\u7f6e\u5750\u6807.","title":"Two-frame Structure from Motion"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#principle-induction","text":"Epipolar Geometry \u5bf9\u6781\u51e0\u4f55 \u5bf9\u6781\u51e0\u4f55\u63cf\u8ff0\u4e86\u4e00\u4e2a\u4e09\u7ef4\u70b9 \\(P\\) \u5728\u4e24\u4e2a\u89c6\u89d2\u7684\u4e8c\u7ef4\u6295\u5f71 \\(u_l, u_r\\) \u4e4b\u95f4\u7684\u51e0\u4f55\u5173\u7cfb. \u5e76\u901a\u8fc7\u8fd9\u4e2a\u51e0\u4f55\u5173\u7cfb\u5efa\u7acb\u4e24\u4e2a\u6444\u50cf\u673a\u4e4b\u95f4\u7684\u53d8\u6362\u5173\u7cfb\uff08\u5373\u7ed9\u51fa\u77e9\u9635 \\(R\\) \u548c \\(\\mathbf{t}\\) \uff09. Definition Epipole \u5bf9\u6781\u70b9 \uff1a\u4e00\u53f0\u6444\u50cf\u673a\u6295\u5f71\u5728\u53e6\u4e00\u53f0\u6444\u50cf\u673a\u4e2d\u7684\u70b9. \u5982\u56fe\u4e2d\u7684 \\(e_l\\) \u548c \\(e_r\\) . \u5bf9\u7ed9\u5b9a\u7684\u4e24\u53f0\u6444\u50cf\u673a\uff0c \\(e_l\\) \u548c \\(e_r\\) \u662f\u552f\u4e00\u7684. Epipolar Plane of Scene Point P \u5173\u4e8e P \u70b9\u7684\u5bf9\u6781\u9762 \uff1a\u7531 Scene Point \\(P\\) \u70b9\u548c\u4e24\u4e2a\u6444\u50cf\u673a \\(O_l\\) \u548c \\(O_r\\) \u5f62\u6210\u7684\u5e73\u9762. \u6bcf\u4e2a scene point \u6709\u552f\u4e00\u5bf9\u5e94\u7684\u5bf9\u6781\u9762. \u5bf9\u6781\u70b9\u5728\u5bf9\u6781\u9762\u4e0a.","title":"Principle Induction \u539f\u7406\u63a8\u5bfc"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#essential-matrix-e","text":"\u5bf9\u4e8e\u4e00\u4e2a\u5bf9\u6781\u9762\uff0c\u4ee5\u4e0b\u7b49\u5f0f\u5173\u7cfb\u6210\u7acb\uff1a \\[ \\begin{aligned} \\mathbf{n} = \\mathbf{t} \\times \\mathbf{x}_l \\\\ \\\\ \\mathbf{x}_l \\cdot (\\mathbf{t} \\times \\mathbf{x}_l) = 0 \\end{aligned} \\] \u5176\u4e2d \\(\\mathbf{n}\\) \u88ab\u79f0\u4e3a Normal Vector. \u5047\u8bbe\u53f3\u8fb9\u7684\u6444\u50cf\u673a\u76f8\u5bf9\u5de6\u8fb9\u7684\u6444\u50cf\u673a\u8fdb\u884c\u521a\u4f53\u53d8\u6362 + \u5e73\u79fb\uff0c\u5373\uff1a \\[ \\begin{aligned} \\mathbf{x}_l &= R \\mathbf{x}_r + \\mathbf{t} \\\\ \\\\ \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\end{bmatrix} &= \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ y_r \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} \\end{aligned} \\] \u8fdb\u4e00\u6b65\u6f14\u7ece\u63a8\u7406\uff0c\u5f97\u5230\uff1a \\[ \\begin{aligned} \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\begin{bmatrix} t_yz_l - t_zy_l \\\\ t_zx_l - t_xz_l \\\\ t_xy_l - t_yx_l \\end{bmatrix} = 0 & \\text{, Cross-product definition} \\\\ \\\\ \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\underbrace{ \\begin{bmatrix} 0 & -t_z & t_y \\\\ t_z & 0 & -t_x \\\\ -t_y & t_x & 0 \\end{bmatrix} }_{T_\\times} \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\end{bmatrix} = 0 & \\text{, Matrix-vector form} \\end{aligned} \\] \u4ee3\u5165\u53d8\u6362\u77e9\u9635 \\[ \\small \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\left( \\underbrace{ \\begin{bmatrix} 0 & -t_z & t_y \\\\ t_z & 0 & -t_x \\\\ -t_y & t_x & 0 \\end{bmatrix} }_{T_\\times} \\underbrace{ \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\end{bmatrix} }_{R} \\begin{bmatrix} x_r \\\\ y_r \\\\ y_r \\end{bmatrix} + \\underbrace{ \\begin{bmatrix} 0 & -t_z & t_y \\\\ t_z & 0 & -t_x \\\\ -t_y & t_x & 0 \\end{bmatrix} \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} }_{\\mathbf{t} \\times \\mathbf{t} = \\mathbf{0}} \\right) = 0 \\] \\[ \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\underbrace{ \\begin{bmatrix} e_{11} & e_{12} & e_{13} \\\\ e_{21} & e_{22} & e_{23} \\\\ e_{31} & e_{32} & e_{33} \\end{bmatrix} }_{\\text{Essential Matrix } E} \\begin{bmatrix} x_r \\\\ y_r \\\\ y_r \\end{bmatrix} \\text{, where } E = T_\\times R \\] \u6ce8\u610f\u5230 \\(T_\\times\\) \u662f\u53cd\u5bf9\u79f0\u77e9\u9635 (skew-symmetric matrix) \u800c \\(R\\) \u662f\u5355\u4f4d\u6b63\u4ea4\u77e9\u9635 (orthonomal matrix)\uff0c\u7531 SVD \u5206\u89e3 (Singule Value Decomposition)\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece \\(E\\) \u552f\u4e00\u5206\u89e3\u5f97\u5230 \\(T_\\times\\) \u548c \\(R\\) .","title":"Essential Matrix \\(E\\) \u57fa\u672c\u77e9\u9635"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#fundemental-matrix-f","text":"\u7531\u6295\u5f71\u5173\u7cfb\u4ee5\u53ca\u5185\u53c2\u77e9\u9635 \\(K\\) \uff0c\u5f97\u5230\uff1a \\[ \\small \\begin{aligned} \\text{Left Camera} && \\text{Right Camera} \\\\ z_l \\begin{bmatrix} u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &= \\underbrace{ \\begin{bmatrix} f_x^{(l)} & 0 & o_x^{(l)} \\\\ 0 & f_y^{(l)} & o_y^{(l)} \\\\ 0 & 0 & 1 \\end{bmatrix} }_{K_l} \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\end{bmatrix} & z_r \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} &= \\underbrace{ \\begin{bmatrix} f_x^{(r)} & 0 & o_x^{(r)} \\\\ 0 & f_y^{(r)} & o_y^{(r)} \\\\ 0 & 0 & 1 \\end{bmatrix} }_{K_r} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\end{bmatrix} \\\\ \\mathbf{x}_l^T &= \\begin{bmatrix} u_l & v_l & 1 \\end{bmatrix} z_l (K_l^{-1})^T & \\mathbf{x}_r &= K_r^{-1} z_r \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} \\end{aligned} \\] \u4ee3\u5165\u4ee5\u4e0a\u5f97\u5230\u7684\uff1a \\[ \\mathbf{x}^T_l E \\mathbf{x}_r = 0 \\] \u6709 \\[ \\begin{bmatrix} u_l & v_l & 1 \\end{bmatrix} z_l (K_l^{-1})^T E K_r^{-1} z_r \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = 0 \\] \u7ea6\u53bb \\(z_l\\) , \\(z_r\\) \uff0c\u5219 \\[ \\begin{bmatrix} u_l & v_l & 1 \\end{bmatrix} F \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = 0 \\text{, where } E = K_l^T F K_r \\]","title":"Fundemental Matrix \\(F\\) \u672c\u771f\u77e9\u9635"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#implementation","text":"","title":"Implementation \u5b9e\u73b0"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-1-find-a-set-m-pairs-of-corresponding-features-m","text":"Least Number of Points At least 8 pairs: \\((u_l^{(i)}, v_l^{(i)})\\) \u2194 \\((u_r^{(i)}, v_r^{(i)})\\) NOTE that one pair only corresponds one equation. (not the same as before)","title":"Step 1. Find a set (\\(m\\) pairs) of corresponding features. \u627e\u5230\u4e00\u7ec4\uff08\\(m\\) \u5bf9\uff09\u5339\u914d\u70b9."},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-2-build-linear-systems-and-solve-for-f-f","text":"\u5bf9\u4e8e\u6bcf\u7ec4\u5339\u914d \\(i\\) \uff0c \\[ \\begin{bmatrix} u_l^{(i)} & v_l^{(i)} & 1 \\end{bmatrix} \\begin{bmatrix} f_{11} & f_{12} & f_{13} \\\\ f_{21} & f_{22} & f_{23} \\\\ f_{31} & f_{32} & f_{33} \\end{bmatrix} \\begin{bmatrix} u_r^{(i)} \\\\ v_r^{(i)} \\\\ 1 \\end{bmatrix} = 0 \\] \u5c55\u5f00\u5f97\uff0c \\[ \\small \\left( f_{11} u_r^{(i)} + f_{12} v_r^{(i)} + f_13 \\right)u_l^{(i)} + \\left( f_{21} u_r^{(i)} + f_{22} v_r^{(i)} + f_23 \\right)v_l^{(i)} + f_{31} u_r^{(i)} + f_{32} v_r^{(i)} + f_33 = 0 \\] \u5bf9\u6240\u6709\u9009\u62e9\u7684\u70b9\uff0c\u6709 \\[ \\small \\begin{bmatrix} u_l^{(1)}u_r^{(1)} & u_l^{(1)}v_r^{(1)} & u_l^{(1)} & v_l^{(1)}u_r^{(1)} & v_l^{(1)}v_r^{(1)} & v_l^{(1)} & u_r^{(1)} & v_r^{(1)} & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ u_l^{(i)}u_r^{(i)} & u_l^{(i)}v_r^{(i)} & u_l^{(i)} & v_l^{(i)}u_r^{(i)} & v_l^{(i)}v_r^{(i)} & v_l^{(i)} & u_r^{(i)} & v_r^{(i)} & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ u_l^{(m)}u_r^{(m)} & u_l^{(m)}v_r^{(m)} & u_l^{(m)} & v_l^{(m)}u_r^{(m)} & v_l^{(m)}v_r^{(m)} & v_l^{(m)} & u_r^{(m)} & v_r^{(m)} & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} f_{11} \\\\ f_{12} \\\\ f_{13} \\\\ f_{21} \\\\ f_{22} \\\\ f_{23} \\\\ f_{31} \\\\ f_{32} \\\\ f_{33} \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\] \\[ A \\mathbf{f} = \\mathbf{0} \\] \u540c\u6837\u5730 \uff0c\u6211\u4eec\u53ef\u4ee5\u7ed9\u51fa\u4e00\u5b9a\u7684\u9650\u5236 constraint. Option 1. \u4ee4 \\(f_{33} = 1\\) . Option 2. \u4ee4 \\(||f|| = 1\\) . \u5f97\u5230\u81ea\u7531\u5ea6\u4e3a \\(8\\) \uff0c\u81f3\u5c11\u9700\u8981 8 \u7ec4\u70b9. \u5e76\u540c\u6837\u8981\u6700\u5c0f\u5316 \\(||\\mathbf{Af - 0}|| = \\mathbf{||Af||}\\) . \u540c\u6837\u5982\u679c\u4f7f\u7528\u7684\u662f\u5411\u91cf\u8303\u6570\u4e3a \\(1\\) \u7684\u9650\u5236\uff0c\u90a3\u4e48\u89e3\u5c31\u662f \\(\\mathbf{A^{T}A}\\) \u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf.","title":"Step 2. Build linear systems and solve for \\(F\\). \u5efa\u7acb\u65b9\u7a0b\u7ec4\u5e76\u6c42\u89e3\u77e9\u9635 \\(F\\)."},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-3-find-e-and-extract-r-mathbft","text":"\\[ E = K_l^T F K_r \\] \u901a\u8fc7 SVD \u5206\u89e3\uff0c \\(E = T_\\times R\\) \u5f97\u5230 \\(T_\\times\\) \u548c \\(R\\) . \\(R\\) \u5373\u4e3a\u65cb\u8f6c\u77e9\u9635\uff0c \\(\\mathbf{t}\\) \u53ef\u4ee5\u901a\u8fc7 \\(T_\\times\\) \u76f4\u63a5\u5f97\u5230.","title":"Step 3. Find \\(E\\) and Extract \\(R, \\mathbf{t}\\)"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-4-find-3d-position-of-scene-points","text":"\u73b0\u5728\u5df2\u7ecf\u6709\u4ee5\u4e0b\u7b49\u5f0f \\[ \\small \\begin{aligned} \\text{Left Camera} \\\\ \\begin{bmatrix} u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} f_x^{(l)} & 0 & o_x^{(l)} & 0 \\\\ 0 & f_y^{(l)} & o_y^{(l)} & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\\\ 1 \\end{bmatrix} ,\\ \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y \\\\ r_{31} & r_{32} & r_{33} & t_z \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\\\ \\begin{bmatrix} u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} f_x^{(l)} & 0 & o_x^{(l)} & 0 \\\\ 0 & f_y^{(l)} & o_y^{(l)} & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y \\\\ r_{31} & r_{32} & r_{33} & t_z \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\\\ \\\\ \\normalsize \\mathbf{\\tilde u}_l &= \\normalsize P_l \\mathbf{\\tilde x}_r \\\\ \\\\ \\small \\text{Right Camera} \\\\ \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} f_x^{(r)} & 0 & o_x^{(r)} & 0 \\\\ 0 & f_y^{(r)} & o_y^{(r)} & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\\\ \\\\ \\normalsize \\mathbf{\\tilde u}_l &= \\normalsize M_{int_r} \\mathbf{\\tilde x}_r \\end{aligned} \\] \u7531 \\[ \\small \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} m_{11} & m_{12} & m_{13} & m_{14} \\\\ m_{21} & m_{22} & m_{23} & m_{24} \\\\ m_{31} & m_{32} & m_{33} & m_{34} \\\\ \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} ,\\ \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\\\ p_{21} & p_{22} & p_{23} & p_{24} \\\\ p_{31} & p_{32} & p_{33} & p_{34} \\\\ \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\] \u6574\u7406\u5f97 \\[ \\underbrace{ \\begin{bmatrix} u_r m_{31} - m_{11} & u_r m_{32} - m_{12} & u_r m_{33} - m_{13} \\\\ v_r m_{31} - m_{21} & v_r m_{32} - m_{22} & v_r m_{33} - m_{23} \\\\ u_l p_{31} - p_{11} & u_l p_{32} - p_{12} & u_l p_{33} - p_{13} \\\\ v_l p_{31} - p_{21} & v_l p_{32} - p_{22} & v_l p_{33} - p_{23} \\end{bmatrix} }_{A_{4\\times 3}} \\underbrace{ \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\end{bmatrix} }_{\\mathbf{x}_r} = \\underbrace{ \\begin{bmatrix} m_{14} - m_{34} \\\\ m_{24} - m_{34} \\\\ p_{14} - p_{34} \\\\ p_{24} - p_{34} \\end{bmatrix} }_{\\mathbf{b}} \\] \u7531\u5206\u6790\u4e0a\u89e3\u7684\u552f\u4e00\u6027\uff0c\u7406\u8bba\u4e0a\u6709\u4e24\u884c\u662f\u7ebf\u6027\u76f8\u5173\u7684. \u4f46\u662f\u5728\u5b9e\u9645\u53d6\u70b9\u4e2d\u53ef\u80fd\u4f1a\u5b58\u5728\u4e00\u5b9a\u7684\u8bef\u5dee\uff0c\u6240\u4ee5\u4e3a\u4e86\u6700\u5927\u5316\u5229\u7528\u6570\u636e\u91cf\uff0c\u8fd8\u662f\u91c7\u7528\u6700\u5c0f\u4e8c\u4e58\u6cd5. \u7531\u6700\u5c0f\u4e8c\u4e58\u6cd5\u5f97\uff0c \\[ \\mathbf{x}_r = (A^TA)^{-1}A^T \\mathbf{b} \\] Non-linear Solution \u4e0a\u9762\u7684\u662f\u901a\u8fc7\u89e3\u7ebf\u6027\u7cfb\u7edf\u7684\u65b9\u6cd5\uff0c\u53e6\u4e00\u79cd\u60f3\u6cd5\u662f\u6700\u5c0f\u5316 \u91cd\u6620\u5c04\u8bef\u5dee (reprojection error) . \\[ \\text{cost}(P) = \\text{dist}(\\mathbf{u}_l, \\mathbf{\\hat u}_l)^2 + \\text{dist}(\\mathbf{u}_r, \\mathbf{\\hat u}_r)^2 \\] \u53e6\u5916\uff0c\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\u6211\u4eec\u4e5f\u53ef\u4ee5\u4f18\u5316 \\(R\\) \u548c \\(\\mathbf{t}\\) .","title":"Step 4. Find 3D Position of Scene Points"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#multi-frame-structure-from-motion","text":"\u4e0e Two-frame SfM \u7c7b\u4f3c\uff0c\u5df2\u77e5 \\(m\\) \u5f20\u56fe\u50cf\uff0c \\(n\\) \u4e2a\u4e09\u7ef4\u70b9\u4ee5\u53ca\u76f8\u673a\u5185\u53c2\u77e9\u9635\uff0c\u6c42\u76f8\u673a\u7684\u4f4d\u7f6e\u4ee5\u53ca\u6240\u62cd\u6444\u4e3b\u4f53\u7684\u4e09\u7ef4\u4f4d\u7f6e\u5750\u6807. \u4e5f\u5c31\u662f\u5bf9\u4e8e\u4ee5\u4e0b\u7b49\u5f0f\uff1a \\[ \\mathbf{u}_j^{(i)} = P_{proj}^{(i)} \\mathbf{P}_j \\text{, where } i = 1, \\dots, m, j = 1, \\dots, n \\] \u7531 \\(mn\\) \u4e2a\u6295\u5f71\u7684\u4e8c\u7ef4\u70b9 \\(\\mathbf{u}_j^{(i)}\\) \u6c42\u89e3 \\(m\\) \u4e2a\u6295\u5f71\u77e9\u9635 \\(P_{proj}^{(i)}\\) \u4e0e \\(n\\) \u4e2a\u4e09\u7ef4\u70b9\u5750\u6807 \\(P_j\\) .","title":"Multi-frame Structure from Motion"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#solution-sequential-structrue-from-motion","text":"","title":"Solution: Sequential Structrue from Motion"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-1-initialize-camera-motion-and-scene-structure","text":"\u9996\u5148\u9009\u4e24\u5f20\u56fe\u505a\u4e00\u6b21 Two-frame SfM.","title":"Step 1. Initialize camera motion and scene structure. \u521d\u59cb\u5316"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-2-deal-with-an-addition-view","text":"\u5bf9\u4e8e\u6bcf\u65b0\u589e\u7684\u4e00\u5f20\u56fe\uff0c \u5bf9\u4e8e\u5df2\u7ecf\u5efa\u7acb\u4e09\u7ef4\u5750\u6807\u7684\u70b9\uff1a PnP \u95ee\u9898 . \u5bf9\u4e8e\u5c1a\u672a\u5efa\u7acb\u4e09\u7ef4\u5750\u6807\u7684\u70b9\uff1a\u505a Two-frame SfM.","title":"Step 2. Deal with an addition view. \u5904\u7406\u4e0b\u4e00\u5f20\u56fe"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-3-refine-structure-and-motion-bundle-adjustment","text":"\u5904\u7406\u5b8c\u6240\u6709\u7684\u56fe\u540e\uff0c\u518d\u901a\u8fc7\u4e00\u6b21 \u96c6\u675f\u8c03\u6574 \uff08\u5177\u4f53\u5b9e\u73b0\u662f LM \u7b97\u6cd5 \uff09\u5bf9\u4e09\u7ef4\u70b9\u5750\u6807\u548c\u76f8\u673a\u53c2\u6570\u505a\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u5373\u6700\u5c0f\u5316 \u91cd\u6620\u5c04\u8bef\u5dee \uff08\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\uff09\uff1a \\[ E(P_{proj}, \\mathbf{P}) = \\sum\\limits_{i=1}^m \\sum\\limits_{j=1}^n \\text{dist} \\left( u_j^{(i)}, P_{proj}^{(i)} \\mathbf{P}_j \\right)^2 \\]","title":"Step 3. Refine structure and motion: Bundle Adjustment. \u96c6\u675f\u8c03\u6574"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#colmap","text":"COLMAP is a general-purpose Structure-from-Motion (SfM) and Multi-View Stereo (MVS) pipeline with a graphical and commandline interface. It offers a wide range of features for reconstruction of ordered and unordered image collections. Pipeline","title":"COLMAP"},{"location":"Computer_Science_Courses/ICV/8_Depth_Estimation_and_3D_Reconstruction/","text":"","title":"Lecture 8"},{"location":"Computer_Science_Courses/ICV/9_Deep_Learning/","text":"","title":"Lecture 9"},{"location":"Mathematics_Basis/","text":"Title Page \u00b6 Abstract This section stores the notes of mathematics basis. Algrebra, analysis, and any other courses or topics concerned about mathematics. Welcome to point out anything wrong and careless mistakes!","title":"Title Page"},{"location":"Mathematics_Basis/#title-page","text":"Abstract This section stores the notes of mathematics basis. Algrebra, analysis, and any other courses or topics concerned about mathematics. Welcome to point out anything wrong and careless mistakes!","title":"Title Page"},{"location":"Mathematics_Basis/DM/Chap_1/","text":"Chapter 1 | The Foundations: Logic and Proofs \u00b6 Proposition \u00b6 Definition Proposition (\u63a8\u65ad) is a statement that is either true or false, but not both. Logical Operators / Connective \u00b6 Negation (\u5426\u5b9a) \\(\\neg p\\) . Conjunction (\u5408\u53d6) \\(p \\wedge q\\) . Disjunction (\u6790\u53d6) \\(p \\vee q\\) . Implication (\u6761\u4ef6 / \u8574\u542b) \\(p \\rightarrow q\\) . \\(p\\) is called the hypothesis (or antecedent or premise) . \\(q\\) is called the conclusion (or consequence) . Common Ways to Express Implication \\(p \\rightarrow q\\) If \\(p\\) then \\(q\\) . \\(p\\) is sufficient for \\(q\\) . \\(p\\) implies \\(q\\) . \\(q\\) is necessary for \\(p\\) . If \\(q\\) whenever \\(p\\) \\(p\\) only if \\(q\\) . Biconditional (\u53cc\u6761\u4ef6) \\(p \\leftrightarrow q\\) . Priorities \\(\\neg\\) , then \\(\\wedge\\ \\vee\\) , then \\(\\rightarrow\\ \\leftrightarrow\\) . Remark There is no such a sign \\(\\leftarrow\\) in the discussion in Dicrete Mathematics. Disjunction is inclusive or . Example inclusive or (\u6216\u3001\u517c\u6216). e.g. I passed mathematics or English. exclusive or (\u5f02\u6216). e.g. Paul was born in 1983 or 1984. \u5408\u53d6\u5426\u5b9a (denoted by Sheffer stroke \u8c22\u8d39\u5c14\u7ad6\u7ebf ) \\(p\\uparrow q \\Leftrightarrow \\neg(p \\wedge q)\\) \uff0c\u6790\u53d6\u5426\u5b9a (denoted by Peirce arrow \u76ae\u5c14\u65af\u7bad\u5934 ) \\(p\\downarrow q \\Leftrightarrow \\neg(p \\vee q)\\) . Propositional Formula \u00b6 Definition (Well Formed) Formula (wff) are one of the follows: Propositional variable / Proposition, \\(T\\) , \\(F\\) . \\(\\neg A\\) , \\((A \\wedge B)\\) , \\((A \\vee B)\\) , \\((A \\rightarrow B)\\) , \\((A \\leftrightarrow B)\\) , if \\(A\\) and \\(B\\) are formulae. Finitely many composed applications above. Definition | Classification tautology (\u91cd\u8a00\u5f0f / \u6c38\u771f\u5f0f): if its truth table contains only true values for every case. contradiction (\u6c38\u5047\u5f0f): if its truth table contains only false values for every case. Contigence: neither a tautology nor a contradiction. Proposition Equivalences \u00b6 Definition Formulae \\(A\\) and \\(B\\) are logical equivalent if \\(A \\leftrightarrow B\\) is tautology, denoted by \\(A \\Leftrightarrow B\\) . Definition if \\(p \\rightarrow q\\) , inverse (\u5426\u547d\u9898) \\(\\neg p \\rightarrow \\neg q\\) . converse (\u9006\u547d\u9898) \\(q \\rightarrow p\\) . contrapositive (\u9006\u5426\u547d\u9898) \\(\\neg q \\rightarrow \\neg p\\) NOTE: \\(p \\rightarrow q \\Leftrightarrow \\neg q \\rightarrow \\neg p\\) . Predicates and Quantifieres \u00b6 Definition Predicates (\u8c13\u8bcd / \u65ad\u8a00) is a statement of the form \\(P(x_1, x_2, \\dots, x_n)\\) , where \\(P\\) is a propositional function at the n -tuple \\((x_1, x_2,\\dots,x_n)\\) . Quantifier \u91cf\u8bcd \u00b6 domain / universe of discourse \u8bba\u57df Universal Quantifier \u5168\u79f0\u91cf\u8bcd For all \\(x\\) , \\(p(x): \\forall xp(x)\\) . Existential Quantifier \u5b58\u5728\u91cf\u8bcd For some \\(x\\) , \\(p(x): \\exists xp(x)\\) . Banding Variables \u00b6 Definition When a quantifier is used on the variable \\(x\\) , then the occurrence of \\(x\\) is bound while others are free . The part of a logical expression to which a quantifier is applied is called the scope (\u8f96\u57df) of this quantifier. Remark If the universe of discourse can be listed, then \\(\\forall xP(x) \\Leftrightarrow P(x_1) \\wedge \\dots \\wedge P(x_n)\\) and \\(\\exists xP(x) \\Leftrightarrow P(x_1) \\vee\\dots \\vee P(x_n)\\) . Properties De Morgan's Laws: \\(\\neg \\forall x p(x) \\Leftrightarrow \\exists x \\neg p(x),\\ \\ \\neg \\exists x p(x) \\Leftrightarrow \\forall x \\neg p(x)\\) . \\[ \\begin{aligned} \\forall x ((p(x) \\wedge q(x)) & \\Leftrightarrow (\\forall x p(x)) \\wedge (\\forall x q(x)). \\\\ exists x ((p(x) \\vee q(x)) & \\Leftrightarrow (\\exists x p(x)) \\vee (\\exists x q(x)). \\end{aligned} \\] \\[ \\begin{aligned} \\forall x q(x) \\rightarrow A & \\Leftrightarrow \\exists x (q(x) \\rightarrow A). \\\\ \\exists x q(x) \\rightarrow A & \\Leftrightarrow \\forall x (q(x) \\rightarrow A). \\end{aligned} \\] Method of Proof \u00b6 Definition An axiom (\u516c\u7406) is a proposition accepted as true without proof. An theorem (\u5b9a\u7406) is a statement that can be shown to be true. A lemma (\u5f15\u7406) is a small theorem used to prove a bigger theorem. A corollary (\u63a8\u8bba) is a theorem proven to be a logical consequnce of another theorem. A conjecture (\u731c\u60f3) is a statement whose truth value is unknown. Valid Arguments \u00b6 Definition Deductive reasoning the process of reaching a conclusion \\(q\\) from a sequence of propositions \\(p_1,\\dots,p_n\\) . - \\(p_1,\\dots,p_n\\) are premises or hypothesis. - \\(q\\) is conclusion. An argument in propositional logic is a sequence of propositions. All but the final proposition in the argument are called premises (\u5047\u8bbe) and the final proposition is called the conclusion (\u7ed3\u8bba) . An argument is valid if when all the hypotheses are true, the conclusion is true. Rules of Inference \u00b6 Glossary Hypothetical Judgement \u5047\u8a00\u63a8\u65ad: Modus ponens \u80af\u5b9a\u524d\u4ef6\u5f0f, Modus tollens \u5426\u5b9a\u540e\u4ef6\u5f0f. Hypothetical syllogism \u5047\u8a00\u4e09\u6bb5\u8bba, Disjunctive syllogism \u6790\u53d6\u4e09\u6bb5\u8bba. Addition \u9644\u52a0\u89c4\u5219, Simplification \u7b80\u5316\u89c4, Conjuction \u5408\u53d6\u89c4\u5219, Resolution \u6d88\u89e3\u539f\u7406. Remark \\((p_1 \\wedge p_2 \\wedge \\dots \\wedge p_n) \\rightarrow (p \\rightarrow q) \\Leftrightarrow (p_1 \\wedge p_2 \\wedge \\dots \\wedge p_n \\wedge p) \\rightarrow q\\) . Contradiction \u53cd\u8bc1\u6cd5 Principle \\((p_1 \\wedge p_2 \\wedge \\dots \\wedge p_n) \\rightarrow q \\Leftrightarrow \\neg (p_1 \\wedge p_2 \\wedge \\dots \\wedge p_n \\wedge p_n \\wedge \\neg q)\\) Steps for proof \\(p \\rightarrow q\\) : Assume \\(p\\) is true and \\(q\\) is false. Show that \\(\\neg p\\) is also true. \\(p \\wedge \\neg p = F\\) , which leads to a contradiction. About Resolution rule - Use for automatic theorem proving. - \\(q\\vee r\\) is called the resolvent . Normal Form \u00b6 Definition A literal is a variable or its negation. Conjunctive clauses (short for clauses) are conjunctions with literals as conjuncts. A formula is in disjunctive normal norm (DNF) if it's wriiten as a disjunction of conjunctions of literals. Disjunctive Normal Form (DNF) \u00b6 Quote Similar discussion can be seen in the canonical form and K-map of Digital Design. \\[ \\bigvee\\limits _{i = 1}^{k} \\bigwedge\\limits _{j = 1}^{n_i} A_{ij} = (A_{11} \\wedge A_{12} \\wedge \\dots \\wedge A_{1n_1}) \\vee \\dots \\vee (A_{k1} \\wedge A_{k2} \\wedge \\dots \\wedge A_{kn_1}) \\] Theorem Any formula is tautologically equivalent to some formula in DNF. Full Disjunctvie Form \u00b6 Definition A minterm is a conjunction of literals in which each variable is represented exactly one. A full disjunctive form is a boolean function expressed as a disjunction of minterms. Remark \\(A \\text{ is tautology} \\Leftrightarrow \\left(A \\Leftrightarrow \\bigvee\\limits _{i = 0}^{2^n - 1}m_i\\right)\\) Full disjunctive form can be obtained by using truth table. \\(\\{\\neg, \\wedge, \\vee\\}\\) is functionally complete . Conjuctive Normal Form (CNF) (DNF similarly) \u00b6 Prenex Normal Form \u524d\u675f\u8303\u5f0f \u00b6 Definition A formula is in prenex normal form if it's of the form \\[ Q_1x_1Q_2x_2\\cdots Q_nx_nB, \\] where \\(Q_i(i = 1, 2, \\cdots, n)\\) is \\(\\forall\\) or \\(\\exists\\) and \\(B\\) is quantifier free. Algorithm of prenex normal form Rename bounded variables. Eliminate all connectives \\(\\rightarrow\\) and \\(\\leftrightarrow\\) . Move all negations inward. Move all quantifiers to the front of the formula.","title":"Chap 1"},{"location":"Mathematics_Basis/DM/Chap_1/#chapter-1-the-foundations-logic-and-proofs","text":"","title":"Chapter 1 | The Foundations: Logic and Proofs"},{"location":"Mathematics_Basis/DM/Chap_1/#proposition","text":"Definition Proposition (\u63a8\u65ad) is a statement that is either true or false, but not both.","title":"Proposition"},{"location":"Mathematics_Basis/DM/Chap_1/#logical-operators-connective","text":"Negation (\u5426\u5b9a) \\(\\neg p\\) . Conjunction (\u5408\u53d6) \\(p \\wedge q\\) . Disjunction (\u6790\u53d6) \\(p \\vee q\\) . Implication (\u6761\u4ef6 / \u8574\u542b) \\(p \\rightarrow q\\) . \\(p\\) is called the hypothesis (or antecedent or premise) . \\(q\\) is called the conclusion (or consequence) . Common Ways to Express Implication \\(p \\rightarrow q\\) If \\(p\\) then \\(q\\) . \\(p\\) is sufficient for \\(q\\) . \\(p\\) implies \\(q\\) . \\(q\\) is necessary for \\(p\\) . If \\(q\\) whenever \\(p\\) \\(p\\) only if \\(q\\) . Biconditional (\u53cc\u6761\u4ef6) \\(p \\leftrightarrow q\\) . Priorities \\(\\neg\\) , then \\(\\wedge\\ \\vee\\) , then \\(\\rightarrow\\ \\leftrightarrow\\) . Remark There is no such a sign \\(\\leftarrow\\) in the discussion in Dicrete Mathematics. Disjunction is inclusive or . Example inclusive or (\u6216\u3001\u517c\u6216). e.g. I passed mathematics or English. exclusive or (\u5f02\u6216). e.g. Paul was born in 1983 or 1984. \u5408\u53d6\u5426\u5b9a (denoted by Sheffer stroke \u8c22\u8d39\u5c14\u7ad6\u7ebf ) \\(p\\uparrow q \\Leftrightarrow \\neg(p \\wedge q)\\) \uff0c\u6790\u53d6\u5426\u5b9a (denoted by Peirce arrow \u76ae\u5c14\u65af\u7bad\u5934 ) \\(p\\downarrow q \\Leftrightarrow \\neg(p \\vee q)\\) .","title":"Logical Operators / Connective"},{"location":"Mathematics_Basis/DM/Chap_1/#propositional-formula","text":"Definition (Well Formed) Formula (wff) are one of the follows: Propositional variable / Proposition, \\(T\\) , \\(F\\) . \\(\\neg A\\) , \\((A \\wedge B)\\) , \\((A \\vee B)\\) , \\((A \\rightarrow B)\\) , \\((A \\leftrightarrow B)\\) , if \\(A\\) and \\(B\\) are formulae. Finitely many composed applications above. Definition | Classification tautology (\u91cd\u8a00\u5f0f / \u6c38\u771f\u5f0f): if its truth table contains only true values for every case. contradiction (\u6c38\u5047\u5f0f): if its truth table contains only false values for every case. Contigence: neither a tautology nor a contradiction.","title":"Propositional Formula"},{"location":"Mathematics_Basis/DM/Chap_1/#proposition-equivalences","text":"Definition Formulae \\(A\\) and \\(B\\) are logical equivalent if \\(A \\leftrightarrow B\\) is tautology, denoted by \\(A \\Leftrightarrow B\\) . Definition if \\(p \\rightarrow q\\) , inverse (\u5426\u547d\u9898) \\(\\neg p \\rightarrow \\neg q\\) . converse (\u9006\u547d\u9898) \\(q \\rightarrow p\\) . contrapositive (\u9006\u5426\u547d\u9898) \\(\\neg q \\rightarrow \\neg p\\) NOTE: \\(p \\rightarrow q \\Leftrightarrow \\neg q \\rightarrow \\neg p\\) .","title":"Proposition Equivalences"},{"location":"Mathematics_Basis/DM/Chap_1/#predicates-and-quantifieres","text":"Definition Predicates (\u8c13\u8bcd / \u65ad\u8a00) is a statement of the form \\(P(x_1, x_2, \\dots, x_n)\\) , where \\(P\\) is a propositional function at the n -tuple \\((x_1, x_2,\\dots,x_n)\\) .","title":"Predicates and Quantifieres"},{"location":"Mathematics_Basis/DM/Chap_1/#quantifier","text":"domain / universe of discourse \u8bba\u57df Universal Quantifier \u5168\u79f0\u91cf\u8bcd For all \\(x\\) , \\(p(x): \\forall xp(x)\\) . Existential Quantifier \u5b58\u5728\u91cf\u8bcd For some \\(x\\) , \\(p(x): \\exists xp(x)\\) .","title":"Quantifier \u91cf\u8bcd"},{"location":"Mathematics_Basis/DM/Chap_1/#banding-variables","text":"Definition When a quantifier is used on the variable \\(x\\) , then the occurrence of \\(x\\) is bound while others are free . The part of a logical expression to which a quantifier is applied is called the scope (\u8f96\u57df) of this quantifier. Remark If the universe of discourse can be listed, then \\(\\forall xP(x) \\Leftrightarrow P(x_1) \\wedge \\dots \\wedge P(x_n)\\) and \\(\\exists xP(x) \\Leftrightarrow P(x_1) \\vee\\dots \\vee P(x_n)\\) . Properties De Morgan's Laws: \\(\\neg \\forall x p(x) \\Leftrightarrow \\exists x \\neg p(x),\\ \\ \\neg \\exists x p(x) \\Leftrightarrow \\forall x \\neg p(x)\\) . \\[ \\begin{aligned} \\forall x ((p(x) \\wedge q(x)) & \\Leftrightarrow (\\forall x p(x)) \\wedge (\\forall x q(x)). \\\\ exists x ((p(x) \\vee q(x)) & \\Leftrightarrow (\\exists x p(x)) \\vee (\\exists x q(x)). \\end{aligned} \\] \\[ \\begin{aligned} \\forall x q(x) \\rightarrow A & \\Leftrightarrow \\exists x (q(x) \\rightarrow A). \\\\ \\exists x q(x) \\rightarrow A & \\Leftrightarrow \\forall x (q(x) \\rightarrow A). \\end{aligned} \\]","title":"Banding Variables"},{"location":"Mathematics_Basis/DM/Chap_1/#method-of-proof","text":"Definition An axiom (\u516c\u7406) is a proposition accepted as true without proof. An theorem (\u5b9a\u7406) is a statement that can be shown to be true. A lemma (\u5f15\u7406) is a small theorem used to prove a bigger theorem. A corollary (\u63a8\u8bba) is a theorem proven to be a logical consequnce of another theorem. A conjecture (\u731c\u60f3) is a statement whose truth value is unknown.","title":"Method of Proof"},{"location":"Mathematics_Basis/DM/Chap_1/#valid-arguments","text":"Definition Deductive reasoning the process of reaching a conclusion \\(q\\) from a sequence of propositions \\(p_1,\\dots,p_n\\) . - \\(p_1,\\dots,p_n\\) are premises or hypothesis. - \\(q\\) is conclusion. An argument in propositional logic is a sequence of propositions. All but the final proposition in the argument are called premises (\u5047\u8bbe) and the final proposition is called the conclusion (\u7ed3\u8bba) . An argument is valid if when all the hypotheses are true, the conclusion is true.","title":"Valid Arguments"},{"location":"Mathematics_Basis/DM/Chap_1/#rules-of-inference","text":"Glossary Hypothetical Judgement \u5047\u8a00\u63a8\u65ad: Modus ponens \u80af\u5b9a\u524d\u4ef6\u5f0f, Modus tollens \u5426\u5b9a\u540e\u4ef6\u5f0f. Hypothetical syllogism \u5047\u8a00\u4e09\u6bb5\u8bba, Disjunctive syllogism \u6790\u53d6\u4e09\u6bb5\u8bba. Addition \u9644\u52a0\u89c4\u5219, Simplification \u7b80\u5316\u89c4, Conjuction \u5408\u53d6\u89c4\u5219, Resolution \u6d88\u89e3\u539f\u7406. Remark \\((p_1 \\wedge p_2 \\wedge \\dots \\wedge p_n) \\rightarrow (p \\rightarrow q) \\Leftrightarrow (p_1 \\wedge p_2 \\wedge \\dots \\wedge p_n \\wedge p) \\rightarrow q\\) . Contradiction \u53cd\u8bc1\u6cd5 Principle \\((p_1 \\wedge p_2 \\wedge \\dots \\wedge p_n) \\rightarrow q \\Leftrightarrow \\neg (p_1 \\wedge p_2 \\wedge \\dots \\wedge p_n \\wedge p_n \\wedge \\neg q)\\) Steps for proof \\(p \\rightarrow q\\) : Assume \\(p\\) is true and \\(q\\) is false. Show that \\(\\neg p\\) is also true. \\(p \\wedge \\neg p = F\\) , which leads to a contradiction. About Resolution rule - Use for automatic theorem proving. - \\(q\\vee r\\) is called the resolvent .","title":"Rules of Inference"},{"location":"Mathematics_Basis/DM/Chap_1/#normal-form","text":"Definition A literal is a variable or its negation. Conjunctive clauses (short for clauses) are conjunctions with literals as conjuncts. A formula is in disjunctive normal norm (DNF) if it's wriiten as a disjunction of conjunctions of literals.","title":"Normal Form"},{"location":"Mathematics_Basis/DM/Chap_1/#disjunctive-normal-form-dnf","text":"Quote Similar discussion can be seen in the canonical form and K-map of Digital Design. \\[ \\bigvee\\limits _{i = 1}^{k} \\bigwedge\\limits _{j = 1}^{n_i} A_{ij} = (A_{11} \\wedge A_{12} \\wedge \\dots \\wedge A_{1n_1}) \\vee \\dots \\vee (A_{k1} \\wedge A_{k2} \\wedge \\dots \\wedge A_{kn_1}) \\] Theorem Any formula is tautologically equivalent to some formula in DNF.","title":"Disjunctive Normal Form (DNF)"},{"location":"Mathematics_Basis/DM/Chap_1/#full-disjunctvie-form","text":"Definition A minterm is a conjunction of literals in which each variable is represented exactly one. A full disjunctive form is a boolean function expressed as a disjunction of minterms. Remark \\(A \\text{ is tautology} \\Leftrightarrow \\left(A \\Leftrightarrow \\bigvee\\limits _{i = 0}^{2^n - 1}m_i\\right)\\) Full disjunctive form can be obtained by using truth table. \\(\\{\\neg, \\wedge, \\vee\\}\\) is functionally complete .","title":"Full Disjunctvie Form"},{"location":"Mathematics_Basis/DM/Chap_1/#conjuctive-normal-form-cnf-dnf-similarly","text":"","title":"Conjuctive Normal Form (CNF) (DNF similarly)"},{"location":"Mathematics_Basis/DM/Chap_1/#prenex-normal-form","text":"Definition A formula is in prenex normal form if it's of the form \\[ Q_1x_1Q_2x_2\\cdots Q_nx_nB, \\] where \\(Q_i(i = 1, 2, \\cdots, n)\\) is \\(\\forall\\) or \\(\\exists\\) and \\(B\\) is quantifier free. Algorithm of prenex normal form Rename bounded variables. Eliminate all connectives \\(\\rightarrow\\) and \\(\\leftrightarrow\\) . Move all negations inward. Move all quantifiers to the front of the formula.","title":"Prenex Normal Form \u524d\u675f\u8303\u5f0f"},{"location":"Mathematics_Basis/DM/Chap_10/","text":"","title":"Chap 10"},{"location":"Mathematics_Basis/DM/Chap_11/","text":"","title":"Chap 11"},{"location":"Mathematics_Basis/DM/Chap_2/","text":"Chapter 2 | Basic Structures: Sets and Functions \u00b6 Set and Subset \u00b6 Definition The object in a set are elements . A set contains its elements. Ways to describe sets: List; Predicate; Venn Diagram. Properties of sets: Certainty; Don't care order and repetition of elements; Finite and Infinite set. Subset \\[ S \\subseteq T \\Leftrightarrow (\\forall x \\in S \\Rightarrow x \\in T). \\] Proper subset (\u771f\u5b50\u96c6) \\[ S \\subset T \\Leftrightarrow (\\forall x \\in S \\Rightarrow x \\in T) \\wedge S \\ne T. \\] ** Empty set \\(\\emptyset\\) and Universal set \\(U\\) ** \\[ \\text{For any set } A,\\ \\ A \\subseteq A,\\ \\ \\emptyset \\subseteq A \\subseteq U. \\] Operations \u00b6 Union \\(A \\cup B = \\{x | x \\in A \\vee x \\in B\\}\\) . Intersection \\(A \\cap B = \\{x | x \\in A \\wedge x \\in B\\}\\) . Difference \\(A -B = \\{x | x \\in A \\wedge x \\notin B\\}\\) . Complement \\(\\overline{A} = U - A\\) . Symmetric Difference \\(A \\oplus B = (A - B) \\cup (B - A)\\) . Properties \\[ A \\oplus B = B \\oplus A, (A \\oplus B) \\oplus C = A \\oplus (B \\oplus C), A \\oplus A = \\emptyset, A \\oplus \\emptyset = A. \\] \\[ A \\oplus B = A \\oplus C \\Rightarrow B = C. \\] Power Set \u00b6 Definition For a set \\(S\\) , the power set of \\(S\\) is the set of all subsets of the set \\(S\\) , denoted by \\[ P(S) = 2^S = \\{T | T \\subseteq S\\}. \\] If a set has \\(n\\) elements, then its power set has \\(2^n\\) elements. Cartesian Product \u00b6 Definition For two sets \\(A\\) and \\(B\\) , the Cartesian product of \\(A\\) and \\(B\\) is the set of all ordered pairs \\((a, b)\\) where \\(a \\in A\\) and \\(b \\in B\\) , denoted by \\[ A \\times B = \\{(a, b) | a \\in A \\wedge b \\in B\\}. \\] Generally for \\(n\\) sets \\(A_1,\\ \\ \\dots,\\ \\ A_n\\) , \\[ A_1 \\times A_2 \\times \\ \\cdots \\times A_n = \\{(a_1, a_2, \\dots a_n) | a_i \\in A_i \\text{ for } i = 1, 2, \\dots, n\\}. \\] Properties \\(A \\times \\emptyset = \\emptyset \\times A = \\emptyset\\) . In general, \\(A \\times B \\ne B \\times A\\) . In general, \\((A \\times B) \\times C \\ne A \\times (B \\times C)\\) , unless we identify \\(((a, b), c)= (a, (b, c))\\) . \\[ \\begin{aligned} A \\times (B \\cup C) = (A \\times B) \\cup (A \\times C), \\\\ A \\times (B \\cap C) = (A \\times B) \\cap (A \\times C). \\end{aligned} \\] If \\(A \\subseteq C\\) and \\(B \\subseteq D\\) , then \\(A \\times B \\subseteq C \\times D\\) , but not vice versa. \\((A \\cap B) \\times (C \\cap D) = (A \\times C) \\cap (B \\times D)\\) . Cardinality of Finite and Infinite Sets \u00b6 Cardinality is the size of a set \\(S\\) , denoted by \\(|S|\\) . !!! theorem | \"Prniciple of Inclusion-exclusion \u5bb9\u65a5\u539f\u7406\" $$ |A \\cup B| = |A| + |B| - |A \\cap B|. $$ More generally, $$ \\left|\\bigcup_{i = 1}^nA_i\\right| = \\sum_{i = 1}^n|A_i| - \\sum_{1 \\le i \\ne j \\le n} |A_i \\cap A_j| + \\cdots + (-1)^{n-1}\\left|\\bigcap_{i = 1]}^n A_i\\right|. $$ Definition Two sets have the same cardinality or say are equinumerous (\u7b49\u52bf\u7684) iff there is an bijective from \\(A\\) to \\(B\\) , i.e. \\(|A| = |B|\\) . Inifinite Sets \u00b6 Countable (denumerable) set A set that is equinumerous with \\(\\mathbb{N}\\) , e.g. \\(\\mathbb{Z}\\) , \\(\\mathbb{Q}\\) , \\(\\mathbb{N} \\times \\mathbb{N}\\) . \\(\\aleph_0\\) is called countable . Properties Countable set is the smallest infinite set. The union of two / finite number of / a countable number of countable sets is countable. Uncountable set Theorem \\(\\left|2^A\\right| > |A|\\) . \\(|\\mathbb{R}| = \\aleph_1 > \\aleph_0\\) . Continuum Hypothesis (CH) \u8fde\u7eed\u7edf\u5047\u8bbe \u00b6 \\[ \\exists ? \\omega,\\ \\ s.t.\\ \\aleph_0 \\lt \\omega \\lt \\aleph_1. \\] Functions \u00b6 \\[ f: A \\rightarrow B \\Leftrightarrow \\forall a \\in A,\\ \\ \\exists b \\in B : f(a) = b \\] \\(f\\) maps \\(A\\) to \\(B\\) . \\(A\\) is the domain of \\(f\\) , \\(\\text{Dom } f = A\\) . \\(B\\) is the codomain of \\(f\\) , \\(\\text{Codom } f = B\\) . \\(f(a) = b,\\ \\ a \\in A,\\ \\ b \\in B\\) . \\(b\\) is the image of \\(a\\) , \\(a\\) is a pre-image of \\(b\\) . \\(\\text{Range}(f) = \\{b \\in B | \\exist a \\in A, f(a) = b\\}\\) . One-to-one function (Injective) \u5355\u5c04 \\[ (\\forall a, b \\in A) \\wedge (a \\ne b) \\Rightarrow f(a) \\ne f(b). \\] Onto function (Subjective) \u6ee1\u5c04 \\[ \\forall b \\in B,\\ \\ \\exist a \\in A,\\ \\ s.t.\\ f(a) = b. \\] One-to-one Correspondence (Bijective) \u53cc\u5c04 one-to-one + onto Inverse and Composition \u00b6 Definition | Inverse Function If \\(f\\) is bijective, \\(f^{-1}:B\\rightarrow A\\) is the inverse function of \\(f\\) . \\[ \\forall a \\in A, b \\in B, (f(a) = b) \\Leftrightarrow (f^{-1}(b) = a). \\] Thus we also call a one-to-one correspondence invertible . Definition | Composition If \\(g: A\\rightarrow B\\) and \\(f:B\\rightarrow C\\) , then \\(f\\circ g : A\\rightarrow C\\) the composition of \\(f\\) and \\(g\\) . \\[ \\forall a \\in A,\\ \\ (f\\circ g)(a) = f(g(a)). \\] Two Important Functions Floor functions \\(\\lfloor x \\rfloor\\) (or by convetion \\([x]\\) , or called greatest integer function). Ceiling functions \\(\\lceil x \\rceil\\) . Growth of Function \u00b6 Quote Similar to Algorithm Analysis in FDS . BIG-O NOTATION, BIG-OMEGA NOTATION and BIG_THETA NOTATION. Theorem If \\[ f(x) = a_nx^n + \\cdots + a_1x + a_0, \\] then \\(f(x)\\) is \\(O(x^n)\\) . Definition If \\(f(x) = \\Theta(g(x))\\) , then we say \\(f(x)\\) is of order \\(g(x)\\) .","title":"Chap 2"},{"location":"Mathematics_Basis/DM/Chap_2/#chapter-2-basic-structures-sets-and-functions","text":"","title":"Chapter 2 | Basic Structures: Sets and Functions"},{"location":"Mathematics_Basis/DM/Chap_2/#set-and-subset","text":"Definition The object in a set are elements . A set contains its elements. Ways to describe sets: List; Predicate; Venn Diagram. Properties of sets: Certainty; Don't care order and repetition of elements; Finite and Infinite set. Subset \\[ S \\subseteq T \\Leftrightarrow (\\forall x \\in S \\Rightarrow x \\in T). \\] Proper subset (\u771f\u5b50\u96c6) \\[ S \\subset T \\Leftrightarrow (\\forall x \\in S \\Rightarrow x \\in T) \\wedge S \\ne T. \\] ** Empty set \\(\\emptyset\\) and Universal set \\(U\\) ** \\[ \\text{For any set } A,\\ \\ A \\subseteq A,\\ \\ \\emptyset \\subseteq A \\subseteq U. \\]","title":"Set and Subset"},{"location":"Mathematics_Basis/DM/Chap_2/#operations","text":"Union \\(A \\cup B = \\{x | x \\in A \\vee x \\in B\\}\\) . Intersection \\(A \\cap B = \\{x | x \\in A \\wedge x \\in B\\}\\) . Difference \\(A -B = \\{x | x \\in A \\wedge x \\notin B\\}\\) . Complement \\(\\overline{A} = U - A\\) . Symmetric Difference \\(A \\oplus B = (A - B) \\cup (B - A)\\) . Properties \\[ A \\oplus B = B \\oplus A, (A \\oplus B) \\oplus C = A \\oplus (B \\oplus C), A \\oplus A = \\emptyset, A \\oplus \\emptyset = A. \\] \\[ A \\oplus B = A \\oplus C \\Rightarrow B = C. \\]","title":"Operations"},{"location":"Mathematics_Basis/DM/Chap_2/#power-set","text":"Definition For a set \\(S\\) , the power set of \\(S\\) is the set of all subsets of the set \\(S\\) , denoted by \\[ P(S) = 2^S = \\{T | T \\subseteq S\\}. \\] If a set has \\(n\\) elements, then its power set has \\(2^n\\) elements.","title":"Power Set"},{"location":"Mathematics_Basis/DM/Chap_2/#cartesian-product","text":"Definition For two sets \\(A\\) and \\(B\\) , the Cartesian product of \\(A\\) and \\(B\\) is the set of all ordered pairs \\((a, b)\\) where \\(a \\in A\\) and \\(b \\in B\\) , denoted by \\[ A \\times B = \\{(a, b) | a \\in A \\wedge b \\in B\\}. \\] Generally for \\(n\\) sets \\(A_1,\\ \\ \\dots,\\ \\ A_n\\) , \\[ A_1 \\times A_2 \\times \\ \\cdots \\times A_n = \\{(a_1, a_2, \\dots a_n) | a_i \\in A_i \\text{ for } i = 1, 2, \\dots, n\\}. \\] Properties \\(A \\times \\emptyset = \\emptyset \\times A = \\emptyset\\) . In general, \\(A \\times B \\ne B \\times A\\) . In general, \\((A \\times B) \\times C \\ne A \\times (B \\times C)\\) , unless we identify \\(((a, b), c)= (a, (b, c))\\) . \\[ \\begin{aligned} A \\times (B \\cup C) = (A \\times B) \\cup (A \\times C), \\\\ A \\times (B \\cap C) = (A \\times B) \\cap (A \\times C). \\end{aligned} \\] If \\(A \\subseteq C\\) and \\(B \\subseteq D\\) , then \\(A \\times B \\subseteq C \\times D\\) , but not vice versa. \\((A \\cap B) \\times (C \\cap D) = (A \\times C) \\cap (B \\times D)\\) .","title":"Cartesian Product"},{"location":"Mathematics_Basis/DM/Chap_2/#cardinality-of-finite-and-infinite-sets","text":"Cardinality is the size of a set \\(S\\) , denoted by \\(|S|\\) . !!! theorem | \"Prniciple of Inclusion-exclusion \u5bb9\u65a5\u539f\u7406\" $$ |A \\cup B| = |A| + |B| - |A \\cap B|. $$ More generally, $$ \\left|\\bigcup_{i = 1}^nA_i\\right| = \\sum_{i = 1}^n|A_i| - \\sum_{1 \\le i \\ne j \\le n} |A_i \\cap A_j| + \\cdots + (-1)^{n-1}\\left|\\bigcap_{i = 1]}^n A_i\\right|. $$ Definition Two sets have the same cardinality or say are equinumerous (\u7b49\u52bf\u7684) iff there is an bijective from \\(A\\) to \\(B\\) , i.e. \\(|A| = |B|\\) .","title":"Cardinality of Finite and Infinite Sets"},{"location":"Mathematics_Basis/DM/Chap_2/#inifinite-sets","text":"Countable (denumerable) set A set that is equinumerous with \\(\\mathbb{N}\\) , e.g. \\(\\mathbb{Z}\\) , \\(\\mathbb{Q}\\) , \\(\\mathbb{N} \\times \\mathbb{N}\\) . \\(\\aleph_0\\) is called countable . Properties Countable set is the smallest infinite set. The union of two / finite number of / a countable number of countable sets is countable. Uncountable set Theorem \\(\\left|2^A\\right| > |A|\\) . \\(|\\mathbb{R}| = \\aleph_1 > \\aleph_0\\) .","title":"Inifinite Sets"},{"location":"Mathematics_Basis/DM/Chap_2/#continuum-hypothesis-ch","text":"\\[ \\exists ? \\omega,\\ \\ s.t.\\ \\aleph_0 \\lt \\omega \\lt \\aleph_1. \\]","title":"Continuum Hypothesis (CH) \u8fde\u7eed\u7edf\u5047\u8bbe"},{"location":"Mathematics_Basis/DM/Chap_2/#functions","text":"\\[ f: A \\rightarrow B \\Leftrightarrow \\forall a \\in A,\\ \\ \\exists b \\in B : f(a) = b \\] \\(f\\) maps \\(A\\) to \\(B\\) . \\(A\\) is the domain of \\(f\\) , \\(\\text{Dom } f = A\\) . \\(B\\) is the codomain of \\(f\\) , \\(\\text{Codom } f = B\\) . \\(f(a) = b,\\ \\ a \\in A,\\ \\ b \\in B\\) . \\(b\\) is the image of \\(a\\) , \\(a\\) is a pre-image of \\(b\\) . \\(\\text{Range}(f) = \\{b \\in B | \\exist a \\in A, f(a) = b\\}\\) . One-to-one function (Injective) \u5355\u5c04 \\[ (\\forall a, b \\in A) \\wedge (a \\ne b) \\Rightarrow f(a) \\ne f(b). \\] Onto function (Subjective) \u6ee1\u5c04 \\[ \\forall b \\in B,\\ \\ \\exist a \\in A,\\ \\ s.t.\\ f(a) = b. \\] One-to-one Correspondence (Bijective) \u53cc\u5c04 one-to-one + onto","title":"Functions"},{"location":"Mathematics_Basis/DM/Chap_2/#inverse-and-composition","text":"Definition | Inverse Function If \\(f\\) is bijective, \\(f^{-1}:B\\rightarrow A\\) is the inverse function of \\(f\\) . \\[ \\forall a \\in A, b \\in B, (f(a) = b) \\Leftrightarrow (f^{-1}(b) = a). \\] Thus we also call a one-to-one correspondence invertible . Definition | Composition If \\(g: A\\rightarrow B\\) and \\(f:B\\rightarrow C\\) , then \\(f\\circ g : A\\rightarrow C\\) the composition of \\(f\\) and \\(g\\) . \\[ \\forall a \\in A,\\ \\ (f\\circ g)(a) = f(g(a)). \\] Two Important Functions Floor functions \\(\\lfloor x \\rfloor\\) (or by convetion \\([x]\\) , or called greatest integer function). Ceiling functions \\(\\lceil x \\rceil\\) .","title":"Inverse and Composition"},{"location":"Mathematics_Basis/DM/Chap_2/#growth-of-function","text":"Quote Similar to Algorithm Analysis in FDS . BIG-O NOTATION, BIG-OMEGA NOTATION and BIG_THETA NOTATION. Theorem If \\[ f(x) = a_nx^n + \\cdots + a_1x + a_0, \\] then \\(f(x)\\) is \\(O(x^n)\\) . Definition If \\(f(x) = \\Theta(g(x))\\) , then we say \\(f(x)\\) is of order \\(g(x)\\) .","title":"Growth of Function"},{"location":"Mathematics_Basis/DM/Chap_3/","text":"The Fundamentals: Algorithms \u00b6 Algorithms \u00b6 Definition An algorithm is a finite set of precise instructions for performing a computation or solving problem. Pseudocode is instructions given in a generic language similar to a computer language. Properties Input and Output Definiteness Correctness Finiteness Effectiveness Generality Complexity \u00b6 Definition Complexity is the amount of time or space needed to execute the algorithm. Space Complexity Time Complexity Types: best-case time; worst-case time; average-case time. P-Class problems can be solved by polynomial time algorithm NP-Class problems for which a solution can be checked in polynomial time NP-Complete Problem If any of these problems can be solved by polynomial worst-case time algorithm, then all can be solved by polynomial worst-case time algorithms.","title":"Chap 3"},{"location":"Mathematics_Basis/DM/Chap_3/#the-fundamentals-algorithms","text":"","title":"The Fundamentals: Algorithms"},{"location":"Mathematics_Basis/DM/Chap_3/#algorithms","text":"Definition An algorithm is a finite set of precise instructions for performing a computation or solving problem. Pseudocode is instructions given in a generic language similar to a computer language. Properties Input and Output Definiteness Correctness Finiteness Effectiveness Generality","title":"Algorithms"},{"location":"Mathematics_Basis/DM/Chap_3/#complexity","text":"Definition Complexity is the amount of time or space needed to execute the algorithm. Space Complexity Time Complexity Types: best-case time; worst-case time; average-case time. P-Class problems can be solved by polynomial time algorithm NP-Class problems for which a solution can be checked in polynomial time NP-Complete Problem If any of these problems can be solved by polynomial worst-case time algorithm, then all can be solved by polynomial worst-case time algorithms.","title":"Complexity"},{"location":"Mathematics_Basis/DM/Chap_4/","text":"","title":"Chap 4"},{"location":"Mathematics_Basis/DM/Chap_5/","text":"Chapter 5 | Induction and Recursion \u00b6 Mathematical Induction \u00b6 The Well-Ordering Property: Every nonnegative integers has a least element . The First Principle of Mathematical Induction \u00b6 To prove by mathematical induction that \\(P(n)\\) is true for every positive integer \\(n\\) , Basic Step. The proposition \\(P(1)\\) is shown to be true. Inductive Step. The implication \\(P(n) \\rightarrow P(n + 1)\\) is shown to be true for every positive integer \\(n\\) . The Second Principle of Mathematical Induction \u00b6 Basic Step. The proposition \\(P(1)\\) is shown to be true. Inductive Step. The implication \\([P(1) \\wedge P(2) \\wedge \\cdots \\wedge P(n)] \\rightarrow P(n + 1)\\) is shown to be true for every positive integer \\(n\\) . Recursion \u00b6 Recursively Defined Functions Recursive or Inductive Definition Specify the value of function at zero. Give a rule for finding its value as an integer from its values at smaller integers. Moreover, we can use recursion to define sets , algorithm and so on. Definition An algorithm is called recursive if it solves a problem by reducing it to an instance of the same problem with smaller input.","title":"Chap 5"},{"location":"Mathematics_Basis/DM/Chap_5/#chapter-5-induction-and-recursion","text":"","title":"Chapter 5 | Induction and Recursion"},{"location":"Mathematics_Basis/DM/Chap_5/#mathematical-induction","text":"The Well-Ordering Property: Every nonnegative integers has a least element .","title":"Mathematical Induction"},{"location":"Mathematics_Basis/DM/Chap_5/#the-first-principle-of-mathematical-induction","text":"To prove by mathematical induction that \\(P(n)\\) is true for every positive integer \\(n\\) , Basic Step. The proposition \\(P(1)\\) is shown to be true. Inductive Step. The implication \\(P(n) \\rightarrow P(n + 1)\\) is shown to be true for every positive integer \\(n\\) .","title":"The First Principle of Mathematical Induction"},{"location":"Mathematics_Basis/DM/Chap_5/#the-second-principle-of-mathematical-induction","text":"Basic Step. The proposition \\(P(1)\\) is shown to be true. Inductive Step. The implication \\([P(1) \\wedge P(2) \\wedge \\cdots \\wedge P(n)] \\rightarrow P(n + 1)\\) is shown to be true for every positive integer \\(n\\) .","title":"The Second Principle of Mathematical Induction"},{"location":"Mathematics_Basis/DM/Chap_5/#recursion","text":"Recursively Defined Functions Recursive or Inductive Definition Specify the value of function at zero. Give a rule for finding its value as an integer from its values at smaller integers. Moreover, we can use recursion to define sets , algorithm and so on. Definition An algorithm is called recursive if it solves a problem by reducing it to an instance of the same problem with smaller input.","title":"Recursion"},{"location":"Mathematics_Basis/DM/Chap_6/","text":"","title":"Chap 6"},{"location":"Mathematics_Basis/DM/Chap_7/","text":"","title":"Chap 7"},{"location":"Mathematics_Basis/DM/Chap_8/","text":"Chapter 8 | Advanced Counting Techniques \u00b6","title":"Chap 8"},{"location":"Mathematics_Basis/DM/Chap_8/#chapter-8-advanced-counting-techniques","text":"","title":"Chapter 8 | Advanced Counting Techniques"},{"location":"Mathematics_Basis/DM/Chap_9/","text":"","title":"Chap 9"},{"location":"Mathematics_Basis/NA/Chap_1/","text":"Chapter 1 | Mathematical Preliminaries \u00b6 Error \u00b6 Truncation Error \u00b6 the error involved using a truncated or finite summation. Roundoff Error \u00b6 the error produced when performing real number calculations. It occurs because the arithmetic performed in a machine involved numbers with a finite number of digits . Chopping & Rounding Given a real number \\(y = 0.d_1d_2\\dots d_kd_{k+1}\\dots \\times 10^N\\) , the floating-point representation of \\(y\\) is \\(fl(y)\\) : \\[ fl(y) = \\left\\{ \\begin{aligned} & 0.d_1d_2\\dots d_k \\times 10^N, & {\\tt Chopping} \\\\ & {\\tt chop}\\left(y + 5 \\times 10^{n - (k + 1)}\\right). & {\\tt Rounding} \\end{aligned} \\right. \\] Definition 1.0 If \\(p^*\\) is an approximation to \\(p\\) , then absolute error is \\(|p - p^*|\\) and relative error is \\(\\dfrac{|p - p^*|}{|p|}\\) . The number \\(p^*\\) is said to be approximate to \\(p\\) to \\(t\\) significant digits if \\(t\\) is the largest nonnegative integer s.t. \\[ \\frac{|p - p^*|}{|p|} < 5 \\times 10^{-t}. \\] Example For the floating-point representation of \\(y\\) , the relative error is \\[ \\left|\\frac{y - fl(y)}{y}\\right|. \\] For chopping representation, \\[ \\left|\\frac{y - fl(y)}{y}\\right| = \\left|\\frac{0.d_{k + 1}d_{k + 2}\\dots}{0.d_1d_2\\dots}\\right| \\times 10^{-k} \\le \\frac{1}{0.1} \\times 10^{-k} = 10^{-k + 1}. \\] For rounding representation, \\[ \\left|\\frac{y - fl(y)}{y}\\right| \\le \\frac{0.5}{0.1} \\times 10^{-k} = 0.5 \\times 10^{-k + 1}. \\] Effect of Error \u00b6 Subtraction may reduce significant digits. e.g. 0.1234 - 0.1233 = 0.001. Division by small number of multiplication by large number magnify the abosolute error without modifying the relative error. Some Solutions to Reduce Error \u00b6 Quadratic Formula The roots of \\(ax^2 + bx + c = 0\\) is \\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}. \\] Sometimes \\(b\\) is closer to \\(\\sqrt{b^2 - 4ac}\\) , which may cause the subtraction to reduce significant digits. An alternate way is to modify the formula to \\[ x = \\frac{-2c}{b \\pm \\sqrt{b^2 - 4ac}}. \\] But it may cause the division by small number. So it's a tradeoff to use one of the two formulae above. Horner's Method \u79e6\u4e5d\u97f6\u7b97\u6cd5 \\[ \\begin{aligned} f(x) &= a_nx^n + a_{n-1}x^{n-1} + \\dots + a_1x + a_0 \\\\ &= (\\dots((a_nx+a_{n-1})x+a_{n-2})x+\\dots+a_1)x+a_0 \\end{aligned} \\] Stable Algorithms and Convergence \u00b6 Definition 1.1 An algorithm that satisfies that small changes in the initial data produce correspondingly small changes in the final results is called stable ; otherwise it is unstable . An algorithm is called conditionally stable if it is stable only for certain choices of initial data. Suppose \\(E_0 > 0\\) denotes an initial errors, \\(E_n\\) denotes the magnitude of an error after \\(n\\) subsequent operations, then we define Linear growth of errors \\(E_n \\approx C n E_0.\\) unavoidable and acceptable. Exponential growth of errors \\(E_n \\approx C^n E_0.\\) unacceptable. Example The recursive equation \\(p_n = \\dfrac{10}{3}p_{n - 1} - p_{n - 2}\\) has the solution \\[ p_n = c_1 \\left(\\frac13\\right)^n + c_23^n. \\] If \\(p_0 = 1, p_1 = \\dfrac13\\) , then the solution is \\[ p_n = \\left(\\frac13\\right)^n. \\] Suppose we have five-digit rounding representation, then \\(\\hat p_0 = 1.0000\\) , \\(\\hat p_1 = 0.33333\\) , and the solution is \\[ \\hat p_n = 1.0000 \\left(\\frac13\\right)^n - 0.12500 \\times 10^{-5} \\cdot 3^n. \\] Then \\[ p_n - \\hat p_n = 0.12500 \\times 10^{-5} \\cdot 3^n \\] grow exponentially with \\(n\\) . On the other hand, the recursive equation \\(p_n = 2p_{n - 1} - p_{n - 2}\\) has the solution \\[ p_n = c_1 + c_2n \\] If \\(p_0 = 1, p_1 = \\dfrac13\\) , then the solution is \\[ p_n = 1 - \\frac23 n. \\] Suppose we have five-digit rounding representation, then \\(\\hat p_0 = 1.0000\\) , \\(\\hat p_1 = 0.33333\\) , and the solution is \\[ \\hat p_n = 1.0000 - 0.66667 n. \\] Then \\[ p_n - \\hat p_n = \\left(0.66667 - \\frac23\\right) n \\] grow linearly with \\(n\\) . Error of floating-point number (IEEE 754 standard) Range of normal representation \u00b6 For Single-Precision Smallest \\(\\text{0/1\\ 00000001\\ 00\\dots00}\\) \\(\\pm 1.0 \\times 2^{-126} \\approx \\pm 1.2 \\times 10^{-38}\\) . Largest \\(\\text{0/1\\ 11111110\\ 11\\dots11}\\) \\(\\pm 2.0 \\times 2^{127} \\approx \\pm 3.4 \\times 10^{38}\\) . For Double-Precision Smallest \\(\\pm 1.0 \\times 2^{-1022} \\approx \\pm 2.2 \\times 10^{-308}\\) . Largest \\(\\pm 2.0 \\times 2^{1023} \\approx \\pm 1.8 \\times 10^{308}\\) . Relative precision \u00b6 Single-Precision: \\(2^{-23}\\) Equivalent to \\(23 \\times \\log_{10}2 \\approx 6\\) decimal digits of precision\uff086 \u4f4d\u6709\u6548\u6570\u5b57\uff09. Double-Precision: \\(2^{-52}\\) Equivalent to \\(52 \\times \\log_{10}2 \\approx 16\\) decimal digits of precision\uff0816 \u4f4d\u6709\u6548\u6570\u5b57\uff09.","title":"Chap 1"},{"location":"Mathematics_Basis/NA/Chap_1/#chapter-1-mathematical-preliminaries","text":"","title":"Chapter 1 | Mathematical Preliminaries"},{"location":"Mathematics_Basis/NA/Chap_1/#error","text":"","title":"Error"},{"location":"Mathematics_Basis/NA/Chap_1/#truncation-error","text":"the error involved using a truncated or finite summation.","title":"Truncation Error"},{"location":"Mathematics_Basis/NA/Chap_1/#roundoff-error","text":"the error produced when performing real number calculations. It occurs because the arithmetic performed in a machine involved numbers with a finite number of digits . Chopping & Rounding Given a real number \\(y = 0.d_1d_2\\dots d_kd_{k+1}\\dots \\times 10^N\\) , the floating-point representation of \\(y\\) is \\(fl(y)\\) : \\[ fl(y) = \\left\\{ \\begin{aligned} & 0.d_1d_2\\dots d_k \\times 10^N, & {\\tt Chopping} \\\\ & {\\tt chop}\\left(y + 5 \\times 10^{n - (k + 1)}\\right). & {\\tt Rounding} \\end{aligned} \\right. \\] Definition 1.0 If \\(p^*\\) is an approximation to \\(p\\) , then absolute error is \\(|p - p^*|\\) and relative error is \\(\\dfrac{|p - p^*|}{|p|}\\) . The number \\(p^*\\) is said to be approximate to \\(p\\) to \\(t\\) significant digits if \\(t\\) is the largest nonnegative integer s.t. \\[ \\frac{|p - p^*|}{|p|} < 5 \\times 10^{-t}. \\] Example For the floating-point representation of \\(y\\) , the relative error is \\[ \\left|\\frac{y - fl(y)}{y}\\right|. \\] For chopping representation, \\[ \\left|\\frac{y - fl(y)}{y}\\right| = \\left|\\frac{0.d_{k + 1}d_{k + 2}\\dots}{0.d_1d_2\\dots}\\right| \\times 10^{-k} \\le \\frac{1}{0.1} \\times 10^{-k} = 10^{-k + 1}. \\] For rounding representation, \\[ \\left|\\frac{y - fl(y)}{y}\\right| \\le \\frac{0.5}{0.1} \\times 10^{-k} = 0.5 \\times 10^{-k + 1}. \\]","title":"Roundoff Error"},{"location":"Mathematics_Basis/NA/Chap_1/#effect-of-error","text":"Subtraction may reduce significant digits. e.g. 0.1234 - 0.1233 = 0.001. Division by small number of multiplication by large number magnify the abosolute error without modifying the relative error.","title":"Effect of Error"},{"location":"Mathematics_Basis/NA/Chap_1/#some-solutions-to-reduce-error","text":"Quadratic Formula The roots of \\(ax^2 + bx + c = 0\\) is \\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}. \\] Sometimes \\(b\\) is closer to \\(\\sqrt{b^2 - 4ac}\\) , which may cause the subtraction to reduce significant digits. An alternate way is to modify the formula to \\[ x = \\frac{-2c}{b \\pm \\sqrt{b^2 - 4ac}}. \\] But it may cause the division by small number. So it's a tradeoff to use one of the two formulae above. Horner's Method \u79e6\u4e5d\u97f6\u7b97\u6cd5 \\[ \\begin{aligned} f(x) &= a_nx^n + a_{n-1}x^{n-1} + \\dots + a_1x + a_0 \\\\ &= (\\dots((a_nx+a_{n-1})x+a_{n-2})x+\\dots+a_1)x+a_0 \\end{aligned} \\]","title":"Some Solutions to Reduce Error"},{"location":"Mathematics_Basis/NA/Chap_1/#stable-algorithms-and-convergence","text":"Definition 1.1 An algorithm that satisfies that small changes in the initial data produce correspondingly small changes in the final results is called stable ; otherwise it is unstable . An algorithm is called conditionally stable if it is stable only for certain choices of initial data. Suppose \\(E_0 > 0\\) denotes an initial errors, \\(E_n\\) denotes the magnitude of an error after \\(n\\) subsequent operations, then we define Linear growth of errors \\(E_n \\approx C n E_0.\\) unavoidable and acceptable. Exponential growth of errors \\(E_n \\approx C^n E_0.\\) unacceptable. Example The recursive equation \\(p_n = \\dfrac{10}{3}p_{n - 1} - p_{n - 2}\\) has the solution \\[ p_n = c_1 \\left(\\frac13\\right)^n + c_23^n. \\] If \\(p_0 = 1, p_1 = \\dfrac13\\) , then the solution is \\[ p_n = \\left(\\frac13\\right)^n. \\] Suppose we have five-digit rounding representation, then \\(\\hat p_0 = 1.0000\\) , \\(\\hat p_1 = 0.33333\\) , and the solution is \\[ \\hat p_n = 1.0000 \\left(\\frac13\\right)^n - 0.12500 \\times 10^{-5} \\cdot 3^n. \\] Then \\[ p_n - \\hat p_n = 0.12500 \\times 10^{-5} \\cdot 3^n \\] grow exponentially with \\(n\\) . On the other hand, the recursive equation \\(p_n = 2p_{n - 1} - p_{n - 2}\\) has the solution \\[ p_n = c_1 + c_2n \\] If \\(p_0 = 1, p_1 = \\dfrac13\\) , then the solution is \\[ p_n = 1 - \\frac23 n. \\] Suppose we have five-digit rounding representation, then \\(\\hat p_0 = 1.0000\\) , \\(\\hat p_1 = 0.33333\\) , and the solution is \\[ \\hat p_n = 1.0000 - 0.66667 n. \\] Then \\[ p_n - \\hat p_n = \\left(0.66667 - \\frac23\\right) n \\] grow linearly with \\(n\\) . Error of floating-point number (IEEE 754 standard)","title":"Stable Algorithms and Convergence"},{"location":"Mathematics_Basis/NA/Chap_1/#range-of-normal-representation","text":"For Single-Precision Smallest \\(\\text{0/1\\ 00000001\\ 00\\dots00}\\) \\(\\pm 1.0 \\times 2^{-126} \\approx \\pm 1.2 \\times 10^{-38}\\) . Largest \\(\\text{0/1\\ 11111110\\ 11\\dots11}\\) \\(\\pm 2.0 \\times 2^{127} \\approx \\pm 3.4 \\times 10^{38}\\) . For Double-Precision Smallest \\(\\pm 1.0 \\times 2^{-1022} \\approx \\pm 2.2 \\times 10^{-308}\\) . Largest \\(\\pm 2.0 \\times 2^{1023} \\approx \\pm 1.8 \\times 10^{308}\\) .","title":"Range of normal representation"},{"location":"Mathematics_Basis/NA/Chap_1/#relative-precision","text":"Single-Precision: \\(2^{-23}\\) Equivalent to \\(23 \\times \\log_{10}2 \\approx 6\\) decimal digits of precision\uff086 \u4f4d\u6709\u6548\u6570\u5b57\uff09. Double-Precision: \\(2^{-52}\\) Equivalent to \\(52 \\times \\log_{10}2 \\approx 16\\) decimal digits of precision\uff0816 \u4f4d\u6709\u6548\u6570\u5b57\uff09.","title":"Relative precision"},{"location":"Mathematics_Basis/NA/Chap_2/","text":"Chapter 2 | Solution of Equations in One Variable \u00b6 Bisection Method \u00b6 Theorem 2.0 Suppose that \\(f \\in C[a, b]\\) and \\(f(a)\\cdot f(b) \\lt 0\\) . The Bisection method generates a sequence \\(\\{p_n\\}\\) \\((n = 0, 1, 2,\\dots)\\) approximating a zero \\(p\\) of \\(f\\) with \\[ |p_n-p| \\le \\frac{b - a}{2^n},\\ \\ \\text{ when }n \\ge 1. \\] Key Points for Algorithm Implementation \u00b6 \\(mid = a + (b - a)\\ /\\ 2, \\text{but not } mid = (a + b)\\ /\\ 2\\) , for accuracy and not exceeding the limit of range. \\(sign(FA)\\cdot sign(FM)\\gt 0, \\text{but not }FA\\cdot FM \\gt 0\\) , for saving time. Pros & Cons \u00b6 Pros Simple premise, only requires a continuous \\(f\\) . Always converges to a solution. Cons Slow to converge, and a good intermediate approximation can be inadvertently discarded. Cannot find multiple roots and complex roots. Fixed-Point Iteration \u00b6 \\[ f(x) = 0 \\leftrightarrow x = g(x) \\] Theorem12.1 | Fixed-Point Theorem Let \\(g \\in C[a,b]\\) be such that \\(g(x) \\in [a,b], \\forall x \\in [a, b]\\) . Suppose \\(g'\\) exists on \\((a,b)\\) and that a constant \\(k\\) \\((0\\lt k \\lt 1)\\) exists, s.t. \\[ \\forall\\ x \\in (a,b),\\ \\ |g'(x)| \\le k. \\] Then \\(\\forall\\ p_0 \\in[a,b]\\) , the sequence \\(\\{p_n\\}\\) defined by \\[ p_n=g(p_{n-1}) \\] converges to the unique fixed-point \\(p\\in [a,b]\\) . . Corollary 2.2 \\[ \\begin{aligned} |p_n - p| &\\le k^n \\max\\{p_0 = a, b - p_0\\}, \\\\ |p_n - p| &\\le \\frac{k^n}{1 - k}|p_1 - p_0|. \\end{aligned} \\] \\(\\Rightarrow\\) The smaller \\(k\\) is, the faster it converges. Newton's Method (Newton-Raphson Method) \u00b6 Newton's method is an improvement of common fixed-point iteration method above. Theorem 2.3 Let \\(f\\in C^2[a,b]\\) and \\(\\exists\\ p\\in[a,b]\\) , s.t. \\(f(p) = 0\\) and \\(f'(p) \\ne 0\\) , then \\(\\exists\\ \\delta \\gt 0\\) , \\(\\forall\\ p_0 \\in [p - \\epsilon, p + \\epsilon]\\) , s.t. the sequence \\(\\{p_n\\}_{n = 1}^\\infty\\) defined by \\[ p_n = p_{n-1} - \\frac{f(p_{n-1})}{f'(p_{n-1})} \\] converges to \\(p\\) . Error Analysis for Iterative Methods \u00b6 Definition 2.0 Suppose \\(\\{p_n\\}\\) is a sequence that converges to \\(p\\) , and \\(\\forall n\\) , \\(p_n \\ne p\\) . If positive constants \\(\\alpha\\) and \\(\\lambda\\) exist with \\[ \\lim_{n \\rightarrow \\infty} \\frac{|p_{n + 1} - p|}{|p_n - p|^\\alpha} = \\lambda, \\] then \\(\\{p_n\\}\\) conveges to \\(p\\) of order \\(\\alpha\\) , with asymptotic error constant \\(\\lambda\\) . Specially, If \\(\\alpha = 1\\) , the sequence is called linearly convergent. If \\(\\alpha = 2\\) , the sequence is called quadratically convergent. Theorem 2.4 The common fixed-point iteration method ( \\(g'(p) \\ne 0\\) ) with the premise in Fixed-Point Theorem is linearly convergent. Proof \\[ \\lim_{n\\rightarrow \\infty}\\frac{|p_{n+1} - p|}{|p_n - p|} = \\lim_{n\\rightarrow \\infty}\\frac{g'(\\xi)|p_n - p|}{|p_n - p|} = |g'(p)|. \\] Theorem 2.5 The Newton's method \\((g'(p)=0)\\) is at least quadratically convergent. Proof \\[ \\begin{aligned} & 0 = f(p) = f(p_n) + f'(p_n)(p - p_n) + \\frac{f''(\\xi _n)}{2!}(p - p_n)^2 \\\\ & \\Rightarrow p = \\underbrace{p_n - \\frac{f(p_n)}{f\"(p_n)}}_{p_{n+1}} - \\frac{f''(\\xi _n)}{2!f'(p_n)}(p - p_n)^2 \\\\ & \\Rightarrow \\frac{|p_{n+1} - p|}{|p_n - p|^2} = \\frac{f''(\\xi _n)}{2f'(p_n)}, \\\\ & \\lim_{n\\rightarrow \\infty}\\frac{|p_{n+1} - p|}{|p_n - p|^2} = \\frac{f''(p_n)}{2f'(p_n)}. \\end{aligned} \\] More commonly, Theorem 2.6 Let \\(p\\) be a fixed point of \\(g(x)\\) . If there exists some constant \\(\\alpha \\ge 2\\) such that \\(g\\in C^\\alpha [p-\\delta, p+\\delta]\\) , \\(g'(p)=\\dots=g^{(\\alpha-1)}(p)=0\\) , \\(g^{(\\alpha)}(p)=0\\) . Then the iterations with \\(p_n = g(p_{n-1})\\) , \\(n \\ge 1\\) , is of order \\(\\alpha\\) . Multiple Roots Situation \u00b6 Notice that from the proof above, Newton's method is quadratically convergent if \\(f'(p_n) \\ne 0\\) . If \\(f'(p) = 0\\) , then the equation has multiple roots at \\(p\\) . If \\(f(x) = (x - p)^mq(x)\\) and \\(q(p)\\ne 0\\) , for Newton's Method, \\[ g'(p) = \\frac{f(p)f''(p)}{f'(p)^2} = \\left.\\frac{f(x)f''(x)}{f'(x)^2}\\right|_{x=p} = \\dots =1 - \\frac{1}{m} \\lt 1 \\] It is still convegent, but not quadratically . A way to speed it up Let \\[ \\mu (x) = \\frac{f(x)}{f'(x)}, \\] then \\(\\mu (x)\\) has the same root as \\(f(x)\\) but no multiple roots anymore. And \\[ g(x) = x - \\frac{\\mu (x)}{\\mu'(x)} = x - \\frac{f(x)f'(x)}{f'(x)^2 - f(x)f''(x)}. \\] Newton's method can be used there again. Pros & Cons \u00b6 Pros Quadratic convegence Cons Requires additional calculation of \\(f''(x)\\) The denominator consists of the difference of the two numbers both close to \\(0\\) . Accelerating Convergence \u00b6 Aitken's \\(\\Delta^2\\) Method \u00b6 Definition 2.1 Forward Difference \\(\\Delta p_n = p_{n+1} - p_n\\) . Similarly \\(\\Delta^kp_n = \\Delta(\\Delta^{k-1}p_n)\\) Representing Aitken's \\(\\Delta^2\\) Method by forward difference, we have \\[ \\hat{p_n} = p_n - \\frac{(\\Delta p_n)^2}{\\Delta^2p_n}. \\] Theorem 2.7 Suppose that \\(\\{p_n\\}\\) is a sequence, \\(\\lim\\limits_{n\\rightarrow \\infty}p_n = p\\) , \\(\\exists N\\) , s.t. \\(\\forall\\ n > N\\) , \\((p_n - p)(p_{n+1} -p) \\gt 0\\) . Then the sequence \\(\\{\\hat{p_n}\\}\\) converges to \\(p\\) faster than \\(\\{p_n\\}\\) in the sense that \\[ \\lim_{n\\rightarrow \\infty}\\frac{\\hat{p_n} - p}{p_n - p} = 0. \\] The algorithm to implement it is called Steffensen\u2019s Acceleration .","title":"Chap 2"},{"location":"Mathematics_Basis/NA/Chap_2/#chapter-2-solution-of-equations-in-one-variable","text":"","title":"Chapter 2 | Solution of Equations in One Variable"},{"location":"Mathematics_Basis/NA/Chap_2/#bisection-method","text":"Theorem 2.0 Suppose that \\(f \\in C[a, b]\\) and \\(f(a)\\cdot f(b) \\lt 0\\) . The Bisection method generates a sequence \\(\\{p_n\\}\\) \\((n = 0, 1, 2,\\dots)\\) approximating a zero \\(p\\) of \\(f\\) with \\[ |p_n-p| \\le \\frac{b - a}{2^n},\\ \\ \\text{ when }n \\ge 1. \\]","title":"Bisection Method"},{"location":"Mathematics_Basis/NA/Chap_2/#key-points-for-algorithm-implementation","text":"\\(mid = a + (b - a)\\ /\\ 2, \\text{but not } mid = (a + b)\\ /\\ 2\\) , for accuracy and not exceeding the limit of range. \\(sign(FA)\\cdot sign(FM)\\gt 0, \\text{but not }FA\\cdot FM \\gt 0\\) , for saving time.","title":"Key Points for Algorithm Implementation"},{"location":"Mathematics_Basis/NA/Chap_2/#pros-cons","text":"Pros Simple premise, only requires a continuous \\(f\\) . Always converges to a solution. Cons Slow to converge, and a good intermediate approximation can be inadvertently discarded. Cannot find multiple roots and complex roots.","title":"Pros &amp; Cons"},{"location":"Mathematics_Basis/NA/Chap_2/#fixed-point-iteration","text":"\\[ f(x) = 0 \\leftrightarrow x = g(x) \\] Theorem12.1 | Fixed-Point Theorem Let \\(g \\in C[a,b]\\) be such that \\(g(x) \\in [a,b], \\forall x \\in [a, b]\\) . Suppose \\(g'\\) exists on \\((a,b)\\) and that a constant \\(k\\) \\((0\\lt k \\lt 1)\\) exists, s.t. \\[ \\forall\\ x \\in (a,b),\\ \\ |g'(x)| \\le k. \\] Then \\(\\forall\\ p_0 \\in[a,b]\\) , the sequence \\(\\{p_n\\}\\) defined by \\[ p_n=g(p_{n-1}) \\] converges to the unique fixed-point \\(p\\in [a,b]\\) . . Corollary 2.2 \\[ \\begin{aligned} |p_n - p| &\\le k^n \\max\\{p_0 = a, b - p_0\\}, \\\\ |p_n - p| &\\le \\frac{k^n}{1 - k}|p_1 - p_0|. \\end{aligned} \\] \\(\\Rightarrow\\) The smaller \\(k\\) is, the faster it converges.","title":"Fixed-Point Iteration"},{"location":"Mathematics_Basis/NA/Chap_2/#newtons-method-newton-raphson-method","text":"Newton's method is an improvement of common fixed-point iteration method above. Theorem 2.3 Let \\(f\\in C^2[a,b]\\) and \\(\\exists\\ p\\in[a,b]\\) , s.t. \\(f(p) = 0\\) and \\(f'(p) \\ne 0\\) , then \\(\\exists\\ \\delta \\gt 0\\) , \\(\\forall\\ p_0 \\in [p - \\epsilon, p + \\epsilon]\\) , s.t. the sequence \\(\\{p_n\\}_{n = 1}^\\infty\\) defined by \\[ p_n = p_{n-1} - \\frac{f(p_{n-1})}{f'(p_{n-1})} \\] converges to \\(p\\) .","title":"Newton's Method (Newton-Raphson Method)"},{"location":"Mathematics_Basis/NA/Chap_2/#error-analysis-for-iterative-methods","text":"Definition 2.0 Suppose \\(\\{p_n\\}\\) is a sequence that converges to \\(p\\) , and \\(\\forall n\\) , \\(p_n \\ne p\\) . If positive constants \\(\\alpha\\) and \\(\\lambda\\) exist with \\[ \\lim_{n \\rightarrow \\infty} \\frac{|p_{n + 1} - p|}{|p_n - p|^\\alpha} = \\lambda, \\] then \\(\\{p_n\\}\\) conveges to \\(p\\) of order \\(\\alpha\\) , with asymptotic error constant \\(\\lambda\\) . Specially, If \\(\\alpha = 1\\) , the sequence is called linearly convergent. If \\(\\alpha = 2\\) , the sequence is called quadratically convergent. Theorem 2.4 The common fixed-point iteration method ( \\(g'(p) \\ne 0\\) ) with the premise in Fixed-Point Theorem is linearly convergent. Proof \\[ \\lim_{n\\rightarrow \\infty}\\frac{|p_{n+1} - p|}{|p_n - p|} = \\lim_{n\\rightarrow \\infty}\\frac{g'(\\xi)|p_n - p|}{|p_n - p|} = |g'(p)|. \\] Theorem 2.5 The Newton's method \\((g'(p)=0)\\) is at least quadratically convergent. Proof \\[ \\begin{aligned} & 0 = f(p) = f(p_n) + f'(p_n)(p - p_n) + \\frac{f''(\\xi _n)}{2!}(p - p_n)^2 \\\\ & \\Rightarrow p = \\underbrace{p_n - \\frac{f(p_n)}{f\"(p_n)}}_{p_{n+1}} - \\frac{f''(\\xi _n)}{2!f'(p_n)}(p - p_n)^2 \\\\ & \\Rightarrow \\frac{|p_{n+1} - p|}{|p_n - p|^2} = \\frac{f''(\\xi _n)}{2f'(p_n)}, \\\\ & \\lim_{n\\rightarrow \\infty}\\frac{|p_{n+1} - p|}{|p_n - p|^2} = \\frac{f''(p_n)}{2f'(p_n)}. \\end{aligned} \\] More commonly, Theorem 2.6 Let \\(p\\) be a fixed point of \\(g(x)\\) . If there exists some constant \\(\\alpha \\ge 2\\) such that \\(g\\in C^\\alpha [p-\\delta, p+\\delta]\\) , \\(g'(p)=\\dots=g^{(\\alpha-1)}(p)=0\\) , \\(g^{(\\alpha)}(p)=0\\) . Then the iterations with \\(p_n = g(p_{n-1})\\) , \\(n \\ge 1\\) , is of order \\(\\alpha\\) .","title":"Error Analysis for Iterative Methods"},{"location":"Mathematics_Basis/NA/Chap_2/#multiple-roots-situation","text":"Notice that from the proof above, Newton's method is quadratically convergent if \\(f'(p_n) \\ne 0\\) . If \\(f'(p) = 0\\) , then the equation has multiple roots at \\(p\\) . If \\(f(x) = (x - p)^mq(x)\\) and \\(q(p)\\ne 0\\) , for Newton's Method, \\[ g'(p) = \\frac{f(p)f''(p)}{f'(p)^2} = \\left.\\frac{f(x)f''(x)}{f'(x)^2}\\right|_{x=p} = \\dots =1 - \\frac{1}{m} \\lt 1 \\] It is still convegent, but not quadratically . A way to speed it up Let \\[ \\mu (x) = \\frac{f(x)}{f'(x)}, \\] then \\(\\mu (x)\\) has the same root as \\(f(x)\\) but no multiple roots anymore. And \\[ g(x) = x - \\frac{\\mu (x)}{\\mu'(x)} = x - \\frac{f(x)f'(x)}{f'(x)^2 - f(x)f''(x)}. \\] Newton's method can be used there again.","title":"Multiple Roots Situation"},{"location":"Mathematics_Basis/NA/Chap_2/#pros-cons_1","text":"Pros Quadratic convegence Cons Requires additional calculation of \\(f''(x)\\) The denominator consists of the difference of the two numbers both close to \\(0\\) .","title":"Pros &amp; Cons"},{"location":"Mathematics_Basis/NA/Chap_2/#accelerating-convergence","text":"","title":"Accelerating Convergence"},{"location":"Mathematics_Basis/NA/Chap_2/#aitkens-delta2-method","text":"Definition 2.1 Forward Difference \\(\\Delta p_n = p_{n+1} - p_n\\) . Similarly \\(\\Delta^kp_n = \\Delta(\\Delta^{k-1}p_n)\\) Representing Aitken's \\(\\Delta^2\\) Method by forward difference, we have \\[ \\hat{p_n} = p_n - \\frac{(\\Delta p_n)^2}{\\Delta^2p_n}. \\] Theorem 2.7 Suppose that \\(\\{p_n\\}\\) is a sequence, \\(\\lim\\limits_{n\\rightarrow \\infty}p_n = p\\) , \\(\\exists N\\) , s.t. \\(\\forall\\ n > N\\) , \\((p_n - p)(p_{n+1} -p) \\gt 0\\) . Then the sequence \\(\\{\\hat{p_n}\\}\\) converges to \\(p\\) faster than \\(\\{p_n\\}\\) in the sense that \\[ \\lim_{n\\rightarrow \\infty}\\frac{\\hat{p_n} - p}{p_n - p} = 0. \\] The algorithm to implement it is called Steffensen\u2019s Acceleration .","title":"Aitken's \\(\\Delta^2\\) Method"},{"location":"Mathematics_Basis/NA/Chap_3/","text":"Chapter 3 | Interpolation and Polynomial Approximation \u00b6 Lagrange Interpolation \u00b6 Suppose we have function \\(y = f(x)\\) with the given points \\((x_0, y_0), (x_1, y_1), \\dots, (x_n, y_n)\\) , and then construct a relatively simple approximating function \\(g(x) \\approx f(x)\\) . Theorem 3.0 If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers and \\(f\\) is a function with given values \\(f(x_0), \\dots, f(x_n)\\) , then a unique polynomial \\(P(x)\\) of degree at most \\(n\\) exists with \\[ P(x_k) = f(x_k),\\ \\ \\text{ for each } k = 0, 1, \\dots, n, \\] and \\[ P(x) = \\sum\\limits_{k = 0}^n f(x_k)L_{n, k}(x), \\] where, for each \\(k = 0, 1, \\dots, n\\) , \\[ L_{n, k}(x) = \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n \\frac{(x - x_i)}{(x_k - x_i)}. \\] \\(L_{n, k}(x)\\) is called the n th Lagrange interpolating polynomial . Proof First we prove for the structure of function \\(L_{n, k}(x)\\) . From the definition of \\(P(x)\\) , \\(L_{n, k}(x)\\) has the following properties. \\(L_{n, k}(x_i) = 0\\) when \\(i \\ne k\\) . \\(L_{n, k}(x_k) = 1\\) . To satisfy the first property, the numerator of \\(L_{n, k}(x)\\) contains the term \\[ (x - x_0)(x - x_1) \\cdots (x - x_{k - 1})(x - x_{k + 1}) \\cdots (x - x_n) = \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n (x - x_i). \\] To satisfy the second property, the denominator of \\(L_{n, k}(x)\\) must be equal to the numerator at \\(x = x_k\\) , thus \\[ \\begin{aligned} L_{n, k}(x) &= \\frac{(x - x_0)(x - x_1) \\cdots (x - x_{k - 1})(x - x_{k + 1}) \\cdots (x - x_n)}{(x_k - x_0)(x - x_1) \\cdots (x_k - x_{k - 1})(x_k - x_{k + 1}) \\cdots (x_k - x_n)} \\\\ &= \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n \\frac{x - x_i}{x_k - x_i}. \\end{aligned} \\] For uniqueness, we prove by contradition. If not, suppose \\(P(x)\\) and \\(Q(x)\\) both satisfying the conditions, then \\(D(x) = P(x) - Q(x)\\) is a polynomial of degree \\(\\text{deg}(D(x)) \\le n\\) , but \\(D(x)\\) has \\(n + 1\\) distinct roots \\(x_0, x_1, \\dots, x_n\\) , which leads to a contradiction. Theorem 3.1 If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers in the interval \\([a, b]\\) and \\(f \\in C^{n + 1}[a, b]\\) . \\(\\forall x \\in [a, b], \\exists \\xi \\in [a, b], s.t.\\) \\[ f(x) = P(x) + \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i), \\] where \\(P(x)\\) is the Lagrange interpolating polynomial. And \\[ R(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] is the truncation error . Proof Since \\(R(x) = f(x) - P(x)\\) has at least \\(n + 1\\) roots, thus \\[ R(x) = K(x)\\prod\\limits_{i = 0}^n (x - x_i). \\] For a fixed \\(x \\ne x_k\\) , define \\(g(t)\\) in \\([a, b]\\) by \\[ g(t) = R(t) - K(x)\\prod\\limits_{i = 0}^n (t - x_i). \\] Since \\(g(t)\\) has \\(n + 2\\) distinct roots \\(x, x_0, \\dots, x_n\\) , by Generalized Rolle's Theorem, we have \\[ \\exists\\ \\xi \\in [a, b],\\ s.t.\\ g^{(n + 1)}(\\xi) = 0. \\] Namely, \\[ f^{(n + 1)}(\\xi) - \\underbrace{P^{n + 1}(\\xi)}_{0} - K(x)(n + 1)! = 0. \\] Thus \\[ K(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!},\\ R(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] Example Suppose a table is to be prepared for the function \\(f(x) = e^x\\) for \\(x\\) in \\([0, 1]\\) . Assume that each entry of the table is accurate up to 8 decimal places and the step size is \\(h\\) . What should \\(h\\) be for linear interpolation to give an absolute error of at most \\(10^{-6}\\) ? Solution. \\[ \\begin{aligned} |f(x) - P(x)| &= \\left|\\frac{f^{(2)}(\\xi)}{2!}(x - x_j)(x - x_{j + 1})\\right| \\\\ &= \\left|\\frac{e^\\xi}{2}(x - kh)(x - (k + 1)h)\\right| \\le \\frac{e}{2} \\cdot \\frac{h^2}{4} = \\frac{eh^2}{8}. \\end{aligned} \\] Thus let \\(\\dfrac{eh^2}{8} \\le 10^{-6}\\) , we have \\(h \\le 1.72 \\times 10^{-3}\\) . To make \\(N = \\dfrac{(1 - 0)}{h}\\) an integer, we can simply choose \\(h = 0.001\\) . Extrapolation Suppose \\(a = \\min\\limits_i \\{x_i\\}\\) , \\(b = \\max\\limits_i \\{x_i\\}\\) . Interpolation estimates value \\(P(x)\\) , \\(x \\in [a, b]\\) , while Extrapolation estimates value \\(P(x)\\) , \\(x \\notin [a, b]\\) . In genernal, interpolation is better than extrapolation. Neville's Method \u00b6 Motivation: When we have more interpolating points, the original Lagrange interpolating method should re-calculate all \\(L_{n, k}\\) , which is not efficient. Definition 3.0 Let \\(f\\) be a function defined at \\(x_0, x_1, \\dots, x_n\\) , and suppose that \\(m_1, \\dots, m_k\\) are \\(k\\) distinct integers with \\(0 \\le m_i \\le n\\) for each \\(i\\) . The Lagrange polynomial that agrees with \\(f(x)\\) at the \\(k\\) points denoted by \\(P_{m_1, m_2, \\dots, m_k}(x)\\) . Thus \\(P(x) = P_{0, 1, \\dots, n}(x)\\) , where \\(P(x)\\) is the n th Lagrange polynomials that interpolate \\(f\\) at \\(k + 1\\) points \\(x_0, \\dots, x_k\\) . Theorem 3.2 Let \\(f\\) be a function defined at \\(x_0, x_1, \\dots, x_n\\) , and \\(x_i \\ne x_j\\) , then \\[ P(x) = \\frac{(x - x_j)P_{0, 1, \\dots, j - 1, j + 1, \\dots k}(x) - (x - x_i)P_{0, 1, \\dots, i - 1, i + 1, \\dots k}(x)}{(x_i - x_j)}. \\] Denote that \\[ Q_{i, j} = P_{i - j, i - j + 1, \\dots, i - 1, i}, \\] then from Theorem 3.2 , the interpolating polynomials can be generated recursively . Newton Interpolation \u00b6 Differing from Langrange polynomials, we try to represent \\(P(x)\\) by the following form: \\[ \\begin{aligned} N(x) = &\\ a_0 + a_1(x - x_0) + a_2(x - x_0)(x - x_1) + \\cdots \\\\ & + a_n(x - x_0)(x - x_1) \\cdots (x - x_{n - 1}). \\end{aligned} \\] Definition 3.1 \\(f[x_i] = f(x_i)\\) is the zeroth divided difference w.r.t. \\(x_i\\) . The k th divided difference w.r.t. \\(x_i, x_{i + 1}, x_{i + k}\\) is defined recursively by \\[ \\small f[x_i, x_{i + 1}, \\dots, x_{i + k - 1}, x_{i + k}] = \\frac{f[x_{i + 1}, x_{i + 2}, \\dots, x_{i + k}] - f[x_i, x_{i + 1}, \\dots, x_{i + k - 1}]}{x_{i + k} - x_i}. \\] Then we derive the Newton's interpolatory divided-difference formula \\[ N(x) = f[x_0] + \\sum\\limits_{k = 1}^n f[x_0, x_1, \\dots, x_k]\\prod\\limits_{i = 0}^{k - 1}(x - x_i). \\] And the divided difference can be generated as below, which is similar to Neville's Method. Theorem 3.3 If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers in the interval \\([a, b]\\) and \\(f \\in C^{n + 1}[a, b]\\) . \\(\\forall x \\in [a, b], \\exists \\xi \\in [a, b], s.t.\\) \\[ f(x) = N(x) + f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i). \\] where \\(N(x)\\) is the Newton's interpolatory divided-difference formula. And \\[ R(x) = f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i). \\] is the truncation error . Proof By definition of divided difference, we have \\[ \\left\\{ \\begin{aligned} f(x) &= f[x_0] + (x - x_0)f[x, x_0], & \\textbf{Eq.1} \\\\ f[x, x_0] &= f[x_0, x_1] + (x - x_1)f[x, x_0, x_1], & \\textbf{Eq.2} \\\\ & \\ \\ \\vdots \\\\ f[x, x_0, \\dots, x_{n - 1}] &= f[x_0, \\dots, x_n] + (x - x_n)f[x, x_0, \\dots, x_n]. & \\textbf{Eq.n - 1} \\end{aligned} \\right. \\] then compute \\[ \\textbf{Eq.1} + (x - x_0) \\times \\textbf{Eq.2} + \\cdots + (x - x_0) \\cdots (x - x_{n - 1}) \\times \\textbf{Eq.n - 1}. \\] i.e. \\[ f(x) = N(x) + \\underbrace{f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i)}_{R(x)}. \\] Note Since the uniqueness of n -th interpolating polynomial, \\(N(x) \\equiv P(x)\\) . They have the same truncation error, which is \\[ f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] Theorem 3.4 Suppose that \\(f \\in C^n[a, b]\\) and \\(x_0, x_1, \\dots, x_n\\) are distinct numbers in \\([a, b]\\) . Then \\(\\exists\\ \\xi \\in (a, b)\\) , s.t. \\[ f[x_0, x_1, \\dots, x_n] = \\frac{f^{(n)}(\\xi)}{n!}. \\] Special Case: Equal Spacing \u00b6 Definition 3.2 Forward Difference \\[ \\begin{aligned} \\Delta f_i &= f_{i + 1} - f_i, \\\\ \\Delta^k f_i &= \\Delta\\left(\\Delta^{k - 1} f_i \\right) = \\Delta^{k - 1}f_{i + 1} - \\Delta^{k - 1}f_i. \\end{aligned} \\] Backward Difference \\[ \\begin{aligned} \\nabla f_i &= f_i - f_{i - 1}, \\\\ \\nabla^k f_i &= \\nabla\\left(\\nabla^{k - 1} f_i \\right) = \\nabla^{k - 1}f_i - \\nabla^{k - 1}f_{i - 1}. \\end{aligned} \\] Property of Forward / Backward Difference Linearity \\(\\Delta (af(x) + bg(x)) = a \\Delta f + b \\Delta g\\) . If \\(\\text{deg}(f(x)) = m\\) , then \\[ \\text{deg}\\left(\\Delta^kf(x)\\right) = \\left\\{ \\begin{aligned} & m - k, & 0 \\le k \\le m, \\\\ & 0, & k > m. \\end{aligned} \\right. \\] Decompose the recursive definition, \\[ \\begin{aligned} \\Delta^n f_k &= \\sum\\limits_{j = 0}^n (-1)^j \\binom{n}{j} f_{n + k - j}, \\\\ \\nabla^n f_k &= \\sum\\limits_{j = 0}^n (-1)^{n - j} \\binom{n}{j} f_{j + k - n}. \\end{aligned} \\] Vice versa, \\[ f_{n + k} = \\sum\\limits_{j = 0}^n \\binom{n}{j} \\Delta^j f_k. \\] Suppose \\(x_0, x_1, \\dots x_n\\) are equally spaced, namely \\(x_i = x_0 + ih\\) . And let \\(x = x_0 + sh\\) , then \\(x - x_i = (s - i)h\\) . Thus \\[ \\begin{aligned} N(x) &= f[x_0] + \\sum\\limits_{k = 1}^n s(s - 1) \\cdots (s - k + 1) h^k f[x_0, x_1, \\dots, x_k] \\\\ &= f[x_0] + \\sum\\limits_{k = 1}^n \\binom{s}{k} k! h^k f[x_0, x_1, \\dots, x_k] \\end{aligned} \\] is called Newton forward divided-difference formula . From mathematical induction, we can derive that \\[ f[x_0, x_1, \\dots, x_k] = \\frac{1}{k!h^k}\\Delta^k f(x_0). \\] Thus we get the Newton Forward-Difference Formula \\[ N(x) = f[x_0] + \\sum\\limits_{k = 1}^n \\binom{s}{k} \\Delta^k f(x_0). \\] Inversely, \\(x = x_n + sh\\) , then \\(x - x_i = (s + n - i)h\\) , Thus \\[ \\begin{aligned} N(x) &= f[x_n] + \\sum\\limits_{k = 1}^n s(s + 1) \\cdots (s + k - 1) h^k f[x_n, x_{n - 1}, \\dots, x_{n - k}] \\\\ &= f[x_n] + \\sum\\limits_{k = 1}^n (-1)^k \\binom{-s}{k} k! h^k f[x_n, x_{n - 1}, \\dots, x_{n - k}]. \\end{aligned} \\] is called Newton backward divided-difference formula . From mathematical induction, we can derive that \\[ f[x_n, x_{n - 1}, \\dots, x_0] = \\frac{1}{k!h^k}\\nabla^k f(x_n). \\] Thus we get the Newton Backward-Difference Formula \\[ N(x) = f[x_n] + \\sum\\limits_{k = 1}^n (-1)^k \\binom{-s}{k} \\nabla^k f(x_n). \\] Hermit Interpolation \u00b6 Definition 3.3 Let \\(x_0, x_1, \\dots, x_n\\) be \\(n + 1\\) distinct numbers in \\([a, b]\\) and \\(m_i \\in \\mathbb{N}\\) . Suppose that \\(f \\in C^m[a, b]\\) , where \\(m = \\max \\{m_i\\}\\) . The osculating polynomial approximating \\(f\\) is the polynomial \\(P(x)\\) of least degree such that \\[ \\frac{d^k P(x_i)}{dx^k} = \\frac{d^k f(x_i)}{dx^k}, \\text{ for each } i = 0, 1, \\dots, n \\text{ and } k = 0, 1, \\dots, m_i. \\] From definition above, we know that when \\(m_i = 0\\) for each \\(i\\) , it's the n -th Lagrange polynomial. And the cases that \\(m_i = 1\\) for each \\(i\\) , then it's Hermit Polynomials . Theorem 3.5 If \\(f \\in C^1[a, b]\\) and \\(x_0, \\dots, x_n \\in [a, b]\\) are distinct, the unique polynomial of least degree agreeing with \\(f\\) and \\(f'\\) at \\(x_0, \\dots, x_n\\) is the Hermit Polynomial of degree at most \\(\\bf 2n + 1\\) defined by \\[ H_{2n + 1}(x) = \\sum\\limits_{j = 0}^n f(x_j) H_{n ,j}(x) + \\sum\\limits_{j = 0}^n f'(x_j) \\hat H_{n, j}(x), \\] where \\[ H_{n, j}(x) = [1 - 2(x - x_j)L'_{n, j}(x_j)]L^2_{n, j}(x), \\] and \\[ \\hat H_{n, j}(x) = (x - x_j)L^2_{n, j}(x). \\] Moreover, if \\(f \\in C^{2n + 2}[a, b]\\) , then \\(\\exists\\ \\xi \\in (a, b)\\) , s.t. \\[ f(x) = H_{2n + 1}(x) + \\underbrace{\\frac{1}{(2n + 2)!}\\prod\\limits_{i = 0}^n (x - x_i)^2 f^{2n + 2}(\\xi)}_{R(x)}. \\] The theorem above gives a complete description of Hermit interpolation. But in pratice, to compute \\(H_{2n + 1}(x)\\) through the formula above is tedious . To make it compute easier, we introduce a method that is similar to Newton's interpolation. Define the sequence \\(\\{z_k\\}_{0}^{2n + 1}\\) by \\[ z_{2i} = z_{2i + 1} = x_i. \\] Based on the Theorem 3.4 , we redefine that \\[ f[z_{2i}, z_{2i + 1}] = f'(z_{2i}) = f'(x_i). \\] Then Hermite polynomial can be represented by \\[ H_{2n + 1}(x) = f[z_0] + \\sum\\limits_{k = 1}^{2n + 1} f[z_0, \\dots, z_k]\\prod\\limits_{i = 0}^{k - 1}(x - z_i). \\] Cubic Spline Interpolation \u00b6 Motivation: For osculating polynomial approximation, we can let \\(m_i\\) be bigger to get high-degree polynomials. It can somehow be better but higher degree tends to causes a fluctuation or say overfitting . An alternative approach is to divide the interval into subintervals and approximate them respectively, which is called piecewise-polynomial approximation . The most common piecewise-polynomial approximation uses cubic polynomials called cubic spline approximation . Definition 3.4 Given a function \\(f\\) defined on \\([a, b]\\) and a set of nodes \\(a = x_0 < x_1 < \\cdots < x_n = b\\) , a cubic spline interpolant \\(S\\) for \\(f\\) is a function that satisfies the following conditions. \\(S(x)\\) is a cubic polynomial, denoted \\(S_j(x)\\) , on the subinterval \\([x_j, x_j + 1]\\) , for each \\(j = 0, 1, \\dots, n - 1\\) ; \\(S(x_j) = f(x_j)\\) for each \\(j = 0, 1, \\dots, n\\) ; \\(S_{j + 1}(x_{j + 1}) = S_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\) ; \\(S'_{j + 1}(x_{j + 1}) = S'_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\) ; \\(S''_{j + 1}(x_{j + 1}) = S''_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\) ; One of the following sets of boundary conditions: \\(S''(x_0) = S''(x_n) = 0\\) ( free or natural boundary ); \\(S'(x_0) = f'(x_0)\\) and \\(S'(x_n) = f'(x_n)\\) ( clamped boundary ). The spline of natural boundary is called natural spline . Theorem 3.6 The cubic spline interpolation of either natural boundary or clamped boundary is unique . (Since the coefficient matrix \\(A\\) is strictly diagonally dominant.) Suppose interpolation function in each subinterval is \\[ S_j(x) = a_j + b_j(x - x_j) + c_j(x - x_j)^2 + d_j(x - x_j)^3. \\] From the conditions in the definition above, by some algebraic process, we can derive the solution with the following equations, \\[ \\begin{aligned} h_j &= x_{j + 1} - x_j; \\\\ a_j &= f(x_j); \\\\ b_j &= \\frac{1}{h_j}(a_{j + 1}) - \\frac{h_j}{3}(2c_j + c_{j + 1}); \\\\ d_j &= \\frac{1}{3h_j}{c_{j + 1} - c_j}. \\end{aligned} \\] While \\(c_j\\) is given by solving the following linear system, \\[ A\\mathbf{x} = \\mathbf{b}, \\] where \\[ \\small A = \\begin{bmatrix} 1 & 0 & 0 & \\cdots & \\cdots & 0 \\\\ h_0 & 2(h_0 + h_1) & h_1 & \\cdots & \\cdots & \\vdots \\\\ 0 & h_1 & 2(h_1 + h_2) & h_2 & \\ddots & \\vdots \\\\ \\vdots & \\ddots & \\ddots & \\ddots & \\ddots & 0 \\\\ \\vdots & \\ddots & \\ddots & h_{n - 2} & 2(h_{n - 2} + h_{n - 1}) & h_{n - 1} \\\\ 0 & \\cdots & \\cdots & 0 & 0 & 1 \\\\ \\end{bmatrix} , \\mathbf{x} = \\begin{bmatrix} c_0 \\\\ c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\] For natural boundary, \\[ \\small \\mathbf{b} = \\begin{bmatrix} 0 \\\\ \\frac{3}{h_1}(a_2 - a_1) - \\frac{3}{h_0}(a_1 - a_0) \\\\ \\vdots \\\\ \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) - \\frac{3}{h_{n - 2}}\\ (a_{n - 1} - a_{n - 2}) \\\\ 0 \\end{bmatrix}. \\] For clamped boundary, \\[ \\small \\mathbf{b} = \\begin{bmatrix} \\frac{3}{h_0}(a_1 - a_0) - 3f'(a) \\\\ \\frac{3}{h_1}(a_2 - a_1) - \\frac{3}{h_0}(a_1 - a_0) \\\\ \\vdots \\\\ \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) - \\frac{3}{h_{n - 2}}\\ (a_{n - 1} - a_{n - 2}) \\\\ 3f'(b) - \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) \\\\ \\end{bmatrix}. \\] For More Accuracy... If \\(f \\in C[a, b]\\) and \\(\\frac{\\max h_i}{\\min h_i} \\le C < \\infty\\) . Then \\(S(x)\\ \\overset{\\text{uniform}}{\\longrightarrow}\\ f(x)\\) when \\(\\max h_i \\rightarrow 0\\) . That is the accuracy of approximation can be improved by adding more nodes without increasing the degree of the splines. Curves \u00b6 We've discussed the interpolation of functions above, but we may encounter the case to interpolate a curve. Straightforward Technique \u00b6 For given points \\((x_0, y_0), (x_1, y_1), \\dots, (x_n, y_n)\\) , we can construct two approximation functions with \\[ x_i = x(t_i),\\ y_i = y(t_i). \\] The interpolation method can be Lagrange, Hermite and Cubic spline, whatever. Bezier Curve \u00b6 In nature, it's piecewise cubic Hermite polynomial , and the curve is called Bezier curve . Similarly, suppose two function \\(x(t)\\) and \\(y(t)\\) at each interval. We have the following condtions. \\[ x(0) = x_0,\\ x(1) = x_1,\\ x'(0) = \\alpha_0,\\ x'(1) = \\alpha_1. \\] \\[ y(0) = y_0,\\ y(1) = y_1,\\ y'(0) = \\beta_0,\\ y'(1) = \\beta_1. \\] The solution is \\[ \\begin{aligned} x(t) &= [2(x_0 - x_1) + (\\alpha_0 + \\alpha_1)]t^3 + [3(x_1 - x_0) - (\\alpha_1 + 2\\alpha_0)]t^2 + \\alpha_0 t + x_0. \\\\ y(t) &= [2(y_0 - y_1) + (\\beta_0 + \\beta_1)]t^3 + [3(y_1 - y_0) - (\\beta_1 + 2\\beta_0)]t^2 + \\alpha_0 t + y_0. \\end{aligned} \\]","title":"Chap 3"},{"location":"Mathematics_Basis/NA/Chap_3/#chapter-3-interpolation-and-polynomial-approximation","text":"","title":"Chapter 3 | Interpolation and Polynomial Approximation"},{"location":"Mathematics_Basis/NA/Chap_3/#lagrange-interpolation","text":"Suppose we have function \\(y = f(x)\\) with the given points \\((x_0, y_0), (x_1, y_1), \\dots, (x_n, y_n)\\) , and then construct a relatively simple approximating function \\(g(x) \\approx f(x)\\) . Theorem 3.0 If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers and \\(f\\) is a function with given values \\(f(x_0), \\dots, f(x_n)\\) , then a unique polynomial \\(P(x)\\) of degree at most \\(n\\) exists with \\[ P(x_k) = f(x_k),\\ \\ \\text{ for each } k = 0, 1, \\dots, n, \\] and \\[ P(x) = \\sum\\limits_{k = 0}^n f(x_k)L_{n, k}(x), \\] where, for each \\(k = 0, 1, \\dots, n\\) , \\[ L_{n, k}(x) = \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n \\frac{(x - x_i)}{(x_k - x_i)}. \\] \\(L_{n, k}(x)\\) is called the n th Lagrange interpolating polynomial . Proof First we prove for the structure of function \\(L_{n, k}(x)\\) . From the definition of \\(P(x)\\) , \\(L_{n, k}(x)\\) has the following properties. \\(L_{n, k}(x_i) = 0\\) when \\(i \\ne k\\) . \\(L_{n, k}(x_k) = 1\\) . To satisfy the first property, the numerator of \\(L_{n, k}(x)\\) contains the term \\[ (x - x_0)(x - x_1) \\cdots (x - x_{k - 1})(x - x_{k + 1}) \\cdots (x - x_n) = \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n (x - x_i). \\] To satisfy the second property, the denominator of \\(L_{n, k}(x)\\) must be equal to the numerator at \\(x = x_k\\) , thus \\[ \\begin{aligned} L_{n, k}(x) &= \\frac{(x - x_0)(x - x_1) \\cdots (x - x_{k - 1})(x - x_{k + 1}) \\cdots (x - x_n)}{(x_k - x_0)(x - x_1) \\cdots (x_k - x_{k - 1})(x_k - x_{k + 1}) \\cdots (x_k - x_n)} \\\\ &= \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n \\frac{x - x_i}{x_k - x_i}. \\end{aligned} \\] For uniqueness, we prove by contradition. If not, suppose \\(P(x)\\) and \\(Q(x)\\) both satisfying the conditions, then \\(D(x) = P(x) - Q(x)\\) is a polynomial of degree \\(\\text{deg}(D(x)) \\le n\\) , but \\(D(x)\\) has \\(n + 1\\) distinct roots \\(x_0, x_1, \\dots, x_n\\) , which leads to a contradiction. Theorem 3.1 If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers in the interval \\([a, b]\\) and \\(f \\in C^{n + 1}[a, b]\\) . \\(\\forall x \\in [a, b], \\exists \\xi \\in [a, b], s.t.\\) \\[ f(x) = P(x) + \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i), \\] where \\(P(x)\\) is the Lagrange interpolating polynomial. And \\[ R(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] is the truncation error . Proof Since \\(R(x) = f(x) - P(x)\\) has at least \\(n + 1\\) roots, thus \\[ R(x) = K(x)\\prod\\limits_{i = 0}^n (x - x_i). \\] For a fixed \\(x \\ne x_k\\) , define \\(g(t)\\) in \\([a, b]\\) by \\[ g(t) = R(t) - K(x)\\prod\\limits_{i = 0}^n (t - x_i). \\] Since \\(g(t)\\) has \\(n + 2\\) distinct roots \\(x, x_0, \\dots, x_n\\) , by Generalized Rolle's Theorem, we have \\[ \\exists\\ \\xi \\in [a, b],\\ s.t.\\ g^{(n + 1)}(\\xi) = 0. \\] Namely, \\[ f^{(n + 1)}(\\xi) - \\underbrace{P^{n + 1}(\\xi)}_{0} - K(x)(n + 1)! = 0. \\] Thus \\[ K(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!},\\ R(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] Example Suppose a table is to be prepared for the function \\(f(x) = e^x\\) for \\(x\\) in \\([0, 1]\\) . Assume that each entry of the table is accurate up to 8 decimal places and the step size is \\(h\\) . What should \\(h\\) be for linear interpolation to give an absolute error of at most \\(10^{-6}\\) ? Solution. \\[ \\begin{aligned} |f(x) - P(x)| &= \\left|\\frac{f^{(2)}(\\xi)}{2!}(x - x_j)(x - x_{j + 1})\\right| \\\\ &= \\left|\\frac{e^\\xi}{2}(x - kh)(x - (k + 1)h)\\right| \\le \\frac{e}{2} \\cdot \\frac{h^2}{4} = \\frac{eh^2}{8}. \\end{aligned} \\] Thus let \\(\\dfrac{eh^2}{8} \\le 10^{-6}\\) , we have \\(h \\le 1.72 \\times 10^{-3}\\) . To make \\(N = \\dfrac{(1 - 0)}{h}\\) an integer, we can simply choose \\(h = 0.001\\) . Extrapolation Suppose \\(a = \\min\\limits_i \\{x_i\\}\\) , \\(b = \\max\\limits_i \\{x_i\\}\\) . Interpolation estimates value \\(P(x)\\) , \\(x \\in [a, b]\\) , while Extrapolation estimates value \\(P(x)\\) , \\(x \\notin [a, b]\\) . In genernal, interpolation is better than extrapolation.","title":"Lagrange Interpolation"},{"location":"Mathematics_Basis/NA/Chap_3/#nevilles-method","text":"Motivation: When we have more interpolating points, the original Lagrange interpolating method should re-calculate all \\(L_{n, k}\\) , which is not efficient. Definition 3.0 Let \\(f\\) be a function defined at \\(x_0, x_1, \\dots, x_n\\) , and suppose that \\(m_1, \\dots, m_k\\) are \\(k\\) distinct integers with \\(0 \\le m_i \\le n\\) for each \\(i\\) . The Lagrange polynomial that agrees with \\(f(x)\\) at the \\(k\\) points denoted by \\(P_{m_1, m_2, \\dots, m_k}(x)\\) . Thus \\(P(x) = P_{0, 1, \\dots, n}(x)\\) , where \\(P(x)\\) is the n th Lagrange polynomials that interpolate \\(f\\) at \\(k + 1\\) points \\(x_0, \\dots, x_k\\) . Theorem 3.2 Let \\(f\\) be a function defined at \\(x_0, x_1, \\dots, x_n\\) , and \\(x_i \\ne x_j\\) , then \\[ P(x) = \\frac{(x - x_j)P_{0, 1, \\dots, j - 1, j + 1, \\dots k}(x) - (x - x_i)P_{0, 1, \\dots, i - 1, i + 1, \\dots k}(x)}{(x_i - x_j)}. \\] Denote that \\[ Q_{i, j} = P_{i - j, i - j + 1, \\dots, i - 1, i}, \\] then from Theorem 3.2 , the interpolating polynomials can be generated recursively .","title":"Neville's Method"},{"location":"Mathematics_Basis/NA/Chap_3/#newton-interpolation","text":"Differing from Langrange polynomials, we try to represent \\(P(x)\\) by the following form: \\[ \\begin{aligned} N(x) = &\\ a_0 + a_1(x - x_0) + a_2(x - x_0)(x - x_1) + \\cdots \\\\ & + a_n(x - x_0)(x - x_1) \\cdots (x - x_{n - 1}). \\end{aligned} \\] Definition 3.1 \\(f[x_i] = f(x_i)\\) is the zeroth divided difference w.r.t. \\(x_i\\) . The k th divided difference w.r.t. \\(x_i, x_{i + 1}, x_{i + k}\\) is defined recursively by \\[ \\small f[x_i, x_{i + 1}, \\dots, x_{i + k - 1}, x_{i + k}] = \\frac{f[x_{i + 1}, x_{i + 2}, \\dots, x_{i + k}] - f[x_i, x_{i + 1}, \\dots, x_{i + k - 1}]}{x_{i + k} - x_i}. \\] Then we derive the Newton's interpolatory divided-difference formula \\[ N(x) = f[x_0] + \\sum\\limits_{k = 1}^n f[x_0, x_1, \\dots, x_k]\\prod\\limits_{i = 0}^{k - 1}(x - x_i). \\] And the divided difference can be generated as below, which is similar to Neville's Method. Theorem 3.3 If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers in the interval \\([a, b]\\) and \\(f \\in C^{n + 1}[a, b]\\) . \\(\\forall x \\in [a, b], \\exists \\xi \\in [a, b], s.t.\\) \\[ f(x) = N(x) + f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i). \\] where \\(N(x)\\) is the Newton's interpolatory divided-difference formula. And \\[ R(x) = f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i). \\] is the truncation error . Proof By definition of divided difference, we have \\[ \\left\\{ \\begin{aligned} f(x) &= f[x_0] + (x - x_0)f[x, x_0], & \\textbf{Eq.1} \\\\ f[x, x_0] &= f[x_0, x_1] + (x - x_1)f[x, x_0, x_1], & \\textbf{Eq.2} \\\\ & \\ \\ \\vdots \\\\ f[x, x_0, \\dots, x_{n - 1}] &= f[x_0, \\dots, x_n] + (x - x_n)f[x, x_0, \\dots, x_n]. & \\textbf{Eq.n - 1} \\end{aligned} \\right. \\] then compute \\[ \\textbf{Eq.1} + (x - x_0) \\times \\textbf{Eq.2} + \\cdots + (x - x_0) \\cdots (x - x_{n - 1}) \\times \\textbf{Eq.n - 1}. \\] i.e. \\[ f(x) = N(x) + \\underbrace{f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i)}_{R(x)}. \\] Note Since the uniqueness of n -th interpolating polynomial, \\(N(x) \\equiv P(x)\\) . They have the same truncation error, which is \\[ f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] Theorem 3.4 Suppose that \\(f \\in C^n[a, b]\\) and \\(x_0, x_1, \\dots, x_n\\) are distinct numbers in \\([a, b]\\) . Then \\(\\exists\\ \\xi \\in (a, b)\\) , s.t. \\[ f[x_0, x_1, \\dots, x_n] = \\frac{f^{(n)}(\\xi)}{n!}. \\]","title":"Newton Interpolation"},{"location":"Mathematics_Basis/NA/Chap_3/#special-case-equal-spacing","text":"Definition 3.2 Forward Difference \\[ \\begin{aligned} \\Delta f_i &= f_{i + 1} - f_i, \\\\ \\Delta^k f_i &= \\Delta\\left(\\Delta^{k - 1} f_i \\right) = \\Delta^{k - 1}f_{i + 1} - \\Delta^{k - 1}f_i. \\end{aligned} \\] Backward Difference \\[ \\begin{aligned} \\nabla f_i &= f_i - f_{i - 1}, \\\\ \\nabla^k f_i &= \\nabla\\left(\\nabla^{k - 1} f_i \\right) = \\nabla^{k - 1}f_i - \\nabla^{k - 1}f_{i - 1}. \\end{aligned} \\] Property of Forward / Backward Difference Linearity \\(\\Delta (af(x) + bg(x)) = a \\Delta f + b \\Delta g\\) . If \\(\\text{deg}(f(x)) = m\\) , then \\[ \\text{deg}\\left(\\Delta^kf(x)\\right) = \\left\\{ \\begin{aligned} & m - k, & 0 \\le k \\le m, \\\\ & 0, & k > m. \\end{aligned} \\right. \\] Decompose the recursive definition, \\[ \\begin{aligned} \\Delta^n f_k &= \\sum\\limits_{j = 0}^n (-1)^j \\binom{n}{j} f_{n + k - j}, \\\\ \\nabla^n f_k &= \\sum\\limits_{j = 0}^n (-1)^{n - j} \\binom{n}{j} f_{j + k - n}. \\end{aligned} \\] Vice versa, \\[ f_{n + k} = \\sum\\limits_{j = 0}^n \\binom{n}{j} \\Delta^j f_k. \\] Suppose \\(x_0, x_1, \\dots x_n\\) are equally spaced, namely \\(x_i = x_0 + ih\\) . And let \\(x = x_0 + sh\\) , then \\(x - x_i = (s - i)h\\) . Thus \\[ \\begin{aligned} N(x) &= f[x_0] + \\sum\\limits_{k = 1}^n s(s - 1) \\cdots (s - k + 1) h^k f[x_0, x_1, \\dots, x_k] \\\\ &= f[x_0] + \\sum\\limits_{k = 1}^n \\binom{s}{k} k! h^k f[x_0, x_1, \\dots, x_k] \\end{aligned} \\] is called Newton forward divided-difference formula . From mathematical induction, we can derive that \\[ f[x_0, x_1, \\dots, x_k] = \\frac{1}{k!h^k}\\Delta^k f(x_0). \\] Thus we get the Newton Forward-Difference Formula \\[ N(x) = f[x_0] + \\sum\\limits_{k = 1}^n \\binom{s}{k} \\Delta^k f(x_0). \\] Inversely, \\(x = x_n + sh\\) , then \\(x - x_i = (s + n - i)h\\) , Thus \\[ \\begin{aligned} N(x) &= f[x_n] + \\sum\\limits_{k = 1}^n s(s + 1) \\cdots (s + k - 1) h^k f[x_n, x_{n - 1}, \\dots, x_{n - k}] \\\\ &= f[x_n] + \\sum\\limits_{k = 1}^n (-1)^k \\binom{-s}{k} k! h^k f[x_n, x_{n - 1}, \\dots, x_{n - k}]. \\end{aligned} \\] is called Newton backward divided-difference formula . From mathematical induction, we can derive that \\[ f[x_n, x_{n - 1}, \\dots, x_0] = \\frac{1}{k!h^k}\\nabla^k f(x_n). \\] Thus we get the Newton Backward-Difference Formula \\[ N(x) = f[x_n] + \\sum\\limits_{k = 1}^n (-1)^k \\binom{-s}{k} \\nabla^k f(x_n). \\]","title":"Special Case: Equal Spacing"},{"location":"Mathematics_Basis/NA/Chap_3/#hermit-interpolation","text":"Definition 3.3 Let \\(x_0, x_1, \\dots, x_n\\) be \\(n + 1\\) distinct numbers in \\([a, b]\\) and \\(m_i \\in \\mathbb{N}\\) . Suppose that \\(f \\in C^m[a, b]\\) , where \\(m = \\max \\{m_i\\}\\) . The osculating polynomial approximating \\(f\\) is the polynomial \\(P(x)\\) of least degree such that \\[ \\frac{d^k P(x_i)}{dx^k} = \\frac{d^k f(x_i)}{dx^k}, \\text{ for each } i = 0, 1, \\dots, n \\text{ and } k = 0, 1, \\dots, m_i. \\] From definition above, we know that when \\(m_i = 0\\) for each \\(i\\) , it's the n -th Lagrange polynomial. And the cases that \\(m_i = 1\\) for each \\(i\\) , then it's Hermit Polynomials . Theorem 3.5 If \\(f \\in C^1[a, b]\\) and \\(x_0, \\dots, x_n \\in [a, b]\\) are distinct, the unique polynomial of least degree agreeing with \\(f\\) and \\(f'\\) at \\(x_0, \\dots, x_n\\) is the Hermit Polynomial of degree at most \\(\\bf 2n + 1\\) defined by \\[ H_{2n + 1}(x) = \\sum\\limits_{j = 0}^n f(x_j) H_{n ,j}(x) + \\sum\\limits_{j = 0}^n f'(x_j) \\hat H_{n, j}(x), \\] where \\[ H_{n, j}(x) = [1 - 2(x - x_j)L'_{n, j}(x_j)]L^2_{n, j}(x), \\] and \\[ \\hat H_{n, j}(x) = (x - x_j)L^2_{n, j}(x). \\] Moreover, if \\(f \\in C^{2n + 2}[a, b]\\) , then \\(\\exists\\ \\xi \\in (a, b)\\) , s.t. \\[ f(x) = H_{2n + 1}(x) + \\underbrace{\\frac{1}{(2n + 2)!}\\prod\\limits_{i = 0}^n (x - x_i)^2 f^{2n + 2}(\\xi)}_{R(x)}. \\] The theorem above gives a complete description of Hermit interpolation. But in pratice, to compute \\(H_{2n + 1}(x)\\) through the formula above is tedious . To make it compute easier, we introduce a method that is similar to Newton's interpolation. Define the sequence \\(\\{z_k\\}_{0}^{2n + 1}\\) by \\[ z_{2i} = z_{2i + 1} = x_i. \\] Based on the Theorem 3.4 , we redefine that \\[ f[z_{2i}, z_{2i + 1}] = f'(z_{2i}) = f'(x_i). \\] Then Hermite polynomial can be represented by \\[ H_{2n + 1}(x) = f[z_0] + \\sum\\limits_{k = 1}^{2n + 1} f[z_0, \\dots, z_k]\\prod\\limits_{i = 0}^{k - 1}(x - z_i). \\]","title":"Hermit Interpolation"},{"location":"Mathematics_Basis/NA/Chap_3/#cubic-spline-interpolation","text":"Motivation: For osculating polynomial approximation, we can let \\(m_i\\) be bigger to get high-degree polynomials. It can somehow be better but higher degree tends to causes a fluctuation or say overfitting . An alternative approach is to divide the interval into subintervals and approximate them respectively, which is called piecewise-polynomial approximation . The most common piecewise-polynomial approximation uses cubic polynomials called cubic spline approximation . Definition 3.4 Given a function \\(f\\) defined on \\([a, b]\\) and a set of nodes \\(a = x_0 < x_1 < \\cdots < x_n = b\\) , a cubic spline interpolant \\(S\\) for \\(f\\) is a function that satisfies the following conditions. \\(S(x)\\) is a cubic polynomial, denoted \\(S_j(x)\\) , on the subinterval \\([x_j, x_j + 1]\\) , for each \\(j = 0, 1, \\dots, n - 1\\) ; \\(S(x_j) = f(x_j)\\) for each \\(j = 0, 1, \\dots, n\\) ; \\(S_{j + 1}(x_{j + 1}) = S_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\) ; \\(S'_{j + 1}(x_{j + 1}) = S'_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\) ; \\(S''_{j + 1}(x_{j + 1}) = S''_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\) ; One of the following sets of boundary conditions: \\(S''(x_0) = S''(x_n) = 0\\) ( free or natural boundary ); \\(S'(x_0) = f'(x_0)\\) and \\(S'(x_n) = f'(x_n)\\) ( clamped boundary ). The spline of natural boundary is called natural spline . Theorem 3.6 The cubic spline interpolation of either natural boundary or clamped boundary is unique . (Since the coefficient matrix \\(A\\) is strictly diagonally dominant.) Suppose interpolation function in each subinterval is \\[ S_j(x) = a_j + b_j(x - x_j) + c_j(x - x_j)^2 + d_j(x - x_j)^3. \\] From the conditions in the definition above, by some algebraic process, we can derive the solution with the following equations, \\[ \\begin{aligned} h_j &= x_{j + 1} - x_j; \\\\ a_j &= f(x_j); \\\\ b_j &= \\frac{1}{h_j}(a_{j + 1}) - \\frac{h_j}{3}(2c_j + c_{j + 1}); \\\\ d_j &= \\frac{1}{3h_j}{c_{j + 1} - c_j}. \\end{aligned} \\] While \\(c_j\\) is given by solving the following linear system, \\[ A\\mathbf{x} = \\mathbf{b}, \\] where \\[ \\small A = \\begin{bmatrix} 1 & 0 & 0 & \\cdots & \\cdots & 0 \\\\ h_0 & 2(h_0 + h_1) & h_1 & \\cdots & \\cdots & \\vdots \\\\ 0 & h_1 & 2(h_1 + h_2) & h_2 & \\ddots & \\vdots \\\\ \\vdots & \\ddots & \\ddots & \\ddots & \\ddots & 0 \\\\ \\vdots & \\ddots & \\ddots & h_{n - 2} & 2(h_{n - 2} + h_{n - 1}) & h_{n - 1} \\\\ 0 & \\cdots & \\cdots & 0 & 0 & 1 \\\\ \\end{bmatrix} , \\mathbf{x} = \\begin{bmatrix} c_0 \\\\ c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\] For natural boundary, \\[ \\small \\mathbf{b} = \\begin{bmatrix} 0 \\\\ \\frac{3}{h_1}(a_2 - a_1) - \\frac{3}{h_0}(a_1 - a_0) \\\\ \\vdots \\\\ \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) - \\frac{3}{h_{n - 2}}\\ (a_{n - 1} - a_{n - 2}) \\\\ 0 \\end{bmatrix}. \\] For clamped boundary, \\[ \\small \\mathbf{b} = \\begin{bmatrix} \\frac{3}{h_0}(a_1 - a_0) - 3f'(a) \\\\ \\frac{3}{h_1}(a_2 - a_1) - \\frac{3}{h_0}(a_1 - a_0) \\\\ \\vdots \\\\ \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) - \\frac{3}{h_{n - 2}}\\ (a_{n - 1} - a_{n - 2}) \\\\ 3f'(b) - \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) \\\\ \\end{bmatrix}. \\] For More Accuracy... If \\(f \\in C[a, b]\\) and \\(\\frac{\\max h_i}{\\min h_i} \\le C < \\infty\\) . Then \\(S(x)\\ \\overset{\\text{uniform}}{\\longrightarrow}\\ f(x)\\) when \\(\\max h_i \\rightarrow 0\\) . That is the accuracy of approximation can be improved by adding more nodes without increasing the degree of the splines.","title":"Cubic Spline Interpolation"},{"location":"Mathematics_Basis/NA/Chap_3/#curves","text":"We've discussed the interpolation of functions above, but we may encounter the case to interpolate a curve.","title":"Curves"},{"location":"Mathematics_Basis/NA/Chap_3/#straightforward-technique","text":"For given points \\((x_0, y_0), (x_1, y_1), \\dots, (x_n, y_n)\\) , we can construct two approximation functions with \\[ x_i = x(t_i),\\ y_i = y(t_i). \\] The interpolation method can be Lagrange, Hermite and Cubic spline, whatever.","title":"Straightforward Technique"},{"location":"Mathematics_Basis/NA/Chap_3/#bezier-curve","text":"In nature, it's piecewise cubic Hermite polynomial , and the curve is called Bezier curve . Similarly, suppose two function \\(x(t)\\) and \\(y(t)\\) at each interval. We have the following condtions. \\[ x(0) = x_0,\\ x(1) = x_1,\\ x'(0) = \\alpha_0,\\ x'(1) = \\alpha_1. \\] \\[ y(0) = y_0,\\ y(1) = y_1,\\ y'(0) = \\beta_0,\\ y'(1) = \\beta_1. \\] The solution is \\[ \\begin{aligned} x(t) &= [2(x_0 - x_1) + (\\alpha_0 + \\alpha_1)]t^3 + [3(x_1 - x_0) - (\\alpha_1 + 2\\alpha_0)]t^2 + \\alpha_0 t + x_0. \\\\ y(t) &= [2(y_0 - y_1) + (\\beta_0 + \\beta_1)]t^3 + [3(y_1 - y_0) - (\\beta_1 + 2\\beta_0)]t^2 + \\alpha_0 t + y_0. \\end{aligned} \\]","title":"Bezier Curve"},{"location":"Mathematics_Basis/NA/Chap_4/","text":"Chapter 4 | Numerical Differentiation and Integration \u00b6 Before everything start, it's necessary to introduce an approach to reduce truncation error: Richardson's extrapolation . Richardson's Extrapolation \u5916\u63a8 \u00b6 Suppose for each \\(h\\) , we have a formula \\(N(h)\\) to approximate an unknown value \\(M\\) . And suppose the truncation error have the form \\[ M - N(h) = K_1 h + K_2 h^2 + K_3 h^3 + \\cdots, \\] for some unknown constants \\(K_1\\) , \\(K_2\\) , \\(K_3\\) , \\(\\dots\\) . This has \\(O(h)\\) approximation. First, we try to make some transformation to reduce the \\(K_1 h\\) term. \\[ \\begin{aligned} M &= N(h) + K_1 h + K_2 h^2 + K_3 h^3 + \\cdots, \\\\ M &= N\\left(\\frac{h}{2}\\right) + K_1 \\frac{h}{2} + K_2 \\frac{h^2}{4} + K_3 \\frac{h^3}{8} + \\cdots, \\end{aligned} \\] Eliminate \\(h\\) , and we have \\[ \\small M = \\left[N\\left(\\frac{h}{2}\\right) + \\left(N\\left(\\frac{h}{2}\\right) - N(h)\\right)\\right] + K_2\\left(\\frac{h^2}{2} - h^2\\right) + K_3 \\left(\\frac{h^3}{4} - h^3\\right) + \\cdots \\] Define \\(N_1(h) \\equiv N(h)\\) and \\[ N_2(h) = N_1 \\left(\\frac{h}{2}\\right) + \\left(N_1\\left(\\frac{h}{2}\\right) - N_1(h)\\right). \\] Then we have \\(O(h^2)\\) approximation formula for \\(M\\) : \\[ M = N_2(h) - \\frac{K_2}{2}h^2 - \\frac{3K_3}{4}h^3 - \\cdots. \\] Repeat this process by eliminating \\(h^2\\) , and then we have \\[ M = N_3(h) + \\frac{K_3}{8}h^3 + \\cdots, \\] where \\[ N_3(h) = N_2 \\left(\\frac{h}{2}\\right) + \\frac{1}{3}\\left(N_2\\left(\\frac{h}{2}\\right) - N_2(h)\\right). \\] We can repeat this process recursively, and finally we get the following conclusion. Richardson's Extrapolation If \\(M\\) is in the form \\[ M = N(h) + \\sum\\limits_{j = 1}^{m - 1}K_jh^j + O(h^m), \\] then for each \\(j = 2, 3, \\dots, m\\) , we have an \\(O(h^j)\\) approximation of the form \\[ N_j(h) = N_{j - 1}\\left(\\frac{h}{2}\\right) + \\frac{1}{2^{j - 1} - 1}\\left(N_{j - 1}\\left(\\frac{h}{2}\\right) - N_{j - 1}(h)\\right). \\] Also it can be used if the truncation error has the form \\[ \\sum\\limits_{j = 1}^{m - 1}K_jh^{\\alpha_j} + O(h^{\\alpha_m}). \\] Numerical Differentiation \u00b6 First Order Differentiation \u00b6 Suppose \\(\\{x_0, x_1, \\dots, x_n\\}\\) are distinct in some interval \\(I\\) and \\(f \\in C^{n + 1}(I)\\) , then we have the Lagrange interpolating polynomials, \\[ f(x) = \\sum\\limits_{k = 0}^{n}f(x_k)L_k(x) + \\frac{f^{(n + 1)}(\\xi(x))}{(n + 1)!}\\prod\\limits_{i = 0}^{n}(x - x_i). \\] Differentiate \\(f(x)\\) and substitute \\(x_j\\) to it, \\[ f'(x_j) = \\sum\\limits_{k = 0}^{n}f(x_k)L_k'(x) + \\frac{f^{(n + 1)}(\\xi(x_j))}{(n + 1)!} \\prod\\limits_{\\substack{k = 0 \\\\ k \\ne j}}^n(x_j - x_k), \\] which is called an (n + 1)-point formula to approximate \\(f'(x_j)\\) . For convenience, we only discuss the equally spaced situation. Suppose the interval is \\([a, b]\\) divided to \\(n\\) parts, and denote \\[ \\begin{aligned} h = \\frac{b - a}{n}, \\\\ x_i = a + ih. \\end{aligned} \\] When \\(n = 1\\) , we simply get the two-point formula, \\[ f'(x_0) = \\frac{f(x_0 + h) - f(x_0)}{h} - \\frac{h}{2}f''(\\xi), \\] which is known as forward-difference formula . Inversely, by replacing \\(h\\) with \\(-h\\) , \\[ f'(x_0) = \\frac{f(x_0) - f(x_0 - h)}{h} + \\frac{h}{2}f''(\\xi), \\] is known as backward-difference formula . When \\(n = 3\\) , we get the three-point formulae . Due to symmetry, there are only two. \\[ \\begin{aligned} f'(x_0) &= \\frac{1}{2h}[-3(f(x_0)) + 4f(x_0 + h) - f(x_0 + 2h)] + \\frac{h^2}{3}f^{(3)}(\\xi), \\\\ & \\text{where some } \\xi \\in [x_0, x_0 + 2h], \\\\ f'(x_0) &= \\frac{1}{2h}[f(x_0 + h) + f(x_0 - h)] - \\frac{h^2}{6}f^{(3)}(\\xi), \\\\ & \\text{where some } \\xi \\in [x_0 - h, x_0 + h]. \\end{aligned} \\] When \\(n = 5\\) , we get the five-point formulae . The following are the useful two of them. \\[ \\begin{aligned} f'(x_0) &= \\frac{1}{12h}[-25f(x_0) + 48f(x_0 + h) - 36f(x_0 + 2h) \\\\ & \\ \\ \\ \\ + 16f(x_0 + 3h) - 3f(x_0 + 4h)] + \\frac{h^4}{5}f^{(5)}(\\xi), \\\\ & \\text{where some } \\xi \\in [x_0, x_0 + 4h], \\\\ f'(x_0) &= \\frac{1}{12h}[f(x_0 - 2h) - 8f(x_0 - h)] + 8f(x_0 + h) - f(x_0 + 2h)] \\\\ & \\ \\ \\ \\ + \\frac{h^4}{30}f^{(5)}(\\xi), \\\\ & \\text{where some } \\xi \\in [x_0 - 2h, x_0 + 2h]. \\end{aligned} \\] Differentiation with Richardson's Extrapolation Consider the three-point formula, \\[ f'(x_0) = \\frac{1}{2h}[f(x_0 + h) + f(x_0 - h)] - \\frac{h^2}{6}f^{(3)}(\\xi) - \\frac{h^4}{120}f^{(5)}(\\xi) - \\cdots. \\] In this case, considering Richardson's extrapolation, we have \\[ N_1(h) = \\frac{1}{2h}[f(x_0 + h) + f(x_0 - h)]. \\] Eliminate \\(h^2\\) , we have \\[ f'(x_0) = N_2(h) + \\frac{h^4}{480}f^{(5)}(x_0) + \\cdots, \\] where \\[ N_2(h) = N_1 \\left(\\frac{h}{2}\\right) + \\frac{1}{3}\\left(N_1\\left(\\frac{h}{2}\\right) - N_1(h)\\right). \\] Continuing this procedure, we have, \\[ N_j(h) = N_{j - 1}\\left(\\frac{h}{2}\\right) + \\frac{1}{4^{j - 1} - 1}\\left(N_{j - 1}\\left(\\frac{h}{2}\\right) - N_{j - 1}(h)\\right). \\] Example Suppose \\(x_0 = 2.0\\) , \\(h = 0.2\\) , \\(f(x) = xe^x\\) , the exact value of \\(f'(x_0)\\) is 22.167168. While the extrapolation process is shown below. Higher Differentiation \u00b6 Take second order differentiation as an example. From Taylor polynomial about point \\(x_0\\) , we have \\[ \\begin{aligned} f(x_0 + h) &= f(x_0) + f'(x_0)h + \\frac{1}{2}f''(x_0)h^2 + \\frac{1}{6}f'''(x_0)h^3 + \\frac{1}{24}f^{(4)}(\\xi_1)h^4, \\\\ f(x_0 - h) &= f(x_0) - f'(x_0)h + \\frac{1}{2}f''(x_0)h^2 - \\frac{1}{6}f'''(x_0)h^3 + \\frac{1}{24}f^{(4)}(\\xi_{-1})h^4, \\\\ \\end{aligned} \\] where \\(x_0 - h < \\xi_{-1} < x_0 < \\xi_1 < x_0 + h\\) . Add these equations and take some transformations, we get \\[ f''(x_0) = \\frac{1}{h^2}[f(x_0 - h) - 2f(x_0) + f(x_0 + h)] - \\frac{h^2}{12}f^{(4)}(\\xi), \\] where \\(x_0 - h < \\xi < x_0 + h\\) , and \\(f^{(4)}(\\xi) = \\dfrac{1}{2}(f^{(4)}(\\xi_1) + f^{(4)}(\\xi_{-1}))\\) . Error Analysis \u00b6 Now we examine the formula below, \\[ f'(x_0) = \\frac{1}{2h}[f(x_0 + h) + f(x_0 - h)] - \\frac{h^2}{6}f^{(3)}(\\xi). \\] Suppose that in evaluation of \\(f(x_0 + h)\\) and \\(f(x_0 - h)\\) , we encounter roundoff errors \\(e(x_0 + h)\\) and \\(e(x_0 - h)\\) . Then our computed values \\(\\tilde f(x_0 + h)\\) and \\(\\tilde f(x_0 - h)\\) satisfy the following formulae, \\[ \\begin{aligned} f(x_0 + h) &= \\tilde f(x_0 + h) + e(x_0 + h), \\\\ f(x_0 - h) &= \\tilde f(x_0 - h) + e(x_0 - h). \\end{aligned} \\] Thus the total error is \\[ R = f'(x_0) - \\frac{\\tilde f(x_0 + h) - \\tilde f(x_0 - h)}{2h} = \\frac{e(x_0 + h) - e(x_0 - h)}{2h} - \\frac{h^2}{6}f^{(3)}(\\xi). \\] Moreover, suppose the roundoff error \\(e\\) are bounded by some number \\(\\varepsilon > 0\\) and \\(f^{(3)}(x)\\) is bounded by some number \\(M > 0\\) , then \\[ |R| \\le \\frac{\\varepsilon}{h} + \\frac{h^2}{6}M. \\] Thus theoretically the best choice of \\(h\\) is \\(\\sqrt[3]{3\\varepsilon / M}\\) . But in reality we cannot compute such an \\(h\\) since we know nothing about \\(f^{(3)}(x)\\) . In all, we should aware that the step size \\(h\\) cannot be too large or too small . Numerical Integration \u00b6 Numerical Quadrature \u00b6 Similarly to the differentiation case, suppose \\(\\{x_0, x_1, \\dots, x_n\\}\\) are distinct in some interval \\(I\\) and \\(f \\in C^{n + 1}(I)\\) , then we have the Lagrange interpolating polynomials, \\[ f(x) = \\sum\\limits_{k = 0}^{n}f(x_k)L_k(x) + \\frac{f^{(n + 1)}(\\xi(x))}{(n + 1)!}\\prod\\limits_{i = 0}^{n}(x - x_i). \\] Integrate \\(f(x)\\) and we get \\[ \\begin{aligned} \\int_a^b f(x) dx &= \\int_a^b \\sum\\limits_{k = 0}^{n}f(x_k)L_k'(x) dx + \\int_a^b \\frac{f^{(n + 1)}(\\xi(x))}{(n + 1)!} \\prod\\limits_{k = 0}^n (x - x_k)dx, \\\\ &= \\sum\\limits_{i = 0}^n A_i f(x_i) + \\underbrace{\\frac{1}{(n + 1)!} \\int_a^b \\prod_{i = 0}^n(x - x_i)f^{(n + 1)}(\\xi(x))dx}_{E(f)}, \\end{aligned} \\] where \\[ A_i = \\int_a^b L_i(x)dx. \\] Definition 4.0 The degree of accuracy , or say precision , of a quadrature formula is the largest positive integer \\(n\\) such that the formula is exact for \\(x^k\\) , for each \\(k = 0, 1, \\dots, n\\) . Similarly, we suppose equally spaced situation here again. Theorem 4.0 | (n + 1)-point closed Newton-Cotes formulae Suppose \\(x_0 = a\\) , \\(x_n = b\\) , and \\(h = (b - a) / n\\) , then \\(\\exists\\ \\xi \\in (a, b)\\) , s.t. \\[ \\int_a^b f(x)dx = \\sum\\limits_{i = 0}^n A_i f(x_i) + \\frac{h^{n + 3}f^{(n + 2)}(\\xi)}{(n + 2)!}\\int_0^n t^2(t - 1)\\cdots(t - n)dt, \\] if \\(n\\) is even and \\(f \\in C^{n + 2}[a, b]\\) , and \\[ \\int_a^b f(x)dx = \\sum\\limits_{i = 0}^n A_i f(x_i) + \\frac{h^{n + 2}f^{(n + 1)}(\\xi)}{(n + 1)!}\\int_0^n t(t - 1)\\cdots(t - n)dt, \\] if \\(n\\) is odd and \\(f \\in C^{n + 1}[a, b]\\) , where \\[ A_i = \\int_a^b L_i(x) dx = \\int_a^b \\prod\\limits_{\\substack{j = 0 \\\\ j \\ne i}}^n \\frac{(x - x_j)}{(x_i - x_j)} dx. \\] (n + 1)-point closed Newton-Cotes formulae \\(n = 1\\) Trapezoidal rule \\[ \\int_{x_0}^{x_1}f(x)dx = \\frac{h}{2}[f(x_0) + f(x_1)] - \\frac{h^3}{12}f''(\\xi) ,\\ \\ \\text{ where some } x_0 < \\xi < x_1. \\] \\(n = 2\\) Simpson's rule \\[ \\int_{x_0}^{x_2}f(x)dx = \\frac{h}{3}[f(x_0) + 4f(x_1) + f(x_2)] - \\frac{h^5}{90}f^{(4)}(\\xi) ,\\ \\ \\text{ where some } x_0 < \\xi < x_2. \\] \\(n = 3\\) Simpson's Three-Eighths rule \\[ \\int_{x_0}^{x_3}f(x)dx = \\frac{3h}{8}[f(x_0) + 3f(x_1) + 3f(x_2) + f(x_3)] - \\frac{3h^5}{80}f^{(4)}(\\xi), \\\\ \\text{where some } x_0 < \\xi < x_3. \\] Trapezoidal rule Simpson's rule Theorem 4.1 | (n + 1)-point open Newton-Cotes formulae Suppose \\(x_{-1} = a\\) , \\(x_{n + 1} = b\\) , and \\(h = (b - a) / (n + 2)\\) , then \\(\\exists\\ \\xi \\in (a, b)\\) , s.t. \\[ \\int_a^b f(x)dx = \\sum\\limits_{i = 0}^n A_i f(x_i) + \\frac{h^{n + 3}f^{(n + 2)}(\\xi)}{(n + 2)!}\\int_{-1}^{n + 1} t^2(t - 1)\\cdots(t - n)dt, \\] if \\(n\\) is even and \\(f \\in C^{n + 2}[a, b]\\) , and \\[ \\int_a^b f(x)dx = \\sum\\limits_{i = 0}^n A_i f(x_i) + \\frac{h^{n + 2}f^{(n + 1)}(\\xi)}{(n + 1)!}\\int_{-1}^{n + 1} t(t - 1)\\cdots(t - n)dt, \\] if \\(n\\) is odd and \\(f \\in C^{n + 1}[a, b]\\) , where \\[ A_i = \\int_a^b L_i(x) dx = \\int_a^b \\prod\\limits_{\\substack{j = 0 \\\\ j \\ne i}}^n \\frac{(x - x_j)}{(x_i - x_j)} dx. \\] (n + 1)-point open Newton-Cotes formulae \\(n = 0\\) Midpoint rule \\[ \\int_{x_{-1}}^{x_1}f(x)dx = 2hf(x_0) + \\frac{h^2}{3}f''(\\xi) \\text{where some } x_{-1} < \\xi < x_1. \\] Composite Numerical Integration \u00b6 Motivation: Although the Newton-Cotes gives a better and better approximation as \\(n\\) increases, since it's based on interpolating polynomials, it also owns the oscillatory nature of high-degree polynomials. Similarly, we discuss a piecewise approach to numerical integration with low-order Newton-Cotes formulae. These are the techniques most often applied . Theorem 4.2 | Composite Trapezoidal rule \\(f \\in C^2[a, b]\\) , \\(h = (b - a) /n\\) , and \\(x_j = a + jh\\) , \\(j = 0, 1, \\dots, n\\) . Then \\(\\exists\\ \\mu \\in (a, b)\\) , s.t. \\[ \\int_a^b f(x)dx = \\frac{h}{2}\\left[f(a) + 2 \\sum\\limits_{j = 1}^{n - 1}f(x_j) + f(b)\\right] - \\frac{b - a}{12}h^2 f''(\\mu). \\] Theorem 4.3 | Composite Simpson's rule \\(f \\in C^2[a, b]\\) , \\(h = (b - a) /n\\) , and \\(x_j = a + jh\\) , \\(j = 0, 1, \\dots, n\\) . Then \\(\\exists\\ \\mu \\in (a, b)\\) , s.t. \\[ \\small \\int_a^b f(x)dx = \\frac{h}{3}\\left[f(a) + 2 \\sum\\limits_{j = 1}^{(n / 2) - 1}f(x_{2j}) + 4 \\sum\\limits_{j = 1}^{n / 2}f(x_{2j - 1}) + f(b)\\right] - \\frac{b - a}{180}h^4 f^{(4)}(\\mu). \\] Theorem 4.4 | Composite Midpoint rule \\(f \\in C^2[a, b]\\) , \\(h = (b - a) / (n + 2)\\) , and \\(x_j = a + (j + 1)h\\) , \\(j = -1, 0, \\dots, n + 1\\) . Then \\(\\exists\\ \\mu \\in (a, b)\\) , s.t. \\[ \\int_a^b f(x)dx = 2h \\sum\\limits_{j = 0}^{n / 2}f(x_{2j}) + \\frac{b - a}{6} h^2 f''(\\mu). \\] Stability Composite integration techniques are all stable w.r.t roundoff error. Example: Composite Simpson's rule Suppose \\(f(x_i)\\) is apporximated by \\(\\tilde f(x_i)\\) with \\[ f(x_i) = \\tilde f(x_i) + e_i. \\] Then the accumulated error is \\[ e(h) = \\left|\\frac{h}{3}\\left[e_0 + 2\\sum\\limits_{j = 1}^{(n / 2) - 1}e_{2j} + 4\\sum\\limits_{j = 1}^{n / 2}e_{2j-1} + e_n\\right]\\right|. \\] Suppose \\(e_i\\) are uniformly bounded by \\(\\varepsilon\\) , then \\[ e(h) \\le \\frac{h}{3}\\left[\\varepsilon + 2 \\left(\\frac{n}{2} - 1\\right) + 4 \\frac{n}{2} \\varepsilon + \\varepsilon \\right] = nh\\varepsilon = (b - a)\\varepsilon. \\] That means even though divide an interval to more parts, the roundoff error will not increase, which is quite stable. Romberg Integration \u00b6 Romberg integration combine the Composite Trapezoidal rule and Richardson's extrapolation to derive a more useful approximation. Suppose we divide the interval \\([a, b]\\) into \\(m_1 = 1\\) , \\(m_2 = 2\\) , \\(\\dots\\) , and \\(m_n = 2^{n - 1}\\) subintervals respectively. For each division, then the step size \\(h_k\\) is \\((b - a) / m_k = (b - a) / 2^{k - 1}\\) . Then we use \\(R_{k, 1}\\) to denote the composite trapezoidal rule, \\[ \\small R_{k, 1} = \\int_a^b f(x) dx = \\frac{h_k}{2} \\left[f(a) + f(b) + 2 \\left(\\sum\\limits_{i = 1}^{2^{k - 1} - 1} f(a + ih_k)\\right)\\right] - \\frac{(b - a)}{12}h^2_kf''(\\mu_k). \\] Mathematically, we have the recursive formula, \\[ R_{k, 1} = \\frac{1}{2}\\left[R_{k - 1, 1} + h_{k - 1}\\sum\\limits_{i = 1}^{2^{k - 2}}f(a+(2i-1)h_k)\\right]. \\] Theorem 4.5 the Composite Trapezoidal rule can represented by an alternative error term in the form \\[ \\int_a^b f(x) dx - R_{k, 1} = \\sum\\limits_{i = 1}^{\\infty} K_i h^{2i}_k, \\] where \\(K_i\\) depends only on \\(f^{(2i-1)}(a)\\) and \\(f^{(2i-1)}(b)\\) . This nice theorem makes Richardson's extrapolation available to reduce the truncation error! Similar to Differentiation with Richardson's Extrapolation , we have the following formula. Romberg Integration \\[ R_{k, j} = R_{k, j - 1} + \\frac{R_{k, j - 1} - R_{k - 1, j - 1}}{4^{j - 1} - 1}, \\] with an \\(O(h^{2j}_k)\\) approximation. Adaptive Quadrature Methods \u00b6 Motivation: On the premise of equal spacing, in some cases, the left half of the interval is well approximated, and maybe we only need to subdivide the right half to approximate better. Here we introduce the Adaptive quadrature methods based on the Composite Simpson's rule. First, we want to derive, if we apply Simpson's rule in two subinterval and add them up, how much precision does it improve compared to only applying Simpson's rule just in the whole interval. From Simpson's rule, we have \\[ \\int_a^b f(x) dx = S(a, b) - \\frac{h^5}{90}f^{(4)}(\\mu), \\] where \\[ S(a, b) = \\frac{h}{3}[f(a) + 4f(a + h) + f(b)]. \\] If we divide \\([a, b]\\) into two subintervals, applying Simpson's rule respectively (namely apply Composite Simpson's rule with \\(n = 4\\) and step size \\(h / 2\\) ), we have \\[ \\int_a^b f(x) dx = S\\left(a, \\frac{a + b}{2}\\right) + S\\left(\\frac{a + b}{2}, b\\right) - \\frac{1}{16}\\left(\\frac{h^5}{90}\\right)f^{(4)}(\\tilde \\mu). \\] Moreover, assume \\(f^{(4)}(\\mu) \\approx f^{(4)}(\\tilde \\mu)\\) , then we have \\[ S\\left(a, \\frac{a + b}{2}\\right) + S\\left(\\frac{a + b}{2}, b\\right) - \\frac{1}{16}\\left(\\frac{h^5}{90}\\right)f^{(4)}(\\tilde \\mu) \\approx S(a, b) - \\frac{h^5}{90}f^{(4)}(\\mu), \\] so \\[ \\frac{h^5}{90}f^{(4)}(\\mu) \\approx \\frac{16}{15}\\left[S(a, b) - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right]. \\] Then, \\[ \\begin{aligned} \\left|\\int_a^b f(x) dx - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right| = \\left|\\frac{1}{16}\\left(\\frac{h^5}{90}\\right)f^{(4)}(\\tilde \\mu)\\right| \\\\ \\approx \\frac{1}{15} \\left|S(a, b) - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right|. \\end{aligned} \\] This result means that the subdivision approximates \\(\\int_a^b f(x) dx\\) about 15 times better than it agree with \\(S(a, b)\\) . Thus suppose we have a tolerance \\(\\varepsilon\\) across the interval \\([a, b]\\) . If \\[ \\left|S(a, b) - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right| < 15\\varepsilon, \\] then we expect to have \\[ \\left|\\int_a^bf(x)dx - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right| < \\varepsilon, \\] and the subdivision is thought to be a better approximation to \\(\\int_a^bf(x)dx\\) . Conclusion Suppose we have a tolerance \\(\\varepsilon\\) on \\([a, b]\\) , and we expect the tolerance is uniform. Thus at the subinterval \\([p, q] \\subseteq [a, b]\\) , with \\(q - p = k(b - a)\\) , we expect the tolerance as \\(k\\varepsilon\\) . Moreover, suppose the approximation of Simpson's rule on \\([p, q]\\) is \\(S\\) while the approxiamtion of Simpson's rule on \\([p, (p + q) / 2]\\) and \\([(p + q) / 2, q]\\) are \\(S_1\\) and \\(S_2\\) respectively. Then the criterion to subdivide is that \\[ |S_1 + S_2 - S| < M \\cdot k \\varepsilon. \\] where \\(M\\) is often taken as 10 but not 15 , which we derive above, since it also consider the error between \\(f^{(4)}(\\mu)\\) and \\(f^{(4)}(\\tilde \\mu)\\) . Gaussian Quadrature \u00b6 Instead of equal spacing in the Newton-Cotes formulae, the selection of the points \\(x_i\\) also become variables. Gaussian Quadrature is aimed to construct a formula \\(\\sum\\limits_{k = 0}^n A_kf(x_k)\\) to approximate \\(\\int_a^b w(x)f(x)dx\\) of precision degree \\(2n + 1\\) with \\(n + 1\\) points, where \\(w(x)\\) is a weight function. (Compare to the equally spaced strategy only have a precision of around \\(n\\) ). That means, to determine \\(x_i\\) and \\(A_i\\) (totally \\(2n + 2\\) unknowns) such that the formula is accurate for \\(f(x) = 1, x, \\dots, x^{2n + 1}\\) (totally \\(2n + 2\\) equations). The selected points \\(x_i\\) are called Gaussian points . Problem Theoretically, since we have \\(2n + 2\\) unknowns and \\(2n + 2\\) eqautions, we can solve out \\(x_i\\) and \\(A_i\\) . But the equations are not linear ! Thus we give the following theorem to find Gaussian points without solving the nonlinear eqautions. Recap the definition of weight function and orthogonality of polynomials, and the construction of the set of orthogonal polynomials. Theorem 4.6 \\(x_0, \\dots, x_n\\) are Gaussian point iff \\(W(x) = \\prod\\limits_{k = 0}^n (x - x_k)\\) is orthogonal to all the polynomials of degree no greater than \\(n\\) on interval \\([a,b]\\) w.r.t the weight function \\(w(x)\\) . Proof \\(\\Rightarrow\\) if \\(x_0, \\dots x_n\\) are Gaussian points, then the degree of precision of the formula \\(\\int_a^b w(x)f(x)dx \\approx \\sum\\limits_{k = 0}^n A_k f(x_k)\\) is at least \\(2n + 1\\) . Then \\(\\forall\\ P(x) \\in \\Pi_n\\) , \\(\\text{deg}(P(x)W(x)) \\le 2n + 1\\) . Thus \\[ \\int_a^b w(x)P(x)W(x)dx = \\sum\\limits_{k = 0}^n A_k P(x_k)\\underbrace{W(x_k)}_{0} = 0. \\] \\(\\Leftarrow\\) \\(\\forall\\ P(x) \\in \\Pi_{2n + 1}\\) , let \\(P(x) = W(x)q(x) + r(x),\\ \\ q(x), r(x) \\in \\Pi_{n}\\) , then \\[ \\begin{aligned} \\int_a^b w(x)P(x) dx &= \\int_a^b w(x)W(x)q(x)dx + \\int_a^b w(x)r(x)dx \\\\ &= \\sum\\limits_{k = 0}^n A_k r(x_k) = \\sum\\limits_{k = 0}^n A_k P(x_k). \\end{aligned} \\] Recap that the set of orthogonal polynomials \\(\\{\\varphi_0, \\dots, \\varphi_n, \\dots\\}\\) is linearly independent and \\(\\varphi_{n + 1}\\) is orthogonal to any polynomials of degree no greater than \\(n\\) . Thus we can take \\(\\varphi_{n + 1}(x)\\) to be \\(W(x)\\) and the roots of \\(\\varphi_{n + 1}(x)\\) are the Gaussian points. Genernal Solution Problem: Assume \\[ \\int_a^b w(x)f(x)dx \\approx \\sum\\limits_{i = 0}^n A_k f(x_k). \\] Step.1 Construct the set of orthogonal polynomial on the interval \\([a, b]\\) by Gram-Schimidt Process from \\(\\varphi_0(x) \\equiv 1\\) to \\(\\varphi_{n}(x)\\) . Step.2 Find the roots of \\(\\varphi_{n}(x)\\) , which are the Gaussian points \\(x_0, \\dots, x_n\\) . Step.3 Solve the linear systems of the equation given by the precision for \\(f(x) = 1, x, \\dots, x^{2n + 1}\\) , and obtain \\(A_0, \\dots, A_n\\) . Although the method above is theoretically available, but it's tedious. But we have some special solutions which have been calculated. With some transformations, we can make them to solve the general problem too. Legendre Polynomials \u00b6 A typical set of orthogonal functions is Legrendre Polynomials. Legrendre Polynomials \\[ P_k(x) = \\frac{1}{2^k k!} \\frac{d^k}{dx^k}(x^2 - 1)^k, \\] or equally defined recursively by \\[ \\begin{aligned} & P_0(x) = 1,\\ P_1(x) = x, \\\\ & P_{k + 1}(x) = \\frac{1}{k + 1}\\left((2k + 1)x P_k(x) - k P_{k - 1}(x)\\right). \\end{aligned} \\] Property \\(\\{P_n(x)\\}\\) are orthogonal on \\([-1, 1]\\) , w.r.t the weight function \\[ w(x) \\equiv 1. \\] \\[ (P_j, P_k) = \\left\\{ \\begin{aligned} & 0, && k \\ne l, \\\\ & \\frac{2}{2k + 1}, && k = l. \\end{aligned} \\right. \\] \\(P_n(x)\\) is a monic polynomial of degree \\(n\\) . \\(P_n(x)\\) is symmetric w.r.t the origin. The \\(n\\) roots of \\(P_n(x)\\) are all on \\([-1, 1]\\) . The first few of them are \\[ \\begin{aligned} P_0(x) &= 1, \\\\ P_1(x) &= x, \\\\ P_2(x) &= x^2 - \\frac13, \\\\ P_3(x) &= x^3 - \\frac35 x, \\\\ P_4(x) &= x^4 - \\frac67 x^2 + \\frac{3}{35}. \\end{aligned} \\] The following table gives the pre-calculated values. And from interval \\([-1, 1]\\) to \\([a, b]\\) , we have a linear map \\[ t = \\frac{2x - a - b}{b - a} \\Leftrightarrow x = \\frac12 [(b - a)t + a + b]. \\] Thus we have \\[ \\int_a^b f(x) dx = \\int_{-1}^1 f\\left(\\frac{(b - a)t + (b + a)}{2}\\right)\\frac{b - a}{2}dt. \\] The formula using the roots of \\(P_{n + 1}(x)\\) is called the Gauss-Legendre quadrature formula. Example Approxiamte \\(\\int_1^{1.5} e^{-x^2}dx\\) (exact value to 7 decimal places is 0.1093643). Solution. \\[ \\int_1^{1.5} e^{-x^2} = \\frac14 \\int_{-1}^1 e^{-(t + 5)^2 / 16}dt, \\] For \\(n = 2\\) , \\[ \\int_1^{1.5} e^{-x^2} \\approx \\frac14 [e^{-(0.57735 + 5)^2 / 16} + e^{-(-57735 + 5)^2 / 16}] = 0.1094003. \\] For \\(n = 3\\) , \\[ \\begin{aligned} \\int_1^{1.5} e^{-x^2} &\\approx \\frac14 [0.55556e^{-(0.77460 + 5)^2 / 16} + 0.88889e^{-(5)^2 / 16} \\\\ & \\ \\ \\ \\ + 0.55556e^{-(-0.77460 + 5)^2 / 16}] \\\\ & = 0.1093642. \\end{aligned} \\] Chebyshev Polynomials \u00b6 Also, Chebyshev polynomials are typical set of orthogonal polynomials too. We don't discuss much about it there. The formula using the roots of \\(T_{n + 1}(x)\\) is called the Gauss-Chebyshev quadrature formula.","title":"Chap 4"},{"location":"Mathematics_Basis/NA/Chap_4/#chapter-4-numerical-differentiation-and-integration","text":"Before everything start, it's necessary to introduce an approach to reduce truncation error: Richardson's extrapolation .","title":"Chapter 4 | Numerical Differentiation and Integration"},{"location":"Mathematics_Basis/NA/Chap_4/#richardsons-extrapolation","text":"Suppose for each \\(h\\) , we have a formula \\(N(h)\\) to approximate an unknown value \\(M\\) . And suppose the truncation error have the form \\[ M - N(h) = K_1 h + K_2 h^2 + K_3 h^3 + \\cdots, \\] for some unknown constants \\(K_1\\) , \\(K_2\\) , \\(K_3\\) , \\(\\dots\\) . This has \\(O(h)\\) approximation. First, we try to make some transformation to reduce the \\(K_1 h\\) term. \\[ \\begin{aligned} M &= N(h) + K_1 h + K_2 h^2 + K_3 h^3 + \\cdots, \\\\ M &= N\\left(\\frac{h}{2}\\right) + K_1 \\frac{h}{2} + K_2 \\frac{h^2}{4} + K_3 \\frac{h^3}{8} + \\cdots, \\end{aligned} \\] Eliminate \\(h\\) , and we have \\[ \\small M = \\left[N\\left(\\frac{h}{2}\\right) + \\left(N\\left(\\frac{h}{2}\\right) - N(h)\\right)\\right] + K_2\\left(\\frac{h^2}{2} - h^2\\right) + K_3 \\left(\\frac{h^3}{4} - h^3\\right) + \\cdots \\] Define \\(N_1(h) \\equiv N(h)\\) and \\[ N_2(h) = N_1 \\left(\\frac{h}{2}\\right) + \\left(N_1\\left(\\frac{h}{2}\\right) - N_1(h)\\right). \\] Then we have \\(O(h^2)\\) approximation formula for \\(M\\) : \\[ M = N_2(h) - \\frac{K_2}{2}h^2 - \\frac{3K_3}{4}h^3 - \\cdots. \\] Repeat this process by eliminating \\(h^2\\) , and then we have \\[ M = N_3(h) + \\frac{K_3}{8}h^3 + \\cdots, \\] where \\[ N_3(h) = N_2 \\left(\\frac{h}{2}\\right) + \\frac{1}{3}\\left(N_2\\left(\\frac{h}{2}\\right) - N_2(h)\\right). \\] We can repeat this process recursively, and finally we get the following conclusion. Richardson's Extrapolation If \\(M\\) is in the form \\[ M = N(h) + \\sum\\limits_{j = 1}^{m - 1}K_jh^j + O(h^m), \\] then for each \\(j = 2, 3, \\dots, m\\) , we have an \\(O(h^j)\\) approximation of the form \\[ N_j(h) = N_{j - 1}\\left(\\frac{h}{2}\\right) + \\frac{1}{2^{j - 1} - 1}\\left(N_{j - 1}\\left(\\frac{h}{2}\\right) - N_{j - 1}(h)\\right). \\] Also it can be used if the truncation error has the form \\[ \\sum\\limits_{j = 1}^{m - 1}K_jh^{\\alpha_j} + O(h^{\\alpha_m}). \\]","title":"Richardson's Extrapolation \u5916\u63a8"},{"location":"Mathematics_Basis/NA/Chap_4/#numerical-differentiation","text":"","title":"Numerical Differentiation"},{"location":"Mathematics_Basis/NA/Chap_4/#first-order-differentiation","text":"Suppose \\(\\{x_0, x_1, \\dots, x_n\\}\\) are distinct in some interval \\(I\\) and \\(f \\in C^{n + 1}(I)\\) , then we have the Lagrange interpolating polynomials, \\[ f(x) = \\sum\\limits_{k = 0}^{n}f(x_k)L_k(x) + \\frac{f^{(n + 1)}(\\xi(x))}{(n + 1)!}\\prod\\limits_{i = 0}^{n}(x - x_i). \\] Differentiate \\(f(x)\\) and substitute \\(x_j\\) to it, \\[ f'(x_j) = \\sum\\limits_{k = 0}^{n}f(x_k)L_k'(x) + \\frac{f^{(n + 1)}(\\xi(x_j))}{(n + 1)!} \\prod\\limits_{\\substack{k = 0 \\\\ k \\ne j}}^n(x_j - x_k), \\] which is called an (n + 1)-point formula to approximate \\(f'(x_j)\\) . For convenience, we only discuss the equally spaced situation. Suppose the interval is \\([a, b]\\) divided to \\(n\\) parts, and denote \\[ \\begin{aligned} h = \\frac{b - a}{n}, \\\\ x_i = a + ih. \\end{aligned} \\] When \\(n = 1\\) , we simply get the two-point formula, \\[ f'(x_0) = \\frac{f(x_0 + h) - f(x_0)}{h} - \\frac{h}{2}f''(\\xi), \\] which is known as forward-difference formula . Inversely, by replacing \\(h\\) with \\(-h\\) , \\[ f'(x_0) = \\frac{f(x_0) - f(x_0 - h)}{h} + \\frac{h}{2}f''(\\xi), \\] is known as backward-difference formula . When \\(n = 3\\) , we get the three-point formulae . Due to symmetry, there are only two. \\[ \\begin{aligned} f'(x_0) &= \\frac{1}{2h}[-3(f(x_0)) + 4f(x_0 + h) - f(x_0 + 2h)] + \\frac{h^2}{3}f^{(3)}(\\xi), \\\\ & \\text{where some } \\xi \\in [x_0, x_0 + 2h], \\\\ f'(x_0) &= \\frac{1}{2h}[f(x_0 + h) + f(x_0 - h)] - \\frac{h^2}{6}f^{(3)}(\\xi), \\\\ & \\text{where some } \\xi \\in [x_0 - h, x_0 + h]. \\end{aligned} \\] When \\(n = 5\\) , we get the five-point formulae . The following are the useful two of them. \\[ \\begin{aligned} f'(x_0) &= \\frac{1}{12h}[-25f(x_0) + 48f(x_0 + h) - 36f(x_0 + 2h) \\\\ & \\ \\ \\ \\ + 16f(x_0 + 3h) - 3f(x_0 + 4h)] + \\frac{h^4}{5}f^{(5)}(\\xi), \\\\ & \\text{where some } \\xi \\in [x_0, x_0 + 4h], \\\\ f'(x_0) &= \\frac{1}{12h}[f(x_0 - 2h) - 8f(x_0 - h)] + 8f(x_0 + h) - f(x_0 + 2h)] \\\\ & \\ \\ \\ \\ + \\frac{h^4}{30}f^{(5)}(\\xi), \\\\ & \\text{where some } \\xi \\in [x_0 - 2h, x_0 + 2h]. \\end{aligned} \\] Differentiation with Richardson's Extrapolation Consider the three-point formula, \\[ f'(x_0) = \\frac{1}{2h}[f(x_0 + h) + f(x_0 - h)] - \\frac{h^2}{6}f^{(3)}(\\xi) - \\frac{h^4}{120}f^{(5)}(\\xi) - \\cdots. \\] In this case, considering Richardson's extrapolation, we have \\[ N_1(h) = \\frac{1}{2h}[f(x_0 + h) + f(x_0 - h)]. \\] Eliminate \\(h^2\\) , we have \\[ f'(x_0) = N_2(h) + \\frac{h^4}{480}f^{(5)}(x_0) + \\cdots, \\] where \\[ N_2(h) = N_1 \\left(\\frac{h}{2}\\right) + \\frac{1}{3}\\left(N_1\\left(\\frac{h}{2}\\right) - N_1(h)\\right). \\] Continuing this procedure, we have, \\[ N_j(h) = N_{j - 1}\\left(\\frac{h}{2}\\right) + \\frac{1}{4^{j - 1} - 1}\\left(N_{j - 1}\\left(\\frac{h}{2}\\right) - N_{j - 1}(h)\\right). \\] Example Suppose \\(x_0 = 2.0\\) , \\(h = 0.2\\) , \\(f(x) = xe^x\\) , the exact value of \\(f'(x_0)\\) is 22.167168. While the extrapolation process is shown below.","title":"First Order Differentiation"},{"location":"Mathematics_Basis/NA/Chap_4/#higher-differentiation","text":"Take second order differentiation as an example. From Taylor polynomial about point \\(x_0\\) , we have \\[ \\begin{aligned} f(x_0 + h) &= f(x_0) + f'(x_0)h + \\frac{1}{2}f''(x_0)h^2 + \\frac{1}{6}f'''(x_0)h^3 + \\frac{1}{24}f^{(4)}(\\xi_1)h^4, \\\\ f(x_0 - h) &= f(x_0) - f'(x_0)h + \\frac{1}{2}f''(x_0)h^2 - \\frac{1}{6}f'''(x_0)h^3 + \\frac{1}{24}f^{(4)}(\\xi_{-1})h^4, \\\\ \\end{aligned} \\] where \\(x_0 - h < \\xi_{-1} < x_0 < \\xi_1 < x_0 + h\\) . Add these equations and take some transformations, we get \\[ f''(x_0) = \\frac{1}{h^2}[f(x_0 - h) - 2f(x_0) + f(x_0 + h)] - \\frac{h^2}{12}f^{(4)}(\\xi), \\] where \\(x_0 - h < \\xi < x_0 + h\\) , and \\(f^{(4)}(\\xi) = \\dfrac{1}{2}(f^{(4)}(\\xi_1) + f^{(4)}(\\xi_{-1}))\\) .","title":"Higher Differentiation"},{"location":"Mathematics_Basis/NA/Chap_4/#error-analysis","text":"Now we examine the formula below, \\[ f'(x_0) = \\frac{1}{2h}[f(x_0 + h) + f(x_0 - h)] - \\frac{h^2}{6}f^{(3)}(\\xi). \\] Suppose that in evaluation of \\(f(x_0 + h)\\) and \\(f(x_0 - h)\\) , we encounter roundoff errors \\(e(x_0 + h)\\) and \\(e(x_0 - h)\\) . Then our computed values \\(\\tilde f(x_0 + h)\\) and \\(\\tilde f(x_0 - h)\\) satisfy the following formulae, \\[ \\begin{aligned} f(x_0 + h) &= \\tilde f(x_0 + h) + e(x_0 + h), \\\\ f(x_0 - h) &= \\tilde f(x_0 - h) + e(x_0 - h). \\end{aligned} \\] Thus the total error is \\[ R = f'(x_0) - \\frac{\\tilde f(x_0 + h) - \\tilde f(x_0 - h)}{2h} = \\frac{e(x_0 + h) - e(x_0 - h)}{2h} - \\frac{h^2}{6}f^{(3)}(\\xi). \\] Moreover, suppose the roundoff error \\(e\\) are bounded by some number \\(\\varepsilon > 0\\) and \\(f^{(3)}(x)\\) is bounded by some number \\(M > 0\\) , then \\[ |R| \\le \\frac{\\varepsilon}{h} + \\frac{h^2}{6}M. \\] Thus theoretically the best choice of \\(h\\) is \\(\\sqrt[3]{3\\varepsilon / M}\\) . But in reality we cannot compute such an \\(h\\) since we know nothing about \\(f^{(3)}(x)\\) . In all, we should aware that the step size \\(h\\) cannot be too large or too small .","title":"Error Analysis"},{"location":"Mathematics_Basis/NA/Chap_4/#numerical-integration","text":"","title":"Numerical Integration"},{"location":"Mathematics_Basis/NA/Chap_4/#numerical-quadrature","text":"Similarly to the differentiation case, suppose \\(\\{x_0, x_1, \\dots, x_n\\}\\) are distinct in some interval \\(I\\) and \\(f \\in C^{n + 1}(I)\\) , then we have the Lagrange interpolating polynomials, \\[ f(x) = \\sum\\limits_{k = 0}^{n}f(x_k)L_k(x) + \\frac{f^{(n + 1)}(\\xi(x))}{(n + 1)!}\\prod\\limits_{i = 0}^{n}(x - x_i). \\] Integrate \\(f(x)\\) and we get \\[ \\begin{aligned} \\int_a^b f(x) dx &= \\int_a^b \\sum\\limits_{k = 0}^{n}f(x_k)L_k'(x) dx + \\int_a^b \\frac{f^{(n + 1)}(\\xi(x))}{(n + 1)!} \\prod\\limits_{k = 0}^n (x - x_k)dx, \\\\ &= \\sum\\limits_{i = 0}^n A_i f(x_i) + \\underbrace{\\frac{1}{(n + 1)!} \\int_a^b \\prod_{i = 0}^n(x - x_i)f^{(n + 1)}(\\xi(x))dx}_{E(f)}, \\end{aligned} \\] where \\[ A_i = \\int_a^b L_i(x)dx. \\] Definition 4.0 The degree of accuracy , or say precision , of a quadrature formula is the largest positive integer \\(n\\) such that the formula is exact for \\(x^k\\) , for each \\(k = 0, 1, \\dots, n\\) . Similarly, we suppose equally spaced situation here again. Theorem 4.0 | (n + 1)-point closed Newton-Cotes formulae Suppose \\(x_0 = a\\) , \\(x_n = b\\) , and \\(h = (b - a) / n\\) , then \\(\\exists\\ \\xi \\in (a, b)\\) , s.t. \\[ \\int_a^b f(x)dx = \\sum\\limits_{i = 0}^n A_i f(x_i) + \\frac{h^{n + 3}f^{(n + 2)}(\\xi)}{(n + 2)!}\\int_0^n t^2(t - 1)\\cdots(t - n)dt, \\] if \\(n\\) is even and \\(f \\in C^{n + 2}[a, b]\\) , and \\[ \\int_a^b f(x)dx = \\sum\\limits_{i = 0}^n A_i f(x_i) + \\frac{h^{n + 2}f^{(n + 1)}(\\xi)}{(n + 1)!}\\int_0^n t(t - 1)\\cdots(t - n)dt, \\] if \\(n\\) is odd and \\(f \\in C^{n + 1}[a, b]\\) , where \\[ A_i = \\int_a^b L_i(x) dx = \\int_a^b \\prod\\limits_{\\substack{j = 0 \\\\ j \\ne i}}^n \\frac{(x - x_j)}{(x_i - x_j)} dx. \\] (n + 1)-point closed Newton-Cotes formulae \\(n = 1\\) Trapezoidal rule \\[ \\int_{x_0}^{x_1}f(x)dx = \\frac{h}{2}[f(x_0) + f(x_1)] - \\frac{h^3}{12}f''(\\xi) ,\\ \\ \\text{ where some } x_0 < \\xi < x_1. \\] \\(n = 2\\) Simpson's rule \\[ \\int_{x_0}^{x_2}f(x)dx = \\frac{h}{3}[f(x_0) + 4f(x_1) + f(x_2)] - \\frac{h^5}{90}f^{(4)}(\\xi) ,\\ \\ \\text{ where some } x_0 < \\xi < x_2. \\] \\(n = 3\\) Simpson's Three-Eighths rule \\[ \\int_{x_0}^{x_3}f(x)dx = \\frac{3h}{8}[f(x_0) + 3f(x_1) + 3f(x_2) + f(x_3)] - \\frac{3h^5}{80}f^{(4)}(\\xi), \\\\ \\text{where some } x_0 < \\xi < x_3. \\] Trapezoidal rule Simpson's rule Theorem 4.1 | (n + 1)-point open Newton-Cotes formulae Suppose \\(x_{-1} = a\\) , \\(x_{n + 1} = b\\) , and \\(h = (b - a) / (n + 2)\\) , then \\(\\exists\\ \\xi \\in (a, b)\\) , s.t. \\[ \\int_a^b f(x)dx = \\sum\\limits_{i = 0}^n A_i f(x_i) + \\frac{h^{n + 3}f^{(n + 2)}(\\xi)}{(n + 2)!}\\int_{-1}^{n + 1} t^2(t - 1)\\cdots(t - n)dt, \\] if \\(n\\) is even and \\(f \\in C^{n + 2}[a, b]\\) , and \\[ \\int_a^b f(x)dx = \\sum\\limits_{i = 0}^n A_i f(x_i) + \\frac{h^{n + 2}f^{(n + 1)}(\\xi)}{(n + 1)!}\\int_{-1}^{n + 1} t(t - 1)\\cdots(t - n)dt, \\] if \\(n\\) is odd and \\(f \\in C^{n + 1}[a, b]\\) , where \\[ A_i = \\int_a^b L_i(x) dx = \\int_a^b \\prod\\limits_{\\substack{j = 0 \\\\ j \\ne i}}^n \\frac{(x - x_j)}{(x_i - x_j)} dx. \\] (n + 1)-point open Newton-Cotes formulae \\(n = 0\\) Midpoint rule \\[ \\int_{x_{-1}}^{x_1}f(x)dx = 2hf(x_0) + \\frac{h^2}{3}f''(\\xi) \\text{where some } x_{-1} < \\xi < x_1. \\]","title":"Numerical Quadrature"},{"location":"Mathematics_Basis/NA/Chap_4/#composite-numerical-integration","text":"Motivation: Although the Newton-Cotes gives a better and better approximation as \\(n\\) increases, since it's based on interpolating polynomials, it also owns the oscillatory nature of high-degree polynomials. Similarly, we discuss a piecewise approach to numerical integration with low-order Newton-Cotes formulae. These are the techniques most often applied . Theorem 4.2 | Composite Trapezoidal rule \\(f \\in C^2[a, b]\\) , \\(h = (b - a) /n\\) , and \\(x_j = a + jh\\) , \\(j = 0, 1, \\dots, n\\) . Then \\(\\exists\\ \\mu \\in (a, b)\\) , s.t. \\[ \\int_a^b f(x)dx = \\frac{h}{2}\\left[f(a) + 2 \\sum\\limits_{j = 1}^{n - 1}f(x_j) + f(b)\\right] - \\frac{b - a}{12}h^2 f''(\\mu). \\] Theorem 4.3 | Composite Simpson's rule \\(f \\in C^2[a, b]\\) , \\(h = (b - a) /n\\) , and \\(x_j = a + jh\\) , \\(j = 0, 1, \\dots, n\\) . Then \\(\\exists\\ \\mu \\in (a, b)\\) , s.t. \\[ \\small \\int_a^b f(x)dx = \\frac{h}{3}\\left[f(a) + 2 \\sum\\limits_{j = 1}^{(n / 2) - 1}f(x_{2j}) + 4 \\sum\\limits_{j = 1}^{n / 2}f(x_{2j - 1}) + f(b)\\right] - \\frac{b - a}{180}h^4 f^{(4)}(\\mu). \\] Theorem 4.4 | Composite Midpoint rule \\(f \\in C^2[a, b]\\) , \\(h = (b - a) / (n + 2)\\) , and \\(x_j = a + (j + 1)h\\) , \\(j = -1, 0, \\dots, n + 1\\) . Then \\(\\exists\\ \\mu \\in (a, b)\\) , s.t. \\[ \\int_a^b f(x)dx = 2h \\sum\\limits_{j = 0}^{n / 2}f(x_{2j}) + \\frac{b - a}{6} h^2 f''(\\mu). \\] Stability Composite integration techniques are all stable w.r.t roundoff error. Example: Composite Simpson's rule Suppose \\(f(x_i)\\) is apporximated by \\(\\tilde f(x_i)\\) with \\[ f(x_i) = \\tilde f(x_i) + e_i. \\] Then the accumulated error is \\[ e(h) = \\left|\\frac{h}{3}\\left[e_0 + 2\\sum\\limits_{j = 1}^{(n / 2) - 1}e_{2j} + 4\\sum\\limits_{j = 1}^{n / 2}e_{2j-1} + e_n\\right]\\right|. \\] Suppose \\(e_i\\) are uniformly bounded by \\(\\varepsilon\\) , then \\[ e(h) \\le \\frac{h}{3}\\left[\\varepsilon + 2 \\left(\\frac{n}{2} - 1\\right) + 4 \\frac{n}{2} \\varepsilon + \\varepsilon \\right] = nh\\varepsilon = (b - a)\\varepsilon. \\] That means even though divide an interval to more parts, the roundoff error will not increase, which is quite stable.","title":"Composite Numerical Integration"},{"location":"Mathematics_Basis/NA/Chap_4/#romberg-integration","text":"Romberg integration combine the Composite Trapezoidal rule and Richardson's extrapolation to derive a more useful approximation. Suppose we divide the interval \\([a, b]\\) into \\(m_1 = 1\\) , \\(m_2 = 2\\) , \\(\\dots\\) , and \\(m_n = 2^{n - 1}\\) subintervals respectively. For each division, then the step size \\(h_k\\) is \\((b - a) / m_k = (b - a) / 2^{k - 1}\\) . Then we use \\(R_{k, 1}\\) to denote the composite trapezoidal rule, \\[ \\small R_{k, 1} = \\int_a^b f(x) dx = \\frac{h_k}{2} \\left[f(a) + f(b) + 2 \\left(\\sum\\limits_{i = 1}^{2^{k - 1} - 1} f(a + ih_k)\\right)\\right] - \\frac{(b - a)}{12}h^2_kf''(\\mu_k). \\] Mathematically, we have the recursive formula, \\[ R_{k, 1} = \\frac{1}{2}\\left[R_{k - 1, 1} + h_{k - 1}\\sum\\limits_{i = 1}^{2^{k - 2}}f(a+(2i-1)h_k)\\right]. \\] Theorem 4.5 the Composite Trapezoidal rule can represented by an alternative error term in the form \\[ \\int_a^b f(x) dx - R_{k, 1} = \\sum\\limits_{i = 1}^{\\infty} K_i h^{2i}_k, \\] where \\(K_i\\) depends only on \\(f^{(2i-1)}(a)\\) and \\(f^{(2i-1)}(b)\\) . This nice theorem makes Richardson's extrapolation available to reduce the truncation error! Similar to Differentiation with Richardson's Extrapolation , we have the following formula. Romberg Integration \\[ R_{k, j} = R_{k, j - 1} + \\frac{R_{k, j - 1} - R_{k - 1, j - 1}}{4^{j - 1} - 1}, \\] with an \\(O(h^{2j}_k)\\) approximation.","title":"Romberg Integration"},{"location":"Mathematics_Basis/NA/Chap_4/#adaptive-quadrature-methods","text":"Motivation: On the premise of equal spacing, in some cases, the left half of the interval is well approximated, and maybe we only need to subdivide the right half to approximate better. Here we introduce the Adaptive quadrature methods based on the Composite Simpson's rule. First, we want to derive, if we apply Simpson's rule in two subinterval and add them up, how much precision does it improve compared to only applying Simpson's rule just in the whole interval. From Simpson's rule, we have \\[ \\int_a^b f(x) dx = S(a, b) - \\frac{h^5}{90}f^{(4)}(\\mu), \\] where \\[ S(a, b) = \\frac{h}{3}[f(a) + 4f(a + h) + f(b)]. \\] If we divide \\([a, b]\\) into two subintervals, applying Simpson's rule respectively (namely apply Composite Simpson's rule with \\(n = 4\\) and step size \\(h / 2\\) ), we have \\[ \\int_a^b f(x) dx = S\\left(a, \\frac{a + b}{2}\\right) + S\\left(\\frac{a + b}{2}, b\\right) - \\frac{1}{16}\\left(\\frac{h^5}{90}\\right)f^{(4)}(\\tilde \\mu). \\] Moreover, assume \\(f^{(4)}(\\mu) \\approx f^{(4)}(\\tilde \\mu)\\) , then we have \\[ S\\left(a, \\frac{a + b}{2}\\right) + S\\left(\\frac{a + b}{2}, b\\right) - \\frac{1}{16}\\left(\\frac{h^5}{90}\\right)f^{(4)}(\\tilde \\mu) \\approx S(a, b) - \\frac{h^5}{90}f^{(4)}(\\mu), \\] so \\[ \\frac{h^5}{90}f^{(4)}(\\mu) \\approx \\frac{16}{15}\\left[S(a, b) - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right]. \\] Then, \\[ \\begin{aligned} \\left|\\int_a^b f(x) dx - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right| = \\left|\\frac{1}{16}\\left(\\frac{h^5}{90}\\right)f^{(4)}(\\tilde \\mu)\\right| \\\\ \\approx \\frac{1}{15} \\left|S(a, b) - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right|. \\end{aligned} \\] This result means that the subdivision approximates \\(\\int_a^b f(x) dx\\) about 15 times better than it agree with \\(S(a, b)\\) . Thus suppose we have a tolerance \\(\\varepsilon\\) across the interval \\([a, b]\\) . If \\[ \\left|S(a, b) - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right| < 15\\varepsilon, \\] then we expect to have \\[ \\left|\\int_a^bf(x)dx - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right| < \\varepsilon, \\] and the subdivision is thought to be a better approximation to \\(\\int_a^bf(x)dx\\) . Conclusion Suppose we have a tolerance \\(\\varepsilon\\) on \\([a, b]\\) , and we expect the tolerance is uniform. Thus at the subinterval \\([p, q] \\subseteq [a, b]\\) , with \\(q - p = k(b - a)\\) , we expect the tolerance as \\(k\\varepsilon\\) . Moreover, suppose the approximation of Simpson's rule on \\([p, q]\\) is \\(S\\) while the approxiamtion of Simpson's rule on \\([p, (p + q) / 2]\\) and \\([(p + q) / 2, q]\\) are \\(S_1\\) and \\(S_2\\) respectively. Then the criterion to subdivide is that \\[ |S_1 + S_2 - S| < M \\cdot k \\varepsilon. \\] where \\(M\\) is often taken as 10 but not 15 , which we derive above, since it also consider the error between \\(f^{(4)}(\\mu)\\) and \\(f^{(4)}(\\tilde \\mu)\\) .","title":"Adaptive Quadrature Methods"},{"location":"Mathematics_Basis/NA/Chap_4/#gaussian-quadrature","text":"Instead of equal spacing in the Newton-Cotes formulae, the selection of the points \\(x_i\\) also become variables. Gaussian Quadrature is aimed to construct a formula \\(\\sum\\limits_{k = 0}^n A_kf(x_k)\\) to approximate \\(\\int_a^b w(x)f(x)dx\\) of precision degree \\(2n + 1\\) with \\(n + 1\\) points, where \\(w(x)\\) is a weight function. (Compare to the equally spaced strategy only have a precision of around \\(n\\) ). That means, to determine \\(x_i\\) and \\(A_i\\) (totally \\(2n + 2\\) unknowns) such that the formula is accurate for \\(f(x) = 1, x, \\dots, x^{2n + 1}\\) (totally \\(2n + 2\\) equations). The selected points \\(x_i\\) are called Gaussian points . Problem Theoretically, since we have \\(2n + 2\\) unknowns and \\(2n + 2\\) eqautions, we can solve out \\(x_i\\) and \\(A_i\\) . But the equations are not linear ! Thus we give the following theorem to find Gaussian points without solving the nonlinear eqautions. Recap the definition of weight function and orthogonality of polynomials, and the construction of the set of orthogonal polynomials. Theorem 4.6 \\(x_0, \\dots, x_n\\) are Gaussian point iff \\(W(x) = \\prod\\limits_{k = 0}^n (x - x_k)\\) is orthogonal to all the polynomials of degree no greater than \\(n\\) on interval \\([a,b]\\) w.r.t the weight function \\(w(x)\\) . Proof \\(\\Rightarrow\\) if \\(x_0, \\dots x_n\\) are Gaussian points, then the degree of precision of the formula \\(\\int_a^b w(x)f(x)dx \\approx \\sum\\limits_{k = 0}^n A_k f(x_k)\\) is at least \\(2n + 1\\) . Then \\(\\forall\\ P(x) \\in \\Pi_n\\) , \\(\\text{deg}(P(x)W(x)) \\le 2n + 1\\) . Thus \\[ \\int_a^b w(x)P(x)W(x)dx = \\sum\\limits_{k = 0}^n A_k P(x_k)\\underbrace{W(x_k)}_{0} = 0. \\] \\(\\Leftarrow\\) \\(\\forall\\ P(x) \\in \\Pi_{2n + 1}\\) , let \\(P(x) = W(x)q(x) + r(x),\\ \\ q(x), r(x) \\in \\Pi_{n}\\) , then \\[ \\begin{aligned} \\int_a^b w(x)P(x) dx &= \\int_a^b w(x)W(x)q(x)dx + \\int_a^b w(x)r(x)dx \\\\ &= \\sum\\limits_{k = 0}^n A_k r(x_k) = \\sum\\limits_{k = 0}^n A_k P(x_k). \\end{aligned} \\] Recap that the set of orthogonal polynomials \\(\\{\\varphi_0, \\dots, \\varphi_n, \\dots\\}\\) is linearly independent and \\(\\varphi_{n + 1}\\) is orthogonal to any polynomials of degree no greater than \\(n\\) . Thus we can take \\(\\varphi_{n + 1}(x)\\) to be \\(W(x)\\) and the roots of \\(\\varphi_{n + 1}(x)\\) are the Gaussian points. Genernal Solution Problem: Assume \\[ \\int_a^b w(x)f(x)dx \\approx \\sum\\limits_{i = 0}^n A_k f(x_k). \\] Step.1 Construct the set of orthogonal polynomial on the interval \\([a, b]\\) by Gram-Schimidt Process from \\(\\varphi_0(x) \\equiv 1\\) to \\(\\varphi_{n}(x)\\) . Step.2 Find the roots of \\(\\varphi_{n}(x)\\) , which are the Gaussian points \\(x_0, \\dots, x_n\\) . Step.3 Solve the linear systems of the equation given by the precision for \\(f(x) = 1, x, \\dots, x^{2n + 1}\\) , and obtain \\(A_0, \\dots, A_n\\) . Although the method above is theoretically available, but it's tedious. But we have some special solutions which have been calculated. With some transformations, we can make them to solve the general problem too.","title":"Gaussian Quadrature"},{"location":"Mathematics_Basis/NA/Chap_4/#legendre-polynomials","text":"A typical set of orthogonal functions is Legrendre Polynomials. Legrendre Polynomials \\[ P_k(x) = \\frac{1}{2^k k!} \\frac{d^k}{dx^k}(x^2 - 1)^k, \\] or equally defined recursively by \\[ \\begin{aligned} & P_0(x) = 1,\\ P_1(x) = x, \\\\ & P_{k + 1}(x) = \\frac{1}{k + 1}\\left((2k + 1)x P_k(x) - k P_{k - 1}(x)\\right). \\end{aligned} \\] Property \\(\\{P_n(x)\\}\\) are orthogonal on \\([-1, 1]\\) , w.r.t the weight function \\[ w(x) \\equiv 1. \\] \\[ (P_j, P_k) = \\left\\{ \\begin{aligned} & 0, && k \\ne l, \\\\ & \\frac{2}{2k + 1}, && k = l. \\end{aligned} \\right. \\] \\(P_n(x)\\) is a monic polynomial of degree \\(n\\) . \\(P_n(x)\\) is symmetric w.r.t the origin. The \\(n\\) roots of \\(P_n(x)\\) are all on \\([-1, 1]\\) . The first few of them are \\[ \\begin{aligned} P_0(x) &= 1, \\\\ P_1(x) &= x, \\\\ P_2(x) &= x^2 - \\frac13, \\\\ P_3(x) &= x^3 - \\frac35 x, \\\\ P_4(x) &= x^4 - \\frac67 x^2 + \\frac{3}{35}. \\end{aligned} \\] The following table gives the pre-calculated values. And from interval \\([-1, 1]\\) to \\([a, b]\\) , we have a linear map \\[ t = \\frac{2x - a - b}{b - a} \\Leftrightarrow x = \\frac12 [(b - a)t + a + b]. \\] Thus we have \\[ \\int_a^b f(x) dx = \\int_{-1}^1 f\\left(\\frac{(b - a)t + (b + a)}{2}\\right)\\frac{b - a}{2}dt. \\] The formula using the roots of \\(P_{n + 1}(x)\\) is called the Gauss-Legendre quadrature formula. Example Approxiamte \\(\\int_1^{1.5} e^{-x^2}dx\\) (exact value to 7 decimal places is 0.1093643). Solution. \\[ \\int_1^{1.5} e^{-x^2} = \\frac14 \\int_{-1}^1 e^{-(t + 5)^2 / 16}dt, \\] For \\(n = 2\\) , \\[ \\int_1^{1.5} e^{-x^2} \\approx \\frac14 [e^{-(0.57735 + 5)^2 / 16} + e^{-(-57735 + 5)^2 / 16}] = 0.1094003. \\] For \\(n = 3\\) , \\[ \\begin{aligned} \\int_1^{1.5} e^{-x^2} &\\approx \\frac14 [0.55556e^{-(0.77460 + 5)^2 / 16} + 0.88889e^{-(5)^2 / 16} \\\\ & \\ \\ \\ \\ + 0.55556e^{-(-0.77460 + 5)^2 / 16}] \\\\ & = 0.1093642. \\end{aligned} \\]","title":"Legendre Polynomials"},{"location":"Mathematics_Basis/NA/Chap_4/#chebyshev-polynomials","text":"Also, Chebyshev polynomials are typical set of orthogonal polynomials too. We don't discuss much about it there. The formula using the roots of \\(T_{n + 1}(x)\\) is called the Gauss-Chebyshev quadrature formula.","title":"Chebyshev Polynomials"},{"location":"Mathematics_Basis/NA/Chap_5/","text":"Chapter 5 | Initial-Value Problems for Ordinary Differential Equations \u00b6 Initial-Value Problem (IVP) Basic IVP (one variable first order) Approximate the solution \\(y(t)\\) to a problem of the form \\[ \\frac{dy}{dt} = f(t, y), t \\in [a, b], \\] subject to an initial condition \\[ y(a) = \\alpha. \\] Higher-Order Systems of First-Order IVP Moreover, adding the number of unknowns, it becomes approximating \\(y_1(t), \\dots y_m(t)\\) to a problem of the form \\[ \\begin{aligned} \\frac{dy_1}{dt} &= f_1(t, y_1, y_2, \\dots, y_m), \\\\ \\frac{dy_2}{dt} &= f_2(t, y_1, y_2, \\dots, y_m), \\\\ & \\vdots \\\\ \\frac{dy_m}{dt} &= f_m(t, y_1, y_2, \\dots, y_m), \\end{aligned} \\] for \\(t \\in [a, b]\\) , subject to the initial conditions \\[ y_1(a) = \\alpha_1, y_2(a) = \\alpha_2, \\dots, y_m(a) = \\alpha_m. \\] Higher-Order IVP Or adding the number of order, it becomes m th-order IVP of the form \\[ y^{(m)} = f(t, y, y', y'', \\dots, y^{(m - 1)}), \\] for \\(t \\in [a, b]\\) , subject to the inital conditions \\[ y(a) = \\alpha_1, y'(a) = \\alpha_2, \\dots, y^{(m)}(a) = \\alpha_m. \\] General Idea \u00b6 Of all the method we introduce below, we can only approximate some points, but not the whole function \\(y(t)\\) . The approximation points are called mesh points . Moreover, we will only approximate the equally spaced points. Suppose we apporximate the values at \\(N\\) points on \\([a, b]\\) , then the mesh points are \\[ t_i = a + ih, \\] where \\(h = (b - a) / N\\) is the step size . To get the value between mesh points, we can use interpolation method. Since we know the derivative value \\(f(t, y)\\) at the mesh point, it's nice to use Hermit interpolation or cubic spline interpolation . Availability and Uniqueness \u00b6 Before finding out the approximation, we need some conditions to guarantee its availability and uniqueness. Definition 5.0 \\(f(t, y)\\) is said to satisfy a Lipschitz condition in the variable \\(y\\) on a set \\(D \\in \\mathbb{R}^2\\) if \\(\\exists\\ L > 0\\) , s.t. \\[ |f(t, y_1) - f(t, y_2)| \\le L|y_1 - y_2|,\\ \\ \\forall\\ (t, y_1), (t, y_2) \\in D. \\] \\(L\\) is called a Lipschitz constant for \\(f\\) . Definition 5.1 A set \\(D \\in \\mathbb{R}^2\\) is said to be convex if \\[ \\forall\\ (t, y_1), (t, y_2) \\in D,\\ \\ \\forall\\ \\lambda \\in [0, 1],\\ \\ ((1 - \\lambda)t_1 + \\lambda t_2, (1 - \\lambda)y_1 + \\lambda y_2) \\in D. \\] Theorem 5.0 | Sufficient Condition \\(f(t, y)\\) is defined on a convex set \\(D \\in \\mathbb{R}^2\\) . If \\(\\exists\\ L > 0\\) s.t. \\[ \\left|\\frac{\\partial f}{\\partial y}(t, y)\\right| \\le L,\\ \\ \\forall\\ (t, y) \\in D, \\] then \\(f\\) satisfies a Lipschitz condition on \\(D\\) in the variable \\(y\\) with Lipschitz constatnt \\(L\\) . Theorem 5.1 | Unique Solution \\(f(t, y)\\) is continuous on \\(D = \\{(t, y) | a \\le t \\le b, -\\infty < y < \\infty\\}\\) ( \\(D\\) is convex). If \\(f\\) satisfies a Lipschitz condition on \\(D\\) in the variable \\(y\\) , then the IVP \\[ y'(t) = f(t, y),\\ \\ a \\le t \\le b,\\ \\ y(a) = \\alpha, \\] has a unique solution \\(y(t)\\) for \\(a \\le t \\le b\\) . Definition 5.2 The IVP \\[ \\frac{dy}{dt} = f(t, y),\\ a \\le t \\le b,\\ y(a) = \\alpha, \\] is said to be a well-posed problem if A unique solution \\(y(t)\\) exists; \\(\\forall\\ \\varepsilon > 0, \\exists\\ k(\\varepsilon) > 0\\) , s.t. \\(\\forall |\\varepsilon_0| < \\varepsilon\\) , and \\(|\\delta(t)|\\) is continuous with \\(|\\delta(t)| < \\varepsilon\\) on \\([a, b]\\) , a unique solution \\(z(t)\\) to the IVP \\[ \\frac{dz}{dt} = f(t, y) + \\delta(t),\\ \\ a \\le t \\le b,\\ \\ y(a) = \\alpha,\\ \\ \\text{ (perturbed problem)} \\] exists with \\[ |z(t) - y(t)| < k(\\varepsilon)\\varepsilon,\\ \\ \\forall\\ t \\in [a, b]. \\] Numerical methods will always be concerned with solving a perturbed problem since roundoff error is unavoidable. Thus we want the problem to be well-posed , which means the perturbation will not affect the result approxiamtion a lot. Theorem 5.2 \\(f(t, y)\\) is continuous on \\(D = \\{(t, y) | a \\le t \\le b, -\\infty < y < \\infty\\}\\) ( \\(D\\) is convex). If \\(f\\) satisfies a Lipschitz condition on \\(D\\) in the variable \\(y\\) , then the IVP \\[ y'(t) = f(t, y),\\ \\ a \\le t \\le b,\\ \\ y(a) = \\alpha, \\] is well-posed . Besides, we also need to discuss the sufficient condition of m th-order system of first-order IVP . Similarly, we have the definition of Lipschitz condition and it's sufficient for the uniqueness of the solution. Definition 5.3 \\(f(t, y_1, \\dots, y_m)\\) define on the convex set \\[ D = \\{(t, y_1, \\dots, y_m | a \\le t \\le b, -\\infty < u_i < \\infty, \\text{ for each } i = 1, 2, \\dots, m\\} \\] is said to satisfy Lipschitz condition on \\(D\\) in the variables \\(y_1, \\dots, y_m\\) if \\(\\exists\\ L > 0\\) , s.t \\[ \\begin{aligned} & \\forall\\ (t, y_1, \\dots, y_n), (t, z_1, \\dots, z_n) \\in D, \\\\ & |f(t, y_1, \\dots, y_m) - f(t, z_1, \\dots, z_m)| \\le L \\sum\\limits_{j = 1}^m |y_j - z_j|. \\end{aligned} \\] Theorem 5.3 if \\(f(t, y_1, \\dots, y_m)\\) satisfy Lipschitz condition, then the m th order systems of first-order IVP has a unique solution \\(y_1(t), \\dots, y_n(t)\\) . Euler's Method \u00b6 To measure the error of Taylor methods (Euler's method is Taylor's method of order 1), we first give the definition of local truncation error , which somehow measure the truncation error at a specified step. Definition 5.4 The difference method \\[ \\begin{aligned} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + h\\phi(t_i, w_i). \\end{aligned} \\] has the local truncation error \\[ \\tau_{i + 1}(h) = \\frac{y_{i + 1} - (y_i + h\\phi(t_i, y_i))}{h} = \\frac{y_{i + 1} - y_i}{h} - \\phi(t_i, y_i). \\] Forward / Explicit Euler's Method \u00b6 We use Taylor's Theorem to derive Euler's method. Suppose \\(y(t)\\) is the unique solution of IVP, for each mesh points, we have \\[ y(t_{i + 1}) = y(t_i) + hy'(t_i) + \\frac{h^2}{2}y''(\\xi_i), \\xi_i \\in [t_i, t_{i + 1}], \\] and since \\(y'(t_i) = f(t_i, y(t_i))\\) , we have \\[ y(t_{i + 1}) = y(t_i) + hf.(t_i, y(t_i)) + \\frac{h^2}{2}y''(\\xi_i), \\xi_i \\in [t_i, t_{i + 1}]. \\] If \\(w_i \\approx y(t_i)\\) and we delete the remainder term, we get the (Explicit) Euler's method . (Explicit) Euler's Method \\[ \\begin{aligned} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + hf(t_i, w_i). \\text{(difference equations)} \\end{aligned} \\] The local truncation error of Euler's Method is \\[ \\tau_{i + 1}(h) = \\frac{hy'(t_i) + \\frac{h^2}{2}y''(\\xi_i)}{h} - f(t_i, y_i) = \\frac{h}{2}y''(\\xi_i) = O(h). \\] Theorem 5.4 The error bound of Euler's method is \\[ |y(t_i) - w_i| \\le \\frac{hM}{2L}\\left[e^{L(t_i - a)} - 1\\right], \\] where \\(M\\) is the upper bound of \\(|y''(t)|\\) . Although it has an exponential term (which is not such an accurate error bound), the most important intuitive is that the error bound is proportional to \\(h\\) . If we consider the roundoff error, we have the following theorem. Theorem 5.5 Suppose there exists roundoff error and \\(u_i = w_i + \\delta_i\\) , \\(i = 0, 1, \\dots\\) , then \\[ |y(t_i) - u_i| \\le \\frac{1}{L}\\left(\\frac{hM}{2} + \\frac{\\delta}{h}\\right)\\left[e^{L(t_i - a)} - 1\\right] + |\\delta_0|e^{L(t_i - a)}, \\] where \\(\\delta\\) is the upper bound of \\(\\delta_i\\) . This comes to a different case, we can't make \\(h\\) too small, and the optimal choice of \\(h\\) is \\[ h = \\sqrt{\\frac{2\\delta}{M}}. \\] Backward/ Implicit Euler's Method \u00b6 Inversely, if we use Taylor's Theorem in the following way, \\[ y(t_i) = y(t_{i + 1}) - hy'(t_{i + 1}) + \\frac{h^2}{2}y''(\\xi_{i + 1}), \\xi_i \\in [t_i, t_{i + 1}]. \\] Thus we get the Implicit Euler's Method , Implicit Euler's Method \\[ \\begin{aligned} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + hf(t_{i + 1}, w_{i + 1}). \\end{aligned} \\] Since \\(w_{i + 1}\\) is on the both side, we may use some methods in Chapter 2 to solve out the equation. The local truncation error of implicit Euler's method is \\[ \\begin{aligned} \\tau_{i + 1}(h) &= \\frac{hy'(t_i) + \\frac{h^2}{2}y''(\\xi_i)}{h} - f(t_{i + 1}, y_{i + 1}) \\\\ &= y'(t_i) - y'(t_{i + 1}) + \\frac{h}{2}y''(\\xi_i) \\\\ &= y'(t_i) - y'(t_i) - hy''(\\xi'_i) + \\frac{h}{2}y''(\\xi_i) \\\\ &= -\\frac{h}{2}y''(\\eta_i) = O(h). \\end{aligned} \\] Trapezoidal Method \u00b6 Notice the local truncation errors of two Euler's method are \\[ \\tau_{i + 1}(h) = \\frac{h}{2}y''(\\xi'),\\ \\ \\tau_{i + 1}(h) = -\\frac{h}{2}y''(\\eta'). \\] If we combine these two, we then obtain a method of \\(O(h^2)\\) local truncation error, which is called Trapezoidal Method . Trapezoidal Method \\[ \\begin{aligned} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + \\frac{h}{2}[f(t_i, w_i) + f(t_{i + 1}, w_{i + 1})]. \\end{aligned} \\] Higher-Order Taylor Method \u00b6 In Euler's methods, we only employ first order Taylor polynomials. If we expand it more to n th Taylor polynomials, we have \\[ \\begin{aligned} y(t_{i + 1}) &= y(t_i) + hy'(t_i) + \\frac{h^2}{2}y''(t_i) + \\cdots + \\frac{h^n}{n!}y^{(n)}(t_i) + \\frac{h^{n + 1}}{(n + 1)!} y^{(n + 1)}(\\xi_i), \\\\ & \\text{ where } \\xi_i \\in [t_i, t_{i + 1}], \\end{aligned} \\] and since \\(y^{(k)}(t) = f^{(k - 1)}(t, y(t))\\) , we have \\[ \\begin{aligned} y(t_{i + 1}) &= y(t_i) + hf(t_i, y(t_i)) + \\frac{h^2}{2}f'(t_i, y(t_i)) + \\cdots + \\frac{h^n}{n!}f^{(n - 1)}(t_i, y(t_i)) \\\\ & \\ \\ \\ \\ + \\frac{h^{n + 1}}{(n + 1)!} f^{(n)}(\\xi_i, y(\\xi_i)). \\end{aligned} \\] Taylor Method of order n \\[ \\begin{aligned} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + hT^{(n)}(t_i, w_i), \\end{aligned} \\] where \\[ T^{(n)}(t_i, w_i) = f(t_i, w_i) + \\frac{h}{2}f'(t_i, w_i) + \\cdots + \\frac{h^{n - 1}}{n!}f^{(n - 1)}(t_i, w_i). \\] Theorem 5.6 The local truncation error of Taylor method of order \\(n\\) is \\(O(h^n)\\) . Runge-Kutta Method \u00b6 Although Taylor method gives more accuracy as \\(n\\) increase, but it's not an easy job to calculate n th derivative of \\(f\\) . Here we introduce a new method called Runge-Kutta Method , which has low local truncation error and doesn't need to compute derivatives. It's based on Taylor's Theorem in two variables . Theorem 5.7 | Taylor's Theorem in two variables (Recap) \\(f(x, y)\\) has continous \\(n + 1\\) partial derivatives on the neigbourhood of \\(A(x_0, y_0)\\) , denoted by \\(U(A)\\) , then \\(\\forall\\ (x, y) \\in U(A)\\) , denote \\(\\Delta x = x - x_0\\) , \\(\\Delta y = y - y_0\\) , \\(\\exists\\ \\theta \\in (0, 1)\\) , s.t. \\[ f(x, y) = P_n(x, y) + R_n(x, y), \\] where \\[ \\small \\begin{aligned} P_n(x, y) &= f(x_0, y_0) + \\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)f(x_0, y_0) + \\frac{1}{2!}\\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)^2f(x_0, y_0) + \\\\ & \\ \\ \\ \\ \\ \\cdots + \\frac{1}{n!}\\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)^nf(x_0, y_0), \\end{aligned} \\] \\[ \\small R_n(x, y) = \\frac{1}{(n + 1)!}\\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)^{n + 1}f(x_0 + \\theta \\Delta x, y_0 + \\theta \\Delta y). \\] and \\[ \\small \\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)^{k}f(x, y) = \\sum\\limits_{i = 0}^k \\binom{k}{i} (\\Delta x)^i (\\Delta y)^{k - i} \\frac{\\partial^k f}{\\partial x^i \\partial y^{k - i}}(x, y). \\] Derivation of Midpoint Method To equate \\[ T^{(2)}(t, y) = f(t, y) + \\frac{h}{2}f'(t, y) = f(t, y) + \\frac{h}{2}\\frac{\\partial f}{\\partial t}(t,y) + \\frac{h}{2}\\frac{\\partial f}{\\partial y}(t,y)\\cdot f(t, y), \\] with \\[ \\begin{aligned} a_1f(t + \\alpha_1, y + \\beta_1) &= a_1f(t, y) + a_1 \\alpha_1 \\frac{\\partial f}{\\partial t}(t, y) \\\\ & \\ \\ \\ \\ + a_1 \\beta_1 \\frac{\\partial f}{\\partial y}(t, y) + a_1 \\cdot R_1(t + \\alpha_1, y + \\beta_1), \\end{aligned} \\] except for the last residual term \\(R_1\\) , we have \\[ a_1 = 1,\\ \\ \\alpha_1 = \\frac{h}{2},\\ \\ beta_1 = \\frac{h}{2}f(t, y). \\] Thus \\[ T^{(2)}(t, y) = f\\left(t + \\frac{h}{2}, y + \\frac{h}{2}f(t, y)\\right) - R_1. \\] Since the term \\(R_1\\) is only concerned with the 2 nd -order partial derivatives of \\(f\\) , if they are bounded, then \\(R_1\\) is \\(O(h^2)\\) , the order of the local truncation error. Replace \\(T^{(2)}(t,y)\\) by the formula above in the Taylor method of order 2, we have the Midpoint Method . Midpoint Method \\[ \\begin{aligned} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + hf\\left(t_i + \\frac{h}{2}, w_i + \\frac{h}{2}f(t_i, w_i)\\right). \\end{aligned} \\] Derivation of Modified Euler Method and Heun's Method Similarly, approximate \\[ T^{(3)}(t, y) = f(t, y) + \\frac{h}{2}f'(t, y) + \\frac{h^2}{6}f''(t,y), \\] with \\[ a_1f(t,y) + a_2f(t + \\alpha_2, y + \\delta_2f(t,y)). \\] But it doesn't have sufficient equation to determine the 4 unknowns \\(a_1, a_2, \\alpha_2, \\delta_2\\) above. Instead we have \\[ a_1 + a_2 = 1, \\ \\alpha_2 = \\delta_2\\ \\overset{\\Delta}{=\\!=}\\ ph. \\] Thus a number of \\(O(h^2)\\) methods can be derived, and they are generally in the form \\[ \\begin{aligned} w_{i + 1} &= w_i + h(a_1 K_1 + a_2 K_2) \\\\ K_1 &= f(t_i, w_i), \\\\ K_2 &= f(t_i + ph, y + ph K_1). \\end{aligned} \\] The most important are the following two. Modified Euler Method \\[ \\begin{aligned} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + \\frac{h}{2}[f(t_i, w_i) + f(t_i + h, w_i + hf(t_i, w_i))]. \\end{aligned} \\] Heun's Method \\[ \\begin{aligned} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + \\frac{h}{4}\\left[f(t_i, w_i) + 3f\\left(t_i + \\frac23 h, w_i + \\frac23 hf(t_i, w_i)\\right)\\right]. \\end{aligned} \\] In a similar manner, approximate \\(T^{(n)}(t, y)\\) with \\[ \\lambda_1 K_1 + \\lambda_2 K_2 + \\cdots + \\lambda_m K_m, \\] where \\[ \\begin{aligned} K_1 &= f(t, y), \\\\ K_2 &= f(t + \\alpha_2 h, y + \\beta_{21} h K_1), \\\\ & \\ \\ \\vdots \\\\ K_m &= f(t + \\alpha_m h, y + \\beta_{m1} h K_1 + \\beta_{m, m - 1} hK_{m - 1}), \\end{aligned} \\] with different \\(m\\) , we can derive quite a lot of Runge-Kutta methods. The most common Runge-Kutta method is given below. Runge-Kutta Order Four \\[ \\begin{aligned} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + \\frac{h}{6}(K_1 + 2K_2 + 2K_3 + K_4), \\\\ K_1 &= f(t_i, w_i), \\\\ K_2 &= f\\left(t_i + \\frac{h}{2}, w_i + \\frac{h}{2}K_1\\right), \\\\ K_3 &= f\\left(t_i + \\frac{h}{2}, w_i + \\frac{h}{2}K_2\\right), \\\\ K_4 &= f\\left(t_i + h, w_i + h K_3\\right). \\end{aligned} \\] The local trancation error of Runge-Kutta are given below, where \\(n\\) is the number of evaluations per step. Property Compared to Taylor's method, evaluate \\(f(t, y)\\) the same times , Runge-Kutta gives the lowest error. Multistep Method \u00b6 In the previous section, the difference equation only relates \\(w_{i + 1}\\) with \\(w_{i}\\) . Multistep Method discuss the situation of relating more term of previous predicted \\(w\\) . Definition 5.5 An m -step multistep method for solving the IVP \\[ y'(t) = f(t, y),\\ \\ a \\le t \\le b,\\ \\ y(a) = \\alpha, \\] has a difference equation \\[ \\begin{aligned} w_{i + 1} &= a_{m - 1}w_i + a_{m - 2}w_{i - 1} + \\cdots a_0 w_{i + 1 - m} + h[b_mf(t_{i + 1}, w_{i + 1}) \\\\ & \\ \\ \\ \\ + b_{m - 1}f(t_i, w_i) + \\cdots + b_0 f(t_{i + 1 - m}, w_{i + 1 - m})], \\end{aligned} \\] with the starting values \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ \\dots,\\ \\ w_{m - 1} = \\alpha_{m - 1}. \\] If \\(b_m = 0\\) , it's called explicit , or open . Otherwise, it's called implicit , or closed . Similarly, we first define local truncation error for multistep method to measure the error at a specified step. Definition 5.6 For a m -step multistep method, the local truncation error at \\((i + 1)\\) th step is \\[ \\begin{aligned} \\tau_{i + 1}(h) &= \\frac{y(t_{i + 1}) - a_{m - 1}y(t_i) - \\cdots - a_0 y(t_{i + 1 - m})}{h} \\\\ & \\ \\ \\ \\ - b_{m - 1}f(t_i, y(t_{i + 1})) + \\cdots + b_0 f(t_{i + 1 - m}, y(t_{i + 1 - m}))]. \\end{aligned} \\] Adams Method \u00b6 In this section, we only consider the case of \\(a_{m - 1} = 1\\) and \\(a_i = 0\\) , \\(i \\ne m - 1\\) . To derive the Adams method, we start from a simple formula. \\[ y(t_{i + 1}) - y(t_i) = \\int_{t_i}^{t_{i + 1}}y'(t)dt = \\int_{t_i}^{t_{i + 1}}f(t, y(t))dt. \\] i.e. \\[ y(t_{i + 1}) = y(t_i) + \\int_{t_i}^{t_{i + 1}}f(t, y(t))dt. \\] We cannot integrate \\(f(t, y(t))\\) without knowing \\(y(t)\\) . Instead we use an interpolating polynomial \\(P(t)\\) by points \\((t_0, w_0),\\ \\dots,\\ (t_i, w_i)\\) to approximate \\(f(t, y)\\) . Thus we approximate \\(y(t_{i + 1})\\) by \\[ y(t_{i + 1}) \\approx w_i + \\int_{t_i}^{t_{i + 1}} P(t) dt. \\] For convenience, we use Newton backward-difference formula to represent \\(P(t)\\) . Adams-Bashforth m -Step Explicit Method \u00b6 To derive an Adam-Bashforth explicit m -step technique , we form the interpolating polynomial \\(P_{m - 1}(t)\\) by \\[ (t_i, f(t_i, y(t_i))),\\ \\ \\dots,\\ \\ (t_{i + 1 - m}, f(t_{i + 1 - m}, y(t_{i + 1 - m}))). \\] Then suppose \\(t = t_i + sh\\) , with \\(dt = hds\\) , we have \\[ \\begin{aligned} \\int_{t_i}^{t_{i + 1}} f(t, y(t)) dt &= \\int_{t_i}^{t_{i + 1}} \\sum\\limits_{k = 0}^{m - 1}(-1)^k \\binom{-s}{k}\\nabla^k f(t_i, y(t_i))dt \\\\ & \\ \\ \\ \\ + \\int_{t_i}^{t_{i + 1}} \\frac{f^{(m)}(\\xi_i, y(\\xi_i))}{m!}\\prod\\limits_{k = 0}^{m - 1}(t - t_{i - k})dt \\\\ \\left(\\text{Note that } dt = hds\\right) &= h\\sum\\limits_{k = 0}^{m - 1}\\nabla^k (-1)^k \\int_0^1 \\binom{-s}{k}ds \\\\ & \\ \\ \\ \\ + h^{m + 1}\\int_0^1 \\binom{-s}{m}f^{(m)}(\\xi_i, y(\\xi_i))ds. \\\\ \\left(\\substack{\\text{Weighted Mean Value} \\\\ \\text{Theorem for Integral}}\\right) &= h\\sum\\limits_{k = 0}^{m - 1}\\nabla^k (-1)^k \\int_0^1 \\binom{-s}{k}ds \\\\ & \\ \\ \\ \\ + h^{m + 1}f^{(m)}(\\mu_i, y(\\mu_i))\\int_0^1 \\binom{-s}{m}ds. \\end{aligned} \\] Since for a given \\(k\\) , we have \\(k\\) \\(0\\) \\(1\\) \\(2\\) \\(3\\) \\(4\\) \\((-1)^k\\int_0^1 \\binom{-s}{k}ds\\) \\(1\\) \\(\\frac12\\) \\(\\frac{5}{12}\\) \\(\\frac38\\) \\(\\frac{251}{720}\\) Thus \\[ \\begin{aligned} y(t_{i + 1}) &= y(t_i) + h\\left[f(t_i, y(t_i)) + \\frac12 \\nabla f(t_i, y(t_i)) + \\frac{5}{12} \\nabla^2 f(t_i, y(t_i)) + \\cdots\\right] \\\\ & \\ \\ \\ \\ + h^{m + 1}f^{(m)}(\\mu_i, y(\\mu_i))\\int_0^1 \\binom{-s}{m}ds. \\end{aligned} \\] Adams-Bashforth m -Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ \\dots,\\ \\ w_{m - 1} = \\alpha_{m - 1}, \\] \\[ w_{i + 1} = w_i + h\\sum\\limits_{k = 0}^{m - 1}\\nabla^k f(t_i, y(t_i)) (-1)^k \\int_0^1 \\binom{-s}{k}ds, \\] with local truncation error \\[ \\tau_{i + 1}(h) = h^{m}y^{(m + 1)}(\\mu_i)(-1)^m\\int_0^1 \\binom{-s}{m}ds. \\] Adams-Bashforth Two-Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1, \\] \\[ w_{i + 1} = w_i + \\frac{h}{2}[3f(t_i, w_i) - f(t_{i - 1}, w_{i - 1})], \\] with local truncation error \\[ \\tau_{i + 1}(h) = \\frac{5}{12}y'''(\\mu_i)h^2. \\] Adams-Bashforth Three-Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2, \\] \\[ w_{i + 1} = w_i + \\frac{h}{12}[23f(t_i, w_i) - 16f(t_{i - 1}, w_{i - 1}) + 5f(t_{i - 2}, w_{i - 2})], \\] with local truncation error \\[ \\tau_{i + 1}(h) = \\frac{3}{8}y^{(4)}(\\mu_i)h^3. \\] Adams-Bashforth Four-Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2,\\ \\ w_3 = \\alpha_3, \\] \\[ \\begin{aligned} w_{i + 1} = w_i + \\frac{h}{24}&[55f(t_i, w_i) - 59f(t_{i - 1}, w_{i - 1}) \\\\ & + 37f(t_{i - 2}, w_{i - 2}) - 9f(t_{i - 3}, w_{i - 3})], \\end{aligned} \\] with local truncation error \\[ \\tau_{i + 1}(h) = \\frac{251}{720}y^{(5)}(\\mu_i)h^4. \\] Adams-Moulton m -Step Implicit Method \u00b6 For implicit case, we add \\((t_{i + 1}, f(t_{i + 1}, y(t_{i + 1})))\\) as one more interpolating node to construct the interpolating polynomials. Let \\(t = t_{i + 1} + sh\\) and there is \\(m\\) points. Similarly we have the conclusions below. Adams-Moulton m -Step Implicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ \\dots,\\ \\ w_{m - 1} = \\alpha_{m - 1}, \\] \\[ w_{i + 1} = w_i + h\\sum\\limits_{k = 0}^{m}\\nabla^k f(t_{i + 1}, y(t_{i + 1})) (-1)^k \\int_{-1}^0 \\binom{-s}{k}ds, \\] with local truncation error \\[ \\tau_{i + 1}(h) = h^{m + 1}y^{(m + 2)}(\\mu_i)(-1)^{m + 1}\\int_{-1}^0 \\binom{-s}{m + 1}ds. \\] Adams-Moulton Two-Step Implicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1, \\] \\[ w_{i + 1} = w_i + \\frac{h}{12}[5f(t_{i + 1}, w_{i + 1}) + 8f(t_i, w_i) - f(t_{i - 1}, w_{i - 1})], \\] with local truncation error \\[ \\tau_{i + 1}(h) = -\\frac{1}{24}y^{(4)}(\\mu_i)h^3. \\] Adams-Moulton Three-Step Implicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2, \\] \\[ w_{i + 1} = w_i + \\frac{h}{24}[9f(t_{i + 1}, w_{i + 1}) + 19f(t_i, w_i) - 5f(t_{i - 1}, w_{i - 1}) + f(t_{i - 2}, w_{i - 2})], \\] with local truncation error \\[ \\tau_{i + 1}(h) = -\\frac{19}{720}y^{(5)}(\\mu_i)h^4. \\] Adams-Moulton Four-Step Implicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2,\\ \\ w_3 = \\alpha_3, \\] \\[ \\begin{aligned} w_{i + 1} = w_i + \\frac{h}{720}&[251f(t_{i + 1}, w_{i + 1}) + 646f(t_i, w_i) - 264f(t_{i - 1}, w_{i - 1}) \\\\ & + 106f(t_{i - 2}, w_{i - 2}) - 19f(t_{i - 3}, w_{i - 3})], \\end{aligned} \\] with local truncation error \\[ \\tau_{i + 1}(h) = -\\frac{3}{160}y^{(6)}(\\mu_i)h^5. \\] Note Comparing an m -step Adams-Bashforth explicit method and (m - 1) -step Adams-Moulton implicit method, they both involve m evaluation of \\(f\\) per step, and both have the local truncation error of the term \\(ky^{(m + 1)}(\\mu_i)h^m\\) . And the latter implicit method has the smaller coefficient \\(k\\) . This leads to greater stability and smaller roundoff errors for implicit methods. Predictor-Corrector Method \u00b6 Implicit method has some advantages but it's hard to calculate. We could use some iterative methods introduced in Chapter 2 to solve it but it complicates the process considerably. In practice, since explicit method is easy to calculate, we combine the explicit and implicit method to predictor-corrector method . Predictor-Corrector Method Step.1 Compute first \\(m\\) initial values by Runge-Kutta method. Step.2 Predict by Adams-Bashforth explicit method. Step.3 Correct by Adams-Moulton implicit method. NOTE: All the formulae used in the three steps have the same order of truncation error. Example Take \\(m = 4\\) which is the most common case as exmaple. Step.1 From initial value \\(w_0 = \\alpha\\) , we use Runge-Kutta method of order four to derive \\(w_1\\) , \\(w_2\\) and \\(w_3\\) . Set \\(i = 3\\) . Step.2 Use four-step Adams-Bashforth explicit method, we have \\[ \\small w_{i + 1}^{(0)} = w_i + \\frac{h}{24}[55f(t_i, w_i) - 59f(t_{i - 1}, w_{i - 1}) + 37f(t_{i - 2}, w_{i - 2}) - 9f(t_{i - 3}, w_{i - 3})], \\] Step.3 Use three-step Adams-Moulton \\[ \\small w_{i + 1}^{(1)} = w_i + \\frac{h}{24}[9f(t_{i + 1}, w_{i + 1}^{(1)}) + 19f(t_i, w_i) - 5f(t_{i - 1}, w_{i - 1}) + f(t_{i - 2}, w_{i - 2})], \\] Then we have two options \\(i = i + 1\\) , and go back to Step. 2 , to approximate the next point. Or, for higher accuracy, we can repeat Step.3 iteratively by \\[ \\small w_{i + 1}^{(k + 1)} = w_i + \\frac{h}{24}[9f(t_{i + 1}, w_{i + 1}^{(k)}) + 19f(t_i, w_i) - 5f(t_{i - 1}, w_{i - 1}) + f(t_{i - 2}, w_{i - 2})]. \\] Other Methods \u00b6 If we derive the multistep method by Taylor expansion , we can have more methods. We take an example to show it. Suppose we want to derive a difference equation in the form \\[ w_{i + 1} = a_0 w_{i - 1} + h(b_2 f(t_{i + 1}, w_{i + 1}) + b_1 f(t_i, w_i) + b_0 f(t_{i - 1}, w_{i - 1})). \\] We use \\(y_i\\) and \\(f_i\\) to denote \\(y(t_i)\\) and \\(f(t_i, y_i)\\) respectively. Expand \\(y_{i - 1}, f_{i + 1}, f_{i}, f_{i - 1}\\) , we have \\[ \\begin{aligned} y_{i - 1} &= y_i - hy_i' + \\frac12 h^2 y_i'' - \\frac16h^3y_i''' + O(h^4), \\\\ f_{i + 1} &= y_i' + hy_i'' + \\frac12 h^2 y_i''' + O(h^3), \\\\ f_i &= y_i', \\\\ f_{i - 1} &= y_i' - hy_i'' + \\frac12 h^2 y_i''' + O(h^3), \\\\ \\end{aligned} \\] and note that \\[ y_{i + 1} = y_i + hy_i' + \\frac12 h^2 y_i'' + \\frac16 h^3 y_i'''+ O(h^4). \\] Equate them with \\[ y_{i + 1} = a_0 y_{i - 1} + h(b_2 f_{i + 1} + b_1 f_i + b_0 f_{i - 1}). \\] We can solve out \\[ a_0 = 1,\\ \\ b_2 = \\frac13,\\ \\ b_1 = \\frac43,\\ \\ b_0 = \\frac13. \\] Thus \\[ w_{i + 1} = w_{i - 1} + \\frac{h}{3}(f(t_{i + 1}, w_{i + 1}) + 4f(t_i, w_i) + f(t_{i - 1}, w_{i - 1})). \\] Simpson Implicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1, \\\\ w_{i + 1} = w_{i - 1} + \\frac{h}{3}(f(t_{i + 1}, w_{i + 1}) + 4f(t_i, w_i) + f(t_{i - 1}, w_{i - 1})), \\] with local truncation error \\[ \\tau_{i + 1}(h) = \\frac{h^4}{90}y^{(5)}(\\xi_i). \\] Note that it's an implicit method, and the most commonly used corresponding predictor is Milne's Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2 \\\\ w_{i + 1} = w_{i - 3} + \\frac{4h}{3}(2f(t_i, w_i) + - f(t_{i - 1}, w_{i - 1}) + 2f(t_{i - 2}, w_{i - 2})), \\] with local truncation error \\[ \\tau_{i + 1}(h) = \\frac{14}{90}h^4 y^{(5)}(\\xi_i). \\] which can also derive by Taylor expansion in the similar manner. Higher-Order Systems of First-Order IVP \u00b6 Actually it just vectorize the method that we've mentioned above. Take Runge-Kutta Order Four as example. Runge-Kutta Method for Systems of Differential Equations \\[ \\begin{aligned} w_{1, 0} &= \\alpha_1, w_{2, 0} = \\alpha_2, \\dots, w_{m, 0} = \\alpha_m. \\\\ w_{i, j + 1} &= w_{i, j} + \\frac{h}{6}(K_{1, i} + 2K_{2, i} + 2K_{3, i} + K_{4, i}), \\\\ K_{1, i} &= f_i(t_j, w_{1, j}, w_{2, j}, \\dots, w_{m, j}), \\\\ K_{2, i} &= f_i\\left(t_j + \\frac{h}{2}, w_{1, j} + \\frac{h}{2}K_{1, 1}, w_{2, j} + \\frac{h}{2}K_{1, 2}, \\dots, w_{m, j} + \\frac{h}{2}K_{1, m}\\right), \\\\ K_{3, i} &= f_i\\left(t_j + \\frac{h}{2}, w_{1, j} + \\frac{h}{2}K_{2, 1}, w_{2, j} + \\frac{h}{2}K_{2, 2}, \\dots, w_{m, j} + \\frac{h}{2}K_{2, m}\\right), \\\\ K_{4, i} &= f_i\\left(t_j + h, w_{1, j} + h K_{3, 1}, w_{2, j} + h K_{3, 2}, \\dots, w_{m, j} + h K_{3. m} \\right). \\end{aligned} \\] Higher-Order IVP \u00b6 We can deal with higher-order IVP the same as higher-order systems of first-order IVP. The only transformation we need to do is to let \\(u_1(t) = y(t)\\) , \\(u_2(t) = y'(t)\\) , \\(\\dots\\) , and \\(u_m(t) = y^{(m - 1)}(t)\\) . This produces the first-order system \\[ \\begin{aligned} \\frac{du_1}{dt} &= \\frac{dy}{dt} = u_2, \\\\ \\frac{du_2}{dt} &= \\frac{dy'}{dt} = u_3, \\\\ & \\vdots \\\\ \\frac{du_{m - 1}}{dt} &= \\frac{dy^{(m - 2)}}{dt} = u_m, \\\\ \\frac{du_m}{dt} &= \\frac{dy^{(m - 1)}}{dt} = y^{(m)} = f(t, y', y'', \\dots, y^{(m - 1)}) = f(t, u_1, \\dots, u_m), \\end{aligned} \\] with initial conditions \\[ u_1(a) = y(a) = \\alpha_1,\\ \\ u_2(a) = y'(a) = \\alpha_2,\\ \\ \\dots,\\ \\ u_m(a) = y^{(m - 1)}(a) = \\alpha_m. \\] Stability \u00b6 Definition 5.7 A one-step difference-equation method with local truncation error \\(\\tau_i(h)\\) is consistent with the differential equation it approximates if \\[ \\lim\\limits_{h \\rightarrow 0} \\max\\limits_{1 \\le i \\le N}|\\tau_i(h)| = 0. \\] For m -step multistep methods, it's also required that \\[ \\lim\\limits_{h \\rightarrow 0} |\\alpha_i - y_i| = 0,\\ \\ i = 1, 2, \\dots, m - 1, \\] since at most cases, these \\(\\alpha_i\\) are derived by one-step methods. Definition 5.8 A one-step / multistep difference-equation method is convergent w.r.t the differential equation it approximates if \\[ \\lim\\limits_{h \\rightarrow 0} \\max\\limits_{1 \\le i \\le N}|w_i - y(t_i)| = 0. \\] Definition 5.9 A stable method is one whose results depent continously on the initial data, in the sense that small changes or perturbations in the intial conditions produce correspondingly small changes in the subsequent approximations. One Step Method \u00b6 It's relatively natural to think the stability is somewhat analogous to the discussion of well-posed , and thus it's not surprising Lipschitz condition appears. The following theorem gives the relation among consistency , convergence and stability . Theorem 5.8 Suppose the IVP is approximated by a one-step method in the form \\[ \\begin{aligned} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + h \\phi(t_i, w_i, h). \\end{aligned} \\] If \\(\\exists\\ h_0 > 0\\) and $ \\phi(t, w, h)$ is continuous and satisfies a Lipschitz condition in the varaible \\(w\\) with Lipschitz constant \\(L\\) on the set \\[ D = \\{(t, w, h) | a \\le t \\le b, -\\infty < w < \\infty, 0 \\le h \\le h_0\\}. \\] Then The method is stable . The difference method is convergent iff it's consistent , which is equivalent to \\[ \\phi(t, y, 0) = f(t, y),\\ \\ t \\in [a, b]. \\] (Relation between Global Error and Local Truncation Error) If \\(\\exists \\tau(h)\\) s.t. \\(|\\tau_i(h)| \\le \\tau(h)\\) whenever \\(0 \\le h \\le h_0\\) , then \\[ |y(t_i) - w_i| \\le \\frac{\\tau(h)}{L}e^{L(t_i - a)}. \\] Multistep Method \u00b6 Theorem 5.9 | Relation between Global Error and Local Truncation Error Suppose the IVP is approximated by a predictor-corrector method with an m -step Adams-Bashforth predictor equation \\[ w_{i + 1} = w_i + h[b_{m - 1}f(t_i, w_i) + \\cdots + b_0f(t_{i + 1 - m}, w_{i + 1 - m})], \\] with local truncation error \\(\\tau_{i + 1}(h)\\) , and an (m - 1) -step Adams-Moulton corrector eqation \\[ w_{i + 1} = w_i + h[\\tilde b_{m - 1}f(t_i, w_i) + \\cdots + \\tilde b_0f(t_{i + 1 - m}, w_{i + 1 - m})], \\] with local truncation error \\(\\tilde \\tau_{i + 1}(h)\\) . In addition, suppose \\(f(t, y)\\) and \\(f_y(t, y)\\) are continous on \\(D = \\{(t, y) | a \\le t \\le b, -\\infty < y < \\infty\\}\\) and \\(f_y\\) is bounded. Then the local truncation error \\(\\sigma_{i + 1}(h)\\) of the predictor-corrector method is \\[ \\small \\sigma_{i + 1}(h) = \\tilde \\tau_{i + 1}(h) + \\tau_{i + 1}(h) \\tilde b_{m - 1} \\frac{\\partial f}{\\partial y}(t_{i + 1}, \\theta_{i + 1}),\\ \\ \\text{ for some } \\theta_{i + 1} \\in (0, h\\tau_{i + 1}(h)). \\] Moreover, \\(\\exists\\ k_1, k_2\\) , s.t. \\[ |w_i - y(t_i)| \\le \\left[\\max\\limits_{0 \\le j \\le m - 1}|w_j - y(t_j)| + k_1\\sigma(h) \\right] e^{k_2(t_i - a)}, \\] where \\(\\sigma(h) = \\max\\limits_{m \\le j \\le N}|\\sigma_j(h)|\\) . For the difference equation of multistep method, we first introduce characteristc polynomial associated with the method, given by \\[ P(\\lambda) = \\lambda^m - a_{m - 1}\\lambda^{m - 1} - a_{m - 2}\\lambda^{m - 2} - \\cdots - a_1 \\lambda - a_0. \\] Definition 5.10 Suppose the roots of the characteristic equation \\[ P(\\lambda) = \\lambda^m - a_{m - 1}\\lambda^{m - 1} - a_{m - 2}\\lambda^{m - 2} - \\cdots - a_1 \\lambda - a_0 = 0 \\] associated with the multistep difference method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ \\dots,\\ \\ w_{m - 1} = \\alpha_{m - 1}, \\\\ w_{i + 1} = a_{m - 1}w_i + a_{m - 2}w_{i - 1} + \\cdots a_0 w_{i + 1 - m} + hF(t_i, h, w_{i + 1}, w_i \\dots, w_{i + 1 - m}). \\] If \\(|\\lambda_i| \\le 1\\) , for each \\(i = 1, 2, \\dots, m\\) and all roots with modulus value \\(1\\) are simple roots, then the difference method is said to satisfy the root condition . Methods that satisfy the root condition and have \\(\\lambda = 1\\) as the only root of the characteristic equation of magnitude one are called strongly stable . Methods that satisfy the root condition and have more than one disctinct root with magnitude one are called weakly stable . Methods that do not satisfy the root condition are called unstable . Theorem 5.10 A multistep method of the form \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ \\dots,\\ \\ w_{m - 1} = \\alpha_{m - 1}, \\\\ w_{i + 1} = a_{m - 1}w_i + a_{m - 2}w_{i - 1} + \\cdots a_0 w_{i + 1 - m} + hF(t_i, h, w_{i + 1}, w_i \\dots, w_{i + 1 - m}). \\] is stable iff it satisfies the root condition. Moreover, if the difference method is consistent with the differential equation, then the method is stable iff it is convergent . Stiff Equations \u00b6 However, a special type of solution is not easy to deal with, when the magnitude of derivative increases but the solution does not. They are call stiff equations . Stiff differential eqaution are characterized as those whose exact solution has a term of the form \\(e^{-ct}\\) , where \\(c\\) is a large positive constant. This is called the transient solution. Actaully there is another important part of solution called steady-state solution. Let's first define test equation to examine what happen with the stiff cases. Definition 5.11 A test equation is the IVP \\[ y' = \\lambda y,\\ \\ y(0) = \\alpha,\\ \\ \\text{ where } \\text{Re}(\\lambda) < 0. \\] The solution is obviously \\(y(t) = \\alpha e^{\\lambda t}\\) , which is the transient solution. At this case, the steady-state solution is zero, thus the approximation characteristics of a method are easy to determine. Example of Euler's Method Take Euler's method as an example, and denote \\(H = h\\lambda\\) . We have \\[ w_0 = \\alpha, \\\\ w_{i + 1} = w_i + h(\\lambda w_i) = (1 + h\\lambda)w_i = (1 + H)w_i, \\] so \\[ w_{i + 1} = (1 + H)^{i + 1}w_0 = (1 + H)^{i + 1}\\alpha. \\] The absolute error is \\[ |y(t_i) - w_i| = |(e^H)^i - (1 + H)^i||\\alpha|. \\] When \\(H < 0\\) , \\((e^H)^i\\) decays to zero as \\(i\\) increases. Thus we want \\((1 + H)^i\\) to decay too, which implies that \\[ |1 + H| < 1. \\] From another perspective, consider the roundoff error of the initial value \\(\\alpha\\) by, \\[ w_0 = \\alpha + \\delta_0. \\] At the i th step the roundoff error is \\[ \\delta_i = (1 + H)^i \\delta_0. \\] To control the error, we also want \\(|1 + H| < 1\\) . In general, when applying to the test equation, we have the difference equation of the form \\[ w_{i + 1} = Q(H)w_i,\\ \\ H = h \\lambda. \\] To make \\(Q(H)\\) approximate \\(e^H\\) , we want at least \\(|Q(H)| < 1\\) to make it decay to 0. The inequality delimit a region in a complex plane. For a multistep method, the difference equation is in the form \\[ (1 - H b_m)w_{i + 1} - (a_{m - 1} + H b_{m - 1})w_i - \\cdots - (a_0 + H b_0)w_{i + 1 - m} = 0. \\] We can define a characteristic polynomial \\[ Q(z, H) = (1 - H b_m)z^m - (a_{m - 1} + H b_{m - 1})z^{m - 1} - \\cdots - (a_0 + H b_0). \\] Suppose \\(\\beta_i\\) are the distinct roots of \\(Q(z, H)\\) , then it can be proved that \\(\\exists\\ c_i\\) , s.t. \\[ w_{i} = \\sum\\limits_{k = 1}^m c_k\\beta_k^i. \\] Again, to make \\(w_i\\) approximate \\((e^H)^i\\) , we want at least all \\(|\\beta_k| < 1\\) to make it decay to \\(0\\) . It also delimit a region in a complex plane. Definition 5.12 The Region R of abosolute stability for a one-step method is \\[ R = \\{H \\in \\mathbb{C} | |Q(H)| < 1\\}, \\] and for a multistep method, \\[ R = \\{H \\in \\mathbb{C} | |\\beta_k| < 1,\\ \\ \\text{ for all zero points of } Q(z, H)\\}. \\] A numerical method is said to be A-stable if its region \\(R\\) of abosolute stability contains left half-plane , which means \\(\\text{Re}(\\lambda) < 0\\) , stable with stiff equation . Method A is said to be more stable than method B if the region of absolute stability of A is larger than that of B. The region of Euler's method is like Thus it's only stable for some cases of stiff equation. When the negative \\(\\lambda\\) becomes smaller and get out of the region, it becomes not stable. Similarly, for Runge-Kutta Order Four method (explicit), applying test equation, we have \\[ w_{i + 1} = \\left(1 + H + \\frac{H^2}{2} + \\frac{H^3}{6} + \\frac{H^4}{24}\\right)w_i. \\] And the region is like Let's consider some implicit method. Say Euler's implicit method, we have \\[ w_{i + 1} = \\frac{1}{1 - H} w_i, \\] whose region is Thus it's A-stable . Also, implicit Trapezoidal method and 2 nd -order Runge-Kutta implict method are A-stable , both with the difference equation \\[ w_{i + 1} = \\frac{2 + H}{2 - H}w_i, \\] whose region is just right covers the left half-plane. Thus an important intuitive is that implicit method is more stable than explicit method in the stiff discussion .","title":"Chap 5"},{"location":"Mathematics_Basis/NA/Chap_5/#chapter-5-initial-value-problems-for-ordinary-differential-equations","text":"Initial-Value Problem (IVP) Basic IVP (one variable first order) Approximate the solution \\(y(t)\\) to a problem of the form \\[ \\frac{dy}{dt} = f(t, y), t \\in [a, b], \\] subject to an initial condition \\[ y(a) = \\alpha. \\] Higher-Order Systems of First-Order IVP Moreover, adding the number of unknowns, it becomes approximating \\(y_1(t), \\dots y_m(t)\\) to a problem of the form \\[ \\begin{aligned} \\frac{dy_1}{dt} &= f_1(t, y_1, y_2, \\dots, y_m), \\\\ \\frac{dy_2}{dt} &= f_2(t, y_1, y_2, \\dots, y_m), \\\\ & \\vdots \\\\ \\frac{dy_m}{dt} &= f_m(t, y_1, y_2, \\dots, y_m), \\end{aligned} \\] for \\(t \\in [a, b]\\) , subject to the initial conditions \\[ y_1(a) = \\alpha_1, y_2(a) = \\alpha_2, \\dots, y_m(a) = \\alpha_m. \\] Higher-Order IVP Or adding the number of order, it becomes m th-order IVP of the form \\[ y^{(m)} = f(t, y, y', y'', \\dots, y^{(m - 1)}), \\] for \\(t \\in [a, b]\\) , subject to the inital conditions \\[ y(a) = \\alpha_1, y'(a) = \\alpha_2, \\dots, y^{(m)}(a) = \\alpha_m. \\]","title":"Chapter 5 | Initial-Value Problems for Ordinary Differential Equations"},{"location":"Mathematics_Basis/NA/Chap_5/#general-idea","text":"Of all the method we introduce below, we can only approximate some points, but not the whole function \\(y(t)\\) . The approximation points are called mesh points . Moreover, we will only approximate the equally spaced points. Suppose we apporximate the values at \\(N\\) points on \\([a, b]\\) , then the mesh points are \\[ t_i = a + ih, \\] where \\(h = (b - a) / N\\) is the step size . To get the value between mesh points, we can use interpolation method. Since we know the derivative value \\(f(t, y)\\) at the mesh point, it's nice to use Hermit interpolation or cubic spline interpolation .","title":"General Idea"},{"location":"Mathematics_Basis/NA/Chap_5/#availability-and-uniqueness","text":"Before finding out the approximation, we need some conditions to guarantee its availability and uniqueness. Definition 5.0 \\(f(t, y)\\) is said to satisfy a Lipschitz condition in the variable \\(y\\) on a set \\(D \\in \\mathbb{R}^2\\) if \\(\\exists\\ L > 0\\) , s.t. \\[ |f(t, y_1) - f(t, y_2)| \\le L|y_1 - y_2|,\\ \\ \\forall\\ (t, y_1), (t, y_2) \\in D. \\] \\(L\\) is called a Lipschitz constant for \\(f\\) . Definition 5.1 A set \\(D \\in \\mathbb{R}^2\\) is said to be convex if \\[ \\forall\\ (t, y_1), (t, y_2) \\in D,\\ \\ \\forall\\ \\lambda \\in [0, 1],\\ \\ ((1 - \\lambda)t_1 + \\lambda t_2, (1 - \\lambda)y_1 + \\lambda y_2) \\in D. \\] Theorem 5.0 | Sufficient Condition \\(f(t, y)\\) is defined on a convex set \\(D \\in \\mathbb{R}^2\\) . If \\(\\exists\\ L > 0\\) s.t. \\[ \\left|\\frac{\\partial f}{\\partial y}(t, y)\\right| \\le L,\\ \\ \\forall\\ (t, y) \\in D, \\] then \\(f\\) satisfies a Lipschitz condition on \\(D\\) in the variable \\(y\\) with Lipschitz constatnt \\(L\\) . Theorem 5.1 | Unique Solution \\(f(t, y)\\) is continuous on \\(D = \\{(t, y) | a \\le t \\le b, -\\infty < y < \\infty\\}\\) ( \\(D\\) is convex). If \\(f\\) satisfies a Lipschitz condition on \\(D\\) in the variable \\(y\\) , then the IVP \\[ y'(t) = f(t, y),\\ \\ a \\le t \\le b,\\ \\ y(a) = \\alpha, \\] has a unique solution \\(y(t)\\) for \\(a \\le t \\le b\\) . Definition 5.2 The IVP \\[ \\frac{dy}{dt} = f(t, y),\\ a \\le t \\le b,\\ y(a) = \\alpha, \\] is said to be a well-posed problem if A unique solution \\(y(t)\\) exists; \\(\\forall\\ \\varepsilon > 0, \\exists\\ k(\\varepsilon) > 0\\) , s.t. \\(\\forall |\\varepsilon_0| < \\varepsilon\\) , and \\(|\\delta(t)|\\) is continuous with \\(|\\delta(t)| < \\varepsilon\\) on \\([a, b]\\) , a unique solution \\(z(t)\\) to the IVP \\[ \\frac{dz}{dt} = f(t, y) + \\delta(t),\\ \\ a \\le t \\le b,\\ \\ y(a) = \\alpha,\\ \\ \\text{ (perturbed problem)} \\] exists with \\[ |z(t) - y(t)| < k(\\varepsilon)\\varepsilon,\\ \\ \\forall\\ t \\in [a, b]. \\] Numerical methods will always be concerned with solving a perturbed problem since roundoff error is unavoidable. Thus we want the problem to be well-posed , which means the perturbation will not affect the result approxiamtion a lot. Theorem 5.2 \\(f(t, y)\\) is continuous on \\(D = \\{(t, y) | a \\le t \\le b, -\\infty < y < \\infty\\}\\) ( \\(D\\) is convex). If \\(f\\) satisfies a Lipschitz condition on \\(D\\) in the variable \\(y\\) , then the IVP \\[ y'(t) = f(t, y),\\ \\ a \\le t \\le b,\\ \\ y(a) = \\alpha, \\] is well-posed . Besides, we also need to discuss the sufficient condition of m th-order system of first-order IVP . Similarly, we have the definition of Lipschitz condition and it's sufficient for the uniqueness of the solution. Definition 5.3 \\(f(t, y_1, \\dots, y_m)\\) define on the convex set \\[ D = \\{(t, y_1, \\dots, y_m | a \\le t \\le b, -\\infty < u_i < \\infty, \\text{ for each } i = 1, 2, \\dots, m\\} \\] is said to satisfy Lipschitz condition on \\(D\\) in the variables \\(y_1, \\dots, y_m\\) if \\(\\exists\\ L > 0\\) , s.t \\[ \\begin{aligned} & \\forall\\ (t, y_1, \\dots, y_n), (t, z_1, \\dots, z_n) \\in D, \\\\ & |f(t, y_1, \\dots, y_m) - f(t, z_1, \\dots, z_m)| \\le L \\sum\\limits_{j = 1}^m |y_j - z_j|. \\end{aligned} \\] Theorem 5.3 if \\(f(t, y_1, \\dots, y_m)\\) satisfy Lipschitz condition, then the m th order systems of first-order IVP has a unique solution \\(y_1(t), \\dots, y_n(t)\\) .","title":"Availability and Uniqueness"},{"location":"Mathematics_Basis/NA/Chap_5/#eulers-method","text":"To measure the error of Taylor methods (Euler's method is Taylor's method of order 1), we first give the definition of local truncation error , which somehow measure the truncation error at a specified step. Definition 5.4 The difference method \\[ \\begin{aligned} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + h\\phi(t_i, w_i). \\end{aligned} \\] has the local truncation error \\[ \\tau_{i + 1}(h) = \\frac{y_{i + 1} - (y_i + h\\phi(t_i, y_i))}{h} = \\frac{y_{i + 1} - y_i}{h} - \\phi(t_i, y_i). \\]","title":"Euler's Method"},{"location":"Mathematics_Basis/NA/Chap_5/#forward-explicit-eulers-method","text":"We use Taylor's Theorem to derive Euler's method. Suppose \\(y(t)\\) is the unique solution of IVP, for each mesh points, we have \\[ y(t_{i + 1}) = y(t_i) + hy'(t_i) + \\frac{h^2}{2}y''(\\xi_i), \\xi_i \\in [t_i, t_{i + 1}], \\] and since \\(y'(t_i) = f(t_i, y(t_i))\\) , we have \\[ y(t_{i + 1}) = y(t_i) + hf.(t_i, y(t_i)) + \\frac{h^2}{2}y''(\\xi_i), \\xi_i \\in [t_i, t_{i + 1}]. \\] If \\(w_i \\approx y(t_i)\\) and we delete the remainder term, we get the (Explicit) Euler's method . (Explicit) Euler's Method \\[ \\begin{aligned} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + hf(t_i, w_i). \\text{(difference equations)} \\end{aligned} \\] The local truncation error of Euler's Method is \\[ \\tau_{i + 1}(h) = \\frac{hy'(t_i) + \\frac{h^2}{2}y''(\\xi_i)}{h} - f(t_i, y_i) = \\frac{h}{2}y''(\\xi_i) = O(h). \\] Theorem 5.4 The error bound of Euler's method is \\[ |y(t_i) - w_i| \\le \\frac{hM}{2L}\\left[e^{L(t_i - a)} - 1\\right], \\] where \\(M\\) is the upper bound of \\(|y''(t)|\\) . Although it has an exponential term (which is not such an accurate error bound), the most important intuitive is that the error bound is proportional to \\(h\\) . If we consider the roundoff error, we have the following theorem. Theorem 5.5 Suppose there exists roundoff error and \\(u_i = w_i + \\delta_i\\) , \\(i = 0, 1, \\dots\\) , then \\[ |y(t_i) - u_i| \\le \\frac{1}{L}\\left(\\frac{hM}{2} + \\frac{\\delta}{h}\\right)\\left[e^{L(t_i - a)} - 1\\right] + |\\delta_0|e^{L(t_i - a)}, \\] where \\(\\delta\\) is the upper bound of \\(\\delta_i\\) . This comes to a different case, we can't make \\(h\\) too small, and the optimal choice of \\(h\\) is \\[ h = \\sqrt{\\frac{2\\delta}{M}}. \\]","title":"Forward / Explicit Euler's Method"},{"location":"Mathematics_Basis/NA/Chap_5/#backward-implicit-eulers-method","text":"Inversely, if we use Taylor's Theorem in the following way, \\[ y(t_i) = y(t_{i + 1}) - hy'(t_{i + 1}) + \\frac{h^2}{2}y''(\\xi_{i + 1}), \\xi_i \\in [t_i, t_{i + 1}]. \\] Thus we get the Implicit Euler's Method , Implicit Euler's Method \\[ \\begin{aligned} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + hf(t_{i + 1}, w_{i + 1}). \\end{aligned} \\] Since \\(w_{i + 1}\\) is on the both side, we may use some methods in Chapter 2 to solve out the equation. The local truncation error of implicit Euler's method is \\[ \\begin{aligned} \\tau_{i + 1}(h) &= \\frac{hy'(t_i) + \\frac{h^2}{2}y''(\\xi_i)}{h} - f(t_{i + 1}, y_{i + 1}) \\\\ &= y'(t_i) - y'(t_{i + 1}) + \\frac{h}{2}y''(\\xi_i) \\\\ &= y'(t_i) - y'(t_i) - hy''(\\xi'_i) + \\frac{h}{2}y''(\\xi_i) \\\\ &= -\\frac{h}{2}y''(\\eta_i) = O(h). \\end{aligned} \\]","title":"Backward/ Implicit Euler's Method"},{"location":"Mathematics_Basis/NA/Chap_5/#trapezoidal-method","text":"Notice the local truncation errors of two Euler's method are \\[ \\tau_{i + 1}(h) = \\frac{h}{2}y''(\\xi'),\\ \\ \\tau_{i + 1}(h) = -\\frac{h}{2}y''(\\eta'). \\] If we combine these two, we then obtain a method of \\(O(h^2)\\) local truncation error, which is called Trapezoidal Method . Trapezoidal Method \\[ \\begin{aligned} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + \\frac{h}{2}[f(t_i, w_i) + f(t_{i + 1}, w_{i + 1})]. \\end{aligned} \\]","title":"Trapezoidal Method"},{"location":"Mathematics_Basis/NA/Chap_5/#higher-order-taylor-method","text":"In Euler's methods, we only employ first order Taylor polynomials. If we expand it more to n th Taylor polynomials, we have \\[ \\begin{aligned} y(t_{i + 1}) &= y(t_i) + hy'(t_i) + \\frac{h^2}{2}y''(t_i) + \\cdots + \\frac{h^n}{n!}y^{(n)}(t_i) + \\frac{h^{n + 1}}{(n + 1)!} y^{(n + 1)}(\\xi_i), \\\\ & \\text{ where } \\xi_i \\in [t_i, t_{i + 1}], \\end{aligned} \\] and since \\(y^{(k)}(t) = f^{(k - 1)}(t, y(t))\\) , we have \\[ \\begin{aligned} y(t_{i + 1}) &= y(t_i) + hf(t_i, y(t_i)) + \\frac{h^2}{2}f'(t_i, y(t_i)) + \\cdots + \\frac{h^n}{n!}f^{(n - 1)}(t_i, y(t_i)) \\\\ & \\ \\ \\ \\ + \\frac{h^{n + 1}}{(n + 1)!} f^{(n)}(\\xi_i, y(\\xi_i)). \\end{aligned} \\] Taylor Method of order n \\[ \\begin{aligned} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + hT^{(n)}(t_i, w_i), \\end{aligned} \\] where \\[ T^{(n)}(t_i, w_i) = f(t_i, w_i) + \\frac{h}{2}f'(t_i, w_i) + \\cdots + \\frac{h^{n - 1}}{n!}f^{(n - 1)}(t_i, w_i). \\] Theorem 5.6 The local truncation error of Taylor method of order \\(n\\) is \\(O(h^n)\\) .","title":"Higher-Order Taylor Method"},{"location":"Mathematics_Basis/NA/Chap_5/#runge-kutta-method","text":"Although Taylor method gives more accuracy as \\(n\\) increase, but it's not an easy job to calculate n th derivative of \\(f\\) . Here we introduce a new method called Runge-Kutta Method , which has low local truncation error and doesn't need to compute derivatives. It's based on Taylor's Theorem in two variables . Theorem 5.7 | Taylor's Theorem in two variables (Recap) \\(f(x, y)\\) has continous \\(n + 1\\) partial derivatives on the neigbourhood of \\(A(x_0, y_0)\\) , denoted by \\(U(A)\\) , then \\(\\forall\\ (x, y) \\in U(A)\\) , denote \\(\\Delta x = x - x_0\\) , \\(\\Delta y = y - y_0\\) , \\(\\exists\\ \\theta \\in (0, 1)\\) , s.t. \\[ f(x, y) = P_n(x, y) + R_n(x, y), \\] where \\[ \\small \\begin{aligned} P_n(x, y) &= f(x_0, y_0) + \\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)f(x_0, y_0) + \\frac{1}{2!}\\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)^2f(x_0, y_0) + \\\\ & \\ \\ \\ \\ \\ \\cdots + \\frac{1}{n!}\\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)^nf(x_0, y_0), \\end{aligned} \\] \\[ \\small R_n(x, y) = \\frac{1}{(n + 1)!}\\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)^{n + 1}f(x_0 + \\theta \\Delta x, y_0 + \\theta \\Delta y). \\] and \\[ \\small \\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)^{k}f(x, y) = \\sum\\limits_{i = 0}^k \\binom{k}{i} (\\Delta x)^i (\\Delta y)^{k - i} \\frac{\\partial^k f}{\\partial x^i \\partial y^{k - i}}(x, y). \\] Derivation of Midpoint Method To equate \\[ T^{(2)}(t, y) = f(t, y) + \\frac{h}{2}f'(t, y) = f(t, y) + \\frac{h}{2}\\frac{\\partial f}{\\partial t}(t,y) + \\frac{h}{2}\\frac{\\partial f}{\\partial y}(t,y)\\cdot f(t, y), \\] with \\[ \\begin{aligned} a_1f(t + \\alpha_1, y + \\beta_1) &= a_1f(t, y) + a_1 \\alpha_1 \\frac{\\partial f}{\\partial t}(t, y) \\\\ & \\ \\ \\ \\ + a_1 \\beta_1 \\frac{\\partial f}{\\partial y}(t, y) + a_1 \\cdot R_1(t + \\alpha_1, y + \\beta_1), \\end{aligned} \\] except for the last residual term \\(R_1\\) , we have \\[ a_1 = 1,\\ \\ \\alpha_1 = \\frac{h}{2},\\ \\ beta_1 = \\frac{h}{2}f(t, y). \\] Thus \\[ T^{(2)}(t, y) = f\\left(t + \\frac{h}{2}, y + \\frac{h}{2}f(t, y)\\right) - R_1. \\] Since the term \\(R_1\\) is only concerned with the 2 nd -order partial derivatives of \\(f\\) , if they are bounded, then \\(R_1\\) is \\(O(h^2)\\) , the order of the local truncation error. Replace \\(T^{(2)}(t,y)\\) by the formula above in the Taylor method of order 2, we have the Midpoint Method . Midpoint Method \\[ \\begin{aligned} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + hf\\left(t_i + \\frac{h}{2}, w_i + \\frac{h}{2}f(t_i, w_i)\\right). \\end{aligned} \\] Derivation of Modified Euler Method and Heun's Method Similarly, approximate \\[ T^{(3)}(t, y) = f(t, y) + \\frac{h}{2}f'(t, y) + \\frac{h^2}{6}f''(t,y), \\] with \\[ a_1f(t,y) + a_2f(t + \\alpha_2, y + \\delta_2f(t,y)). \\] But it doesn't have sufficient equation to determine the 4 unknowns \\(a_1, a_2, \\alpha_2, \\delta_2\\) above. Instead we have \\[ a_1 + a_2 = 1, \\ \\alpha_2 = \\delta_2\\ \\overset{\\Delta}{=\\!=}\\ ph. \\] Thus a number of \\(O(h^2)\\) methods can be derived, and they are generally in the form \\[ \\begin{aligned} w_{i + 1} &= w_i + h(a_1 K_1 + a_2 K_2) \\\\ K_1 &= f(t_i, w_i), \\\\ K_2 &= f(t_i + ph, y + ph K_1). \\end{aligned} \\] The most important are the following two. Modified Euler Method \\[ \\begin{aligned} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + \\frac{h}{2}[f(t_i, w_i) + f(t_i + h, w_i + hf(t_i, w_i))]. \\end{aligned} \\] Heun's Method \\[ \\begin{aligned} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + \\frac{h}{4}\\left[f(t_i, w_i) + 3f\\left(t_i + \\frac23 h, w_i + \\frac23 hf(t_i, w_i)\\right)\\right]. \\end{aligned} \\] In a similar manner, approximate \\(T^{(n)}(t, y)\\) with \\[ \\lambda_1 K_1 + \\lambda_2 K_2 + \\cdots + \\lambda_m K_m, \\] where \\[ \\begin{aligned} K_1 &= f(t, y), \\\\ K_2 &= f(t + \\alpha_2 h, y + \\beta_{21} h K_1), \\\\ & \\ \\ \\vdots \\\\ K_m &= f(t + \\alpha_m h, y + \\beta_{m1} h K_1 + \\beta_{m, m - 1} hK_{m - 1}), \\end{aligned} \\] with different \\(m\\) , we can derive quite a lot of Runge-Kutta methods. The most common Runge-Kutta method is given below. Runge-Kutta Order Four \\[ \\begin{aligned} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + \\frac{h}{6}(K_1 + 2K_2 + 2K_3 + K_4), \\\\ K_1 &= f(t_i, w_i), \\\\ K_2 &= f\\left(t_i + \\frac{h}{2}, w_i + \\frac{h}{2}K_1\\right), \\\\ K_3 &= f\\left(t_i + \\frac{h}{2}, w_i + \\frac{h}{2}K_2\\right), \\\\ K_4 &= f\\left(t_i + h, w_i + h K_3\\right). \\end{aligned} \\] The local trancation error of Runge-Kutta are given below, where \\(n\\) is the number of evaluations per step. Property Compared to Taylor's method, evaluate \\(f(t, y)\\) the same times , Runge-Kutta gives the lowest error.","title":"Runge-Kutta Method"},{"location":"Mathematics_Basis/NA/Chap_5/#multistep-method","text":"In the previous section, the difference equation only relates \\(w_{i + 1}\\) with \\(w_{i}\\) . Multistep Method discuss the situation of relating more term of previous predicted \\(w\\) . Definition 5.5 An m -step multistep method for solving the IVP \\[ y'(t) = f(t, y),\\ \\ a \\le t \\le b,\\ \\ y(a) = \\alpha, \\] has a difference equation \\[ \\begin{aligned} w_{i + 1} &= a_{m - 1}w_i + a_{m - 2}w_{i - 1} + \\cdots a_0 w_{i + 1 - m} + h[b_mf(t_{i + 1}, w_{i + 1}) \\\\ & \\ \\ \\ \\ + b_{m - 1}f(t_i, w_i) + \\cdots + b_0 f(t_{i + 1 - m}, w_{i + 1 - m})], \\end{aligned} \\] with the starting values \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ \\dots,\\ \\ w_{m - 1} = \\alpha_{m - 1}. \\] If \\(b_m = 0\\) , it's called explicit , or open . Otherwise, it's called implicit , or closed . Similarly, we first define local truncation error for multistep method to measure the error at a specified step. Definition 5.6 For a m -step multistep method, the local truncation error at \\((i + 1)\\) th step is \\[ \\begin{aligned} \\tau_{i + 1}(h) &= \\frac{y(t_{i + 1}) - a_{m - 1}y(t_i) - \\cdots - a_0 y(t_{i + 1 - m})}{h} \\\\ & \\ \\ \\ \\ - b_{m - 1}f(t_i, y(t_{i + 1})) + \\cdots + b_0 f(t_{i + 1 - m}, y(t_{i + 1 - m}))]. \\end{aligned} \\]","title":"Multistep Method"},{"location":"Mathematics_Basis/NA/Chap_5/#adams-method","text":"In this section, we only consider the case of \\(a_{m - 1} = 1\\) and \\(a_i = 0\\) , \\(i \\ne m - 1\\) . To derive the Adams method, we start from a simple formula. \\[ y(t_{i + 1}) - y(t_i) = \\int_{t_i}^{t_{i + 1}}y'(t)dt = \\int_{t_i}^{t_{i + 1}}f(t, y(t))dt. \\] i.e. \\[ y(t_{i + 1}) = y(t_i) + \\int_{t_i}^{t_{i + 1}}f(t, y(t))dt. \\] We cannot integrate \\(f(t, y(t))\\) without knowing \\(y(t)\\) . Instead we use an interpolating polynomial \\(P(t)\\) by points \\((t_0, w_0),\\ \\dots,\\ (t_i, w_i)\\) to approximate \\(f(t, y)\\) . Thus we approximate \\(y(t_{i + 1})\\) by \\[ y(t_{i + 1}) \\approx w_i + \\int_{t_i}^{t_{i + 1}} P(t) dt. \\] For convenience, we use Newton backward-difference formula to represent \\(P(t)\\) .","title":"Adams Method"},{"location":"Mathematics_Basis/NA/Chap_5/#adams-bashforth-m-step-explicit-method","text":"To derive an Adam-Bashforth explicit m -step technique , we form the interpolating polynomial \\(P_{m - 1}(t)\\) by \\[ (t_i, f(t_i, y(t_i))),\\ \\ \\dots,\\ \\ (t_{i + 1 - m}, f(t_{i + 1 - m}, y(t_{i + 1 - m}))). \\] Then suppose \\(t = t_i + sh\\) , with \\(dt = hds\\) , we have \\[ \\begin{aligned} \\int_{t_i}^{t_{i + 1}} f(t, y(t)) dt &= \\int_{t_i}^{t_{i + 1}} \\sum\\limits_{k = 0}^{m - 1}(-1)^k \\binom{-s}{k}\\nabla^k f(t_i, y(t_i))dt \\\\ & \\ \\ \\ \\ + \\int_{t_i}^{t_{i + 1}} \\frac{f^{(m)}(\\xi_i, y(\\xi_i))}{m!}\\prod\\limits_{k = 0}^{m - 1}(t - t_{i - k})dt \\\\ \\left(\\text{Note that } dt = hds\\right) &= h\\sum\\limits_{k = 0}^{m - 1}\\nabla^k (-1)^k \\int_0^1 \\binom{-s}{k}ds \\\\ & \\ \\ \\ \\ + h^{m + 1}\\int_0^1 \\binom{-s}{m}f^{(m)}(\\xi_i, y(\\xi_i))ds. \\\\ \\left(\\substack{\\text{Weighted Mean Value} \\\\ \\text{Theorem for Integral}}\\right) &= h\\sum\\limits_{k = 0}^{m - 1}\\nabla^k (-1)^k \\int_0^1 \\binom{-s}{k}ds \\\\ & \\ \\ \\ \\ + h^{m + 1}f^{(m)}(\\mu_i, y(\\mu_i))\\int_0^1 \\binom{-s}{m}ds. \\end{aligned} \\] Since for a given \\(k\\) , we have \\(k\\) \\(0\\) \\(1\\) \\(2\\) \\(3\\) \\(4\\) \\((-1)^k\\int_0^1 \\binom{-s}{k}ds\\) \\(1\\) \\(\\frac12\\) \\(\\frac{5}{12}\\) \\(\\frac38\\) \\(\\frac{251}{720}\\) Thus \\[ \\begin{aligned} y(t_{i + 1}) &= y(t_i) + h\\left[f(t_i, y(t_i)) + \\frac12 \\nabla f(t_i, y(t_i)) + \\frac{5}{12} \\nabla^2 f(t_i, y(t_i)) + \\cdots\\right] \\\\ & \\ \\ \\ \\ + h^{m + 1}f^{(m)}(\\mu_i, y(\\mu_i))\\int_0^1 \\binom{-s}{m}ds. \\end{aligned} \\] Adams-Bashforth m -Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ \\dots,\\ \\ w_{m - 1} = \\alpha_{m - 1}, \\] \\[ w_{i + 1} = w_i + h\\sum\\limits_{k = 0}^{m - 1}\\nabla^k f(t_i, y(t_i)) (-1)^k \\int_0^1 \\binom{-s}{k}ds, \\] with local truncation error \\[ \\tau_{i + 1}(h) = h^{m}y^{(m + 1)}(\\mu_i)(-1)^m\\int_0^1 \\binom{-s}{m}ds. \\] Adams-Bashforth Two-Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1, \\] \\[ w_{i + 1} = w_i + \\frac{h}{2}[3f(t_i, w_i) - f(t_{i - 1}, w_{i - 1})], \\] with local truncation error \\[ \\tau_{i + 1}(h) = \\frac{5}{12}y'''(\\mu_i)h^2. \\] Adams-Bashforth Three-Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2, \\] \\[ w_{i + 1} = w_i + \\frac{h}{12}[23f(t_i, w_i) - 16f(t_{i - 1}, w_{i - 1}) + 5f(t_{i - 2}, w_{i - 2})], \\] with local truncation error \\[ \\tau_{i + 1}(h) = \\frac{3}{8}y^{(4)}(\\mu_i)h^3. \\] Adams-Bashforth Four-Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2,\\ \\ w_3 = \\alpha_3, \\] \\[ \\begin{aligned} w_{i + 1} = w_i + \\frac{h}{24}&[55f(t_i, w_i) - 59f(t_{i - 1}, w_{i - 1}) \\\\ & + 37f(t_{i - 2}, w_{i - 2}) - 9f(t_{i - 3}, w_{i - 3})], \\end{aligned} \\] with local truncation error \\[ \\tau_{i + 1}(h) = \\frac{251}{720}y^{(5)}(\\mu_i)h^4. \\]","title":"Adams-Bashforth m-Step Explicit Method"},{"location":"Mathematics_Basis/NA/Chap_5/#adams-moulton-m-step-implicit-method","text":"For implicit case, we add \\((t_{i + 1}, f(t_{i + 1}, y(t_{i + 1})))\\) as one more interpolating node to construct the interpolating polynomials. Let \\(t = t_{i + 1} + sh\\) and there is \\(m\\) points. Similarly we have the conclusions below. Adams-Moulton m -Step Implicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ \\dots,\\ \\ w_{m - 1} = \\alpha_{m - 1}, \\] \\[ w_{i + 1} = w_i + h\\sum\\limits_{k = 0}^{m}\\nabla^k f(t_{i + 1}, y(t_{i + 1})) (-1)^k \\int_{-1}^0 \\binom{-s}{k}ds, \\] with local truncation error \\[ \\tau_{i + 1}(h) = h^{m + 1}y^{(m + 2)}(\\mu_i)(-1)^{m + 1}\\int_{-1}^0 \\binom{-s}{m + 1}ds. \\] Adams-Moulton Two-Step Implicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1, \\] \\[ w_{i + 1} = w_i + \\frac{h}{12}[5f(t_{i + 1}, w_{i + 1}) + 8f(t_i, w_i) - f(t_{i - 1}, w_{i - 1})], \\] with local truncation error \\[ \\tau_{i + 1}(h) = -\\frac{1}{24}y^{(4)}(\\mu_i)h^3. \\] Adams-Moulton Three-Step Implicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2, \\] \\[ w_{i + 1} = w_i + \\frac{h}{24}[9f(t_{i + 1}, w_{i + 1}) + 19f(t_i, w_i) - 5f(t_{i - 1}, w_{i - 1}) + f(t_{i - 2}, w_{i - 2})], \\] with local truncation error \\[ \\tau_{i + 1}(h) = -\\frac{19}{720}y^{(5)}(\\mu_i)h^4. \\] Adams-Moulton Four-Step Implicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2,\\ \\ w_3 = \\alpha_3, \\] \\[ \\begin{aligned} w_{i + 1} = w_i + \\frac{h}{720}&[251f(t_{i + 1}, w_{i + 1}) + 646f(t_i, w_i) - 264f(t_{i - 1}, w_{i - 1}) \\\\ & + 106f(t_{i - 2}, w_{i - 2}) - 19f(t_{i - 3}, w_{i - 3})], \\end{aligned} \\] with local truncation error \\[ \\tau_{i + 1}(h) = -\\frac{3}{160}y^{(6)}(\\mu_i)h^5. \\] Note Comparing an m -step Adams-Bashforth explicit method and (m - 1) -step Adams-Moulton implicit method, they both involve m evaluation of \\(f\\) per step, and both have the local truncation error of the term \\(ky^{(m + 1)}(\\mu_i)h^m\\) . And the latter implicit method has the smaller coefficient \\(k\\) . This leads to greater stability and smaller roundoff errors for implicit methods.","title":"Adams-Moulton m-Step Implicit Method"},{"location":"Mathematics_Basis/NA/Chap_5/#predictor-corrector-method","text":"Implicit method has some advantages but it's hard to calculate. We could use some iterative methods introduced in Chapter 2 to solve it but it complicates the process considerably. In practice, since explicit method is easy to calculate, we combine the explicit and implicit method to predictor-corrector method . Predictor-Corrector Method Step.1 Compute first \\(m\\) initial values by Runge-Kutta method. Step.2 Predict by Adams-Bashforth explicit method. Step.3 Correct by Adams-Moulton implicit method. NOTE: All the formulae used in the three steps have the same order of truncation error. Example Take \\(m = 4\\) which is the most common case as exmaple. Step.1 From initial value \\(w_0 = \\alpha\\) , we use Runge-Kutta method of order four to derive \\(w_1\\) , \\(w_2\\) and \\(w_3\\) . Set \\(i = 3\\) . Step.2 Use four-step Adams-Bashforth explicit method, we have \\[ \\small w_{i + 1}^{(0)} = w_i + \\frac{h}{24}[55f(t_i, w_i) - 59f(t_{i - 1}, w_{i - 1}) + 37f(t_{i - 2}, w_{i - 2}) - 9f(t_{i - 3}, w_{i - 3})], \\] Step.3 Use three-step Adams-Moulton \\[ \\small w_{i + 1}^{(1)} = w_i + \\frac{h}{24}[9f(t_{i + 1}, w_{i + 1}^{(1)}) + 19f(t_i, w_i) - 5f(t_{i - 1}, w_{i - 1}) + f(t_{i - 2}, w_{i - 2})], \\] Then we have two options \\(i = i + 1\\) , and go back to Step. 2 , to approximate the next point. Or, for higher accuracy, we can repeat Step.3 iteratively by \\[ \\small w_{i + 1}^{(k + 1)} = w_i + \\frac{h}{24}[9f(t_{i + 1}, w_{i + 1}^{(k)}) + 19f(t_i, w_i) - 5f(t_{i - 1}, w_{i - 1}) + f(t_{i - 2}, w_{i - 2})]. \\]","title":"Predictor-Corrector Method"},{"location":"Mathematics_Basis/NA/Chap_5/#other-methods","text":"If we derive the multistep method by Taylor expansion , we can have more methods. We take an example to show it. Suppose we want to derive a difference equation in the form \\[ w_{i + 1} = a_0 w_{i - 1} + h(b_2 f(t_{i + 1}, w_{i + 1}) + b_1 f(t_i, w_i) + b_0 f(t_{i - 1}, w_{i - 1})). \\] We use \\(y_i\\) and \\(f_i\\) to denote \\(y(t_i)\\) and \\(f(t_i, y_i)\\) respectively. Expand \\(y_{i - 1}, f_{i + 1}, f_{i}, f_{i - 1}\\) , we have \\[ \\begin{aligned} y_{i - 1} &= y_i - hy_i' + \\frac12 h^2 y_i'' - \\frac16h^3y_i''' + O(h^4), \\\\ f_{i + 1} &= y_i' + hy_i'' + \\frac12 h^2 y_i''' + O(h^3), \\\\ f_i &= y_i', \\\\ f_{i - 1} &= y_i' - hy_i'' + \\frac12 h^2 y_i''' + O(h^3), \\\\ \\end{aligned} \\] and note that \\[ y_{i + 1} = y_i + hy_i' + \\frac12 h^2 y_i'' + \\frac16 h^3 y_i'''+ O(h^4). \\] Equate them with \\[ y_{i + 1} = a_0 y_{i - 1} + h(b_2 f_{i + 1} + b_1 f_i + b_0 f_{i - 1}). \\] We can solve out \\[ a_0 = 1,\\ \\ b_2 = \\frac13,\\ \\ b_1 = \\frac43,\\ \\ b_0 = \\frac13. \\] Thus \\[ w_{i + 1} = w_{i - 1} + \\frac{h}{3}(f(t_{i + 1}, w_{i + 1}) + 4f(t_i, w_i) + f(t_{i - 1}, w_{i - 1})). \\] Simpson Implicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1, \\\\ w_{i + 1} = w_{i - 1} + \\frac{h}{3}(f(t_{i + 1}, w_{i + 1}) + 4f(t_i, w_i) + f(t_{i - 1}, w_{i - 1})), \\] with local truncation error \\[ \\tau_{i + 1}(h) = \\frac{h^4}{90}y^{(5)}(\\xi_i). \\] Note that it's an implicit method, and the most commonly used corresponding predictor is Milne's Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2 \\\\ w_{i + 1} = w_{i - 3} + \\frac{4h}{3}(2f(t_i, w_i) + - f(t_{i - 1}, w_{i - 1}) + 2f(t_{i - 2}, w_{i - 2})), \\] with local truncation error \\[ \\tau_{i + 1}(h) = \\frac{14}{90}h^4 y^{(5)}(\\xi_i). \\] which can also derive by Taylor expansion in the similar manner.","title":"Other Methods"},{"location":"Mathematics_Basis/NA/Chap_5/#higher-order-systems-of-first-order-ivp","text":"Actually it just vectorize the method that we've mentioned above. Take Runge-Kutta Order Four as example. Runge-Kutta Method for Systems of Differential Equations \\[ \\begin{aligned} w_{1, 0} &= \\alpha_1, w_{2, 0} = \\alpha_2, \\dots, w_{m, 0} = \\alpha_m. \\\\ w_{i, j + 1} &= w_{i, j} + \\frac{h}{6}(K_{1, i} + 2K_{2, i} + 2K_{3, i} + K_{4, i}), \\\\ K_{1, i} &= f_i(t_j, w_{1, j}, w_{2, j}, \\dots, w_{m, j}), \\\\ K_{2, i} &= f_i\\left(t_j + \\frac{h}{2}, w_{1, j} + \\frac{h}{2}K_{1, 1}, w_{2, j} + \\frac{h}{2}K_{1, 2}, \\dots, w_{m, j} + \\frac{h}{2}K_{1, m}\\right), \\\\ K_{3, i} &= f_i\\left(t_j + \\frac{h}{2}, w_{1, j} + \\frac{h}{2}K_{2, 1}, w_{2, j} + \\frac{h}{2}K_{2, 2}, \\dots, w_{m, j} + \\frac{h}{2}K_{2, m}\\right), \\\\ K_{4, i} &= f_i\\left(t_j + h, w_{1, j} + h K_{3, 1}, w_{2, j} + h K_{3, 2}, \\dots, w_{m, j} + h K_{3. m} \\right). \\end{aligned} \\]","title":"Higher-Order Systems of First-Order IVP"},{"location":"Mathematics_Basis/NA/Chap_5/#higher-order-ivp","text":"We can deal with higher-order IVP the same as higher-order systems of first-order IVP. The only transformation we need to do is to let \\(u_1(t) = y(t)\\) , \\(u_2(t) = y'(t)\\) , \\(\\dots\\) , and \\(u_m(t) = y^{(m - 1)}(t)\\) . This produces the first-order system \\[ \\begin{aligned} \\frac{du_1}{dt} &= \\frac{dy}{dt} = u_2, \\\\ \\frac{du_2}{dt} &= \\frac{dy'}{dt} = u_3, \\\\ & \\vdots \\\\ \\frac{du_{m - 1}}{dt} &= \\frac{dy^{(m - 2)}}{dt} = u_m, \\\\ \\frac{du_m}{dt} &= \\frac{dy^{(m - 1)}}{dt} = y^{(m)} = f(t, y', y'', \\dots, y^{(m - 1)}) = f(t, u_1, \\dots, u_m), \\end{aligned} \\] with initial conditions \\[ u_1(a) = y(a) = \\alpha_1,\\ \\ u_2(a) = y'(a) = \\alpha_2,\\ \\ \\dots,\\ \\ u_m(a) = y^{(m - 1)}(a) = \\alpha_m. \\]","title":"Higher-Order IVP"},{"location":"Mathematics_Basis/NA/Chap_5/#stability","text":"Definition 5.7 A one-step difference-equation method with local truncation error \\(\\tau_i(h)\\) is consistent with the differential equation it approximates if \\[ \\lim\\limits_{h \\rightarrow 0} \\max\\limits_{1 \\le i \\le N}|\\tau_i(h)| = 0. \\] For m -step multistep methods, it's also required that \\[ \\lim\\limits_{h \\rightarrow 0} |\\alpha_i - y_i| = 0,\\ \\ i = 1, 2, \\dots, m - 1, \\] since at most cases, these \\(\\alpha_i\\) are derived by one-step methods. Definition 5.8 A one-step / multistep difference-equation method is convergent w.r.t the differential equation it approximates if \\[ \\lim\\limits_{h \\rightarrow 0} \\max\\limits_{1 \\le i \\le N}|w_i - y(t_i)| = 0. \\] Definition 5.9 A stable method is one whose results depent continously on the initial data, in the sense that small changes or perturbations in the intial conditions produce correspondingly small changes in the subsequent approximations.","title":"Stability"},{"location":"Mathematics_Basis/NA/Chap_5/#one-step-method","text":"It's relatively natural to think the stability is somewhat analogous to the discussion of well-posed , and thus it's not surprising Lipschitz condition appears. The following theorem gives the relation among consistency , convergence and stability . Theorem 5.8 Suppose the IVP is approximated by a one-step method in the form \\[ \\begin{aligned} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + h \\phi(t_i, w_i, h). \\end{aligned} \\] If \\(\\exists\\ h_0 > 0\\) and $ \\phi(t, w, h)$ is continuous and satisfies a Lipschitz condition in the varaible \\(w\\) with Lipschitz constant \\(L\\) on the set \\[ D = \\{(t, w, h) | a \\le t \\le b, -\\infty < w < \\infty, 0 \\le h \\le h_0\\}. \\] Then The method is stable . The difference method is convergent iff it's consistent , which is equivalent to \\[ \\phi(t, y, 0) = f(t, y),\\ \\ t \\in [a, b]. \\] (Relation between Global Error and Local Truncation Error) If \\(\\exists \\tau(h)\\) s.t. \\(|\\tau_i(h)| \\le \\tau(h)\\) whenever \\(0 \\le h \\le h_0\\) , then \\[ |y(t_i) - w_i| \\le \\frac{\\tau(h)}{L}e^{L(t_i - a)}. \\]","title":"One Step Method"},{"location":"Mathematics_Basis/NA/Chap_5/#multistep-method_1","text":"Theorem 5.9 | Relation between Global Error and Local Truncation Error Suppose the IVP is approximated by a predictor-corrector method with an m -step Adams-Bashforth predictor equation \\[ w_{i + 1} = w_i + h[b_{m - 1}f(t_i, w_i) + \\cdots + b_0f(t_{i + 1 - m}, w_{i + 1 - m})], \\] with local truncation error \\(\\tau_{i + 1}(h)\\) , and an (m - 1) -step Adams-Moulton corrector eqation \\[ w_{i + 1} = w_i + h[\\tilde b_{m - 1}f(t_i, w_i) + \\cdots + \\tilde b_0f(t_{i + 1 - m}, w_{i + 1 - m})], \\] with local truncation error \\(\\tilde \\tau_{i + 1}(h)\\) . In addition, suppose \\(f(t, y)\\) and \\(f_y(t, y)\\) are continous on \\(D = \\{(t, y) | a \\le t \\le b, -\\infty < y < \\infty\\}\\) and \\(f_y\\) is bounded. Then the local truncation error \\(\\sigma_{i + 1}(h)\\) of the predictor-corrector method is \\[ \\small \\sigma_{i + 1}(h) = \\tilde \\tau_{i + 1}(h) + \\tau_{i + 1}(h) \\tilde b_{m - 1} \\frac{\\partial f}{\\partial y}(t_{i + 1}, \\theta_{i + 1}),\\ \\ \\text{ for some } \\theta_{i + 1} \\in (0, h\\tau_{i + 1}(h)). \\] Moreover, \\(\\exists\\ k_1, k_2\\) , s.t. \\[ |w_i - y(t_i)| \\le \\left[\\max\\limits_{0 \\le j \\le m - 1}|w_j - y(t_j)| + k_1\\sigma(h) \\right] e^{k_2(t_i - a)}, \\] where \\(\\sigma(h) = \\max\\limits_{m \\le j \\le N}|\\sigma_j(h)|\\) . For the difference equation of multistep method, we first introduce characteristc polynomial associated with the method, given by \\[ P(\\lambda) = \\lambda^m - a_{m - 1}\\lambda^{m - 1} - a_{m - 2}\\lambda^{m - 2} - \\cdots - a_1 \\lambda - a_0. \\] Definition 5.10 Suppose the roots of the characteristic equation \\[ P(\\lambda) = \\lambda^m - a_{m - 1}\\lambda^{m - 1} - a_{m - 2}\\lambda^{m - 2} - \\cdots - a_1 \\lambda - a_0 = 0 \\] associated with the multistep difference method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ \\dots,\\ \\ w_{m - 1} = \\alpha_{m - 1}, \\\\ w_{i + 1} = a_{m - 1}w_i + a_{m - 2}w_{i - 1} + \\cdots a_0 w_{i + 1 - m} + hF(t_i, h, w_{i + 1}, w_i \\dots, w_{i + 1 - m}). \\] If \\(|\\lambda_i| \\le 1\\) , for each \\(i = 1, 2, \\dots, m\\) and all roots with modulus value \\(1\\) are simple roots, then the difference method is said to satisfy the root condition . Methods that satisfy the root condition and have \\(\\lambda = 1\\) as the only root of the characteristic equation of magnitude one are called strongly stable . Methods that satisfy the root condition and have more than one disctinct root with magnitude one are called weakly stable . Methods that do not satisfy the root condition are called unstable . Theorem 5.10 A multistep method of the form \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ \\dots,\\ \\ w_{m - 1} = \\alpha_{m - 1}, \\\\ w_{i + 1} = a_{m - 1}w_i + a_{m - 2}w_{i - 1} + \\cdots a_0 w_{i + 1 - m} + hF(t_i, h, w_{i + 1}, w_i \\dots, w_{i + 1 - m}). \\] is stable iff it satisfies the root condition. Moreover, if the difference method is consistent with the differential equation, then the method is stable iff it is convergent .","title":"Multistep Method"},{"location":"Mathematics_Basis/NA/Chap_5/#stiff-equations","text":"However, a special type of solution is not easy to deal with, when the magnitude of derivative increases but the solution does not. They are call stiff equations . Stiff differential eqaution are characterized as those whose exact solution has a term of the form \\(e^{-ct}\\) , where \\(c\\) is a large positive constant. This is called the transient solution. Actaully there is another important part of solution called steady-state solution. Let's first define test equation to examine what happen with the stiff cases. Definition 5.11 A test equation is the IVP \\[ y' = \\lambda y,\\ \\ y(0) = \\alpha,\\ \\ \\text{ where } \\text{Re}(\\lambda) < 0. \\] The solution is obviously \\(y(t) = \\alpha e^{\\lambda t}\\) , which is the transient solution. At this case, the steady-state solution is zero, thus the approximation characteristics of a method are easy to determine. Example of Euler's Method Take Euler's method as an example, and denote \\(H = h\\lambda\\) . We have \\[ w_0 = \\alpha, \\\\ w_{i + 1} = w_i + h(\\lambda w_i) = (1 + h\\lambda)w_i = (1 + H)w_i, \\] so \\[ w_{i + 1} = (1 + H)^{i + 1}w_0 = (1 + H)^{i + 1}\\alpha. \\] The absolute error is \\[ |y(t_i) - w_i| = |(e^H)^i - (1 + H)^i||\\alpha|. \\] When \\(H < 0\\) , \\((e^H)^i\\) decays to zero as \\(i\\) increases. Thus we want \\((1 + H)^i\\) to decay too, which implies that \\[ |1 + H| < 1. \\] From another perspective, consider the roundoff error of the initial value \\(\\alpha\\) by, \\[ w_0 = \\alpha + \\delta_0. \\] At the i th step the roundoff error is \\[ \\delta_i = (1 + H)^i \\delta_0. \\] To control the error, we also want \\(|1 + H| < 1\\) . In general, when applying to the test equation, we have the difference equation of the form \\[ w_{i + 1} = Q(H)w_i,\\ \\ H = h \\lambda. \\] To make \\(Q(H)\\) approximate \\(e^H\\) , we want at least \\(|Q(H)| < 1\\) to make it decay to 0. The inequality delimit a region in a complex plane. For a multistep method, the difference equation is in the form \\[ (1 - H b_m)w_{i + 1} - (a_{m - 1} + H b_{m - 1})w_i - \\cdots - (a_0 + H b_0)w_{i + 1 - m} = 0. \\] We can define a characteristic polynomial \\[ Q(z, H) = (1 - H b_m)z^m - (a_{m - 1} + H b_{m - 1})z^{m - 1} - \\cdots - (a_0 + H b_0). \\] Suppose \\(\\beta_i\\) are the distinct roots of \\(Q(z, H)\\) , then it can be proved that \\(\\exists\\ c_i\\) , s.t. \\[ w_{i} = \\sum\\limits_{k = 1}^m c_k\\beta_k^i. \\] Again, to make \\(w_i\\) approximate \\((e^H)^i\\) , we want at least all \\(|\\beta_k| < 1\\) to make it decay to \\(0\\) . It also delimit a region in a complex plane. Definition 5.12 The Region R of abosolute stability for a one-step method is \\[ R = \\{H \\in \\mathbb{C} | |Q(H)| < 1\\}, \\] and for a multistep method, \\[ R = \\{H \\in \\mathbb{C} | |\\beta_k| < 1,\\ \\ \\text{ for all zero points of } Q(z, H)\\}. \\] A numerical method is said to be A-stable if its region \\(R\\) of abosolute stability contains left half-plane , which means \\(\\text{Re}(\\lambda) < 0\\) , stable with stiff equation . Method A is said to be more stable than method B if the region of absolute stability of A is larger than that of B. The region of Euler's method is like Thus it's only stable for some cases of stiff equation. When the negative \\(\\lambda\\) becomes smaller and get out of the region, it becomes not stable. Similarly, for Runge-Kutta Order Four method (explicit), applying test equation, we have \\[ w_{i + 1} = \\left(1 + H + \\frac{H^2}{2} + \\frac{H^3}{6} + \\frac{H^4}{24}\\right)w_i. \\] And the region is like Let's consider some implicit method. Say Euler's implicit method, we have \\[ w_{i + 1} = \\frac{1}{1 - H} w_i, \\] whose region is Thus it's A-stable . Also, implicit Trapezoidal method and 2 nd -order Runge-Kutta implict method are A-stable , both with the difference equation \\[ w_{i + 1} = \\frac{2 + H}{2 - H}w_i, \\] whose region is just right covers the left half-plane. Thus an important intuitive is that implicit method is more stable than explicit method in the stiff discussion .","title":"Stiff Equations"},{"location":"Mathematics_Basis/NA/Chap_6/","text":"Chapter 6 | Direct Methods for Solving Linear Systems \u00b6 Linear Systems of Equations \u00b6 Gaussian Elimination with Backward Substitution \u00b6 \\[ A\\mathbf{x} = \\mathbf{b} \\] Let \\(A^{(1)} = A, \\mathbf{b}^{(1)} = \\mathbf{b}\\) . Elimination \u00b6 For Step \\(k\\ (1\\le k \\le n-1)\\) , if \\(a^{(k)}_{kk}\\ne0\\) ( pivot element ), compute \\(m_{ik} = \\dfrac{a^{(k)}_{ik}}{a^{(k)}_{kk}}\\) and \\[ \\left\\{ \\begin{aligned} a_{ij}^{(k+1)} = a_{ij}^{(k)} - m_{ik}a_{kj}^{(k)} \\\\ b_i^{(k+1)} = b_i^{(k)} - m_{ik}b_k^{(k)} \\end{aligned} ,\\ \\ \\text{ where } i, j = k+1, \\dots, n. \\right. \\] After \\(n-1\\) steps, \\[ \\begin{bmatrix} a^{(1)}_{11} & a^{(1)}_{12} & \\cdots & a^{(1)}_{1n} \\\\ & a^{(2)}_{22} & \\cdots & a^{(2)}_{2n} \\\\ & & \\cdots & \\vdots \\\\ & & & a^{(n)}_{nn} \\\\ \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix} b^{(1)}_1 \\\\ b^{(2)}_2 \\\\ \\vdots \\\\ b^{(n)}_n \\end{bmatrix}. \\] Backward Substitution \u00b6 Then, \\[ x_n = \\frac{b_n^{(n)}}{a^{(n)}_{nn}},\\ \\ x_i = \\frac{1}{a_{ii}^{(i)}}\\left(b^{(i)}_i - \\sum_{j = i + 1}^n a^{(i)}_{ij} x_j\\right),\\ \\ \\text{ where }i = n-1, \\dots, 1. \\] Complexity \u00b6 Recap that we have \\[ \\sum\\limits_{i = 1}^n 1 = n,\\ \\ \\sum\\limits_{i = 1}^n i = \\frac{n(n+1)}{2},\\ \\ \\sum\\limits_{i = 1}^n i^2 = \\frac{n(n+1)(2n+1)}{6}.\\ \\ \\] For Elimination, Multiplications/divisions \\[ \\sum\\limits_{k=1}^{n-1}((n - k) + (n - k)(n - k + 2)) = \\frac{n^3}{3} + \\frac{n^2}{2} - \\frac{5}{6}n. \\] Addtion/subtraction \\[ \\sum\\limits_{k = 1}^{n-1}(n - k)(n - k + 1) = \\frac{n^3}{3} - \\frac{n}{3}. \\] For Backward-substitution, Multiplications/divisions \\[ 1 + \\sum\\limits_{k=1}^{n-1}(n - k + 1) = \\frac{n^2}{2} + \\frac{n}{2}. \\] Addtion/subtraction \\[ \\sum\\limits_{i = 1}^{n - 1}((n - i + 1) + 1) = \\frac{n^2}{2} - \\frac{n}{2}. \\] In total, Multiplications/divisions \\[ \\frac{n^3}{3} + n^2 - \\frac{n}{3}. \\] Addtion/subtraction \\[ \\frac{n^3}{3} + \\frac{n^2}{2}-\\frac{5n}{6}. \\] Pivoting Strategies \u00b6 Motivation: For Gaussian Elimination with Backward Substituion, if the pivot element \\(a_{kk}^{(k)}\\) is small compared to \\(a_{ik}^{(k)}\\) , then \\(m_{ik}\\) is large with high roundoff error . Thus we need some transformation to improve the accuracy. Partial Pivoting (a.k.a Maximal Column Pivoting) \u00b6 Determine the smallest \\(p \\ge k\\) such that \\[ \\left|a_{pk}^{(k)}\\right| = \\max_{k \\le i \\le n} \\left|a_{ik}^{(k)}\\right|, \\] and interchange row \\(p\\) and row \\(k\\) . Requires \\(O(N^2)\\) additional comparisons . Scaled Partial Pivoting (a.k.a Scaled-Column Pivoting) \u00b6 Determine the smallest \\(p \\ge k\\) such that \\[ \\frac{\\left|a_{pk}^{(k)}\\right|}{s_p} = \\max\\limits_{k \\le i \\le n} \\frac{\\left|a_{ik}^{(k)}\\right|}{s_i}, \\] and interchange row \\(p\\) and row \\(k\\) , where \\(s_i = \\max\\limits_{1 \\le j \\le n} \\left|a_{ij}\\right|\\) . (Simply put, place the element in the pivot position that is largest relative to the entries in its row.) Requires \\(O(N^2)\\) additional comparisons and \\(O(N^2)\\) divisions . Complete Pivoting (a.k.a Maximal Pivoting) \u00b6 Search all the entries \\(a_{ij}\\) for \\(i,j = k, \\dots,n\\) to find the entry with the largest magnitude. Both row and column interchanges are performed to bring this entry to the pivot position. Requires \\(O\\left(\\dfrac{1}{3}N^3\\right)\\) additional comparisons . LU Factorization \u00b6 Considering the matrix form of Gaussian Elimination, for total \\(n-1\\) steps, we have \\[ L_{n-1}L_{n-2}\\dots L_1[A\\ \\textbf{b}] = \\begin{bmatrix} a^{(1)}_{11} & a^{(1)}_{12} & \\cdots & a^{(1)}_{1n} & b_1^{(1)} \\\\ & a^{(2)}_{22} & \\cdots & a^{(2)}_{2n} & b_2^{(2)} \\\\ & & \\ddots & \\vdots & \\vdots \\\\ & & & a^{(n)}_{nn} & b_n^{(n)} \\\\ \\end{bmatrix}, \\] where \\[ L_k = \\begin{bmatrix} 1 & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\ 0 & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \\vdots & \\ddots & 1 & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots & -m_{k+1, k} & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots& \\vdots & \\ddots & \\ddots & 0 \\\\ 0 & \\cdots & -m_{n, k} & \\cdots & \\cdots & 1 \\end{bmatrix}. \\] It's simple to compute that \\[ L_k^{-1} = \\begin{bmatrix} 1 & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\ 0 & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \\vdots & \\ddots & 1 & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots & m_{k+1, k} & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots& \\vdots & \\ddots & \\ddots & 0 \\\\ 0 & \\cdots & m_{n, k} & \\cdots & \\cdots & 1 \\end{bmatrix}. \\] Thus we let \\[ L_1^{-1}L_2^{-1}\\dots L_{n-1}^{-1} = \\begin{bmatrix} 1 & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\ m_{2,1} & 1 & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \\vdots & \\ddots & 1 & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots &\\ddots& \\ddots & 1 & 0\\\\ m_{n,1} & \\cdots & \\cdots & \\cdots & m_{n, n-1} & 1 \\end{bmatrix} = L, \\] and \\[ U = \\begin{bmatrix} a^{(1)}_{11} & a^{(1)}_{12} & \\cdots & a^{(1)}_{1n} \\\\ 0 & a^{(2)}_{22} & \\cdots & a^{(2)}_{2n} \\\\ \\vdots & \\ddots & \\ddots & \\vdots \\\\ 0 & \\cdots & 0 & a^{(n)}_{nn} \\\\ \\end{bmatrix}. \\] Then we get \\[ A = LU. \\] Theorem 6.0 If Gaussian elimination can be performed on the linear system \\(A\\mathbf{x} = \\mathbf{b}\\) without row interchanges , then the matrix \\(A\\) can be factored into the product of a lower-triangular matrix \\(L\\) and an upper-triangular matrix \\(U\\) . If \\(L\\) has to be unitary , then the factorization is unique . Special Types of Matrices \u00b6 Strictly Diagonally Dominant Matrix \u4e25\u683c\u4e3b\u5bf9\u89d2\u5360\u4f18\u77e9\u9635 \u00b6 Definition 6.0 The \\(n \\times n\\) matrix \\(A\\) is said to be strictly diagnoally dominant when \\[ |a_{ii}| \\gt \\sum\\limits_{\\substack{j=1 \\\\ j\\ne i}}^{n}|a_{ij}|,\\text{ for each } i = 1, \\dots, n. \\] Theorem 6.1 A strictly diagonally dominant matrix is nonsingular . And Gaussian elimination can be performed without row or column interchanges, and computations will be stable with respect to the growth of roundoff errors.\uff08\u6ee1\u79e9\u3001\u65e0\u9700\u4ea4\u6362\u884c\u5217\u3001\u8bef\u5dee\u7a33\u5b9a\uff09 Positive Definite Matrix \u6b63\u5b9a\u77e9\u9635 \u00b6 Definition 6.1 (Recap) A matrix \\(A\\) is positive definite if it's symmetric and if \\(\\forall\\ (\\mathbf{0} \\ne) \\mathbf{x} \\in \\mathbb{R}^n\\) , \\(\\mathbf{x}^tA\\mathbf{x} > 0\\) . Theorem 6.2 If \\(A\\) is an \\(n \\times n\\) positive definite matrix, then \\(A\\) is nonsingular; \\(a_{ii} > 0\\) , for each \\(i = 1, 2, \\dots, n\\) ; \\(\\max\\limits_{1 \\le k, j \\le n}|a_{kj}| \\le \\max\\limits_{1 \\le i \\le n}|a_{ii}|\\) ; \\((a_{ij})^2 < a_{ii}a_{jj}\\) , for each \\(i \\ne j\\) . Choleski's Method (LDLt factorization) \u00b6 Further decompose \\(U\\) to \\(D\\tilde U\\) . \\(A\\) is symmetric \\(\\Rightarrow\\) \\(L = \\tilde U^t\\) . Thus \\[ A = LU = LD\\tilde U = LDL^t.\\ \\ (\u4e0b\u4e09\u89d2\u77e9\u9635\u4e58\u5bf9\u89d2\u77e9\u9635\u518d\u4e58\u5176\u8f6c\u7f6e) \\] Let \\[ D^{1/2} = \\begin{bmatrix} \\sqrt{u_{11}} & & & \\\\ & \\sqrt{u_{22}} & & \\\\ & & \\ddots & \\\\ & & & \\sqrt{u_{nn}} \\end{bmatrix}, \\] and \\(\\widetilde{L} = LD^{1/2}\\) . Then \\[ A = \\tilde L \\tilde L^t.\\ \\ (\u4e0b\u4e09\u89d2\u77e9\u9635\u4e58\u5176\u8f6c\u7f6e) \\] Tridiagonal Linear System \u4e09\u5bf9\u89d2\u77e9\u9635 \u00b6 Definition 6.2 An \\(n \\times n\\) matrix \\(A\\) is called a band matrix if \\(\\exists\\ p, q\\) \\((1 < p, q < n)\\) , s.t. whenever \\(i + p \\le j\\) or \\(j + q \\le i\\) , \\(a_{ij} = 0\\) . And \\(w = p + q - 1\\) is called the bandwidth . Specially, if \\(p = q = 2\\) , then \\(A\\) is called tridiagonal , with the following form, \\[ \\begin{bmatrix} b_1 & c_1 & & & \\\\ a_2 & b_2 & c_2 & & \\\\ & \\ddots & \\ddots & \\ddots \\\\ & & a_{n-1} & b_{n-1} & c_{n-1} \\\\ & & & a_n & b_n \\\\ \\end{bmatrix} \\] Crout Factorization \u00b6 \\[ A = LU = \\begin{bmatrix} l_{11} \\\\ l_{21} & l_{22} \\\\ & \\ddots & \\ddots \\\\ & & \\ddots & \\ddots \\\\ & & & l_{n, n- 1} & l_{n, n} \\end{bmatrix} \\begin{bmatrix} 1 & u_{12}\\\\ & 1 & u_{23} \\\\ & & \\ddots & \\ddots \\\\ & & & \\ddots & u_{n-1,n}\\\\ & & & & 1 \\end{bmatrix}. \\] the time complexity is \\(O(N)\\) .","title":"Chap 6"},{"location":"Mathematics_Basis/NA/Chap_6/#chapter-6-direct-methods-for-solving-linear-systems","text":"","title":"Chapter 6 | Direct Methods for Solving Linear Systems"},{"location":"Mathematics_Basis/NA/Chap_6/#linear-systems-of-equations","text":"","title":"Linear Systems of Equations"},{"location":"Mathematics_Basis/NA/Chap_6/#gaussian-elimination-with-backward-substitution","text":"\\[ A\\mathbf{x} = \\mathbf{b} \\] Let \\(A^{(1)} = A, \\mathbf{b}^{(1)} = \\mathbf{b}\\) .","title":"Gaussian Elimination with Backward Substitution"},{"location":"Mathematics_Basis/NA/Chap_6/#elimination","text":"For Step \\(k\\ (1\\le k \\le n-1)\\) , if \\(a^{(k)}_{kk}\\ne0\\) ( pivot element ), compute \\(m_{ik} = \\dfrac{a^{(k)}_{ik}}{a^{(k)}_{kk}}\\) and \\[ \\left\\{ \\begin{aligned} a_{ij}^{(k+1)} = a_{ij}^{(k)} - m_{ik}a_{kj}^{(k)} \\\\ b_i^{(k+1)} = b_i^{(k)} - m_{ik}b_k^{(k)} \\end{aligned} ,\\ \\ \\text{ where } i, j = k+1, \\dots, n. \\right. \\] After \\(n-1\\) steps, \\[ \\begin{bmatrix} a^{(1)}_{11} & a^{(1)}_{12} & \\cdots & a^{(1)}_{1n} \\\\ & a^{(2)}_{22} & \\cdots & a^{(2)}_{2n} \\\\ & & \\cdots & \\vdots \\\\ & & & a^{(n)}_{nn} \\\\ \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix} b^{(1)}_1 \\\\ b^{(2)}_2 \\\\ \\vdots \\\\ b^{(n)}_n \\end{bmatrix}. \\]","title":"Elimination"},{"location":"Mathematics_Basis/NA/Chap_6/#backward-substitution","text":"Then, \\[ x_n = \\frac{b_n^{(n)}}{a^{(n)}_{nn}},\\ \\ x_i = \\frac{1}{a_{ii}^{(i)}}\\left(b^{(i)}_i - \\sum_{j = i + 1}^n a^{(i)}_{ij} x_j\\right),\\ \\ \\text{ where }i = n-1, \\dots, 1. \\]","title":"Backward Substitution"},{"location":"Mathematics_Basis/NA/Chap_6/#complexity","text":"Recap that we have \\[ \\sum\\limits_{i = 1}^n 1 = n,\\ \\ \\sum\\limits_{i = 1}^n i = \\frac{n(n+1)}{2},\\ \\ \\sum\\limits_{i = 1}^n i^2 = \\frac{n(n+1)(2n+1)}{6}.\\ \\ \\] For Elimination, Multiplications/divisions \\[ \\sum\\limits_{k=1}^{n-1}((n - k) + (n - k)(n - k + 2)) = \\frac{n^3}{3} + \\frac{n^2}{2} - \\frac{5}{6}n. \\] Addtion/subtraction \\[ \\sum\\limits_{k = 1}^{n-1}(n - k)(n - k + 1) = \\frac{n^3}{3} - \\frac{n}{3}. \\] For Backward-substitution, Multiplications/divisions \\[ 1 + \\sum\\limits_{k=1}^{n-1}(n - k + 1) = \\frac{n^2}{2} + \\frac{n}{2}. \\] Addtion/subtraction \\[ \\sum\\limits_{i = 1}^{n - 1}((n - i + 1) + 1) = \\frac{n^2}{2} - \\frac{n}{2}. \\] In total, Multiplications/divisions \\[ \\frac{n^3}{3} + n^2 - \\frac{n}{3}. \\] Addtion/subtraction \\[ \\frac{n^3}{3} + \\frac{n^2}{2}-\\frac{5n}{6}. \\]","title":"Complexity"},{"location":"Mathematics_Basis/NA/Chap_6/#pivoting-strategies","text":"Motivation: For Gaussian Elimination with Backward Substituion, if the pivot element \\(a_{kk}^{(k)}\\) is small compared to \\(a_{ik}^{(k)}\\) , then \\(m_{ik}\\) is large with high roundoff error . Thus we need some transformation to improve the accuracy.","title":"Pivoting Strategies"},{"location":"Mathematics_Basis/NA/Chap_6/#partial-pivoting-aka-maximal-column-pivoting","text":"Determine the smallest \\(p \\ge k\\) such that \\[ \\left|a_{pk}^{(k)}\\right| = \\max_{k \\le i \\le n} \\left|a_{ik}^{(k)}\\right|, \\] and interchange row \\(p\\) and row \\(k\\) . Requires \\(O(N^2)\\) additional comparisons .","title":"Partial Pivoting (a.k.a Maximal Column Pivoting)"},{"location":"Mathematics_Basis/NA/Chap_6/#scaled-partial-pivoting-aka-scaled-column-pivoting","text":"Determine the smallest \\(p \\ge k\\) such that \\[ \\frac{\\left|a_{pk}^{(k)}\\right|}{s_p} = \\max\\limits_{k \\le i \\le n} \\frac{\\left|a_{ik}^{(k)}\\right|}{s_i}, \\] and interchange row \\(p\\) and row \\(k\\) , where \\(s_i = \\max\\limits_{1 \\le j \\le n} \\left|a_{ij}\\right|\\) . (Simply put, place the element in the pivot position that is largest relative to the entries in its row.) Requires \\(O(N^2)\\) additional comparisons and \\(O(N^2)\\) divisions .","title":"Scaled Partial Pivoting (a.k.a Scaled-Column Pivoting)"},{"location":"Mathematics_Basis/NA/Chap_6/#complete-pivoting-aka-maximal-pivoting","text":"Search all the entries \\(a_{ij}\\) for \\(i,j = k, \\dots,n\\) to find the entry with the largest magnitude. Both row and column interchanges are performed to bring this entry to the pivot position. Requires \\(O\\left(\\dfrac{1}{3}N^3\\right)\\) additional comparisons .","title":"Complete Pivoting (a.k.a Maximal Pivoting)"},{"location":"Mathematics_Basis/NA/Chap_6/#lu-factorization","text":"Considering the matrix form of Gaussian Elimination, for total \\(n-1\\) steps, we have \\[ L_{n-1}L_{n-2}\\dots L_1[A\\ \\textbf{b}] = \\begin{bmatrix} a^{(1)}_{11} & a^{(1)}_{12} & \\cdots & a^{(1)}_{1n} & b_1^{(1)} \\\\ & a^{(2)}_{22} & \\cdots & a^{(2)}_{2n} & b_2^{(2)} \\\\ & & \\ddots & \\vdots & \\vdots \\\\ & & & a^{(n)}_{nn} & b_n^{(n)} \\\\ \\end{bmatrix}, \\] where \\[ L_k = \\begin{bmatrix} 1 & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\ 0 & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \\vdots & \\ddots & 1 & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots & -m_{k+1, k} & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots& \\vdots & \\ddots & \\ddots & 0 \\\\ 0 & \\cdots & -m_{n, k} & \\cdots & \\cdots & 1 \\end{bmatrix}. \\] It's simple to compute that \\[ L_k^{-1} = \\begin{bmatrix} 1 & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\ 0 & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \\vdots & \\ddots & 1 & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots & m_{k+1, k} & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots& \\vdots & \\ddots & \\ddots & 0 \\\\ 0 & \\cdots & m_{n, k} & \\cdots & \\cdots & 1 \\end{bmatrix}. \\] Thus we let \\[ L_1^{-1}L_2^{-1}\\dots L_{n-1}^{-1} = \\begin{bmatrix} 1 & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\ m_{2,1} & 1 & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \\vdots & \\ddots & 1 & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots &\\ddots& \\ddots & 1 & 0\\\\ m_{n,1} & \\cdots & \\cdots & \\cdots & m_{n, n-1} & 1 \\end{bmatrix} = L, \\] and \\[ U = \\begin{bmatrix} a^{(1)}_{11} & a^{(1)}_{12} & \\cdots & a^{(1)}_{1n} \\\\ 0 & a^{(2)}_{22} & \\cdots & a^{(2)}_{2n} \\\\ \\vdots & \\ddots & \\ddots & \\vdots \\\\ 0 & \\cdots & 0 & a^{(n)}_{nn} \\\\ \\end{bmatrix}. \\] Then we get \\[ A = LU. \\] Theorem 6.0 If Gaussian elimination can be performed on the linear system \\(A\\mathbf{x} = \\mathbf{b}\\) without row interchanges , then the matrix \\(A\\) can be factored into the product of a lower-triangular matrix \\(L\\) and an upper-triangular matrix \\(U\\) . If \\(L\\) has to be unitary , then the factorization is unique .","title":"LU Factorization"},{"location":"Mathematics_Basis/NA/Chap_6/#special-types-of-matrices","text":"","title":"Special Types of Matrices"},{"location":"Mathematics_Basis/NA/Chap_6/#strictly-diagonally-dominant-matrix","text":"Definition 6.0 The \\(n \\times n\\) matrix \\(A\\) is said to be strictly diagnoally dominant when \\[ |a_{ii}| \\gt \\sum\\limits_{\\substack{j=1 \\\\ j\\ne i}}^{n}|a_{ij}|,\\text{ for each } i = 1, \\dots, n. \\] Theorem 6.1 A strictly diagonally dominant matrix is nonsingular . And Gaussian elimination can be performed without row or column interchanges, and computations will be stable with respect to the growth of roundoff errors.\uff08\u6ee1\u79e9\u3001\u65e0\u9700\u4ea4\u6362\u884c\u5217\u3001\u8bef\u5dee\u7a33\u5b9a\uff09","title":"Strictly Diagonally Dominant Matrix \u4e25\u683c\u4e3b\u5bf9\u89d2\u5360\u4f18\u77e9\u9635"},{"location":"Mathematics_Basis/NA/Chap_6/#positive-definite-matrix","text":"Definition 6.1 (Recap) A matrix \\(A\\) is positive definite if it's symmetric and if \\(\\forall\\ (\\mathbf{0} \\ne) \\mathbf{x} \\in \\mathbb{R}^n\\) , \\(\\mathbf{x}^tA\\mathbf{x} > 0\\) . Theorem 6.2 If \\(A\\) is an \\(n \\times n\\) positive definite matrix, then \\(A\\) is nonsingular; \\(a_{ii} > 0\\) , for each \\(i = 1, 2, \\dots, n\\) ; \\(\\max\\limits_{1 \\le k, j \\le n}|a_{kj}| \\le \\max\\limits_{1 \\le i \\le n}|a_{ii}|\\) ; \\((a_{ij})^2 < a_{ii}a_{jj}\\) , for each \\(i \\ne j\\) .","title":"Positive Definite Matrix \u6b63\u5b9a\u77e9\u9635"},{"location":"Mathematics_Basis/NA/Chap_6/#choleskis-method-ldlt-factorization","text":"Further decompose \\(U\\) to \\(D\\tilde U\\) . \\(A\\) is symmetric \\(\\Rightarrow\\) \\(L = \\tilde U^t\\) . Thus \\[ A = LU = LD\\tilde U = LDL^t.\\ \\ (\u4e0b\u4e09\u89d2\u77e9\u9635\u4e58\u5bf9\u89d2\u77e9\u9635\u518d\u4e58\u5176\u8f6c\u7f6e) \\] Let \\[ D^{1/2} = \\begin{bmatrix} \\sqrt{u_{11}} & & & \\\\ & \\sqrt{u_{22}} & & \\\\ & & \\ddots & \\\\ & & & \\sqrt{u_{nn}} \\end{bmatrix}, \\] and \\(\\widetilde{L} = LD^{1/2}\\) . Then \\[ A = \\tilde L \\tilde L^t.\\ \\ (\u4e0b\u4e09\u89d2\u77e9\u9635\u4e58\u5176\u8f6c\u7f6e) \\]","title":"Choleski's Method (LDLt factorization)"},{"location":"Mathematics_Basis/NA/Chap_6/#tridiagonal-linear-system","text":"Definition 6.2 An \\(n \\times n\\) matrix \\(A\\) is called a band matrix if \\(\\exists\\ p, q\\) \\((1 < p, q < n)\\) , s.t. whenever \\(i + p \\le j\\) or \\(j + q \\le i\\) , \\(a_{ij} = 0\\) . And \\(w = p + q - 1\\) is called the bandwidth . Specially, if \\(p = q = 2\\) , then \\(A\\) is called tridiagonal , with the following form, \\[ \\begin{bmatrix} b_1 & c_1 & & & \\\\ a_2 & b_2 & c_2 & & \\\\ & \\ddots & \\ddots & \\ddots \\\\ & & a_{n-1} & b_{n-1} & c_{n-1} \\\\ & & & a_n & b_n \\\\ \\end{bmatrix} \\]","title":"Tridiagonal Linear System \u4e09\u5bf9\u89d2\u77e9\u9635"},{"location":"Mathematics_Basis/NA/Chap_6/#crout-factorization","text":"\\[ A = LU = \\begin{bmatrix} l_{11} \\\\ l_{21} & l_{22} \\\\ & \\ddots & \\ddots \\\\ & & \\ddots & \\ddots \\\\ & & & l_{n, n- 1} & l_{n, n} \\end{bmatrix} \\begin{bmatrix} 1 & u_{12}\\\\ & 1 & u_{23} \\\\ & & \\ddots & \\ddots \\\\ & & & \\ddots & u_{n-1,n}\\\\ & & & & 1 \\end{bmatrix}. \\] the time complexity is \\(O(N)\\) .","title":"Crout Factorization"},{"location":"Mathematics_Basis/NA/Chap_7/","text":"Chapter 7 | Iterative Techniques in Matrix Algebra \u00b6 Norms of Vectors and Matrices \u00b6 Vector Norms \u00b6 Definition 7.0 A vector norm on \\(\\mathbb{R}^n\\) is a function \\(||\\cdot||\\) , from \\(\\mathbb{R}^n\\) into \\(\\mathbb{R}\\) with the following properties. \\[ \\begin{aligned} \\forall \\mathbf{x, y} \\in \\mathbb{R}^n, \\alpha \\in \\mathbb{R}, & (1)\\ ||\\mathbf{x}|| \\ge 0;\\ ||\\mathbf{x}|| = 0 \\Leftrightarrow \\mathbf{x} = \\mathbf{0}; \\\\ & (2)\\ ||\\alpha \\mathbf{x}|| = |\\alpha| \\cdot ||\\mathbf{x}||; \\\\ & (3)\\ ||\\mathbf{x} + \\mathbf{y}|| \\le ||\\mathbf{x}|| + ||\\mathbf{y}||. \\\\ \\end{aligned} \\] Commonly used examples L1 Norm: \\(||\\mathbf{x}||_1 = \\sum\\limits_{i = 1}^n|x_i|\\) . L2 Norm / Euclidean Norm: \\(||\\mathbf{x}||_2 = \\sqrt{\\sum\\limits_{i = 1}^n|x_i|^2}\\) . p-Norm: \\(||\\mathbf{x}||_p = \\left(\\sum\\limits_{i = 1}^n|x_i|^p\\right)^{1/p}\\) . Infinity Norm: \\(||\\mathbf{x}||_\\infty = \\max\\limits_{1 \\le i \\le n} |x_i|\\) . Convergence of Vector \u00b6 Definition 7.1 Similarly with a scalar, a sequence of vectors \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) in \\(\\mathbb{R}^n\\) is said to converge to \\(\\mathbf{x}\\) with respect to norm \\(||\\cdot||\\) , if \\[ \\forall\\ \\varepsilon \\gt 0,\\ \\ \\exists\\ N \\in \\mathbb{N},\\ \\ \\text{ s.t. } \\forall\\ k \\gt N,\\ \\ ||\\mathbf{x}^{(k)} - \\mathbf{x}|| \\lt \\varepsilon. \\] Theorem 7.0 \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) in \\(\\mathbb{R}^n\\) converges to \\(\\mathbf{x}\\) with respect to norm \\(||\\cdot||_\\infty\\) iff \\[ \\forall\\ i,\\ \\ \\lim\\limits_{k \\rightarrow \\infty}x_i^{(k)} = x_i. \\] Equivalence \u00b6 Definition 7.2 \\(||\\mathbf{x}||_A\\) and \\(||\\mathbf{x}||_B\\) are equivalent , if \\[ \\exists\\ C_1, C_2,\\ \\ \\text{ s.t. } C_1||\\mathbf{x}||_B \\le ||\\mathbf{x}||_A \\le C_2||\\mathbf{x}||_B. \\] Theorem 7.1 All the vector norms on \\(\\mathbb{R}^n\\) are equivalent. Matrix Norms \u00b6 Definition 7.3 A matrix norm on \\(\\mathbb{R}^n\\) is a function \\(||\\cdot||\\) , from \\(M_n(\\mathbb{R})\\) matrices into \\(\\mathbb{R}\\) with the following properties. \\[ \\begin{aligned} \\forall A, B \\in M_n(\\mathbb{R}), \\alpha \\in \\mathbb{R}, & (1)\\ ||A|| \\ge 0;\\ ||A|| = 0 \\Leftrightarrow A = \\mathbf{O}; \\\\ & (2)\\ ||\\alpha A|| = |\\alpha| \\cdot ||A||; \\\\ & (3)\\ ||A + B|| \\le ||A|| + ||B||; \\\\ & (4)\\ ||AB|| \\le ||A|| \\cdot ||B||. \\end{aligned} \\] Commonly used examples Frobenius Norm: \\(||A||_F = \\sqrt{\\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^n|a_{ij}|^2}\\) . Natural Norm: \\(||A||_p = \\max\\limits_{\\mathbf{x}\\ne \\mathbf{0}} \\dfrac{||A\\mathbf{x}||_p}{||\\mathbf{x}||_p} = \\max\\limits_{||\\mathbf{x}||_p = 1} ||A\\mathbf{x}||_p\\) , where \\(||\\cdot||_p\\) is the vector norm. \\(||A||_\\infty = \\max\\limits_{1\\le i \\le n}\\sum\\limits_{j=1}^n|a_{ij}|\\) . \\(||A||_1= \\max\\limits_{1\\le j \\le n}\\sum\\limits_{i=1}^n|a_{ij}|\\) . (Spectral Norm) \\(||A||_2= \\sqrt{\\lambda_{max}(A^TA)}\\) . Corollary 7.2 For any vector \\(\\mathbf{x} \\ne 0\\) , matrix \\(A\\) , and any natural norm \\(||\\cdot||\\) , we have \\[ ||A\\mathbf{x}|| \\le ||A|| \\cdot ||\\mathbf{x}||. \\] Eigenvalues and Eigenvectors \u00b6 Definition 7.4 (Recap) If \\(A\\) is a square matrix, the characteristic polynomial of \\(A\\) is defined by \\[ p(\\lambda) = \\text{det}(A - \\lambda I). \\] The roots of \\(p\\) are eigenvalues . If \\(\\lambda\\) is an eigenvalue and \\(\\mathbf{x} \\ne 0\\) satisfies \\((A - \\lambda I)\\mathbf{x} = \\mathbf{0}\\) , then \\(\\mathbf{x}\\) is an eigenvector . Spectral Radius \u00b6 Definition 7.5 The spectral radius of a matrix \\(A\\) is defined by \\[ \\rho(A) = \\max|\\lambda|,\\ \\ \\text{ where $\\lambda$ is an eigenvalue of $A$}. \\] (Recap that for complex \\(\\lambda = \\alpha + \\beta i\\) , \\(|\\lambda| = \\sqrt{\\alpha^2 + \\beta^2}\\) .) Theorem 7.3 \\(\\forall\\ A \\in M_n(\\mathbb{R})\\) , \\(||A||_2 = \\sqrt{\\rho(A^tA)}\\) . \\(\\rho(A) \\le ||A||\\) , for any natural norm \\(||\\cdot||\\) . Proof A proof for the second property. Suppose \\(\\lambda\\) is an eigenvalue of \\(A\\) with eigenvector \\(\\mathbf{x}\\) and \\(||\\mathbf{x}|| = 1\\) , \\[ |\\lambda| = |\\lambda| \\cdot ||\\mathbf{x}|| = ||\\lambda \\mathbf{x}|| \\le ||A|| \\cdot ||\\mathbf{x}|| = ||A||. \\] Thus, \\[ \\rho(A) = \\max|\\lambda| \\le ||A||. \\] Convergence of Matrix \u00b6 Definition 7.6 \\(A \\in M_n(\\mathbb{R}))\\) is convergent if \\[ \\lim_{k \\rightarrow \\infty}\\left(A^k\\right)_{ij} = 0,\\ \\ \\text{ for each } i = 1, 2, \\dots, n \\text{ and } j = 1, 2, \\dots, n. \\] Theorem 7.4 The following statements are equivalent. \\(A\\) is a convergent matrix. \\(\\lim\\limits_{n \\rightarrow \\infty} ||A^n|| = 0\\) , for some natural norm. \\(\\lim\\limits_{n \\rightarrow \\infty} ||A^n|| = 0\\) , for all natural norms. \\(\\rho(A) < 1\\) . \\(\\forall\\ \\mathbf{x}\\) , \\(\\lim\\limits_{n \\rightarrow \\infty} ||A^n \\mathbf{x}|| = \\mathbf{0}\\) . Iterative Techniques for Solving Linear Systems \u00b6 \\[ A\\mathbf{x} = \\mathbf{b} \\Leftrightarrow (D - L - U)\\mathbf{x} = \\mathbf{b} \\Leftrightarrow D\\mathbf{x} = (L + U)\\mathbf{x} + \\mathbf{b} \\\\ \\] Thus, \\[ \\mathbf{x} = D^{-1}(L + U)\\mathbf{x} + D^{-1}\\mathbf{b}. \\] Jacobi Iterative Method \u00b6 Let \\(T_j = D^{-1}(L+U)\\) and \\(\\mathbf{c}_\\mathbf{j} = D^{-1}\\mathbf{b}\\) , then \\[ \\mathbf{x}^{(k)} = T_j\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\mathbf{j}. \\] Gauss-Seidel Iterative Method \u00b6 \\[ \\small \\mathbf{x}^{(k)} = D^{-1}(L\\mathbf{x}^{(k)} + U\\mathbf{x}^{(k - 1)}) + D^{-1}\\mathbf{b} \\Leftrightarrow \\mathbf{x}^{(k)} = (D - L)^{-1}U\\mathbf{x}^{(k - 1)} + (D - L)^{-1}\\mathbf{b} \\] Let \\(T_g = (D - L)^{-1}U\\) and \\(\\mathbf{c}_\\mathbf{g} = (D - L)^{-1}\\mathbf{b}\\) , then \\[ \\mathbf{x}^{(k)} = T_g\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\mathbf{g}. \\] Convergence \u00b6 Consider the following formula \\[ \\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c}, \\] where \\(\\mathbf{x}^{(0)}\\) is arbitrary. Lemma 7.5 If \\(\\rho(T) \\lt 1\\) , then \\((I - T)^{-1}\\) exists and \\[ (I - T)^{-1} = \\sum\\limits_{j = 0}^\\infty T^j. \\] Proof Suppose \\(\\lambda\\) is an eigenvalue of \\(T\\) with eigenvector \\(\\mathbf{x}\\) , then since \\(T\\mathbf{x} = \\lambda \\mathbf{x} \\Leftrightarrow (I - T)\\mathbf{x} = (1 - \\lambda)\\mathbf{x}\\) , thus \\(1 - \\lambda\\) is an eigenvalue of \\(I - T\\) . Since \\(|\\lambda| \\le \\rho(T) < 1\\) , thus \\(\\lambda = 1\\) is not an eigenvalue of \\(T\\) and \\(0\\) is not an eigenvalue of \\(I - T\\) . Hence, \\((I - T)^{-1}\\) exists. Let \\(S_m = I + T + T^2 + \\cdots + T^m\\) , then \\[ (I - T)S_m = (1 + T + \\cdots + T^m) - (T + T^2 + \\cdots + T^{m + 1}) = I - T^{m + 1}. \\] Since \\(T\\) is convergent, thus \\[ \\lim\\limits_{m \\rightarrow \\infty} (I - T)S_m = \\lim\\limits_{m \\rightarrow \\infty}(I - T^{m + 1}) = I. \\] Thus, \\((I - T)^{-1} = \\lim\\limits_{m \\rightarrow \\infty}S_m = \\sum\\limits_{j = 0}^\\infty T^j\\) . Theorem 7.6 \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) defined by \\(\\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c}\\) converges to the unique solution of \\[ \\mathbf{x} = T\\mathbf{x} + \\mathbf{c} \\] iff \\[ \\rho(T) \\lt 1. \\] Proof \\(\\Rightarrow\\) : Define error \\(\\mathbf{e}^{(k)} = \\mathbf{x} - \\mathbf{x}^{(k)}\\) , then \\[ \\mathbf{e}^{(k)} = (T\\mathbf{x} + c) - (T\\mathbf{x}^{(k - 1)} + c) = T(\\mathbf{x} - \\mathbf{x}^{(k - 1)})T\\mathbf{e}^{(k - 1)} \\Rightarrow \\mathbf{e}^{(k)} = T^k \\mathbf{e}^{(0)}. \\] Since it converges, thus \\[ \\lim\\limits_{k \\rightarrow \\infty} \\mathbf{e}^{(k)} = 0 \\Rightarrow \\forall\\ \\mathbf{e}^{(0)},\\ \\ \\lim\\limits_{k \\rightarrow \\infty} T^k \\mathbf{e}^{(0)} = 0 \\] \\[ \\Leftrightarrow \\rho(T) < 1. \\] \\(\\Leftarrow\\) : \\[ \\mathbf{x}^{(k)} = T^{k}\\mathbf{x}^{(0)} + (T^{k - 1} + \\cdots + T + I) \\mathbf{c}. \\] Since \\(\\rho(T) < 1\\) , \\(T\\) is convergent and \\[ \\lim\\limits_{k \\rightarrow \\infty} T^k \\mathbf{x}^{(0)} = \\mathbf{0}. \\] From Lemma 7.5 , we have \\[ \\lim\\limits_{k \\rightarrow \\infty} \\mathbf{x}^{(k)} = \\lim\\limits_{k \\rightarrow \\infty} T^k\\mathbf{x}^{(0)} + \\left(\\sum\\limits_{j = 0}^\\infty T^j \\right)\\mathbf{c} = \\mathbf{0} + (I - T)^{-1}\\mathbf{c} = (I - T)^{-1}\\mathbf{c}. \\] Thus \\(\\{\\mathbf{x}^{(k)}\\}\\) converges to \\(\\mathbf{x} \\equiv (I - T)^{-1} \\Leftrightarrow \\mathbf{x} = T\\mathbf{x} + c\\) . Corollary 7.7 If \\(||T||\\lt 1\\) for any matrix norm and \\(\\mathbf{c}\\) is a given vector, then \\(\\forall\\ \\mathbf{x}^{(0)}\\in \\mathbb{R}^n\\) and \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) defined by \\(\\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c}\\) converges to \\(\\mathbf{x}\\) , and the following error bounds hold \\(||\\mathbf{x} - \\mathbf{x}^{(k)}|| \\le ||T||^k||\\mathbf{x}^{(0)} - \\mathbf{x}||\\) . \\(||\\mathbf{x} - \\mathbf{x}^{(k)}|| \\le \\dfrac{||T||^k}{1 - ||T||}||\\mathbf{x}^{(1)} - \\mathbf{x}||\\) . Theorem 7.8 Suppose \\(A\\) is strictly diagonally dominant, then \\(\\forall\\ \\mathbf{x}^{(0)}\\) , both Jacobi and Gauss-Seidel methods give \\(\\{\\mathbf{x}^{(k)}\\}_{k=0}^\\infty\\) that converge to the unique solution of \\(A\\mathbf{x} = \\mathbf{b}\\) . Relaxation Methods \u00b6 Definition 7.7 Suppose \\(\\mathbf{\\tilde x}\\) is an approximation to the solution of \\(A\\mathbf{x} = \\mathbf{b}\\) , then the residual vector for \\(\\mathbf{\\tilde x}\\) w.r.t this linear system is \\[ \\mathbf{r} = \\mathbf{b} - A\\mathbf{\\tilde x}. \\] Further examine Gauss-Seidel method. \\[ x_i^{(k)} = x_i^{(k - 1)} + \\frac{r_i^{(k)}}{a_{ii}},\\ \\ \\text{ where } r_i^{(k)} = b_i - \\sum_{j \\lt i} a_{ij}x_j^{(k)} - \\sum_{j \\ge i} a_{ij}x_j^{(k - 1)}. \\] Let \\(x_i^{(k)} = x_i^{(k - 1)} + \\omega\\dfrac{r_i^{(k)}}{a_{ii}}\\) , by modifying the value of \\(\\omega\\) , we can somehow get faster convergence. \\(0 \\lt \\omega \\lt 1\\) Under-Relaxation Method \\(\\omega = 1\\) Gauss-Seidel Method \\(\\omega \\gt 1\\) Successive Over-Relaxation Method (SOR) In matrix form, \\[ \\mathbf{x}^{(k)} = (D - \\omega L)^{-1}[(1 - \\omega)D + \\omega U]\\mathbf{x}^{(k - 1)} + (D - \\omega L)^{-1}\\mathbf{b}. \\] Let \\(T_\\omega = (D - \\omega L)^{-1}[(1 - \\omega)D + \\omega U]\\) and \\(\\mathbf{c}_\\omega = (D - \\omega L)^{-1}\\mathbf{b}\\) , then \\[ \\mathbf{x}^{(k)} = T_\\omega\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\omega. \\] Theorem 7.9 (Kahan) If \\(a_{ii} \\ne 0\\) , then \\(\\rho(T_\\omega)\\ge |\\omega -1 |\\) , which implies that SOR method can converge only if \\[ 0 \\lt \\omega \\lt 2. \\] Proof Recap that upper and lower triangular determinant are equal to the product of the entries at its diagnoal. Since \\(D\\) is diagonal, \\(L\\) and \\(U\\) are lower and upper triangular matrix, thus \\[ \\begin{aligned} \\text{det}(T_\\omega) &= \\text{det}((D - \\omega L)^{-1}) \\cdot \\text{det}((1 - \\omega)D + \\omega U) \\\\ &= \\text{det}(D^{-1})(1-\\omega)^n\\text{det}(D) = (1 - \\omega)^n. \\end{aligned} \\] On the other hand, recap that \\[ \\text{det}(T_\\omega) = \\prod\\limits_{i = 1}^n \\lambda_i,\\ \\ \\text{ where $\\lambda_i$ are eigenvalues of $T$}. \\] Thus \\[ \\rho(T_\\omega) = \\max\\limits_{1 \\le i \\le n} |\\lambda_i| \\ge |\\omega - 1|. \\] Theorem 7.10 (Ostrowski-Reich) If \\(A\\) is positive definite and \\(0 \\lt \\omega \\lt 2\\) , the SOR method converges for any choice of initial approximation vector \\(\\mathbf{x}^{(0)}\\) . Theorem 7.11 If \\(A\\) is positive definite and tridiagonal, then \\(\\rho(T_g) = \\rho^2(T_j)\\lt1\\) , and the optimal choice of \\(\\omega\\) for the SOR method is \\[ \\omega = \\frac{2}{1 + \\sqrt{1 - \\rho(T_j)^2}}. \\] With this choice of \\(\\omega\\) , we have \\(\\rho(T_\\omega) = \\omega - 1\\) . Error Bounds and Iterative Refinement \u00b6 Definition 7.8 The conditional number of the nonsigular matrix \\(A\\) relative to a norm \\(||\\cdot||\\) is \\[ K(A) = ||A|| \\cdot ||A^{-1}||. \\] A matrix \\(A\\) is well-conditioned if \\(K(A)\\) is close to \\(1\\) , and is ill-conditioned when \\(K(A)\\) is significantly greater than \\(1\\) . Proposition If \\(A\\) is symmetric, then \\(K(A)_2 = \\dfrac{\\max|\\lambda|}{\\min|\\lambda|}\\) . \\(K(A)_2 = 1\\) if \\(A\\) is orthogonal. \\(\\forall\\ \\text{ orthogonal matrix } R\\) , \\(K(RA)_2 = K(AR)_2 = K(A)_2\\) . \\(\\forall\\ \\text{ natural norm } ||\\cdot||_p\\) , \\(K(A)_p \\ge 1\\) . \\(K(\\alpha A) = K(A)\\) . Theorem 7.12 For any natural norm \\(||\\cdot||\\) , \\[ ||\\mathbf{x} - \\mathbf{\\tilde x}|| \\le ||\\mathbf{r}|| \\cdot ||A^{-1}||, \\] and if \\(\\mathbf{x} \\ne \\mathbf{0}\\) and \\(\\mathbf{b} \\ne \\mathbf{0}\\) , \\[ \\frac{||\\mathbf{x} - \\mathbf{\\tilde x}||}{||\\mathbf{x}||} \\le ||A||\\cdot||A^{-1}|| \\frac{||\\mathbf{r}||}{||\\mathbf{b}||} = K(A)\\frac{||\\mathbf{r}||}{||\\mathbf{b}||}. \\] Iterative Refinement Step.1 Solve \\(A\\mathbf{x} = \\mathbf{b}\\) and get an approximation solution \\(\\mathbf{x}_{0}\\) . Let \\(i = 1\\) . Step.2 Let \\(\\mathbf{r} = \\mathbf{b} - A\\mathbf{x}_{i - 1}\\) . Step.3 Solve \\(A\\mathbf{d} = \\mathbf{r}\\) and get the solution \\(\\mathbf{d}\\) . Step.4 The better approximation is \\(\\mathbf{x}_{i} = \\mathbf{x}_{i - 1} + \\mathbf{d}.\\) Step.5 Judge whether it's precise enough. If not, let \\(i = i + 1\\) and then repeat from Step.2 . In reality, \\(A\\) and \\(\\mathbf{b}\\) may be perturbed by an amount \\(\\delta A\\) and \\(\\delta \\mathbf{b}\\) . For \\(A(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b} + \\delta\\mathbf{b}\\) , \\[ \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le ||A|| \\cdot ||A^{-1}|| \\cdot \\frac{||\\delta \\mathbf{b}||}{||\\mathbf{b}||}. \\] For \\((A + \\delta A)(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b}\\) , \\[ \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le \\frac{||A^{-1}|| \\cdot ||\\delta A||}{1 - ||A^{-1}|| \\cdot ||\\delta A||} = \\frac{||A|| \\cdot ||A^{-1}|| \\cdot \\frac{||\\delta A||}{||A||}}{1 - ||A|| \\cdot ||A^{-1}|| \\cdot\\frac{||\\delta A||}{||A||}}. \\] Theorem 7.13 If \\(A\\) is nonsingular and \\[ ||\\delta A|| \\lt \\frac{1}{||A^{-1}||}, \\] then \\((A + \\delta A)(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b} + \\delta\\mathbf{b}\\) with the error estimate \\[ \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le \\frac{K(A)}{1 - K(A)\\frac{||\\delta A||}{||A||}}\\left(\\frac{||\\delta A||}{||A||} + \\frac{||\\delta\\mathbf{b}||}{||\\mathbf{b}||}\\right). \\]","title":"Chap 7"},{"location":"Mathematics_Basis/NA/Chap_7/#chapter-7-iterative-techniques-in-matrix-algebra","text":"","title":"Chapter 7 | Iterative Techniques in Matrix Algebra"},{"location":"Mathematics_Basis/NA/Chap_7/#norms-of-vectors-and-matrices","text":"","title":"Norms of Vectors and Matrices"},{"location":"Mathematics_Basis/NA/Chap_7/#vector-norms","text":"Definition 7.0 A vector norm on \\(\\mathbb{R}^n\\) is a function \\(||\\cdot||\\) , from \\(\\mathbb{R}^n\\) into \\(\\mathbb{R}\\) with the following properties. \\[ \\begin{aligned} \\forall \\mathbf{x, y} \\in \\mathbb{R}^n, \\alpha \\in \\mathbb{R}, & (1)\\ ||\\mathbf{x}|| \\ge 0;\\ ||\\mathbf{x}|| = 0 \\Leftrightarrow \\mathbf{x} = \\mathbf{0}; \\\\ & (2)\\ ||\\alpha \\mathbf{x}|| = |\\alpha| \\cdot ||\\mathbf{x}||; \\\\ & (3)\\ ||\\mathbf{x} + \\mathbf{y}|| \\le ||\\mathbf{x}|| + ||\\mathbf{y}||. \\\\ \\end{aligned} \\] Commonly used examples L1 Norm: \\(||\\mathbf{x}||_1 = \\sum\\limits_{i = 1}^n|x_i|\\) . L2 Norm / Euclidean Norm: \\(||\\mathbf{x}||_2 = \\sqrt{\\sum\\limits_{i = 1}^n|x_i|^2}\\) . p-Norm: \\(||\\mathbf{x}||_p = \\left(\\sum\\limits_{i = 1}^n|x_i|^p\\right)^{1/p}\\) . Infinity Norm: \\(||\\mathbf{x}||_\\infty = \\max\\limits_{1 \\le i \\le n} |x_i|\\) .","title":"Vector Norms"},{"location":"Mathematics_Basis/NA/Chap_7/#convergence-of-vector","text":"Definition 7.1 Similarly with a scalar, a sequence of vectors \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) in \\(\\mathbb{R}^n\\) is said to converge to \\(\\mathbf{x}\\) with respect to norm \\(||\\cdot||\\) , if \\[ \\forall\\ \\varepsilon \\gt 0,\\ \\ \\exists\\ N \\in \\mathbb{N},\\ \\ \\text{ s.t. } \\forall\\ k \\gt N,\\ \\ ||\\mathbf{x}^{(k)} - \\mathbf{x}|| \\lt \\varepsilon. \\] Theorem 7.0 \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) in \\(\\mathbb{R}^n\\) converges to \\(\\mathbf{x}\\) with respect to norm \\(||\\cdot||_\\infty\\) iff \\[ \\forall\\ i,\\ \\ \\lim\\limits_{k \\rightarrow \\infty}x_i^{(k)} = x_i. \\]","title":"Convergence of Vector"},{"location":"Mathematics_Basis/NA/Chap_7/#equivalence","text":"Definition 7.2 \\(||\\mathbf{x}||_A\\) and \\(||\\mathbf{x}||_B\\) are equivalent , if \\[ \\exists\\ C_1, C_2,\\ \\ \\text{ s.t. } C_1||\\mathbf{x}||_B \\le ||\\mathbf{x}||_A \\le C_2||\\mathbf{x}||_B. \\] Theorem 7.1 All the vector norms on \\(\\mathbb{R}^n\\) are equivalent.","title":"Equivalence"},{"location":"Mathematics_Basis/NA/Chap_7/#matrix-norms","text":"Definition 7.3 A matrix norm on \\(\\mathbb{R}^n\\) is a function \\(||\\cdot||\\) , from \\(M_n(\\mathbb{R})\\) matrices into \\(\\mathbb{R}\\) with the following properties. \\[ \\begin{aligned} \\forall A, B \\in M_n(\\mathbb{R}), \\alpha \\in \\mathbb{R}, & (1)\\ ||A|| \\ge 0;\\ ||A|| = 0 \\Leftrightarrow A = \\mathbf{O}; \\\\ & (2)\\ ||\\alpha A|| = |\\alpha| \\cdot ||A||; \\\\ & (3)\\ ||A + B|| \\le ||A|| + ||B||; \\\\ & (4)\\ ||AB|| \\le ||A|| \\cdot ||B||. \\end{aligned} \\] Commonly used examples Frobenius Norm: \\(||A||_F = \\sqrt{\\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^n|a_{ij}|^2}\\) . Natural Norm: \\(||A||_p = \\max\\limits_{\\mathbf{x}\\ne \\mathbf{0}} \\dfrac{||A\\mathbf{x}||_p}{||\\mathbf{x}||_p} = \\max\\limits_{||\\mathbf{x}||_p = 1} ||A\\mathbf{x}||_p\\) , where \\(||\\cdot||_p\\) is the vector norm. \\(||A||_\\infty = \\max\\limits_{1\\le i \\le n}\\sum\\limits_{j=1}^n|a_{ij}|\\) . \\(||A||_1= \\max\\limits_{1\\le j \\le n}\\sum\\limits_{i=1}^n|a_{ij}|\\) . (Spectral Norm) \\(||A||_2= \\sqrt{\\lambda_{max}(A^TA)}\\) . Corollary 7.2 For any vector \\(\\mathbf{x} \\ne 0\\) , matrix \\(A\\) , and any natural norm \\(||\\cdot||\\) , we have \\[ ||A\\mathbf{x}|| \\le ||A|| \\cdot ||\\mathbf{x}||. \\]","title":"Matrix Norms"},{"location":"Mathematics_Basis/NA/Chap_7/#eigenvalues-and-eigenvectors","text":"Definition 7.4 (Recap) If \\(A\\) is a square matrix, the characteristic polynomial of \\(A\\) is defined by \\[ p(\\lambda) = \\text{det}(A - \\lambda I). \\] The roots of \\(p\\) are eigenvalues . If \\(\\lambda\\) is an eigenvalue and \\(\\mathbf{x} \\ne 0\\) satisfies \\((A - \\lambda I)\\mathbf{x} = \\mathbf{0}\\) , then \\(\\mathbf{x}\\) is an eigenvector .","title":"Eigenvalues and Eigenvectors"},{"location":"Mathematics_Basis/NA/Chap_7/#spectral-radius","text":"Definition 7.5 The spectral radius of a matrix \\(A\\) is defined by \\[ \\rho(A) = \\max|\\lambda|,\\ \\ \\text{ where $\\lambda$ is an eigenvalue of $A$}. \\] (Recap that for complex \\(\\lambda = \\alpha + \\beta i\\) , \\(|\\lambda| = \\sqrt{\\alpha^2 + \\beta^2}\\) .) Theorem 7.3 \\(\\forall\\ A \\in M_n(\\mathbb{R})\\) , \\(||A||_2 = \\sqrt{\\rho(A^tA)}\\) . \\(\\rho(A) \\le ||A||\\) , for any natural norm \\(||\\cdot||\\) . Proof A proof for the second property. Suppose \\(\\lambda\\) is an eigenvalue of \\(A\\) with eigenvector \\(\\mathbf{x}\\) and \\(||\\mathbf{x}|| = 1\\) , \\[ |\\lambda| = |\\lambda| \\cdot ||\\mathbf{x}|| = ||\\lambda \\mathbf{x}|| \\le ||A|| \\cdot ||\\mathbf{x}|| = ||A||. \\] Thus, \\[ \\rho(A) = \\max|\\lambda| \\le ||A||. \\]","title":"Spectral Radius"},{"location":"Mathematics_Basis/NA/Chap_7/#convergence-of-matrix","text":"Definition 7.6 \\(A \\in M_n(\\mathbb{R}))\\) is convergent if \\[ \\lim_{k \\rightarrow \\infty}\\left(A^k\\right)_{ij} = 0,\\ \\ \\text{ for each } i = 1, 2, \\dots, n \\text{ and } j = 1, 2, \\dots, n. \\] Theorem 7.4 The following statements are equivalent. \\(A\\) is a convergent matrix. \\(\\lim\\limits_{n \\rightarrow \\infty} ||A^n|| = 0\\) , for some natural norm. \\(\\lim\\limits_{n \\rightarrow \\infty} ||A^n|| = 0\\) , for all natural norms. \\(\\rho(A) < 1\\) . \\(\\forall\\ \\mathbf{x}\\) , \\(\\lim\\limits_{n \\rightarrow \\infty} ||A^n \\mathbf{x}|| = \\mathbf{0}\\) .","title":"Convergence of Matrix"},{"location":"Mathematics_Basis/NA/Chap_7/#iterative-techniques-for-solving-linear-systems","text":"\\[ A\\mathbf{x} = \\mathbf{b} \\Leftrightarrow (D - L - U)\\mathbf{x} = \\mathbf{b} \\Leftrightarrow D\\mathbf{x} = (L + U)\\mathbf{x} + \\mathbf{b} \\\\ \\] Thus, \\[ \\mathbf{x} = D^{-1}(L + U)\\mathbf{x} + D^{-1}\\mathbf{b}. \\]","title":"Iterative Techniques for Solving Linear Systems"},{"location":"Mathematics_Basis/NA/Chap_7/#jacobi-iterative-method","text":"Let \\(T_j = D^{-1}(L+U)\\) and \\(\\mathbf{c}_\\mathbf{j} = D^{-1}\\mathbf{b}\\) , then \\[ \\mathbf{x}^{(k)} = T_j\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\mathbf{j}. \\]","title":"Jacobi Iterative Method"},{"location":"Mathematics_Basis/NA/Chap_7/#gauss-seidel-iterative-method","text":"\\[ \\small \\mathbf{x}^{(k)} = D^{-1}(L\\mathbf{x}^{(k)} + U\\mathbf{x}^{(k - 1)}) + D^{-1}\\mathbf{b} \\Leftrightarrow \\mathbf{x}^{(k)} = (D - L)^{-1}U\\mathbf{x}^{(k - 1)} + (D - L)^{-1}\\mathbf{b} \\] Let \\(T_g = (D - L)^{-1}U\\) and \\(\\mathbf{c}_\\mathbf{g} = (D - L)^{-1}\\mathbf{b}\\) , then \\[ \\mathbf{x}^{(k)} = T_g\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\mathbf{g}. \\]","title":"Gauss-Seidel Iterative Method"},{"location":"Mathematics_Basis/NA/Chap_7/#convergence","text":"Consider the following formula \\[ \\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c}, \\] where \\(\\mathbf{x}^{(0)}\\) is arbitrary. Lemma 7.5 If \\(\\rho(T) \\lt 1\\) , then \\((I - T)^{-1}\\) exists and \\[ (I - T)^{-1} = \\sum\\limits_{j = 0}^\\infty T^j. \\] Proof Suppose \\(\\lambda\\) is an eigenvalue of \\(T\\) with eigenvector \\(\\mathbf{x}\\) , then since \\(T\\mathbf{x} = \\lambda \\mathbf{x} \\Leftrightarrow (I - T)\\mathbf{x} = (1 - \\lambda)\\mathbf{x}\\) , thus \\(1 - \\lambda\\) is an eigenvalue of \\(I - T\\) . Since \\(|\\lambda| \\le \\rho(T) < 1\\) , thus \\(\\lambda = 1\\) is not an eigenvalue of \\(T\\) and \\(0\\) is not an eigenvalue of \\(I - T\\) . Hence, \\((I - T)^{-1}\\) exists. Let \\(S_m = I + T + T^2 + \\cdots + T^m\\) , then \\[ (I - T)S_m = (1 + T + \\cdots + T^m) - (T + T^2 + \\cdots + T^{m + 1}) = I - T^{m + 1}. \\] Since \\(T\\) is convergent, thus \\[ \\lim\\limits_{m \\rightarrow \\infty} (I - T)S_m = \\lim\\limits_{m \\rightarrow \\infty}(I - T^{m + 1}) = I. \\] Thus, \\((I - T)^{-1} = \\lim\\limits_{m \\rightarrow \\infty}S_m = \\sum\\limits_{j = 0}^\\infty T^j\\) . Theorem 7.6 \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) defined by \\(\\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c}\\) converges to the unique solution of \\[ \\mathbf{x} = T\\mathbf{x} + \\mathbf{c} \\] iff \\[ \\rho(T) \\lt 1. \\] Proof \\(\\Rightarrow\\) : Define error \\(\\mathbf{e}^{(k)} = \\mathbf{x} - \\mathbf{x}^{(k)}\\) , then \\[ \\mathbf{e}^{(k)} = (T\\mathbf{x} + c) - (T\\mathbf{x}^{(k - 1)} + c) = T(\\mathbf{x} - \\mathbf{x}^{(k - 1)})T\\mathbf{e}^{(k - 1)} \\Rightarrow \\mathbf{e}^{(k)} = T^k \\mathbf{e}^{(0)}. \\] Since it converges, thus \\[ \\lim\\limits_{k \\rightarrow \\infty} \\mathbf{e}^{(k)} = 0 \\Rightarrow \\forall\\ \\mathbf{e}^{(0)},\\ \\ \\lim\\limits_{k \\rightarrow \\infty} T^k \\mathbf{e}^{(0)} = 0 \\] \\[ \\Leftrightarrow \\rho(T) < 1. \\] \\(\\Leftarrow\\) : \\[ \\mathbf{x}^{(k)} = T^{k}\\mathbf{x}^{(0)} + (T^{k - 1} + \\cdots + T + I) \\mathbf{c}. \\] Since \\(\\rho(T) < 1\\) , \\(T\\) is convergent and \\[ \\lim\\limits_{k \\rightarrow \\infty} T^k \\mathbf{x}^{(0)} = \\mathbf{0}. \\] From Lemma 7.5 , we have \\[ \\lim\\limits_{k \\rightarrow \\infty} \\mathbf{x}^{(k)} = \\lim\\limits_{k \\rightarrow \\infty} T^k\\mathbf{x}^{(0)} + \\left(\\sum\\limits_{j = 0}^\\infty T^j \\right)\\mathbf{c} = \\mathbf{0} + (I - T)^{-1}\\mathbf{c} = (I - T)^{-1}\\mathbf{c}. \\] Thus \\(\\{\\mathbf{x}^{(k)}\\}\\) converges to \\(\\mathbf{x} \\equiv (I - T)^{-1} \\Leftrightarrow \\mathbf{x} = T\\mathbf{x} + c\\) . Corollary 7.7 If \\(||T||\\lt 1\\) for any matrix norm and \\(\\mathbf{c}\\) is a given vector, then \\(\\forall\\ \\mathbf{x}^{(0)}\\in \\mathbb{R}^n\\) and \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) defined by \\(\\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c}\\) converges to \\(\\mathbf{x}\\) , and the following error bounds hold \\(||\\mathbf{x} - \\mathbf{x}^{(k)}|| \\le ||T||^k||\\mathbf{x}^{(0)} - \\mathbf{x}||\\) . \\(||\\mathbf{x} - \\mathbf{x}^{(k)}|| \\le \\dfrac{||T||^k}{1 - ||T||}||\\mathbf{x}^{(1)} - \\mathbf{x}||\\) . Theorem 7.8 Suppose \\(A\\) is strictly diagonally dominant, then \\(\\forall\\ \\mathbf{x}^{(0)}\\) , both Jacobi and Gauss-Seidel methods give \\(\\{\\mathbf{x}^{(k)}\\}_{k=0}^\\infty\\) that converge to the unique solution of \\(A\\mathbf{x} = \\mathbf{b}\\) .","title":"Convergence"},{"location":"Mathematics_Basis/NA/Chap_7/#relaxation-methods","text":"Definition 7.7 Suppose \\(\\mathbf{\\tilde x}\\) is an approximation to the solution of \\(A\\mathbf{x} = \\mathbf{b}\\) , then the residual vector for \\(\\mathbf{\\tilde x}\\) w.r.t this linear system is \\[ \\mathbf{r} = \\mathbf{b} - A\\mathbf{\\tilde x}. \\] Further examine Gauss-Seidel method. \\[ x_i^{(k)} = x_i^{(k - 1)} + \\frac{r_i^{(k)}}{a_{ii}},\\ \\ \\text{ where } r_i^{(k)} = b_i - \\sum_{j \\lt i} a_{ij}x_j^{(k)} - \\sum_{j \\ge i} a_{ij}x_j^{(k - 1)}. \\] Let \\(x_i^{(k)} = x_i^{(k - 1)} + \\omega\\dfrac{r_i^{(k)}}{a_{ii}}\\) , by modifying the value of \\(\\omega\\) , we can somehow get faster convergence. \\(0 \\lt \\omega \\lt 1\\) Under-Relaxation Method \\(\\omega = 1\\) Gauss-Seidel Method \\(\\omega \\gt 1\\) Successive Over-Relaxation Method (SOR) In matrix form, \\[ \\mathbf{x}^{(k)} = (D - \\omega L)^{-1}[(1 - \\omega)D + \\omega U]\\mathbf{x}^{(k - 1)} + (D - \\omega L)^{-1}\\mathbf{b}. \\] Let \\(T_\\omega = (D - \\omega L)^{-1}[(1 - \\omega)D + \\omega U]\\) and \\(\\mathbf{c}_\\omega = (D - \\omega L)^{-1}\\mathbf{b}\\) , then \\[ \\mathbf{x}^{(k)} = T_\\omega\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\omega. \\] Theorem 7.9 (Kahan) If \\(a_{ii} \\ne 0\\) , then \\(\\rho(T_\\omega)\\ge |\\omega -1 |\\) , which implies that SOR method can converge only if \\[ 0 \\lt \\omega \\lt 2. \\] Proof Recap that upper and lower triangular determinant are equal to the product of the entries at its diagnoal. Since \\(D\\) is diagonal, \\(L\\) and \\(U\\) are lower and upper triangular matrix, thus \\[ \\begin{aligned} \\text{det}(T_\\omega) &= \\text{det}((D - \\omega L)^{-1}) \\cdot \\text{det}((1 - \\omega)D + \\omega U) \\\\ &= \\text{det}(D^{-1})(1-\\omega)^n\\text{det}(D) = (1 - \\omega)^n. \\end{aligned} \\] On the other hand, recap that \\[ \\text{det}(T_\\omega) = \\prod\\limits_{i = 1}^n \\lambda_i,\\ \\ \\text{ where $\\lambda_i$ are eigenvalues of $T$}. \\] Thus \\[ \\rho(T_\\omega) = \\max\\limits_{1 \\le i \\le n} |\\lambda_i| \\ge |\\omega - 1|. \\] Theorem 7.10 (Ostrowski-Reich) If \\(A\\) is positive definite and \\(0 \\lt \\omega \\lt 2\\) , the SOR method converges for any choice of initial approximation vector \\(\\mathbf{x}^{(0)}\\) . Theorem 7.11 If \\(A\\) is positive definite and tridiagonal, then \\(\\rho(T_g) = \\rho^2(T_j)\\lt1\\) , and the optimal choice of \\(\\omega\\) for the SOR method is \\[ \\omega = \\frac{2}{1 + \\sqrt{1 - \\rho(T_j)^2}}. \\] With this choice of \\(\\omega\\) , we have \\(\\rho(T_\\omega) = \\omega - 1\\) .","title":"Relaxation Methods"},{"location":"Mathematics_Basis/NA/Chap_7/#error-bounds-and-iterative-refinement","text":"Definition 7.8 The conditional number of the nonsigular matrix \\(A\\) relative to a norm \\(||\\cdot||\\) is \\[ K(A) = ||A|| \\cdot ||A^{-1}||. \\] A matrix \\(A\\) is well-conditioned if \\(K(A)\\) is close to \\(1\\) , and is ill-conditioned when \\(K(A)\\) is significantly greater than \\(1\\) . Proposition If \\(A\\) is symmetric, then \\(K(A)_2 = \\dfrac{\\max|\\lambda|}{\\min|\\lambda|}\\) . \\(K(A)_2 = 1\\) if \\(A\\) is orthogonal. \\(\\forall\\ \\text{ orthogonal matrix } R\\) , \\(K(RA)_2 = K(AR)_2 = K(A)_2\\) . \\(\\forall\\ \\text{ natural norm } ||\\cdot||_p\\) , \\(K(A)_p \\ge 1\\) . \\(K(\\alpha A) = K(A)\\) . Theorem 7.12 For any natural norm \\(||\\cdot||\\) , \\[ ||\\mathbf{x} - \\mathbf{\\tilde x}|| \\le ||\\mathbf{r}|| \\cdot ||A^{-1}||, \\] and if \\(\\mathbf{x} \\ne \\mathbf{0}\\) and \\(\\mathbf{b} \\ne \\mathbf{0}\\) , \\[ \\frac{||\\mathbf{x} - \\mathbf{\\tilde x}||}{||\\mathbf{x}||} \\le ||A||\\cdot||A^{-1}|| \\frac{||\\mathbf{r}||}{||\\mathbf{b}||} = K(A)\\frac{||\\mathbf{r}||}{||\\mathbf{b}||}. \\] Iterative Refinement Step.1 Solve \\(A\\mathbf{x} = \\mathbf{b}\\) and get an approximation solution \\(\\mathbf{x}_{0}\\) . Let \\(i = 1\\) . Step.2 Let \\(\\mathbf{r} = \\mathbf{b} - A\\mathbf{x}_{i - 1}\\) . Step.3 Solve \\(A\\mathbf{d} = \\mathbf{r}\\) and get the solution \\(\\mathbf{d}\\) . Step.4 The better approximation is \\(\\mathbf{x}_{i} = \\mathbf{x}_{i - 1} + \\mathbf{d}.\\) Step.5 Judge whether it's precise enough. If not, let \\(i = i + 1\\) and then repeat from Step.2 . In reality, \\(A\\) and \\(\\mathbf{b}\\) may be perturbed by an amount \\(\\delta A\\) and \\(\\delta \\mathbf{b}\\) . For \\(A(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b} + \\delta\\mathbf{b}\\) , \\[ \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le ||A|| \\cdot ||A^{-1}|| \\cdot \\frac{||\\delta \\mathbf{b}||}{||\\mathbf{b}||}. \\] For \\((A + \\delta A)(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b}\\) , \\[ \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le \\frac{||A^{-1}|| \\cdot ||\\delta A||}{1 - ||A^{-1}|| \\cdot ||\\delta A||} = \\frac{||A|| \\cdot ||A^{-1}|| \\cdot \\frac{||\\delta A||}{||A||}}{1 - ||A|| \\cdot ||A^{-1}|| \\cdot\\frac{||\\delta A||}{||A||}}. \\] Theorem 7.13 If \\(A\\) is nonsingular and \\[ ||\\delta A|| \\lt \\frac{1}{||A^{-1}||}, \\] then \\((A + \\delta A)(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b} + \\delta\\mathbf{b}\\) with the error estimate \\[ \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le \\frac{K(A)}{1 - K(A)\\frac{||\\delta A||}{||A||}}\\left(\\frac{||\\delta A||}{||A||} + \\frac{||\\delta\\mathbf{b}||}{||\\mathbf{b}||}\\right). \\]","title":"Error Bounds and Iterative Refinement"},{"location":"Mathematics_Basis/NA/Chap_8/","text":"Chapter 8 | Approximation Theory \u00b6 Abstract Target \u00b6 Given \\(x_1, \\dots, x_m\\) and \\(y_1, \\dots, y_m\\) sampled from a funciton \\(y = f(x)\\) , or the continuous function \\(f(x)\\) , \\(x \\in [a, b]\\) itself, find a simpler function \\(P(x) \\approx f(x)\\) . Measurement of Error \u00b6 ( Minimax ) minimize (discrete) \\(E_\\infty = \\max\\limits_{1 \\le i \\le m}|P(x_i) - y_i|\\) . (continuous) \\(E_\\infty = \\max\\limits_{a \\le x \\le b}|P(x) - f(x)|\\) . ( Absolute Deviation ) minimize (discrete) \\(E_1 = \\sum\\limits_{i = 1}^{m}|P(x_i) - y_i|\\) . (continuous) \\(E_1 = \\int\\nolimits_{a}^{b}|P(x) - f(x)|dx\\) . ( Least Squares Method ) minimize (discrete) \\(E_2 = \\sum\\limits_{i = 1}^{m}|P(x_i) - y_i|^2\\) . (continuous) \\(E_2 = \\int\\nolimits_{a}^{b}|P(x) - f(x)|^2dx\\) . In this course, we only discuss the minimax and least square parts, and only make \\(P(x)\\) a polynomial function. General Least Squares Approximation \u00b6 Definition 8.0 \\(w\\) is called a weight function if (discrete) \\[ \\forall\\ i \\in \\mathbb{N},\\ \\ w_i > 0. \\] (continuous) \\(w\\) is an integrable function and on the interval \\(I\\) , \\[ \\forall\\ x \\in I,\\ \\ w(x) \\ge 0, \\] \\[ \\forall\\ I' \\subseteq I,\\ \\ w(x) \\not\\equiv 0. \\] Considering the weight function, the least square method can be more general as below. (discrete) \\(E_2 = \\sum\\limits_{i = 1}^{m}w_i|P(x_i) - y_i|^2\\) . (continuous) \\(E_2 = \\int\\nolimits_{a}^{b}w(x)|P(x) - f(x)|^2dx\\) . Discrete Least Squares Approximation \u00b6 Target \u00b6 Approximate a set of data \\(\\{(x_i, y_i) | i = 1, 2, \\dots, m\\}\\) , with an algebraic polynomial \\[ P_n(x) = a_nx^n + a_{n - 1}x^{n - 1} + \\cdots + a_1x + a_0, \\] of degree \\(n < m - 1\\) (in most case \\(n \\ll m\\) ), with the least squares measurement, w.r.t. and weight function \\(w_i \\equiv 1\\) . Solution \u00b6 \\[ \\begin{aligned} E_2 &= \\sum\\limits_{i = 1}^m (y_i - P_n(x_i))^2 \\\\ &= \\sum\\limits_{i = 1}^m y_i^2 - 2\\sum\\limits_{i = 1}^m P_n(x_i)y_i + \\sum\\limits_{i = 1}^m P_n^2(x_i) \\\\ &= \\sum\\limits_{i = 1}^m y_i^2 - 2\\sum\\limits_{i = 1}^m \\left(\\sum\\limits_{j = 0}^n a_jx_i^j\\right)y_i + \\sum\\limits_{i = 1}^m \\left(\\sum\\limits_{j = 0}^n a_jx_i^j \\right)^2 \\\\ &= \\sum\\limits_{i = 1}^m y_i^2 - 2\\sum\\limits_{j = 0}^n a_j \\left( \\sum\\limits_{i = 1}^m y_i x_i^j \\right) + \\sum\\limits_{j = 0}^n \\sum\\limits_{k = 0}^n a_ja_k \\left(\\sum\\limits_{i = 1}^m x_i^{j + k} \\right). \\end{aligned} \\] The necessary condition to minimize \\(E_2\\) is \\[ 0 = \\frac{\\partial E_2}{\\partial a_j} = -2 \\sum\\limits_{i = 1}^m y_i x_i^j + 2 \\sum\\limits_{k = 0}^n a_k \\sum\\limits_{i = 1}^m x_i^{j + k}. \\] Then we get the \\(n + 1\\) normal equations with \\(n + 1\\) unknown \\(a_j\\) , \\[ \\sum\\limits_{k = 0}^n a_k \\sum\\limits_{i = 1}^m x_i^{j + k} = \\sum\\limits_{i = 1}^m y_i x_i^j. \\] Let \\(b_k = \\sum\\limits_{i = 1}^m x_i^k\\) and \\(c_k = \\sum\\limits_{i = 1}^m y_i x_i^k\\) , we can represent the normal equations by \\[ \\begin{bmatrix} b_{0 + 0} & \\cdots & b_{0 + n} \\\\ \\vdots & \\ddots & \\vdots \\\\ b_{n + 0} & \\cdots & b_{n + n} \\end{bmatrix} \\begin{bmatrix} a_0 \\\\ \\vdots \\\\ a_n \\end{bmatrix} = \\begin{bmatrix} c_0 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\] Theorem 8.0 Normal equations have a unique solution if \\(x_i\\) are distinct . Proof Suppose \\(X\\) is an \\(n + 1 \\times m\\) Vandermonde Matrix , which is \\[ X = \\begin{bmatrix} 1 & x_1 & x_1^2 & \\cdots & x_1^n \\\\ 1 & x_2 & x_2^2 & \\cdots & x_2^n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_m & x_m^2 & \\cdots & x_m^n \\\\ \\end{bmatrix}, \\] and let \\(\\mathbf{y} = (y_1, y_1, \\cdots, y_m)^T\\) . Then the normal equations can be represented by (notice the dimension of matrices and vectors), \\[ X X^T \\mathbf{a} = X \\mathbf{y}. \\] Since \\(x_i\\) are distinct, \\(X\\) is a column full rank matrix, namely \\[ \\text{rank}(X) = n + 1. \\] Since \\(x_i \\in \\mathbb{R}\\) , thus \\[ \\text{rank}(X X^T) = \\text{rank}(X) = n + 1. \\] Hence the normal equations have a unique solution. Logarithmic Linear Least Squares \u00b6 To approximate by the function of the form \\[ y = be^{ax}, \\] or \\[ y = bx^a, \\] we can consider the logarithm of the equation by \\[ \\ln y = \\ln b + ax, \\] and \\[ \\ln y = \\ln b + a \\ln x. \\] Note It's a simple algrebric transformation. But we should point out that it minimize the logarithmic linear least squares, but not linear least squares . Just consider the arguments to minimize the following two errors, \\[ E = \\sum\\limits_{i = 1}^m (y_i - be^{ax_i})^2, \\] and \\[ E' = \\sum\\limits_{i = 1}^m (\\ln y - (\\ln b + ax))^2, \\] they are slightly different actually. Continuous Least Squares Approximation \u00b6 Now we consider the continuous function instead of discrete points. Target \u00b6 Approxiate function \\(f \\in C[a, b]\\) , with an algebraic polynomial \\[ P_n(x) = a_nx^n + a_{n - 1}x^{n - 1} + \\cdots + a_1x + a_0, \\] with the least squares measurement, w.r.t the weight function \\(w(x) \\equiv 1\\) . Solution \u00b6 Problem Similarly to the discrete situation, we can derive the normal equations by making \\[ 0 = \\frac{\\partial E}{\\partial a_j}, \\] and we get the normal equations \\[ \\sum\\limits_{k = 0}^n a_k \\int_{a}^{b} x^{j + k} dx = \\int_{a}^{b} x^j f(x)dx. \\] However , notice that \\[ \\int_{a}^{b} x^{j + k} dx = \\frac{b^{j + k + 1} - a^{j + k + 1}}{j + k + 1}. \\] Thus the coefficient of the linear system is a Hilbert matrix , which has a large conditional number . In actual numerical calculation, this gives a large roundoff error. Another disadvantage is that we can not easily get \\(P_{n + 1}(x)\\) from \\(P_{n}(x)\\) , similarly with the discussion of Lagrange interpolation. Hence we introduce a different solution based on the concept of orthogonal polynomials . Definition 8.1 The set of function \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is linearly independent on \\([a, b]\\) if, whenever \\[ \\forall\\ x \\in [a, b], c_0\\varphi_0(x) + c_1\\varphi_1(x) + \\cdots + c_n\\varphi_n(x) = 0 \\] we have \\(c_0 = c_1 = \\cdots = c_n = 0\\) . Otherwise it's linearly dependent . Theorem 8.1 If \\(\\text{deg}(\\varphi_j(x)) = j\\) , then \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is linearly independent on any interval \\([a, b]\\) . Theorem 8.2 \\(\\Pi_n\\) is the linear space spanned by \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) , where \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is linear independent, and \\(\\Pi_n\\) is the set of all polynomials of degree at most n . Definition 8.2 For the linear independent set \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) , \\(\\forall P(x) \\in \\Pi_n\\) , \\(P(x) = \\sum\\limits_{j = 0}^n \\alpha_j \\varphi_j(x)\\) is called a generalized polynomial . Definition 8.3 Inner product w.r.t the weight function \\(w\\) is defined and denoted by (discrete) \\((f, g) = \\sum\\limits_{i = 1}^{m} w_i f(x_i)g(x_i)\\) (continuous) \\((f, g) = \\int\\nolimits_{a}^{b} w(x) f(x) g(x) dx\\) . \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is an orthogonal set of functions for the interval \\([a,b]\\) w.r.t the weight function \\(w\\) if \\[ (\\varphi_j, \\varphi_k) = \\left\\{ \\begin{aligned} & 0, && j \\ne k, \\\\ & \\alpha_k > 0, && j = k. \\end{aligned} \\right. \\] In addition, if \\(\\alpha_k = 1\\) , then the set is orthonormal \uff08\u5355\u4f4d\u6b63\u4ea4\uff09 . Motivation of Orthogonality Considering \\(w(x)\\) , then the normal equations can be represented by \\[ \\int_a^b w(x)f(x)\\varphi_j(x)dx = \\sum\\limits_{k = 0}^n a_k \\int_a^b w(x)\\varphi_k(x)\\varphi_j(x)dx. \\] If we define the orthogonal set of functions as above, the equations reduce to \\[ \\int_a^b w(x)f(x)\\varphi_j(x)dx = a_j \\alpha_j. \\] Theorem 8.3 \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is an orthogonal set of functions on the interval \\([a, b]\\) w.r.t. the weight function \\(w\\) , then the least squares approximation to \\(f\\) on \\([a, b]\\) w.r.t. \\(w\\) is \\[ P(x) = \\sum\\limits_{k = 0}^n a_k \\varphi_k(x), \\] where, for each \\(k = 0, 1, \\dots, n\\) , \\[ a_k = \\frac{\\int_a^b w(x) \\varphi_k(x)f(x)dx}{\\int_a^b w(x) \\varphi_k^2(x)dx} = \\frac{(\\varphi_k, f)}{(\\varphi_{k}, \\varphi_{k})} = \\frac{(\\varphi_k, f)}{\\alpha_k}. \\] Base on the Gram-Schmidt Process , we have the following theorem to construct the orthogonal polynomials on \\([a, b]\\) w.r.t a weight function \\(w\\) . Theorem 8.4 The set of polynomial functions \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) defined in the following way is orthogonal on \\([a, b]\\) w.r.t. the weight function \\(w\\) . \\[ \\begin{aligned} & \\varphi_0(x) \\equiv 1,\\ \\varphi_1(x) = x - B_1, \\\\ & \\varphi_k(x) = (x - B_k) \\varphi_{k - 1}(x) - C_k \\varphi_{k - 2}(x), k \\ge 2. \\\\ \\end{aligned} \\] where \\[ B_k = \\frac{(x\\varphi_{k - 1}, \\varphi_{k - 1})}{\\varphi_{k - 1}, \\varphi_{k - 1}},\\ \\ C_k = \\frac{(x\\varphi_{k - 1}, \\varphi_{k - 2})}{\\varphi_{k - 2}, \\varphi_{k - 2}}. \\] Minimax Approximation \u00b6 The minimax approximation to minimize \\(||P - f||_\\infty||\\) has the following properties. Proposition If \\(f \\in C[a, b]\\) and \\(f\\) is not a polynomial of degree \\(n\\) , then there exists a unique polynomial \\(P(x)\\) s.t. \\(||P - f||\\infty\\) is minimized. \\(P(x)\\) exists and must have both \\(+\\) and \\(-\\) deviation points. (Chebyshev Theorem) The \\(n\\) degree \\(P(x)\\) minimizes \\(||P - f||_\\infty\\) \\(\\Leftrightarrow\\) \\(P(x)\\) has at least \\(n + 2\\) alternating \\(+\\) and \\(-\\) deviation points w.r.t. \\(f\\) . That is, there is a set of points \\(a \\le t_1 < \\dots < t_{n + 2} \\le b\\) (called the Chebyshev alternating sequence ) s.t. \\[ P(t_k) - f(t_k) = (-1)^k||P - f||_\\infty. \\] Here we introduce Chebyshev polynomials to deal with \\(E_\\infty\\) error, and by the way, we can use it to economize the power series. Chebyshev Polynomials \u00b6 Chebyshev polynomials are defined concisely by \\[ T_n(x) = \\cos(n \\arccos x), \\] or equally defined recursively by \\[ \\begin{aligned} & T_0(x) = 1,\\ T_1(x) = x, \\\\ & T_{n + 1}(x) = 2x T_{n}(x) - T_{n - 1}(x). \\end{aligned} \\] Property \\(\\{T_n(x)\\}\\) are orthogonal on \\((-1, 1)\\) w.r.t the weight function \\[ w(x) = \\frac{1}{\\sqrt{1 - x^2}}. \\] \\[ (T_n, T_m) = \\int_{-1}^1 \\frac{T_n(x)T_m(x)}{\\sqrt{1 - x^2}}dx = \\left\\{ \\begin{aligned} & 0, && n \\ne m, \\\\ & \\pi, && n = m = 0, \\\\ & \\frac{\\pi}{2}, && n = m \\ne 0. \\end{aligned} \\right. \\] \\(T_n(x)\\) is a polynomial of degree \\(n\\) with the leading coefficient \\(2^{n - 1}\\) . \\(T_n(x)\\) has \\(n\\) zero points at \\[ \\bar{x}_k = \\cos \\left(\\frac{2k-1}{2n}\\pi\\right),\\ k = 1, 2, \\dots, n. \\] \\(T_n(x)\\) has extrema at \\[ \\bar{x}'_k = \\cos \\frac{k\\pi}n \\text{, with } T(\\bar{x}'_k) = (-1)^k,\\ k = 1, 2, \\dots, n. \\] Monic Chebyshev Polynomials \u00b6 Monic chebyshev polynomials are defined by \\[ \\tilde T_0(x) = 1,\\ \\tilde T_n(x) = \\frac{1}{2^{n - 1}} T_n(x). \\] The following is an important theorem for the position of Chebyshev polynomials. Theorem 8.5 \\[ \\frac{1}{2^{n - 1}} = \\max\\limits_{x \\in [-1, 1]}|\\tilde T_n(x)| \\le \\max\\limits_{x \\in [-1, 1]}|P_n(x)|,\\ \\forall\\ P_n(x) \\in \\tilde \\Pi_n, \\] where \\(\\tilde \\Pi_n\\) denotes the set of all monic polynomials of degree n . From theorem 8.5 , we can answer where to place interpolating points to minimize the error in Lagrange interpolation. Recap that \\[ R(x) = f(x) - P(x) = \\frac{f^{n + 1}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^{n}(x - x_i). \\] To minimize \\(R(x)\\) , since there is no control over \\(\\xi\\) , so we only need to minimize \\[ |w_n(x)| = \\left|\\prod\\limits_{i = 0}^{n}(x - x_i)\\right|. \\] Since \\(w_n(x)\\) is a monic polynomial of degree \\((n + 1)\\) , we can obtain minimum when \\(w_n(x) = \\tilde T_{n + 1}(x)\\) . To make it equal, we can simply make their zero points equal, namely \\[ x_k = \\bar{x}_{k + 1} = \\cos \\frac{2k + 1}{2(n + 1)}\\pi. \\] Corollary 8.6 \\[ \\max\\limits_{x \\in [-1, 1]} |f(x) - P(x)| \\le \\frac{\\max\\limits_{x \\in [-1, 1]}\\left|f^{(n + 1)}(x)\\right|}{2^n (n + 1)!}. \\] Solution of Minimax Approximation \u00b6 Now we come back to the minimax approximation. To minimize \\[ E_\\infty = \\max\\limits_{a \\le x \\le b}|P(x) - f(x)|, \\] with a polynomial of degree \\(n\\) on the interval \\([a, b]\\) , we need the following steps. Step.1 Find the roots of \\(T_{n + 1}(t)\\) on the interval \\([-1, 1]\\) , denoted by \\(t_0, \\dots, t_n\\) . Step.2 Extend it to the interval \\([a, b]\\) by \\[ x_i = \\frac12[(b - a)t_i + a + b]. \\] Step.3 Substitue \\(x_i\\) into \\(f(x)\\) to get \\(y_i\\) . Step.4 Compute the Langrange polynomial \\(P(x)\\) of the interpolating points \\((x_i, y_i)\\) . Ecomomization of Power Series \u00b6 Chebyshev polynomials can also be used to reduce the degree of an approximating polynomials with a minimal loss of accuracy . Consider approximating \\[ P_n(x) = a_nx^n + a_{n - 1}x^{n - 1} + \\cdots + a_1x + a_0 \\] on [-1, 1]. Then the target is to minimize \\[ \\max_{x\\in[-1,1]}|P_n(x) - P_{n - 1}(x)|. \\] Solution \u00b6 Since \\(\\frac{1}{a_n}(P_n(x) - P_{n - 1}(x))\\) is monic, thus \\[ \\max_{x\\in[-1,1]}\\left|\\frac{1}{a_n}((P_n(x) - P_{n - 1}(x))\\right| \\ge \\frac{1}{2^{n - 1}}. \\] Equality occurs when \\[ \\frac{1}{a_n}((P_n(x) - P_{n - 1}(x)) = \\tilde T_n(x). \\] Thus we can choose \\[ P_{n - 1}(x) = P_n(x) - a_n \\tilde T_n(x) \\] with the minimum error value of \\[ \\max_{x\\in[-1,1]}|P_n(x) - P_{n - 1}(x)| = |a_n| \\max_{x\\in[-1,1]}\\left|\\frac{1}{a_n}((P_n(x) - P_{n - 1}(x))\\right| = \\frac{|a_n|}{2^{n - 1}}. \\]","title":"Chap 8"},{"location":"Mathematics_Basis/NA/Chap_8/#chapter-8-approximation-theory","text":"Abstract","title":"Chapter 8 | Approximation Theory"},{"location":"Mathematics_Basis/NA/Chap_8/#target","text":"Given \\(x_1, \\dots, x_m\\) and \\(y_1, \\dots, y_m\\) sampled from a funciton \\(y = f(x)\\) , or the continuous function \\(f(x)\\) , \\(x \\in [a, b]\\) itself, find a simpler function \\(P(x) \\approx f(x)\\) .","title":"Target"},{"location":"Mathematics_Basis/NA/Chap_8/#measurement-of-error","text":"( Minimax ) minimize (discrete) \\(E_\\infty = \\max\\limits_{1 \\le i \\le m}|P(x_i) - y_i|\\) . (continuous) \\(E_\\infty = \\max\\limits_{a \\le x \\le b}|P(x) - f(x)|\\) . ( Absolute Deviation ) minimize (discrete) \\(E_1 = \\sum\\limits_{i = 1}^{m}|P(x_i) - y_i|\\) . (continuous) \\(E_1 = \\int\\nolimits_{a}^{b}|P(x) - f(x)|dx\\) . ( Least Squares Method ) minimize (discrete) \\(E_2 = \\sum\\limits_{i = 1}^{m}|P(x_i) - y_i|^2\\) . (continuous) \\(E_2 = \\int\\nolimits_{a}^{b}|P(x) - f(x)|^2dx\\) . In this course, we only discuss the minimax and least square parts, and only make \\(P(x)\\) a polynomial function.","title":"Measurement of Error"},{"location":"Mathematics_Basis/NA/Chap_8/#general-least-squares-approximation","text":"Definition 8.0 \\(w\\) is called a weight function if (discrete) \\[ \\forall\\ i \\in \\mathbb{N},\\ \\ w_i > 0. \\] (continuous) \\(w\\) is an integrable function and on the interval \\(I\\) , \\[ \\forall\\ x \\in I,\\ \\ w(x) \\ge 0, \\] \\[ \\forall\\ I' \\subseteq I,\\ \\ w(x) \\not\\equiv 0. \\] Considering the weight function, the least square method can be more general as below. (discrete) \\(E_2 = \\sum\\limits_{i = 1}^{m}w_i|P(x_i) - y_i|^2\\) . (continuous) \\(E_2 = \\int\\nolimits_{a}^{b}w(x)|P(x) - f(x)|^2dx\\) .","title":"General Least Squares Approximation"},{"location":"Mathematics_Basis/NA/Chap_8/#discrete-least-squares-approximation","text":"","title":"Discrete Least Squares Approximation"},{"location":"Mathematics_Basis/NA/Chap_8/#target_1","text":"Approximate a set of data \\(\\{(x_i, y_i) | i = 1, 2, \\dots, m\\}\\) , with an algebraic polynomial \\[ P_n(x) = a_nx^n + a_{n - 1}x^{n - 1} + \\cdots + a_1x + a_0, \\] of degree \\(n < m - 1\\) (in most case \\(n \\ll m\\) ), with the least squares measurement, w.r.t. and weight function \\(w_i \\equiv 1\\) .","title":"Target"},{"location":"Mathematics_Basis/NA/Chap_8/#solution","text":"\\[ \\begin{aligned} E_2 &= \\sum\\limits_{i = 1}^m (y_i - P_n(x_i))^2 \\\\ &= \\sum\\limits_{i = 1}^m y_i^2 - 2\\sum\\limits_{i = 1}^m P_n(x_i)y_i + \\sum\\limits_{i = 1}^m P_n^2(x_i) \\\\ &= \\sum\\limits_{i = 1}^m y_i^2 - 2\\sum\\limits_{i = 1}^m \\left(\\sum\\limits_{j = 0}^n a_jx_i^j\\right)y_i + \\sum\\limits_{i = 1}^m \\left(\\sum\\limits_{j = 0}^n a_jx_i^j \\right)^2 \\\\ &= \\sum\\limits_{i = 1}^m y_i^2 - 2\\sum\\limits_{j = 0}^n a_j \\left( \\sum\\limits_{i = 1}^m y_i x_i^j \\right) + \\sum\\limits_{j = 0}^n \\sum\\limits_{k = 0}^n a_ja_k \\left(\\sum\\limits_{i = 1}^m x_i^{j + k} \\right). \\end{aligned} \\] The necessary condition to minimize \\(E_2\\) is \\[ 0 = \\frac{\\partial E_2}{\\partial a_j} = -2 \\sum\\limits_{i = 1}^m y_i x_i^j + 2 \\sum\\limits_{k = 0}^n a_k \\sum\\limits_{i = 1}^m x_i^{j + k}. \\] Then we get the \\(n + 1\\) normal equations with \\(n + 1\\) unknown \\(a_j\\) , \\[ \\sum\\limits_{k = 0}^n a_k \\sum\\limits_{i = 1}^m x_i^{j + k} = \\sum\\limits_{i = 1}^m y_i x_i^j. \\] Let \\(b_k = \\sum\\limits_{i = 1}^m x_i^k\\) and \\(c_k = \\sum\\limits_{i = 1}^m y_i x_i^k\\) , we can represent the normal equations by \\[ \\begin{bmatrix} b_{0 + 0} & \\cdots & b_{0 + n} \\\\ \\vdots & \\ddots & \\vdots \\\\ b_{n + 0} & \\cdots & b_{n + n} \\end{bmatrix} \\begin{bmatrix} a_0 \\\\ \\vdots \\\\ a_n \\end{bmatrix} = \\begin{bmatrix} c_0 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\] Theorem 8.0 Normal equations have a unique solution if \\(x_i\\) are distinct . Proof Suppose \\(X\\) is an \\(n + 1 \\times m\\) Vandermonde Matrix , which is \\[ X = \\begin{bmatrix} 1 & x_1 & x_1^2 & \\cdots & x_1^n \\\\ 1 & x_2 & x_2^2 & \\cdots & x_2^n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_m & x_m^2 & \\cdots & x_m^n \\\\ \\end{bmatrix}, \\] and let \\(\\mathbf{y} = (y_1, y_1, \\cdots, y_m)^T\\) . Then the normal equations can be represented by (notice the dimension of matrices and vectors), \\[ X X^T \\mathbf{a} = X \\mathbf{y}. \\] Since \\(x_i\\) are distinct, \\(X\\) is a column full rank matrix, namely \\[ \\text{rank}(X) = n + 1. \\] Since \\(x_i \\in \\mathbb{R}\\) , thus \\[ \\text{rank}(X X^T) = \\text{rank}(X) = n + 1. \\] Hence the normal equations have a unique solution.","title":"Solution"},{"location":"Mathematics_Basis/NA/Chap_8/#logarithmic-linear-least-squares","text":"To approximate by the function of the form \\[ y = be^{ax}, \\] or \\[ y = bx^a, \\] we can consider the logarithm of the equation by \\[ \\ln y = \\ln b + ax, \\] and \\[ \\ln y = \\ln b + a \\ln x. \\] Note It's a simple algrebric transformation. But we should point out that it minimize the logarithmic linear least squares, but not linear least squares . Just consider the arguments to minimize the following two errors, \\[ E = \\sum\\limits_{i = 1}^m (y_i - be^{ax_i})^2, \\] and \\[ E' = \\sum\\limits_{i = 1}^m (\\ln y - (\\ln b + ax))^2, \\] they are slightly different actually.","title":"Logarithmic Linear Least Squares"},{"location":"Mathematics_Basis/NA/Chap_8/#continuous-least-squares-approximation","text":"Now we consider the continuous function instead of discrete points.","title":"Continuous Least Squares Approximation"},{"location":"Mathematics_Basis/NA/Chap_8/#target_2","text":"Approxiate function \\(f \\in C[a, b]\\) , with an algebraic polynomial \\[ P_n(x) = a_nx^n + a_{n - 1}x^{n - 1} + \\cdots + a_1x + a_0, \\] with the least squares measurement, w.r.t the weight function \\(w(x) \\equiv 1\\) .","title":"Target"},{"location":"Mathematics_Basis/NA/Chap_8/#solution_1","text":"Problem Similarly to the discrete situation, we can derive the normal equations by making \\[ 0 = \\frac{\\partial E}{\\partial a_j}, \\] and we get the normal equations \\[ \\sum\\limits_{k = 0}^n a_k \\int_{a}^{b} x^{j + k} dx = \\int_{a}^{b} x^j f(x)dx. \\] However , notice that \\[ \\int_{a}^{b} x^{j + k} dx = \\frac{b^{j + k + 1} - a^{j + k + 1}}{j + k + 1}. \\] Thus the coefficient of the linear system is a Hilbert matrix , which has a large conditional number . In actual numerical calculation, this gives a large roundoff error. Another disadvantage is that we can not easily get \\(P_{n + 1}(x)\\) from \\(P_{n}(x)\\) , similarly with the discussion of Lagrange interpolation. Hence we introduce a different solution based on the concept of orthogonal polynomials . Definition 8.1 The set of function \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is linearly independent on \\([a, b]\\) if, whenever \\[ \\forall\\ x \\in [a, b], c_0\\varphi_0(x) + c_1\\varphi_1(x) + \\cdots + c_n\\varphi_n(x) = 0 \\] we have \\(c_0 = c_1 = \\cdots = c_n = 0\\) . Otherwise it's linearly dependent . Theorem 8.1 If \\(\\text{deg}(\\varphi_j(x)) = j\\) , then \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is linearly independent on any interval \\([a, b]\\) . Theorem 8.2 \\(\\Pi_n\\) is the linear space spanned by \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) , where \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is linear independent, and \\(\\Pi_n\\) is the set of all polynomials of degree at most n . Definition 8.2 For the linear independent set \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) , \\(\\forall P(x) \\in \\Pi_n\\) , \\(P(x) = \\sum\\limits_{j = 0}^n \\alpha_j \\varphi_j(x)\\) is called a generalized polynomial . Definition 8.3 Inner product w.r.t the weight function \\(w\\) is defined and denoted by (discrete) \\((f, g) = \\sum\\limits_{i = 1}^{m} w_i f(x_i)g(x_i)\\) (continuous) \\((f, g) = \\int\\nolimits_{a}^{b} w(x) f(x) g(x) dx\\) . \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is an orthogonal set of functions for the interval \\([a,b]\\) w.r.t the weight function \\(w\\) if \\[ (\\varphi_j, \\varphi_k) = \\left\\{ \\begin{aligned} & 0, && j \\ne k, \\\\ & \\alpha_k > 0, && j = k. \\end{aligned} \\right. \\] In addition, if \\(\\alpha_k = 1\\) , then the set is orthonormal \uff08\u5355\u4f4d\u6b63\u4ea4\uff09 . Motivation of Orthogonality Considering \\(w(x)\\) , then the normal equations can be represented by \\[ \\int_a^b w(x)f(x)\\varphi_j(x)dx = \\sum\\limits_{k = 0}^n a_k \\int_a^b w(x)\\varphi_k(x)\\varphi_j(x)dx. \\] If we define the orthogonal set of functions as above, the equations reduce to \\[ \\int_a^b w(x)f(x)\\varphi_j(x)dx = a_j \\alpha_j. \\] Theorem 8.3 \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is an orthogonal set of functions on the interval \\([a, b]\\) w.r.t. the weight function \\(w\\) , then the least squares approximation to \\(f\\) on \\([a, b]\\) w.r.t. \\(w\\) is \\[ P(x) = \\sum\\limits_{k = 0}^n a_k \\varphi_k(x), \\] where, for each \\(k = 0, 1, \\dots, n\\) , \\[ a_k = \\frac{\\int_a^b w(x) \\varphi_k(x)f(x)dx}{\\int_a^b w(x) \\varphi_k^2(x)dx} = \\frac{(\\varphi_k, f)}{(\\varphi_{k}, \\varphi_{k})} = \\frac{(\\varphi_k, f)}{\\alpha_k}. \\] Base on the Gram-Schmidt Process , we have the following theorem to construct the orthogonal polynomials on \\([a, b]\\) w.r.t a weight function \\(w\\) . Theorem 8.4 The set of polynomial functions \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) defined in the following way is orthogonal on \\([a, b]\\) w.r.t. the weight function \\(w\\) . \\[ \\begin{aligned} & \\varphi_0(x) \\equiv 1,\\ \\varphi_1(x) = x - B_1, \\\\ & \\varphi_k(x) = (x - B_k) \\varphi_{k - 1}(x) - C_k \\varphi_{k - 2}(x), k \\ge 2. \\\\ \\end{aligned} \\] where \\[ B_k = \\frac{(x\\varphi_{k - 1}, \\varphi_{k - 1})}{\\varphi_{k - 1}, \\varphi_{k - 1}},\\ \\ C_k = \\frac{(x\\varphi_{k - 1}, \\varphi_{k - 2})}{\\varphi_{k - 2}, \\varphi_{k - 2}}. \\]","title":"Solution"},{"location":"Mathematics_Basis/NA/Chap_8/#minimax-approximation","text":"The minimax approximation to minimize \\(||P - f||_\\infty||\\) has the following properties. Proposition If \\(f \\in C[a, b]\\) and \\(f\\) is not a polynomial of degree \\(n\\) , then there exists a unique polynomial \\(P(x)\\) s.t. \\(||P - f||\\infty\\) is minimized. \\(P(x)\\) exists and must have both \\(+\\) and \\(-\\) deviation points. (Chebyshev Theorem) The \\(n\\) degree \\(P(x)\\) minimizes \\(||P - f||_\\infty\\) \\(\\Leftrightarrow\\) \\(P(x)\\) has at least \\(n + 2\\) alternating \\(+\\) and \\(-\\) deviation points w.r.t. \\(f\\) . That is, there is a set of points \\(a \\le t_1 < \\dots < t_{n + 2} \\le b\\) (called the Chebyshev alternating sequence ) s.t. \\[ P(t_k) - f(t_k) = (-1)^k||P - f||_\\infty. \\] Here we introduce Chebyshev polynomials to deal with \\(E_\\infty\\) error, and by the way, we can use it to economize the power series.","title":"Minimax Approximation"},{"location":"Mathematics_Basis/NA/Chap_8/#chebyshev-polynomials","text":"Chebyshev polynomials are defined concisely by \\[ T_n(x) = \\cos(n \\arccos x), \\] or equally defined recursively by \\[ \\begin{aligned} & T_0(x) = 1,\\ T_1(x) = x, \\\\ & T_{n + 1}(x) = 2x T_{n}(x) - T_{n - 1}(x). \\end{aligned} \\] Property \\(\\{T_n(x)\\}\\) are orthogonal on \\((-1, 1)\\) w.r.t the weight function \\[ w(x) = \\frac{1}{\\sqrt{1 - x^2}}. \\] \\[ (T_n, T_m) = \\int_{-1}^1 \\frac{T_n(x)T_m(x)}{\\sqrt{1 - x^2}}dx = \\left\\{ \\begin{aligned} & 0, && n \\ne m, \\\\ & \\pi, && n = m = 0, \\\\ & \\frac{\\pi}{2}, && n = m \\ne 0. \\end{aligned} \\right. \\] \\(T_n(x)\\) is a polynomial of degree \\(n\\) with the leading coefficient \\(2^{n - 1}\\) . \\(T_n(x)\\) has \\(n\\) zero points at \\[ \\bar{x}_k = \\cos \\left(\\frac{2k-1}{2n}\\pi\\right),\\ k = 1, 2, \\dots, n. \\] \\(T_n(x)\\) has extrema at \\[ \\bar{x}'_k = \\cos \\frac{k\\pi}n \\text{, with } T(\\bar{x}'_k) = (-1)^k,\\ k = 1, 2, \\dots, n. \\]","title":"Chebyshev Polynomials"},{"location":"Mathematics_Basis/NA/Chap_8/#monic-chebyshev-polynomials","text":"Monic chebyshev polynomials are defined by \\[ \\tilde T_0(x) = 1,\\ \\tilde T_n(x) = \\frac{1}{2^{n - 1}} T_n(x). \\] The following is an important theorem for the position of Chebyshev polynomials. Theorem 8.5 \\[ \\frac{1}{2^{n - 1}} = \\max\\limits_{x \\in [-1, 1]}|\\tilde T_n(x)| \\le \\max\\limits_{x \\in [-1, 1]}|P_n(x)|,\\ \\forall\\ P_n(x) \\in \\tilde \\Pi_n, \\] where \\(\\tilde \\Pi_n\\) denotes the set of all monic polynomials of degree n . From theorem 8.5 , we can answer where to place interpolating points to minimize the error in Lagrange interpolation. Recap that \\[ R(x) = f(x) - P(x) = \\frac{f^{n + 1}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^{n}(x - x_i). \\] To minimize \\(R(x)\\) , since there is no control over \\(\\xi\\) , so we only need to minimize \\[ |w_n(x)| = \\left|\\prod\\limits_{i = 0}^{n}(x - x_i)\\right|. \\] Since \\(w_n(x)\\) is a monic polynomial of degree \\((n + 1)\\) , we can obtain minimum when \\(w_n(x) = \\tilde T_{n + 1}(x)\\) . To make it equal, we can simply make their zero points equal, namely \\[ x_k = \\bar{x}_{k + 1} = \\cos \\frac{2k + 1}{2(n + 1)}\\pi. \\] Corollary 8.6 \\[ \\max\\limits_{x \\in [-1, 1]} |f(x) - P(x)| \\le \\frac{\\max\\limits_{x \\in [-1, 1]}\\left|f^{(n + 1)}(x)\\right|}{2^n (n + 1)!}. \\]","title":"Monic Chebyshev Polynomials"},{"location":"Mathematics_Basis/NA/Chap_8/#solution-of-minimax-approximation","text":"Now we come back to the minimax approximation. To minimize \\[ E_\\infty = \\max\\limits_{a \\le x \\le b}|P(x) - f(x)|, \\] with a polynomial of degree \\(n\\) on the interval \\([a, b]\\) , we need the following steps. Step.1 Find the roots of \\(T_{n + 1}(t)\\) on the interval \\([-1, 1]\\) , denoted by \\(t_0, \\dots, t_n\\) . Step.2 Extend it to the interval \\([a, b]\\) by \\[ x_i = \\frac12[(b - a)t_i + a + b]. \\] Step.3 Substitue \\(x_i\\) into \\(f(x)\\) to get \\(y_i\\) . Step.4 Compute the Langrange polynomial \\(P(x)\\) of the interpolating points \\((x_i, y_i)\\) .","title":"Solution of Minimax Approximation"},{"location":"Mathematics_Basis/NA/Chap_8/#ecomomization-of-power-series","text":"Chebyshev polynomials can also be used to reduce the degree of an approximating polynomials with a minimal loss of accuracy . Consider approximating \\[ P_n(x) = a_nx^n + a_{n - 1}x^{n - 1} + \\cdots + a_1x + a_0 \\] on [-1, 1]. Then the target is to minimize \\[ \\max_{x\\in[-1,1]}|P_n(x) - P_{n - 1}(x)|. \\]","title":"Ecomomization of Power Series"},{"location":"Mathematics_Basis/NA/Chap_8/#solution_2","text":"Since \\(\\frac{1}{a_n}(P_n(x) - P_{n - 1}(x))\\) is monic, thus \\[ \\max_{x\\in[-1,1]}\\left|\\frac{1}{a_n}((P_n(x) - P_{n - 1}(x))\\right| \\ge \\frac{1}{2^{n - 1}}. \\] Equality occurs when \\[ \\frac{1}{a_n}((P_n(x) - P_{n - 1}(x)) = \\tilde T_n(x). \\] Thus we can choose \\[ P_{n - 1}(x) = P_n(x) - a_n \\tilde T_n(x) \\] with the minimum error value of \\[ \\max_{x\\in[-1,1]}|P_n(x) - P_{n - 1}(x)| = |a_n| \\max_{x\\in[-1,1]}\\left|\\frac{1}{a_n}((P_n(x) - P_{n - 1}(x))\\right| = \\frac{|a_n|}{2^{n - 1}}. \\]","title":"Solution"},{"location":"Mathematics_Basis/NA/Chap_9/","text":"Chapter 9 | Approximating Eigenvalues \u00b6 Power Method \u00b6 The Power Method is an iterative technique used to determine the dominant eigenvalue of a matrix (the eigenvalue with the largest magnitude). Suppose \\(A \\in M_n(\\mathbb{R})\\) with eigenvalues satisfying \\(|\\lambda_1| \\gt |\\lambda_2| \\ge \\dots \\ge |\\lambda_n|\\) and their corresponding linearly independent eigenvectors \\(\\mathbf{v}_\\mathbf{1}, \\dots, \\mathbf{v}_\\mathbf{n}\\) . Original Method Step.1 Start from any \\(\\mathbf{x}^{(0)} \\ne \\mathbf{0}\\) and \\((\\mathbf{x}^{(0)}, \\mathbf{v}_\\mathbf{1}) \\ne 0\\) , and suppose \\(\\mathbf{x}^{(0)} = \\sum\\limits_{j = 1}^n\\beta_j\\mathbf{v}_\\mathbf{j}\\) , \\(\\beta_1 \\ne 0.\\) Step.2 \\(\\mathbf{x}^{(k)} = A \\mathbf{x}^{(k - 1)} = \\sum\\limits_{j = 1}^n\\beta_j\\lambda_j^k\\mathbf{v}_\\mathbf{j} = \\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}\\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\lambda_1^k\\beta_1\\mathbf{v}_\\mathbf{1}\\) . Step.3 \\(A\\mathbf{x}^{(k)} \\approx \\lambda_1^k\\beta_1A\\mathbf{v}_\\mathbf{1} \\approx \\lambda_1\\mathbf{x}^{(k)}\\) , thus \\[ \\lambda_1 = \\frac{\\mathbf{x}^{(k + 1)}_i}{\\mathbf{x}^{(k)}_i}, \\mathbf{v}_\\mathbf{1} = \\frac{\\mathbf{x}^{(k)}}{\\lambda_1^k\\beta_1}. \\] Motivation for Normalization: In the original method, if \\(|\\lambda| > 1\\) then \\(\\mathbf{x}^{(k)}\\) diverges. If \\(|\\lambda| < 1\\) then \\(\\mathbf{x}^{(k)}\\) converges to \\(0\\) . Both cases are not suitable for actual computing. Thus we need normalization to make sure \\(||\\mathbf{x}||_\\infty = 1\\) at each step to guarantee the stability . Normalization Let \\(\\mathbf{u}^{(k-1)} = \\dfrac{\\mathbf{x}^{(k-1)}}{||\\mathbf{x}^{(k-1)}||_\\infty}, \\mathbf{x}^{(k)} = A\\mathbf{u}^{(k - 1)}\\) , then \\[ \\begin{aligned} & \\mathbf{u}^{(k)} = \\frac{\\mathbf{x}^{(k)}}{||\\mathbf{x}^{(k)}||_\\infty} = \\frac{\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}}{\\left|\\left|\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}\\right|\\right|_\\infty} \\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty}, \\\\ & \\mathbf{x}^{(k)} = \\frac{A^k\\mathbf{x}_\\mathbf{0}}{||A^{k - 1}\\mathbf{x}_\\mathbf{0}||} = \\frac{\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}}{\\left|\\left|\\lambda_1^{k - 1} \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^{k - 1}\\mathbf{v}_\\mathbf{j}\\right|\\right|_\\infty} \\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\lambda_1 \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty}=\\lambda_1 \\mathbf{u}^{(k)}. \\end{aligned} \\] Thus \\[ \\begin{aligned} & \\lambda_1 = \\frac{\\mathbf{x}_i^{(k)}}{\\mathbf{u}_i^{(k)}},\\ \\ \\text{ or more directly }, \\lambda_1 = \\mathbf{x}^{(k)}_{p_k},\\ \\ \\text{ where } \\mathbf{u}^{(k)}_{p_k} = 1 = ||\\mathbf{u}^{(k)}||_\\infty, \\\\ & \\hat{\\mathbf{v}_\\mathbf{1}} = \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty}= \\mathbf{u}^{(k)}. \\end{aligned} \\] Remark For multiple eigenvalues \\(\\lambda_1 = \\lambda_2 = \\dots = \\lambda_r\\) , \\[ \\mathbf{x}^{(k)} = \\lambda_1^k\\left( \\sum_{j = 1}^r\\beta_j\\mathbf{v}_\\mathbf{j} + \\sum_{j = r + 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j} \\right)\\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\lambda_1^k\\sum_{j = 1}^r\\beta_j\\mathbf{v}_\\mathbf{j} \\] The method fails to converge if \\(\\lambda_1 = -\\lambda_2\\) . Aitken's \\(\\Delta^2\\) procedure can be used to speed up the convergence. Rate of Convergence \u00b6 Examine \\(\\textbf{x}^{(k)} = \\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\dfrac{\\lambda_j}{\\lambda_1}\\right)^k\\textbf{v}_\\textbf{j}\\) . We find that \\(\\left(\\dfrac{\\lambda_j}{\\lambda_1}\\right)^k\\) determines the rate of convergence, especially \\(\\left|\\dfrac{\\lambda_2}{\\lambda_1}\\right|^k\\) . Thus, suppose \\[ \\mu^{(k)} = \\mathbf{x}^{(k)}_{p_k},\\ \\ \\text{ where } \\mathbf{u}^{(k)}_{p_k} = 1 = ||\\mathbf{u}^{(k)}||_\\infty, \\\\ \\] Then there is a constant \\(L\\) , s.t. for some large \\(m\\) , \\[ |\\mu^{(m)} - \\lambda_1| \\approx L \\left|\\frac{\\lambda_2}{\\lambda_1}\\right|^m, \\] which implies that \\[ \\lim\\limits_{m \\rightarrow \\infty} \\frac{|\\mu^{(m + 1)} - \\lambda_1|}{|\\mu^{(m)} - \\lambda_1|} \\approx \\left|\\frac{\\lambda_2}{\\lambda_1}\\right| < 1. \\] Hence the seqence \\(\\{\\mu^{(m)}\\}\\) converges linearly to \\(\\lambda_1\\) and Aitken's \\(\\Delta^2\\) procedure can be applied. Aitken's \\(\\Delta^2\\) procedure Initially set \\(\\mu_0 = 0\\) , \\(\\mu_1 = 0\\) . For each step \\(k\\) , set \\(\\mu = \\mathbf{x}^{(k)}_p\\) , where \\(\\mathbf{u}^{(k - 1)}_p = 1\\) . set the predicted eigenvalue to \\(\\lambda_1\\) be \\[ \\hat\\mu = \\mu_0 - \\frac{(\\mu_1-\\mu_0)^2}{\\mu - 2\\mu_1 + \\mu_0}. \\] set \\(\\mu_0 = \\mu_1\\) , \\(\\mu_1 = \\mu\\) . Target: Make \\(\\left|\\dfrac{\\lambda_2}{\\lambda_1}\\right|\\) as small as possible. Example Let \\(p = \\dfrac12 (\\lambda_2 + \\lambda_n)\\) , and \\(B = A - pI\\) , then \\(B\\) has the eigenvalues \\[ \\lambda_1 - p, \\lambda_2 - p, \\dots, \\lambda_n - p. \\] And since \\[ \\left|\\frac{\\lambda_2 - p}{\\lambda_1 - p}\\right| < \\left|\\frac{\\lambda_2}{\\lambda_1}\\right|, \\] the iteration for finding the eigenvalues of \\(B\\) converges much faster than that of \\(A\\) . But how to find \\(p\\) ...? Inverse Power Method \u00b6 If \\(A\\) has eigenvalues satisfying \\(|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\gt |\\lambda_n|\\) , then \\(A^{-1}\\) has \\[ \\left|\\frac{1}{\\lambda_n}\\right| \\gt \\left|\\frac{1}{\\lambda_{n-1}}\\right| \\ge \\cdots \\ge \\left|\\frac{1}{\\lambda_1}\\right|. \\] The dominant eigenvalue of \\(A^{-1}\\) is the eigenvalue with the smallest magnitude of \\(A\\) . A way to find eigenvalue \\(\\lambda\\) closest to a given value \\(q\\) Use power method for matrix \\((A - qI)^{-1}\\) , which has the eigenvalues \\[ \\left|\\frac{1}{\\lambda_1 - q}\\right| \\gt \\left|\\frac{1}{\\lambda_2 - q}\\right| \\ge \\cdots \\ge \\left|\\frac{1}{\\lambda_n - q}\\right|. \\] where \\(\\lambda_1\\) is the closest to \\(q\\) . Also, inverse power method can sometimes accelerate to solve \\(\\lambda_1\\) of \\(A\\) . By initialize \\(q\\) with \\[ q =\\frac{\\mathbf{x}^{(0)t}A\\mathbf{x}^{(0)}}{\\mathbf{x}^{(0)t}\\mathbf{x}^{(0)}}. \\] This choice is considered by the property of eigenvalue \\(\\lambda\\) if \\(\\mathbf{x}\\) is an eigenvector of \\(A\\) . \\[ \\lambda =\\frac{\\mathbf{x}^{t}A\\mathbf{x}}{\\mathbf{x}^{t}\\mathbf{x}} = \\frac{\\mathbf{x}^{t}A\\mathbf{x}}{||\\mathbf{x}||^2_2}. \\] Wielandt Deflation \u00b6 Our target is now to solve the eigenvalue of the second largest magnitude associated with its eigenvector. We introduce deflation techniques here, which forming a new matrix \\(B\\) with eigenvalue \\(0, \\lambda_2, \\dots, \\lambda_n\\) , where \\(\\lambda_1\\) is replaced by \\(0\\) . Theorem 9.0 Suppose \\(\\lambda_1, \\dots, \\lambda_n\\) are eigenvalues of \\(A\\) associated with eigenvectors \\(\\mathbf{v}^{(1)}, \\dots, \\mathbf{v}^{(n)}\\) , and \\(\\lambda_1\\) has multiplicity \\(1\\) . Let \\(\\mathbf{x}\\) be a vector with \\(\\mathbf{x}^{t}\\mathbf{v}^{(1)} = 1\\) . Then the matrix \\[ B = A - \\lambda_1\\mathbf{v}^{(1)}\\mathbf{x}^t \\] has eigenvalues \\(0, \\lambda_2, \\dots, \\lambda_n\\) with associated eigenvectors \\(\\mathbf{v}^{(1)}, \\mathbf{w}^{(2)}, \\dots, \\mathbf{w}^{(n)}\\) , where \\(\\mathbf{v}^{(i)}\\) and \\(\\mathbf{w}^{(i)}\\) are related by the equation \\[ \\mathbf{v}^{(i)} = (\\lambda_i - \\lambda_1) \\mathbf{w}^{(i)} + \\lambda_1(\\mathbf{x}^t\\mathbf{w}^{(i)})\\mathbf{v}^{(1)}. \\] With the Theorem 9.0 , the only thing we need to do is to choose vector \\(\\mathbf{x}\\) . Wielandt deflation chooses \\(\\mathbf{x}\\) by the following formula. \\[ \\mathbf{x} = \\frac{1}{\\lambda_1 v_i^{(1)}}(a_{i1}, \\dots, a_{in})^t, \\] where \\(v_i^{(1)}\\) is a nonzero coordinate of the eigenvector \\(\\mathbf{v}^{(1)}\\) and \\(a_{i1}, \\dots a_{in}\\) are the entries of i -th row of \\(A\\) . It's easy to verify that \\[ \\mathbf{x}^t\\mathbf{v}^{(1)} = \\frac{1}{\\lambda_1 v_i^{(1)}}(\\lambda_1 v_i^{(1)}) = 1. \\] Moreover, by this definition, the i -th row of \\(B\\) consists entirely of zero entries . This means that the i -th coordinate of eigenvector \\(\\mathbf{w}\\) is \\(0\\) . Consequently i -th column of \\(B\\) makes no contribution for calculating eigenvector. Thus we can replace \\(B\\) by an \\((n - 1)\\times(n - 1)\\) matrix \\(B'\\) , which deletes the i -th row and i -th column and has eigenvalues \\(\\lambda_2, \\dots, \\lambda_n\\) . Wielandt Deflation Step.1 Find \\(\\lambda_1\\) , the eigenvalue of the largest magnitude, and its associated eigenvector \\(\\mathbf{v}^{(1)}\\) by Power Method. Step.2 Choose \\(\\mathbf{x}\\) and Construct \\(B = A - \\lambda_1 \\mathbf{v}^{(1)} \\mathbf{x}^{(t)}\\) . Step.3 Delete i -th row and i -th column, which gives \\(B'\\) . Step.4 Find \\(\\lambda_2\\) and its associated eigenvector \\(\\mathbf{w}^{(2)}\\) of \\(B'\\) by Power Method. Step.5 Use the formula of Theorem 9.0 to get the associated eigenvector \\(\\mathbf{v}^{(2)}\\) of \\(A\\) . \\[ \\mathbf{v}^{(2)} = (\\lambda_2 - \\lambda_1) \\mathbf{w}^{(2)} + \\lambda_1(\\mathbf{x}^t\\mathbf{w}^{(2)})\\mathbf{v}^{(1)}. \\] Example Find the eigenvalue of the second largest magnitude and its associated eigenvector of \\[ A = \\begin{bmatrix} -4 & 14 & 0 \\\\ -5 & 13 & 0 \\\\ -1 & 0 & 2 \\end{bmatrix}. \\] Solution. Step.1 Find \\(\\lambda_1 = 6\\) and its associated eigenvector \\(\\mathbf{v}^{(1)} = \\left(1, \\dfrac{5}{7}, -\\dfrac14\\right)^t\\) by Power Method of \\(A\\) . Step.2 Choose \\(i = 1\\) , then \\[ \\mathbf{x} = \\frac{1}{6 \\times 1}(-4, 14, 0)^t = \\left(-\\frac23, \\frac83, 0\\right)^t, \\] \\[ B = A - \\lambda_1\\mathbf{v}^{(1)}\\mathbf{x}^t = \\begin{bmatrix} 0 & 0 & 0 \\\\ -\\frac{15}{7} & 3 & 0 \\\\ -2 & \\frac72 & 2 \\end{bmatrix}. \\] Step.3 Delete the i -th row and i -th column, \\[ B' = \\begin{bmatrix} 3 & 0 \\\\ \\frac72 & 2 \\end{bmatrix}. \\] Step.4 Find \\(\\lambda_2 = 3\\) and its associated eigenvector \\(\\mathbf{w}^{(2)'} = \\left(\\dfrac{2}{7}, 1\\right)^t\\) by Power Method of \\(B'\\) . Step.5 Add the dimension of \\(\\mathbf{w}^{(2)'}\\) and \\(\\mathbf{w}^{(2)} = \\left(0, \\dfrac{2}{7}, 1\\right)^t\\) . \\[ \\begin{aligned} \\mathbf{v}^{(2)} &= (\\lambda_2 - \\lambda_1) \\mathbf{w}^{(2)} + \\lambda_1(\\mathbf{x}^t\\mathbf{w}^{(2)})\\mathbf{v}^{(1)} \\\\ &= (3 - 6)\\left(0, \\dfrac{2}{7}, 1\\right)^t + 6\\left[\\left(-\\frac23, \\frac83, 0\\right)\\left(0, \\dfrac{2}{7}, 1\\right)^t\\right]\\left(1, \\dfrac{5}{7}, -\\dfrac14\\right)^t \\\\ &= (4, 2, -4)^t \\overset{\\text{Normalized by } ||\\cdot||_\\infty}{=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=} (1, 0.5, -1)^t. \\end{aligned} \\]","title":"Chap 9"},{"location":"Mathematics_Basis/NA/Chap_9/#chapter-9-approximating-eigenvalues","text":"","title":"Chapter 9 | Approximating Eigenvalues"},{"location":"Mathematics_Basis/NA/Chap_9/#power-method","text":"The Power Method is an iterative technique used to determine the dominant eigenvalue of a matrix (the eigenvalue with the largest magnitude). Suppose \\(A \\in M_n(\\mathbb{R})\\) with eigenvalues satisfying \\(|\\lambda_1| \\gt |\\lambda_2| \\ge \\dots \\ge |\\lambda_n|\\) and their corresponding linearly independent eigenvectors \\(\\mathbf{v}_\\mathbf{1}, \\dots, \\mathbf{v}_\\mathbf{n}\\) . Original Method Step.1 Start from any \\(\\mathbf{x}^{(0)} \\ne \\mathbf{0}\\) and \\((\\mathbf{x}^{(0)}, \\mathbf{v}_\\mathbf{1}) \\ne 0\\) , and suppose \\(\\mathbf{x}^{(0)} = \\sum\\limits_{j = 1}^n\\beta_j\\mathbf{v}_\\mathbf{j}\\) , \\(\\beta_1 \\ne 0.\\) Step.2 \\(\\mathbf{x}^{(k)} = A \\mathbf{x}^{(k - 1)} = \\sum\\limits_{j = 1}^n\\beta_j\\lambda_j^k\\mathbf{v}_\\mathbf{j} = \\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}\\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\lambda_1^k\\beta_1\\mathbf{v}_\\mathbf{1}\\) . Step.3 \\(A\\mathbf{x}^{(k)} \\approx \\lambda_1^k\\beta_1A\\mathbf{v}_\\mathbf{1} \\approx \\lambda_1\\mathbf{x}^{(k)}\\) , thus \\[ \\lambda_1 = \\frac{\\mathbf{x}^{(k + 1)}_i}{\\mathbf{x}^{(k)}_i}, \\mathbf{v}_\\mathbf{1} = \\frac{\\mathbf{x}^{(k)}}{\\lambda_1^k\\beta_1}. \\] Motivation for Normalization: In the original method, if \\(|\\lambda| > 1\\) then \\(\\mathbf{x}^{(k)}\\) diverges. If \\(|\\lambda| < 1\\) then \\(\\mathbf{x}^{(k)}\\) converges to \\(0\\) . Both cases are not suitable for actual computing. Thus we need normalization to make sure \\(||\\mathbf{x}||_\\infty = 1\\) at each step to guarantee the stability . Normalization Let \\(\\mathbf{u}^{(k-1)} = \\dfrac{\\mathbf{x}^{(k-1)}}{||\\mathbf{x}^{(k-1)}||_\\infty}, \\mathbf{x}^{(k)} = A\\mathbf{u}^{(k - 1)}\\) , then \\[ \\begin{aligned} & \\mathbf{u}^{(k)} = \\frac{\\mathbf{x}^{(k)}}{||\\mathbf{x}^{(k)}||_\\infty} = \\frac{\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}}{\\left|\\left|\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}\\right|\\right|_\\infty} \\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty}, \\\\ & \\mathbf{x}^{(k)} = \\frac{A^k\\mathbf{x}_\\mathbf{0}}{||A^{k - 1}\\mathbf{x}_\\mathbf{0}||} = \\frac{\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}}{\\left|\\left|\\lambda_1^{k - 1} \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^{k - 1}\\mathbf{v}_\\mathbf{j}\\right|\\right|_\\infty} \\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\lambda_1 \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty}=\\lambda_1 \\mathbf{u}^{(k)}. \\end{aligned} \\] Thus \\[ \\begin{aligned} & \\lambda_1 = \\frac{\\mathbf{x}_i^{(k)}}{\\mathbf{u}_i^{(k)}},\\ \\ \\text{ or more directly }, \\lambda_1 = \\mathbf{x}^{(k)}_{p_k},\\ \\ \\text{ where } \\mathbf{u}^{(k)}_{p_k} = 1 = ||\\mathbf{u}^{(k)}||_\\infty, \\\\ & \\hat{\\mathbf{v}_\\mathbf{1}} = \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty}= \\mathbf{u}^{(k)}. \\end{aligned} \\] Remark For multiple eigenvalues \\(\\lambda_1 = \\lambda_2 = \\dots = \\lambda_r\\) , \\[ \\mathbf{x}^{(k)} = \\lambda_1^k\\left( \\sum_{j = 1}^r\\beta_j\\mathbf{v}_\\mathbf{j} + \\sum_{j = r + 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j} \\right)\\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\lambda_1^k\\sum_{j = 1}^r\\beta_j\\mathbf{v}_\\mathbf{j} \\] The method fails to converge if \\(\\lambda_1 = -\\lambda_2\\) . Aitken's \\(\\Delta^2\\) procedure can be used to speed up the convergence.","title":"Power Method"},{"location":"Mathematics_Basis/NA/Chap_9/#rate-of-convergence","text":"Examine \\(\\textbf{x}^{(k)} = \\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\dfrac{\\lambda_j}{\\lambda_1}\\right)^k\\textbf{v}_\\textbf{j}\\) . We find that \\(\\left(\\dfrac{\\lambda_j}{\\lambda_1}\\right)^k\\) determines the rate of convergence, especially \\(\\left|\\dfrac{\\lambda_2}{\\lambda_1}\\right|^k\\) . Thus, suppose \\[ \\mu^{(k)} = \\mathbf{x}^{(k)}_{p_k},\\ \\ \\text{ where } \\mathbf{u}^{(k)}_{p_k} = 1 = ||\\mathbf{u}^{(k)}||_\\infty, \\\\ \\] Then there is a constant \\(L\\) , s.t. for some large \\(m\\) , \\[ |\\mu^{(m)} - \\lambda_1| \\approx L \\left|\\frac{\\lambda_2}{\\lambda_1}\\right|^m, \\] which implies that \\[ \\lim\\limits_{m \\rightarrow \\infty} \\frac{|\\mu^{(m + 1)} - \\lambda_1|}{|\\mu^{(m)} - \\lambda_1|} \\approx \\left|\\frac{\\lambda_2}{\\lambda_1}\\right| < 1. \\] Hence the seqence \\(\\{\\mu^{(m)}\\}\\) converges linearly to \\(\\lambda_1\\) and Aitken's \\(\\Delta^2\\) procedure can be applied. Aitken's \\(\\Delta^2\\) procedure Initially set \\(\\mu_0 = 0\\) , \\(\\mu_1 = 0\\) . For each step \\(k\\) , set \\(\\mu = \\mathbf{x}^{(k)}_p\\) , where \\(\\mathbf{u}^{(k - 1)}_p = 1\\) . set the predicted eigenvalue to \\(\\lambda_1\\) be \\[ \\hat\\mu = \\mu_0 - \\frac{(\\mu_1-\\mu_0)^2}{\\mu - 2\\mu_1 + \\mu_0}. \\] set \\(\\mu_0 = \\mu_1\\) , \\(\\mu_1 = \\mu\\) . Target: Make \\(\\left|\\dfrac{\\lambda_2}{\\lambda_1}\\right|\\) as small as possible. Example Let \\(p = \\dfrac12 (\\lambda_2 + \\lambda_n)\\) , and \\(B = A - pI\\) , then \\(B\\) has the eigenvalues \\[ \\lambda_1 - p, \\lambda_2 - p, \\dots, \\lambda_n - p. \\] And since \\[ \\left|\\frac{\\lambda_2 - p}{\\lambda_1 - p}\\right| < \\left|\\frac{\\lambda_2}{\\lambda_1}\\right|, \\] the iteration for finding the eigenvalues of \\(B\\) converges much faster than that of \\(A\\) . But how to find \\(p\\) ...?","title":"Rate of Convergence"},{"location":"Mathematics_Basis/NA/Chap_9/#inverse-power-method","text":"If \\(A\\) has eigenvalues satisfying \\(|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\gt |\\lambda_n|\\) , then \\(A^{-1}\\) has \\[ \\left|\\frac{1}{\\lambda_n}\\right| \\gt \\left|\\frac{1}{\\lambda_{n-1}}\\right| \\ge \\cdots \\ge \\left|\\frac{1}{\\lambda_1}\\right|. \\] The dominant eigenvalue of \\(A^{-1}\\) is the eigenvalue with the smallest magnitude of \\(A\\) . A way to find eigenvalue \\(\\lambda\\) closest to a given value \\(q\\) Use power method for matrix \\((A - qI)^{-1}\\) , which has the eigenvalues \\[ \\left|\\frac{1}{\\lambda_1 - q}\\right| \\gt \\left|\\frac{1}{\\lambda_2 - q}\\right| \\ge \\cdots \\ge \\left|\\frac{1}{\\lambda_n - q}\\right|. \\] where \\(\\lambda_1\\) is the closest to \\(q\\) . Also, inverse power method can sometimes accelerate to solve \\(\\lambda_1\\) of \\(A\\) . By initialize \\(q\\) with \\[ q =\\frac{\\mathbf{x}^{(0)t}A\\mathbf{x}^{(0)}}{\\mathbf{x}^{(0)t}\\mathbf{x}^{(0)}}. \\] This choice is considered by the property of eigenvalue \\(\\lambda\\) if \\(\\mathbf{x}\\) is an eigenvector of \\(A\\) . \\[ \\lambda =\\frac{\\mathbf{x}^{t}A\\mathbf{x}}{\\mathbf{x}^{t}\\mathbf{x}} = \\frac{\\mathbf{x}^{t}A\\mathbf{x}}{||\\mathbf{x}||^2_2}. \\]","title":"Inverse Power Method"},{"location":"Mathematics_Basis/NA/Chap_9/#wielandt-deflation","text":"Our target is now to solve the eigenvalue of the second largest magnitude associated with its eigenvector. We introduce deflation techniques here, which forming a new matrix \\(B\\) with eigenvalue \\(0, \\lambda_2, \\dots, \\lambda_n\\) , where \\(\\lambda_1\\) is replaced by \\(0\\) . Theorem 9.0 Suppose \\(\\lambda_1, \\dots, \\lambda_n\\) are eigenvalues of \\(A\\) associated with eigenvectors \\(\\mathbf{v}^{(1)}, \\dots, \\mathbf{v}^{(n)}\\) , and \\(\\lambda_1\\) has multiplicity \\(1\\) . Let \\(\\mathbf{x}\\) be a vector with \\(\\mathbf{x}^{t}\\mathbf{v}^{(1)} = 1\\) . Then the matrix \\[ B = A - \\lambda_1\\mathbf{v}^{(1)}\\mathbf{x}^t \\] has eigenvalues \\(0, \\lambda_2, \\dots, \\lambda_n\\) with associated eigenvectors \\(\\mathbf{v}^{(1)}, \\mathbf{w}^{(2)}, \\dots, \\mathbf{w}^{(n)}\\) , where \\(\\mathbf{v}^{(i)}\\) and \\(\\mathbf{w}^{(i)}\\) are related by the equation \\[ \\mathbf{v}^{(i)} = (\\lambda_i - \\lambda_1) \\mathbf{w}^{(i)} + \\lambda_1(\\mathbf{x}^t\\mathbf{w}^{(i)})\\mathbf{v}^{(1)}. \\] With the Theorem 9.0 , the only thing we need to do is to choose vector \\(\\mathbf{x}\\) . Wielandt deflation chooses \\(\\mathbf{x}\\) by the following formula. \\[ \\mathbf{x} = \\frac{1}{\\lambda_1 v_i^{(1)}}(a_{i1}, \\dots, a_{in})^t, \\] where \\(v_i^{(1)}\\) is a nonzero coordinate of the eigenvector \\(\\mathbf{v}^{(1)}\\) and \\(a_{i1}, \\dots a_{in}\\) are the entries of i -th row of \\(A\\) . It's easy to verify that \\[ \\mathbf{x}^t\\mathbf{v}^{(1)} = \\frac{1}{\\lambda_1 v_i^{(1)}}(\\lambda_1 v_i^{(1)}) = 1. \\] Moreover, by this definition, the i -th row of \\(B\\) consists entirely of zero entries . This means that the i -th coordinate of eigenvector \\(\\mathbf{w}\\) is \\(0\\) . Consequently i -th column of \\(B\\) makes no contribution for calculating eigenvector. Thus we can replace \\(B\\) by an \\((n - 1)\\times(n - 1)\\) matrix \\(B'\\) , which deletes the i -th row and i -th column and has eigenvalues \\(\\lambda_2, \\dots, \\lambda_n\\) . Wielandt Deflation Step.1 Find \\(\\lambda_1\\) , the eigenvalue of the largest magnitude, and its associated eigenvector \\(\\mathbf{v}^{(1)}\\) by Power Method. Step.2 Choose \\(\\mathbf{x}\\) and Construct \\(B = A - \\lambda_1 \\mathbf{v}^{(1)} \\mathbf{x}^{(t)}\\) . Step.3 Delete i -th row and i -th column, which gives \\(B'\\) . Step.4 Find \\(\\lambda_2\\) and its associated eigenvector \\(\\mathbf{w}^{(2)}\\) of \\(B'\\) by Power Method. Step.5 Use the formula of Theorem 9.0 to get the associated eigenvector \\(\\mathbf{v}^{(2)}\\) of \\(A\\) . \\[ \\mathbf{v}^{(2)} = (\\lambda_2 - \\lambda_1) \\mathbf{w}^{(2)} + \\lambda_1(\\mathbf{x}^t\\mathbf{w}^{(2)})\\mathbf{v}^{(1)}. \\] Example Find the eigenvalue of the second largest magnitude and its associated eigenvector of \\[ A = \\begin{bmatrix} -4 & 14 & 0 \\\\ -5 & 13 & 0 \\\\ -1 & 0 & 2 \\end{bmatrix}. \\] Solution. Step.1 Find \\(\\lambda_1 = 6\\) and its associated eigenvector \\(\\mathbf{v}^{(1)} = \\left(1, \\dfrac{5}{7}, -\\dfrac14\\right)^t\\) by Power Method of \\(A\\) . Step.2 Choose \\(i = 1\\) , then \\[ \\mathbf{x} = \\frac{1}{6 \\times 1}(-4, 14, 0)^t = \\left(-\\frac23, \\frac83, 0\\right)^t, \\] \\[ B = A - \\lambda_1\\mathbf{v}^{(1)}\\mathbf{x}^t = \\begin{bmatrix} 0 & 0 & 0 \\\\ -\\frac{15}{7} & 3 & 0 \\\\ -2 & \\frac72 & 2 \\end{bmatrix}. \\] Step.3 Delete the i -th row and i -th column, \\[ B' = \\begin{bmatrix} 3 & 0 \\\\ \\frac72 & 2 \\end{bmatrix}. \\] Step.4 Find \\(\\lambda_2 = 3\\) and its associated eigenvector \\(\\mathbf{w}^{(2)'} = \\left(\\dfrac{2}{7}, 1\\right)^t\\) by Power Method of \\(B'\\) . Step.5 Add the dimension of \\(\\mathbf{w}^{(2)'}\\) and \\(\\mathbf{w}^{(2)} = \\left(0, \\dfrac{2}{7}, 1\\right)^t\\) . \\[ \\begin{aligned} \\mathbf{v}^{(2)} &= (\\lambda_2 - \\lambda_1) \\mathbf{w}^{(2)} + \\lambda_1(\\mathbf{x}^t\\mathbf{w}^{(2)})\\mathbf{v}^{(1)} \\\\ &= (3 - 6)\\left(0, \\dfrac{2}{7}, 1\\right)^t + 6\\left[\\left(-\\frac23, \\frac83, 0\\right)\\left(0, \\dfrac{2}{7}, 1\\right)^t\\right]\\left(1, \\dfrac{5}{7}, -\\dfrac14\\right)^t \\\\ &= (4, 2, -4)^t \\overset{\\text{Normalized by } ||\\cdot||_\\infty}{=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=} (1, 0.5, -1)^t. \\end{aligned} \\]","title":"Wielandt Deflation"},{"location":"Mathematics_Basis/NA/class_notes/","text":"Class Notes \u00b6 Lecturer \u8bb8\u5a01\u5a01 Textbook \u00b6 Lecture Grade (100 %) \u00b6 Laboratory Projects (36%) 8 labs. Release at a time. Classroom Quiz (4%) Twice, 2% for each test Can also answer questions in class (1% for each) Research Topics (15%) Done in groups 18 topics to choose from In class presentations (5~10 minutes) Homework (5%) Final exam (40%)","title":"Class Notes"},{"location":"Mathematics_Basis/NA/class_notes/#class-notes","text":"Lecturer \u8bb8\u5a01\u5a01","title":"Class Notes"},{"location":"Mathematics_Basis/NA/class_notes/#textbook","text":"","title":"Textbook"},{"location":"Mathematics_Basis/NA/class_notes/#lecture-grade-100","text":"Laboratory Projects (36%) 8 labs. Release at a time. Classroom Quiz (4%) Twice, 2% for each test Can also answer questions in class (1% for each) Research Topics (15%) Done in groups 18 topics to choose from In class presentations (5~10 minutes) Homework (5%) Final exam (40%)","title":"Lecture Grade (100 %)"}]}