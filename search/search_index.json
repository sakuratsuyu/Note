{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u3053\u3053\u306f \u2014\u2014 \u306a\u3093\u3068! \u00b6 A note book by @sakuratsuyu. \u5076\u5c14\u5199\u70b9\u4e1c\u897f\uff0c\u5e0c\u671b\u4f60\u559c\u6b22\u3002 \\[ \\tt{Transfering\\ content\\ from\\ other\\ places\\ these\\ days\\ } \\cdots\\ \\&\\$@\\%\\&\\$@\\%\\&\\$@\\% \\] About me All ids I used: sakuratsuyu , \u685c\u3064\u3086 , \u66e6\u9732 and Mslease . Maybe you can find me somewhere else! Sophomore as a ZJU CS Undergraduate. ZJUSCT Freshman. (\u6478\u5927\u9c7c) MADer. (MAD \u5728\u505a\u4e86\u5728\u505a\u4e86.jpg) Hope you can find anything useful and interesting here! \u2728\u30ad\u30e9\u30ad\u30e9\u76ee\u2728","title":"\u3053\u3053\u306f \u2014\u2014 \u306a\u3093\u3068!"},{"location":"#_1","text":"A note book by @sakuratsuyu. \u5076\u5c14\u5199\u70b9\u4e1c\u897f\uff0c\u5e0c\u671b\u4f60\u559c\u6b22\u3002 \\[ \\tt{Transfering\\ content\\ from\\ other\\ places\\ these\\ days\\ } \\cdots\\ \\&\\$@\\%\\&\\$@\\%\\&\\$@\\% \\] About me All ids I used: sakuratsuyu , \u685c\u3064\u3086 , \u66e6\u9732 and Mslease . Maybe you can find me somewhere else! Sophomore as a ZJU CS Undergraduate. ZJUSCT Freshman. (\u6478\u5927\u9c7c) MADer. (MAD \u5728\u505a\u4e86\u5728\u505a\u4e86.jpg) Hope you can find anything useful and interesting here! \u2728\u30ad\u30e9\u30ad\u30e9\u76ee\u2728","title":"\u3053\u3053\u306f \u2014\u2014 \u306a\u3093\u3068!"},{"location":"comming_soon/","text":"Waiting... \u00b6 Failure Things there are comming soon.","title":"Waiting..."},{"location":"comming_soon/#waiting","text":"Failure Things there are comming soon.","title":"Waiting..."},{"location":"discussion/","text":"Discussion \u00b6 Free talk here about anything! Maybe I can put some blogrolls there... Anyone wants? Blogrolls wants more friends! Isshiki \u4fee's Notebook","title":"Blogroll"},{"location":"discussion/#discussion","text":"Free talk here about anything! Maybe I can put some blogrolls there... Anyone wants? Blogrolls wants more friends! Isshiki \u4fee's Notebook","title":"Discussion"},{"location":"tasklist/","text":"Task List \u00b6 Make a logo! Mathematics Basis Discrete Mathematics | DM Numerical Analysis | NA Abstract Algrebra Computer Science Courses The Missing Semester of your CS education \u2192 Cheat Sheets / Tools Introduction to Computer Systems | ICS HPC 101 Introduction to Computer Vision | ICV Fundemental of Data Strcuture | FDS Digital Logic Design Cheat Sheets / Tools Git Markdown Latex Vim / Neovim Shell Data Wrangling Profiling Pot-pourri Simple Cryptography Guidance to Configure Manjaro + i3wm","title":"Plan"},{"location":"tasklist/#task-list","text":"Make a logo! Mathematics Basis Discrete Mathematics | DM Numerical Analysis | NA Abstract Algrebra Computer Science Courses The Missing Semester of your CS education \u2192 Cheat Sheets / Tools Introduction to Computer Systems | ICS HPC 101 Introduction to Computer Vision | ICV Fundemental of Data Strcuture | FDS Digital Logic Design Cheat Sheets / Tools Git Markdown Latex Vim / Neovim Shell Data Wrangling Profiling Pot-pourri Simple Cryptography Guidance to Configure Manjaro + i3wm","title":"Task List"},{"location":"Computer_Science_Courses/","text":"Title Page \u00b6 Abstract This section stores the notes of the courses that I've learned about computer scicence (CS) from ZJU or other platforms like MIT and Standford. It helps me to build my knowledge system and hope it helps you too. Welcome to point out anything wrong and careless mistakes!","title":"Title Page"},{"location":"Computer_Science_Courses/#title-page","text":"Abstract This section stores the notes of the courses that I've learned about computer scicence (CS) from ZJU or other platforms like MIT and Standford. It helps me to build my knowledge system and hope it helps you too. Welcome to point out anything wrong and careless mistakes!","title":"Title Page"},{"location":"Computer_Science_Courses/ICV/10_Recognition/","text":"","title":"Lecture 10"},{"location":"Computer_Science_Courses/ICV/11_3D_Deep_Learning/","text":"","title":"Lecture 11"},{"location":"Computer_Science_Courses/ICV/12_13_Computational_Photography/","text":"","title":"Lecture 12/13"},{"location":"Computer_Science_Courses/ICV/1_Introduction/","text":"Lecture 1 Introduction \u00b6 Review of Linear Algrebra \u00b6 Affine Transformations \u4eff\u5c04\u53d8\u6362 \u00b6 Affine map = linear map + translation \\[ \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix} \\] Using homogenous coordinates \uff08\u9f50\u6b21\u5750\u6807\uff09 \\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a & b & t_x \\\\ c & d & t_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\] Eigenvectors and Eigenvalues \u00b6 The eigenvalues of symmetric matrices are real numbers. The eigenvalues of positive definite matrices are positive numbers.","title":"Lecture 1"},{"location":"Computer_Science_Courses/ICV/1_Introduction/#lecture-1-introduction","text":"","title":"Lecture 1 Introduction"},{"location":"Computer_Science_Courses/ICV/1_Introduction/#review-of-linear-algrebra","text":"","title":"Review of Linear Algrebra"},{"location":"Computer_Science_Courses/ICV/1_Introduction/#affine-transformations","text":"Affine map = linear map + translation \\[ \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix} \\] Using homogenous coordinates \uff08\u9f50\u6b21\u5750\u6807\uff09 \\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a & b & t_x \\\\ c & d & t_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\]","title":"Affine Transformations \u4eff\u5c04\u53d8\u6362"},{"location":"Computer_Science_Courses/ICV/1_Introduction/#eigenvectors-and-eigenvalues","text":"The eigenvalues of symmetric matrices are real numbers. The eigenvalues of positive definite matrices are positive numbers.","title":"Eigenvectors and Eigenvalues"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/","text":"Lecture 2 Image Formation \u00b6 Camera and Lens \u00b6 Pinhole Camera \u00b6 A barrier to block off most of the rays. The opening is the aperture \u5149\u5708 . Flaws Less light gets through. Diffraction effects\uff08\u884d\u5c04\uff09 Lens \u900f\u955c \u00b6 \\[ \\frac{1}{i} + \\frac{1}{o} = \\frac{1}{f} \\] Focal length \\(f\\) \u7126\u8ddd \u00b6 if \\(o = \\infty\\) , then \\(f = i\\) . Magnification \\(m\\) \u653e\u5927\u7387 \u00b6 \\[ m = \\frac{h_i}{h_o} \\] Field of View (FOV) \u89c6\u91ce \u00b6 Factor \u00b6 Focal Length Longer focal length, Narrower angle of view. Vice versa. Note 50mm / 46\u00b0 (and full frame) is the most similar FOV with human eyes. Thus 50mm lens is called standard lens \uff08\u6807\u51c6\u955c\u5934\uff09. telephoto lens \uff08\u957f\u7126\u955c\u5934\uff0c\u671b\u8fdc\u955c\u5934\uff09\uff1a\u89c6\u91ce\u5c0f\uff0c\u653e\u5927\u7387\u5927 short focal lens \uff08\u77ed\u7126\u955c\u5934\uff0c\u5e7f\u89d2\u955c\u5934\uff09\uff1a\u89c6\u91ce\u5927\uff0c\u653e\u5927\u7387\u5c0f Sensor Size Bigger sensor size, Wider angle of view. Vice versa. Aperture \u5149\u5708 \u00b6 The representation of aperture is its Diameter \\(D\\) . F-Number \\[ N = \\frac{f}{D} \\text{ (mostly greater than 1, around 1.8 ~ 22)} \\] Lens Defocus \u00b6 Blur Circle Diameter (\u5149\u6591\u534a\u5f84) \\[ b = \\frac{D}{i'}|i' -i|, b \\propto D \\propto \\frac{1}{N} \\] Focusing \u5bf9\u7126 \u00b6 Depth of Field (DoF) \u666f\u6df1 \u00b6 \\[ {\\tt DoF} = o_2 - o_1 = \\frac{2of^2cN(o-f)}{f^4-c^2N^2(o-f)^2} \\] From the equation above DoF is almost proportional to \\(N\\) , and thus the Larger aperture, the Smaller F-Number and the Smaller DoF. How to blur the background \u00b6 Large aperture Long focal length Near foreground Far background Geometric Image Formation\uff08\u5b9a\u4f4d\uff09 \u00b6 Camera model \\[ \\begin{bmatrix} u \\\\ v \\end{bmatrix} = \\begin{bmatrix} f \\frac{x}{z} \\\\ f \\frac{y}{z} \\end{bmatrix} \\] Homogeneous Coordinates / Projective Coordinates \u00b6 Suppose that \\(\\begin{bmatrix} x \\\\ y \\\\ w \\end{bmatrix}\\) is the same as \\(\\begin{bmatrix} x/w \\\\ y/w \\\\ 1 \\end{bmatrix}\\) , then we get \\[ \\begin{bmatrix} f & 0 & 0 & 0 \\\\ 0 & f & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} fx \\\\ fy \\\\ z \\\\ \\end{bmatrix} \\cong \\begin{bmatrix} f\\frac{x}{z} \\\\ f\\frac{y}{z} \\\\ 1 \\end{bmatrix} \\] We can also put the image plane in front of the camera (opposite to the previous picture). Perspective Projection \u00b6 Preserevd - Straight lines are still straight Lost - Length and Angle Vanishing Points \u00b6 Properties Any wo parallel lines have the same vanishing point v. v can be outside the image frame or at infinity. Line from C to v is parallel to the lines. Vanishing Lines \u00b6 Multiple vanishing points The direction of the vanishing line tells us the orientation of the plane. Distortion \u00b6 Converging verticals Problem and Solution (View Camera \u79fb\u8f74\u76f8\u673a) Exterior columns appear bigger Due to lens flaws Radial distortion Due to imperfect lens \\[ \\begin{align} r^2 &= {x'}_n^{2} + {y'}_n^{2} \\\\ {x'}_d &= {x'}_n(1 + \\kappa_1r^2 + \\kappa_2r^4) \\\\ {y'}_d &= {y'}_n(1 + \\kappa_1r^2 + \\kappa_2r^4) \\\\ \\end{align} \\] Solution Take a photo of a grid at the same point and then use the mathmatics to calculate and correct radial distortion. Orthographic Projection \u00b6 \\[ \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\\\ 1 \\\\ \\end{bmatrix} \\Rightarrow (x, y) \\] Photometric Image Formation\uff08\u5b9a\u989c\u8272/\u4eae\u5ea6\uff09 \u00b6 Image Sensor \u00b6 CMOS CCD (Charge Coupled Device) Shutter \u00b6 Shutter speed controls exposure time. Color Sensing \u00b6 Color Spaces RGB HSV Practical Color Sensing: Bayer Filter Shading \u7740\u8272 \u00b6 BRDF (Bidirectional Reflectance Distribution Function) \u00b6 \\[ L_r(\\hat{\\textbf{v}_r};\\lambda) = \\int L_i(\\hat{\\textbf{i}_r};\\lambda)f_r(\\hat{\\textbf{v}_r}, \\hat{\\textbf{v}_i}, \\hat{\\textbf{n}}; \\lambda)\\cos^+\\theta_i\\ d\\hat{\\textbf{v}_i} \\] Diffuse (Lambertian) Reflection \u00b6 Shading independent of view direction Specular Term \u00b6 Intensity depends on view direction Blinn-Phong Reflection Model \u00b6 \\[ L = L_a + L_d + L_s = k_aI_a + k_d(I/r^2)\\max(0, \\textbf{n}\\cdot\\textbf{l}) + k_s(I/r^2)\\max(0, \\textbf{n}\\cdot\\textbf{h})^p \\]","title":"Lecture 2"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#lecture-2-image-formation","text":"","title":"Lecture 2 Image Formation"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#camera-and-lens","text":"","title":"Camera and Lens"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#pinhole-camera","text":"A barrier to block off most of the rays. The opening is the aperture \u5149\u5708 . Flaws Less light gets through. Diffraction effects\uff08\u884d\u5c04\uff09","title":"Pinhole Camera"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#lens","text":"\\[ \\frac{1}{i} + \\frac{1}{o} = \\frac{1}{f} \\]","title":"Lens \u900f\u955c"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#focal-length-f","text":"if \\(o = \\infty\\) , then \\(f = i\\) .","title":"Focal length \\(f\\)  \u7126\u8ddd"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#magnification-m","text":"\\[ m = \\frac{h_i}{h_o} \\]","title":"Magnification \\(m\\) \u653e\u5927\u7387"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#field-of-view-fov","text":"","title":"Field of View (FOV) \u89c6\u91ce"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#factor","text":"Focal Length Longer focal length, Narrower angle of view. Vice versa. Note 50mm / 46\u00b0 (and full frame) is the most similar FOV with human eyes. Thus 50mm lens is called standard lens \uff08\u6807\u51c6\u955c\u5934\uff09. telephoto lens \uff08\u957f\u7126\u955c\u5934\uff0c\u671b\u8fdc\u955c\u5934\uff09\uff1a\u89c6\u91ce\u5c0f\uff0c\u653e\u5927\u7387\u5927 short focal lens \uff08\u77ed\u7126\u955c\u5934\uff0c\u5e7f\u89d2\u955c\u5934\uff09\uff1a\u89c6\u91ce\u5927\uff0c\u653e\u5927\u7387\u5c0f Sensor Size Bigger sensor size, Wider angle of view. Vice versa.","title":"Factor"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#aperture","text":"The representation of aperture is its Diameter \\(D\\) . F-Number \\[ N = \\frac{f}{D} \\text{ (mostly greater than 1, around 1.8 ~ 22)} \\]","title":"Aperture \u5149\u5708"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#lens-defocus","text":"Blur Circle Diameter (\u5149\u6591\u534a\u5f84) \\[ b = \\frac{D}{i'}|i' -i|, b \\propto D \\propto \\frac{1}{N} \\]","title":"Lens Defocus"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#focusing","text":"","title":"Focusing \u5bf9\u7126"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#depth-of-field-dof","text":"\\[ {\\tt DoF} = o_2 - o_1 = \\frac{2of^2cN(o-f)}{f^4-c^2N^2(o-f)^2} \\] From the equation above DoF is almost proportional to \\(N\\) , and thus the Larger aperture, the Smaller F-Number and the Smaller DoF.","title":"Depth of Field (DoF) \u666f\u6df1"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#how-to-blur-the-background","text":"Large aperture Long focal length Near foreground Far background","title":"How to blur the background"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#geometric-image-formation","text":"Camera model \\[ \\begin{bmatrix} u \\\\ v \\end{bmatrix} = \\begin{bmatrix} f \\frac{x}{z} \\\\ f \\frac{y}{z} \\end{bmatrix} \\]","title":"Geometric Image Formation\uff08\u5b9a\u4f4d\uff09"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#homogeneous-coordinates-projective-coordinates","text":"Suppose that \\(\\begin{bmatrix} x \\\\ y \\\\ w \\end{bmatrix}\\) is the same as \\(\\begin{bmatrix} x/w \\\\ y/w \\\\ 1 \\end{bmatrix}\\) , then we get \\[ \\begin{bmatrix} f & 0 & 0 & 0 \\\\ 0 & f & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} fx \\\\ fy \\\\ z \\\\ \\end{bmatrix} \\cong \\begin{bmatrix} f\\frac{x}{z} \\\\ f\\frac{y}{z} \\\\ 1 \\end{bmatrix} \\] We can also put the image plane in front of the camera (opposite to the previous picture).","title":"Homogeneous Coordinates / Projective Coordinates"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#perspective-projection","text":"Preserevd - Straight lines are still straight Lost - Length and Angle","title":"Perspective Projection"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#vanishing-points","text":"Properties Any wo parallel lines have the same vanishing point v. v can be outside the image frame or at infinity. Line from C to v is parallel to the lines.","title":"Vanishing Points"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#vanishing-lines","text":"Multiple vanishing points The direction of the vanishing line tells us the orientation of the plane.","title":"Vanishing Lines"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#distortion","text":"Converging verticals Problem and Solution (View Camera \u79fb\u8f74\u76f8\u673a) Exterior columns appear bigger Due to lens flaws Radial distortion Due to imperfect lens \\[ \\begin{align} r^2 &= {x'}_n^{2} + {y'}_n^{2} \\\\ {x'}_d &= {x'}_n(1 + \\kappa_1r^2 + \\kappa_2r^4) \\\\ {y'}_d &= {y'}_n(1 + \\kappa_1r^2 + \\kappa_2r^4) \\\\ \\end{align} \\] Solution Take a photo of a grid at the same point and then use the mathmatics to calculate and correct radial distortion.","title":"Distortion"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#orthographic-projection","text":"\\[ \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\\\ 1 \\\\ \\end{bmatrix} \\Rightarrow (x, y) \\]","title":"Orthographic Projection"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#photometric-image-formation","text":"","title":"Photometric Image Formation\uff08\u5b9a\u989c\u8272/\u4eae\u5ea6\uff09"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#image-sensor","text":"CMOS CCD (Charge Coupled Device)","title":"Image Sensor"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#shutter","text":"Shutter speed controls exposure time.","title":"Shutter"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#color-sensing","text":"Color Spaces RGB HSV Practical Color Sensing: Bayer Filter","title":"Color Sensing"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#shading","text":"","title":"Shading \u7740\u8272"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#brdf-bidirectional-reflectance-distribution-function","text":"\\[ L_r(\\hat{\\textbf{v}_r};\\lambda) = \\int L_i(\\hat{\\textbf{i}_r};\\lambda)f_r(\\hat{\\textbf{v}_r}, \\hat{\\textbf{v}_i}, \\hat{\\textbf{n}}; \\lambda)\\cos^+\\theta_i\\ d\\hat{\\textbf{v}_i} \\]","title":"BRDF (Bidirectional Reflectance Distribution Function)"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#diffuse-lambertian-reflection","text":"Shading independent of view direction","title":"Diffuse (Lambertian) Reflection"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#specular-term","text":"Intensity depends on view direction","title":"Specular Term"},{"location":"Computer_Science_Courses/ICV/2_Image_Formation/#blinn-phong-reflection-model","text":"\\[ L = L_a + L_d + L_s = k_aI_a + k_d(I/r^2)\\max(0, \\textbf{n}\\cdot\\textbf{l}) + k_s(I/r^2)\\max(0, \\textbf{n}\\cdot\\textbf{h})^p \\]","title":"Blinn-Phong Reflection Model"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/","text":"Lecture 3 Imae Processing \u00b6 Image Processing Basis \u00b6 Operations: Incresing contrast, Invert, Blur, Sharpen, Edge Detection Convolution \u00b6 \\[ (f * g)(x) = \\int_{-\\infty}^\\infty f(y)g(x-y)dy \\] discrete 2D form \\[ (f * g)(x) = \\sum_{i, j =-\\infty}^\\infty f(i, j)g(x - i, y - j) \\] Gaussian Blur \u00b6 2D Gaussian function \\[ \\large f(i, j) = \\frac{1}{2\\pi\\sigma^2}e^{-\\frac{i^2 + j^2}{2\\sigma^2}} \\] Usage Define a window size (commmly a square, say \\(n \\times n\\) , and let \\(r = \\lfloor n / 2 \\rfloor\\) ). Select a point, say \\((x, y)\\) and then a window around it. Apply the Gaussian function at each point in the window, and sum them up, namely \\(G(x, y) = \\sum\\limits_{i = x - r}^{x + r}\\sum\\limits_{j = y - r}^{y + r}f(i,j)\\) . Then the \"blurred\" value of point \\((x, y)\\) is \\(G(x, y)\\) . Sharpen \u00b6 An example of kernel matrix \\[ f = \\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 5 & -1 \\\\ 0 & -1 & 0 \\end{bmatrix} \\] An insight Let \\(I\\) be the original image, then the sharpen image \\(I' = I + (I - \\text{blur}(I))\\) , where \\(I - \\text{blur}(I)\\) can be regarded as the high frequency content. Extract Gradients \u00b6 Examples of kernel matrix \\(f = \\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1\\end{bmatrix}\\) extracts horizontal gradients. \\(f = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\) extracts vertical gradients. Bilateral Filters \u00b6 Kernel depends on image content. Better performance but lower efficiency. A trick: Separable Filter A filter is separable if it can be wriiten as the outer product of two other filters. Example \\[ \\frac19 \\begin{bmatrix} 1 & 1 & 1\\\\ 1 & 1 & 1\\\\ 1 & 1 & 1 \\end{bmatrix} = \\frac13 \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\times \\frac13 \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix} \\] Purpose / Advantages : speed up the calculation Image Sampling \u00b6 Abstract Down-sampling \u2192 Reducing image size Aliasing \u00b6 Aliasing is the artifacts due to sampling Higher frequencies need faster sampling. Undersampling creates frequency aliases. Fourier Transform \u00b6 Simly put, fourier transform represents a function as a weighted sum of sines and cosines , where the sines and consines are in various frequencies . Convolution Theorem From the view of Fourier Transform ... \u00b6 Sampling and Aliasing \u00b6 Sampling is repeating frequency contents. Aliasing is mixed frequency contents. Method to reduce aliasing Increase sampling rate Nyquist-Shannon theorem the signal can be perfectly reconstructed if sampled with a frequency larger than \\(2f_0\\) . Anti-aliasing Filtering, then sampling Example Image Magnification \u00b6 Abstract Up-sampling - Inverse of down-sampling An important method: Interpolation . 1D Interpolation \u00b6 Nearest-neighbour Interpolation Not continuous; Not smooth Linear Interpolation Continuous; Not smooth Cubic Interpolation Continuous; Smooth 2D Interpolation \u00b6 (similar to 1D cases) Nearest-neighbour Interpolation Bilinear Interpolation define \\(\\text{lerp}(x, v_0,v_1) = v_0 + x(v_1 - v_0)\\) . Suppose the point in the rectangle surrounded by four points \\(u_{00},u_{01},u_{10},u_{11}\\) . then \\(f(x, y) = \\text{lerp}(t, u_0, u_1)\\) , where \\(u_0 = \\text{lerp}(s, u_{00}, u_{10})\\) and \\(u_1 = \\text{lerp}(s, u_{01}, u_{11})\\) . Bicubic Interpolation Super-Resolution \u00b6 Remains Change Aspect Ratio \u00b6 Remains","title":"Lecture 3"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#lecture-3-imae-processing","text":"","title":"Lecture 3 Imae Processing"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#image-processing-basis","text":"Operations: Incresing contrast, Invert, Blur, Sharpen, Edge Detection","title":"Image Processing Basis"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#convolution","text":"\\[ (f * g)(x) = \\int_{-\\infty}^\\infty f(y)g(x-y)dy \\] discrete 2D form \\[ (f * g)(x) = \\sum_{i, j =-\\infty}^\\infty f(i, j)g(x - i, y - j) \\]","title":"Convolution"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#gaussian-blur","text":"2D Gaussian function \\[ \\large f(i, j) = \\frac{1}{2\\pi\\sigma^2}e^{-\\frac{i^2 + j^2}{2\\sigma^2}} \\] Usage Define a window size (commmly a square, say \\(n \\times n\\) , and let \\(r = \\lfloor n / 2 \\rfloor\\) ). Select a point, say \\((x, y)\\) and then a window around it. Apply the Gaussian function at each point in the window, and sum them up, namely \\(G(x, y) = \\sum\\limits_{i = x - r}^{x + r}\\sum\\limits_{j = y - r}^{y + r}f(i,j)\\) . Then the \"blurred\" value of point \\((x, y)\\) is \\(G(x, y)\\) .","title":"Gaussian Blur"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#sharpen","text":"An example of kernel matrix \\[ f = \\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 5 & -1 \\\\ 0 & -1 & 0 \\end{bmatrix} \\] An insight Let \\(I\\) be the original image, then the sharpen image \\(I' = I + (I - \\text{blur}(I))\\) , where \\(I - \\text{blur}(I)\\) can be regarded as the high frequency content.","title":"Sharpen"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#extract-gradients","text":"Examples of kernel matrix \\(f = \\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1\\end{bmatrix}\\) extracts horizontal gradients. \\(f = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\) extracts vertical gradients.","title":"Extract Gradients"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#bilateral-filters","text":"Kernel depends on image content. Better performance but lower efficiency. A trick: Separable Filter A filter is separable if it can be wriiten as the outer product of two other filters. Example \\[ \\frac19 \\begin{bmatrix} 1 & 1 & 1\\\\ 1 & 1 & 1\\\\ 1 & 1 & 1 \\end{bmatrix} = \\frac13 \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\times \\frac13 \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix} \\] Purpose / Advantages : speed up the calculation","title":"Bilateral Filters"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#image-sampling","text":"Abstract Down-sampling \u2192 Reducing image size","title":"Image Sampling"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#aliasing","text":"Aliasing is the artifacts due to sampling Higher frequencies need faster sampling. Undersampling creates frequency aliases.","title":"Aliasing"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#fourier-transform","text":"Simly put, fourier transform represents a function as a weighted sum of sines and cosines , where the sines and consines are in various frequencies . Convolution Theorem","title":"Fourier Transform"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#from-the-view-of-fourier-transform","text":"","title":"From the view of Fourier Transform ..."},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#sampling-and-aliasing","text":"Sampling is repeating frequency contents. Aliasing is mixed frequency contents. Method to reduce aliasing Increase sampling rate Nyquist-Shannon theorem the signal can be perfectly reconstructed if sampled with a frequency larger than \\(2f_0\\) . Anti-aliasing Filtering, then sampling Example","title":"Sampling and Aliasing"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#image-magnification","text":"Abstract Up-sampling - Inverse of down-sampling An important method: Interpolation .","title":"Image Magnification"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#1d-interpolation","text":"Nearest-neighbour Interpolation Not continuous; Not smooth Linear Interpolation Continuous; Not smooth Cubic Interpolation Continuous; Smooth","title":"1D Interpolation"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#2d-interpolation","text":"(similar to 1D cases) Nearest-neighbour Interpolation Bilinear Interpolation define \\(\\text{lerp}(x, v_0,v_1) = v_0 + x(v_1 - v_0)\\) . Suppose the point in the rectangle surrounded by four points \\(u_{00},u_{01},u_{10},u_{11}\\) . then \\(f(x, y) = \\text{lerp}(t, u_0, u_1)\\) , where \\(u_0 = \\text{lerp}(s, u_{00}, u_{10})\\) and \\(u_1 = \\text{lerp}(s, u_{01}, u_{11})\\) . Bicubic Interpolation","title":"2D Interpolation"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#super-resolution","text":"Remains","title":"Super-Resolution"},{"location":"Computer_Science_Courses/ICV/3_Image_Processing/#change-aspect-ratio","text":"Remains","title":"Change Aspect Ratio"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/","text":"Lecture 4 Model Fitting and Optimization \u00b6 Optimization \u00b6 \\[ \\begin{align} & \\text{Minimize } & f_0(x) \\\\ & \\text{Subject to } &f_i(x) & \\le 0,\\ i = 1, \\dots, m \\\\ & & g_i(x) & = 0,\\ i = 1,\\dots, p \\end{align} \\] \\(x \\in \\mathbb{R}^n\\) is a vector variable to be chosen. \\(f_0\\) is the objective function to be minimized. \\(f_1, \\dots, f_m\\) are the inequality constraint functions. \\(g_1, \\dots, g_p\\) are the equality constraint functions. Model Fitting \u00b6 A typical approach: Minimize the Mean Square Error ( MSE ) \\[ \\hat x = \\mathop{\\arg\\min}\\limits_x \\sum\\limits_i(b_i - a_i^Tx)^2 \\] Reasons to choose MSE \u00b6 Key Assumptions: MSE = MLE with Gaussian noise From Maximum Likelihood Estimation ( MLE ) , the data is assumed to be with Gaussian noise . \\[ b_i = a_i^Tx + n,\\ n \\sim G(0, \\sigma) \\] The likelihood of observing \\((a_i, b_i)\\) is \\[ P[(a_i, b_i)|x] = P[b_i - a_i^Tx] \\propto \\exp\\left(-\\frac{(b_i - a_i^Tx)^2}{2\\sigma^2}\\right) \\] Note If the data points are independent , \\[ \\begin{align} P[(a_1, b_1)(a_2, b_2)\\dots|x] & = \\prod\\limits_iP[(a_i, b_i)|x] = \\prod\\limits_iP[b_i - a_{i^{Tx]}} \\\\ & \\propto \\exp\\left(-\\frac{\\sum_i(b_i - a_i^Tx)^2}{2\\sigma^2}\\right) = \\exp\\left(-\\frac{||Ax-b||^2_2}{2\\sigma^2}\\right) \\end{align} \\] \\[ \\begin{align} \\hat x &= \\mathop{\\arg\\max}\\limits_x P[(a_1, b_1)(a_2, b_2)\\dots|x] \\\\ &= \\mathop{\\arg\\max}\\limits_x \\exp\\left(-\\frac{||Ax-b||^2_2}{2\\sigma^2}\\right) = \\mathop{\\arg\\min}\\limits_x||Ax - b||_2^2 \\end{align} \\] Numerical Methods \u00b6 Analytical Solution \u89e3\u6790\u89e3 \u00b6 The derivative of \\(||Ax-b||^2_2\\) is \\(2A^T(Ax - b)\\) , let it be 0. Then we get \\(\\hat x\\) satisfying \\[ A^TAx = A^Tb \\] But, if no analytical solution ... Approximate Solution \u8fd1\u4f3c\u89e3 \u00b6 Method \\(x \\leftarrow x_0\\) \\(\\text{while not converge}\\) \\(p \\leftarrow \\text{descending_direction(x)}\\) \\(\\alpha \\leftarrow \\text{descending_step(x)}\\) \\(x \\leftarrow x + \\alpha p\\) Gradient Descent (GD) \u00b6 Steepest Descent Method \u00b6 \\[ F(x_0 + \\Delta x) \\approx F(x_0) + J_F\\Delta x \\] Direction \\(\\Delta x = -J^T_F\\) Step To minimize \\(\\phi(a)\\) Backtracking Algorithm Initialize \\(\\alpha\\) with a large value. Decrease \\(\\alpha\\) until \\(\\phi(\\alpha)\\le\\phi(0) + \\gamma\\phi'(0)\\alpha\\) . Advantage Easy to implement Perform well when far from the minimum Disadvantage Converge slowly when near the minimum, which wastes a lot of computation Newton Method \u00b6 \\[ F(x_0 + \\Delta x) \\approx F(x_0) + J_F\\Delta x + \\frac12\\Delta x^TH_F\\Delta x \\] \\(\\Delta x = -H_F^{-1}J^T_F\\) Advantage Faster convergence Disadvantage Hessian matrix requires a lot of computation Gauss-Newton Method \u00b6 For \\(\\hat x = \\mathop{\\arg\\min}\\limits_x ||Ax-b||^2_2 \\overset{\\Delta}{=\\!=}\\mathop{\\arg\\min}\\limits_x||R(x)||^2_2\\) , expand \\(R(x)\\) . \\[ \\begin{align} ||R(x_k+\\Delta x)||^2_2 &\\approx ||R(x_k) + J_R\\Delta x||^2_2 \\newline &= ||R(x_k)||^2_2 + 2R(x_k)^TJ_R\\Delta x + \\Delta x^TJ^T_RJ_R\\Delta x \\end{align} \\] \\(\\Delta x = -(J_R^TJ_R)^{-1}J_R^TR(x_k) = -(J_R^TJ_R)^{-1}J_F^T\\) Compared to Newton Method, \\(J_R^TJ_R\\) is used to approximate \\(H_F\\) . Advantage No need to compute Hessian matrix Faster to converge Disadvantage \\(J^T_RJ_R\\) may be singular. Levenberg-Marquardt (LM) \u00b6 \\(\\Delta x = -(J_R^TJ_R + \\lambda I)^{-1}J_R^TR(x_k)\\) \\(\\lambda \\rightarrow \\infty\\) Steepest Descent \\(\\lambda \\rightarrow 0\\) Gauss-Newton Advantage Start and converge quickly Local Minimum and Global Minimum Convex Optimization \u00b6 Remains Robust Estimation \u00b6 Inlier obeys the model assumption. Outlier differs significantly rom the assumption. Outlier makes MSE fail. To reduce its effect, we can use other loss functions, called robust functions. RANSAC (Random Sample Concensus) \u00b6 The most powerful method to handle outliers. Key ideas The distribution of inliers is similar while outliers differ a lot. Use data point pairs to vote ill-posed Problem \u75c5\u6001\u95ee\u9898 / \u591a\u89e3\u95ee\u9898 \u00b6 The solution is not unique. To make it unique: L2 regularization Suppress redundant variables \\(\\min_x||Ax-b||^2_2,\\ s.t. ||x||_2 \\le 1\\) (the same as \\(\\min_x||Ax-b||^2_2 + \\lambda||x||_2\\) ) L1 regularization Make \\(x\\) sparse \\(\\min_x||Ax-b||^2_2,\\ s.t. ||x||_1 \\le 1\\) (the same as \\(\\min_x||Ax-b||^2_2 + \\lambda||x||_1\\) ) Graphcut and MRF \u00b6 Note A key idea: Neighboring pixels tend to take the same label. Images as Graphs A vertex for each pixel An edge between each pair, weighted by the affinity or similarity between its two vertices Pixel Dissimilarity \\(S(\\textbf{f}_i, \\textbf{f}_j) = \\sqrt{\\sum_k(f_{ik} - f_{jk})^2}\\) Pixel Affinity \\(w(i, j) = A(\\textbf{f}_i, \\textbf{f}_j) = \\exp\\left(\\frac{-1}{2\\sigma^2}S(\\textbf{f}_i, \\textbf{f}_j)\\right)\\) Graph Notation \\(G = (V, E)\\) \\(V\\) : a set of vertices \\(E\\) : a set of edges Graph Cut \u00b6 Cut \\(C=(V_A, V_B)\\) is a parition of vertices \\(V\\) of a graph \\(G\\) into two disjoint subsets \\(V_A\\) and \\(V_B\\) . Cost of Cut \\(\\text{cut}(V_A, V_B) = \\sum_{u\\in V_A, v\\in V_B} w(u, v)\\) Problem with min-cut \u00b6 Bias to cut small, isolated segments Solution: Normalized cut Compute how strongly verices \\(V_A\\) are associated with vertices \\(V\\) \\[ \\text{assoc}(V_A, V) = \\sum_{u\\in V_A, v\\in V} w(u, v) \\] \\[ \\text{NCut}(V_A, V_B) = \\frac{\\text{cut}(V_A, V_B)}{\\text{assoc}(V_A, V)} + \\frac{\\text{cut}(V_A, V_B)}{\\text{assoc}(V_B, V)} \\] Markow Random Field ( MRF ) \u00b6 Graphcut is an exception of MRF . Question","title":"Lecture 4"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#lecture-4-model-fitting-and-optimization","text":"","title":"Lecture 4 Model Fitting and Optimization"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#optimization","text":"\\[ \\begin{align} & \\text{Minimize } & f_0(x) \\\\ & \\text{Subject to } &f_i(x) & \\le 0,\\ i = 1, \\dots, m \\\\ & & g_i(x) & = 0,\\ i = 1,\\dots, p \\end{align} \\] \\(x \\in \\mathbb{R}^n\\) is a vector variable to be chosen. \\(f_0\\) is the objective function to be minimized. \\(f_1, \\dots, f_m\\) are the inequality constraint functions. \\(g_1, \\dots, g_p\\) are the equality constraint functions.","title":"Optimization"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#model-fitting","text":"A typical approach: Minimize the Mean Square Error ( MSE ) \\[ \\hat x = \\mathop{\\arg\\min}\\limits_x \\sum\\limits_i(b_i - a_i^Tx)^2 \\]","title":"Model Fitting"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#reasons-to-choose-mse","text":"Key Assumptions: MSE = MLE with Gaussian noise From Maximum Likelihood Estimation ( MLE ) , the data is assumed to be with Gaussian noise . \\[ b_i = a_i^Tx + n,\\ n \\sim G(0, \\sigma) \\] The likelihood of observing \\((a_i, b_i)\\) is \\[ P[(a_i, b_i)|x] = P[b_i - a_i^Tx] \\propto \\exp\\left(-\\frac{(b_i - a_i^Tx)^2}{2\\sigma^2}\\right) \\] Note If the data points are independent , \\[ \\begin{align} P[(a_1, b_1)(a_2, b_2)\\dots|x] & = \\prod\\limits_iP[(a_i, b_i)|x] = \\prod\\limits_iP[b_i - a_{i^{Tx]}} \\\\ & \\propto \\exp\\left(-\\frac{\\sum_i(b_i - a_i^Tx)^2}{2\\sigma^2}\\right) = \\exp\\left(-\\frac{||Ax-b||^2_2}{2\\sigma^2}\\right) \\end{align} \\] \\[ \\begin{align} \\hat x &= \\mathop{\\arg\\max}\\limits_x P[(a_1, b_1)(a_2, b_2)\\dots|x] \\\\ &= \\mathop{\\arg\\max}\\limits_x \\exp\\left(-\\frac{||Ax-b||^2_2}{2\\sigma^2}\\right) = \\mathop{\\arg\\min}\\limits_x||Ax - b||_2^2 \\end{align} \\]","title":"Reasons to choose MSE"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#numerical-methods","text":"","title":"Numerical Methods"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#analytical-solution","text":"The derivative of \\(||Ax-b||^2_2\\) is \\(2A^T(Ax - b)\\) , let it be 0. Then we get \\(\\hat x\\) satisfying \\[ A^TAx = A^Tb \\] But, if no analytical solution ...","title":"Analytical Solution \u89e3\u6790\u89e3"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#approximate-solution","text":"Method \\(x \\leftarrow x_0\\) \\(\\text{while not converge}\\) \\(p \\leftarrow \\text{descending_direction(x)}\\) \\(\\alpha \\leftarrow \\text{descending_step(x)}\\) \\(x \\leftarrow x + \\alpha p\\)","title":"Approximate Solution \u8fd1\u4f3c\u89e3"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#gradient-descent-gd","text":"","title":"Gradient Descent (GD)"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#steepest-descent-method","text":"\\[ F(x_0 + \\Delta x) \\approx F(x_0) + J_F\\Delta x \\] Direction \\(\\Delta x = -J^T_F\\) Step To minimize \\(\\phi(a)\\) Backtracking Algorithm Initialize \\(\\alpha\\) with a large value. Decrease \\(\\alpha\\) until \\(\\phi(\\alpha)\\le\\phi(0) + \\gamma\\phi'(0)\\alpha\\) . Advantage Easy to implement Perform well when far from the minimum Disadvantage Converge slowly when near the minimum, which wastes a lot of computation","title":"Steepest Descent Method"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#newton-method","text":"\\[ F(x_0 + \\Delta x) \\approx F(x_0) + J_F\\Delta x + \\frac12\\Delta x^TH_F\\Delta x \\] \\(\\Delta x = -H_F^{-1}J^T_F\\) Advantage Faster convergence Disadvantage Hessian matrix requires a lot of computation","title":"Newton Method"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#gauss-newton-method","text":"For \\(\\hat x = \\mathop{\\arg\\min}\\limits_x ||Ax-b||^2_2 \\overset{\\Delta}{=\\!=}\\mathop{\\arg\\min}\\limits_x||R(x)||^2_2\\) , expand \\(R(x)\\) . \\[ \\begin{align} ||R(x_k+\\Delta x)||^2_2 &\\approx ||R(x_k) + J_R\\Delta x||^2_2 \\newline &= ||R(x_k)||^2_2 + 2R(x_k)^TJ_R\\Delta x + \\Delta x^TJ^T_RJ_R\\Delta x \\end{align} \\] \\(\\Delta x = -(J_R^TJ_R)^{-1}J_R^TR(x_k) = -(J_R^TJ_R)^{-1}J_F^T\\) Compared to Newton Method, \\(J_R^TJ_R\\) is used to approximate \\(H_F\\) . Advantage No need to compute Hessian matrix Faster to converge Disadvantage \\(J^T_RJ_R\\) may be singular.","title":"Gauss-Newton Method"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#levenberg-marquardt-lm","text":"\\(\\Delta x = -(J_R^TJ_R + \\lambda I)^{-1}J_R^TR(x_k)\\) \\(\\lambda \\rightarrow \\infty\\) Steepest Descent \\(\\lambda \\rightarrow 0\\) Gauss-Newton Advantage Start and converge quickly Local Minimum and Global Minimum","title":"Levenberg-Marquardt (LM)"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#convex-optimization","text":"Remains","title":"Convex Optimization"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#robust-estimation","text":"Inlier obeys the model assumption. Outlier differs significantly rom the assumption. Outlier makes MSE fail. To reduce its effect, we can use other loss functions, called robust functions.","title":"Robust Estimation"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#ransac-random-sample-concensus","text":"The most powerful method to handle outliers. Key ideas The distribution of inliers is similar while outliers differ a lot. Use data point pairs to vote","title":"RANSAC (Random Sample Concensus)"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#ill-posed-problem","text":"The solution is not unique. To make it unique: L2 regularization Suppress redundant variables \\(\\min_x||Ax-b||^2_2,\\ s.t. ||x||_2 \\le 1\\) (the same as \\(\\min_x||Ax-b||^2_2 + \\lambda||x||_2\\) ) L1 regularization Make \\(x\\) sparse \\(\\min_x||Ax-b||^2_2,\\ s.t. ||x||_1 \\le 1\\) (the same as \\(\\min_x||Ax-b||^2_2 + \\lambda||x||_1\\) )","title":"ill-posed Problem \u75c5\u6001\u95ee\u9898 / \u591a\u89e3\u95ee\u9898"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#graphcut-and-mrf","text":"Note A key idea: Neighboring pixels tend to take the same label. Images as Graphs A vertex for each pixel An edge between each pair, weighted by the affinity or similarity between its two vertices Pixel Dissimilarity \\(S(\\textbf{f}_i, \\textbf{f}_j) = \\sqrt{\\sum_k(f_{ik} - f_{jk})^2}\\) Pixel Affinity \\(w(i, j) = A(\\textbf{f}_i, \\textbf{f}_j) = \\exp\\left(\\frac{-1}{2\\sigma^2}S(\\textbf{f}_i, \\textbf{f}_j)\\right)\\) Graph Notation \\(G = (V, E)\\) \\(V\\) : a set of vertices \\(E\\) : a set of edges","title":"Graphcut and MRF"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#graph-cut","text":"Cut \\(C=(V_A, V_B)\\) is a parition of vertices \\(V\\) of a graph \\(G\\) into two disjoint subsets \\(V_A\\) and \\(V_B\\) . Cost of Cut \\(\\text{cut}(V_A, V_B) = \\sum_{u\\in V_A, v\\in V_B} w(u, v)\\)","title":"Graph Cut"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#problem-with-min-cut","text":"Bias to cut small, isolated segments Solution: Normalized cut Compute how strongly verices \\(V_A\\) are associated with vertices \\(V\\) \\[ \\text{assoc}(V_A, V) = \\sum_{u\\in V_A, v\\in V} w(u, v) \\] \\[ \\text{NCut}(V_A, V_B) = \\frac{\\text{cut}(V_A, V_B)}{\\text{assoc}(V_A, V)} + \\frac{\\text{cut}(V_A, V_B)}{\\text{assoc}(V_B, V)} \\]","title":"Problem with min-cut"},{"location":"Computer_Science_Courses/ICV/4_Model_Fitting_and_Optimization/#markow-random-field-mrf","text":"Graphcut is an exception of MRF . Question","title":"Markow Random Field (MRF)"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/","text":"Lecture 5 Image Matching and Motion Estimation \u00b6 Image Matching \u00b6 Abstract Dectection Description Matching Learned based matching 1. Detection \u00b6 We want uniqueness. Corner Detection \u00b6 Local measures of uniqueness \u00b6 shifting the window in any direction causes a big change distribution of gradients Principal Component Analysis (PCA) \u4e3b\u6210\u5206\u5206\u6790 To describe the distribution of gradients by two eigenvectors. Method Compute the covariance matrix \\(H\\) at each point and compute its eigenvalues \\(\\lambda_1\\) , \\(\\lambda_2\\) . Classify Flat - \\(\\lambda_1\\) and \\(lambda_2\\) are both small. Edge - \\(\\lambda_1 \\gg \\lambda_2\\) or \\(\\lambda_1 \\ll \\lambda_2\\) . Corner - \\(\\lambda_1\\) and $lambda_2are both large. Harri Corner Detector However, computing eigenvalues are expensive. Instead, we use Harris corner detector to indicate it. Compute derivatives at each pixel. Compute covariance matrix \\(H\\) in a Gaussian window around each pixel. Compute corner response function \\(f\\) , given by \\[ f = \\frac{\\lambda_1\\lambda_2}{\\lambda_1 + \\lambda_2} = \\frac{\\det(H)}{\\text{tr}(H)} \\] Threshold \\(f\\) . Find local maxima of response function. Invariance Properties Invariant to intensity shift, translation, rotation Not invariant to intensity scaling, scaling How to find the correct scale? Try each scale and find the scale of maximum of \\(f\\) . Implementation \u2014\u2014 image pyramid with a fixed window size. Blob Detection \u00b6 convolution! Laplacian of Gaussian (LoG) Filter \u00b6 Example \\[ f = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & -4 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix} \\] Laplacian is sensitive to noise. To solve this flaw, we often Smooth image with a Gaussian filter Compute Laplacian \\[ \\nabla^2(f*g) = f*\\nabla^2g, \\text{ where $f$ is the Laplacian and $g$ is the Gaussian} \\] Scale Selection (the same problem as corner detection) \u00b6 The same solution as corner detection - Try and find maximum. Implementation: Difference of Gaussian (DoG) \u00b6 A way to acclerate computation, since LoG can be approximated by DoG. \\[ \\nabla^2G_\\sigma \\approx G_{\\sigma_1} - G_{\\sigma_2} \\] 2. Description \u00b6 We mainly focus on the SIFT (Scale Invariant Feature Transform) descriptor. Orientation Normalization \u00b6 Compute orientation histogram Select dominant orientation Normalize: rotate to fixed orientation Properites of SIFT \u00b6 Extraordinaraily robust matching technique handle changes in viewpoint Theoretically invariant to scale and rotation handle significant changes in illumination Fast Other dectectores and descriptors HOG SURF FAST ORB Fast Retina Key-point 3. Matching \u00b6 Feature matching \u00b6 Define distance function to compare two descriptors. Test all to find the minimum distance. Problem: repeated elements To find that the problem happens: Ratio Test Ratio score = \\(\\frac{||f_1 - f_2||}{||f_1 - f_2'||}\\) best match \\(f_2\\) , second best match \\(f'_2\\) . Another strategy: Mutual Nearest Neighbor \\(f_2\\) is the nearest neighbour of \\(f_1\\) in \\(I_2\\) . \\(f_1\\) is the nearest neighbour of \\(f_2\\) in \\(I_1\\) . 4. Learning based matching \u00b6 Question Motion Estimation \u00b6 Problems Feature-tracking Optical flow Method: Lucas-Kanade Method \u00b6 Assumptions \u00b6 Small motion Brightness constancy Spatial coherence Brightness constancy \u00b6 \\[ \\begin{align} I(x, y, t) &= I(x + u, y + v, t + 1) \\\\ I(x + u, y + v, t + 1) &\\approx I(x, y, t) + I_x u + I_y v + I_t \\\\ I_x u + I_y v + I_t &\\approx 0 \\\\ \\nabla I \\cdot [u, v]^T + I_t &= 0 \\end{align} \\] Aperture Problem \u00b6 Idea: To get more equations \u2192 Spatial coherence constraint Assume the pixel's neighbors have the same motion \\([u, v]\\) . If we use an \\(n\\times n\\) window, \\[ \\begin{bmatrix} I_x(\\textbf{p}_\\textbf{1}) & I_y(\\textbf{p}_\\textbf{1}) \\\\ \\vdots & \\vdots\\\\ I_x(\\textbf{p}_\\textbf{1}) & I_y(\\textbf{p}_{\\textbf{n}^\\textbf{2}}) \\\\ \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = - \\begin{bmatrix} I_t(\\textbf{p}_\\textbf{1}) \\\\ \\vdots \\\\ I_t(\\textbf{p}_{\\textbf{n}^\\textbf{2}}) \\end{bmatrix} \\Rightarrow Ad=b \\] More equations than variables. Thus we solve \\(\\min_d||Ad-b||^2\\) . Solution is given by \\((A^TA)d = A^Tb\\) , when solvable, which is \\(A^TA\\) should be invertible and well-conditioned (eigenvalues should not be too small, similarly with criteria for Harris corner detector). Thus it's easier to estimate the motion of a corner, then edge, then low texture region (flat). Small Motion Assumption \u00b6 For taylor expansion, we want the motion as small as possbile. But probably not - at least larger than one pixel. Solution \u00b6 Reduce the resolution Use the image pyramid","title":"Lecture 5"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#lecture-5-image-matching-and-motion-estimation","text":"","title":"Lecture 5 Image Matching and Motion Estimation"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#image-matching","text":"Abstract Dectection Description Matching Learned based matching","title":"Image Matching"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#1-detection","text":"We want uniqueness.","title":"1. Detection"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#corner-detection","text":"","title":"Corner Detection"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#local-measures-of-uniqueness","text":"shifting the window in any direction causes a big change distribution of gradients Principal Component Analysis (PCA) \u4e3b\u6210\u5206\u5206\u6790 To describe the distribution of gradients by two eigenvectors. Method Compute the covariance matrix \\(H\\) at each point and compute its eigenvalues \\(\\lambda_1\\) , \\(\\lambda_2\\) . Classify Flat - \\(\\lambda_1\\) and \\(lambda_2\\) are both small. Edge - \\(\\lambda_1 \\gg \\lambda_2\\) or \\(\\lambda_1 \\ll \\lambda_2\\) . Corner - \\(\\lambda_1\\) and $lambda_2are both large. Harri Corner Detector However, computing eigenvalues are expensive. Instead, we use Harris corner detector to indicate it. Compute derivatives at each pixel. Compute covariance matrix \\(H\\) in a Gaussian window around each pixel. Compute corner response function \\(f\\) , given by \\[ f = \\frac{\\lambda_1\\lambda_2}{\\lambda_1 + \\lambda_2} = \\frac{\\det(H)}{\\text{tr}(H)} \\] Threshold \\(f\\) . Find local maxima of response function. Invariance Properties Invariant to intensity shift, translation, rotation Not invariant to intensity scaling, scaling How to find the correct scale? Try each scale and find the scale of maximum of \\(f\\) . Implementation \u2014\u2014 image pyramid with a fixed window size.","title":"Local measures of uniqueness"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#blob-detection","text":"convolution!","title":"Blob Detection"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#laplacian-of-gaussian-log-filter","text":"Example \\[ f = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & -4 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix} \\] Laplacian is sensitive to noise. To solve this flaw, we often Smooth image with a Gaussian filter Compute Laplacian \\[ \\nabla^2(f*g) = f*\\nabla^2g, \\text{ where $f$ is the Laplacian and $g$ is the Gaussian} \\]","title":"Laplacian of Gaussian (LoG) Filter"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#scale-selection-the-same-problem-as-corner-detection","text":"The same solution as corner detection - Try and find maximum.","title":"Scale Selection (the same problem as corner detection)"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#implementation-difference-of-gaussian-dog","text":"A way to acclerate computation, since LoG can be approximated by DoG. \\[ \\nabla^2G_\\sigma \\approx G_{\\sigma_1} - G_{\\sigma_2} \\]","title":"Implementation: Difference of Gaussian (DoG)"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#2-description","text":"We mainly focus on the SIFT (Scale Invariant Feature Transform) descriptor.","title":"2. Description"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#orientation-normalization","text":"Compute orientation histogram Select dominant orientation Normalize: rotate to fixed orientation","title":"Orientation Normalization"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#properites-of-sift","text":"Extraordinaraily robust matching technique handle changes in viewpoint Theoretically invariant to scale and rotation handle significant changes in illumination Fast Other dectectores and descriptors HOG SURF FAST ORB Fast Retina Key-point","title":"Properites of SIFT"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#3-matching","text":"","title":"3. Matching"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#feature-matching","text":"Define distance function to compare two descriptors. Test all to find the minimum distance. Problem: repeated elements To find that the problem happens: Ratio Test Ratio score = \\(\\frac{||f_1 - f_2||}{||f_1 - f_2'||}\\) best match \\(f_2\\) , second best match \\(f'_2\\) . Another strategy: Mutual Nearest Neighbor \\(f_2\\) is the nearest neighbour of \\(f_1\\) in \\(I_2\\) . \\(f_1\\) is the nearest neighbour of \\(f_2\\) in \\(I_1\\) .","title":"Feature matching"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#4-learning-based-matching","text":"Question","title":"4. Learning based matching"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#motion-estimation","text":"Problems Feature-tracking Optical flow","title":"Motion Estimation"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#method-lucas-kanade-method","text":"","title":"Method: Lucas-Kanade Method"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#assumptions","text":"Small motion Brightness constancy Spatial coherence","title":"Assumptions"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#brightness-constancy","text":"\\[ \\begin{align} I(x, y, t) &= I(x + u, y + v, t + 1) \\\\ I(x + u, y + v, t + 1) &\\approx I(x, y, t) + I_x u + I_y v + I_t \\\\ I_x u + I_y v + I_t &\\approx 0 \\\\ \\nabla I \\cdot [u, v]^T + I_t &= 0 \\end{align} \\]","title":"Brightness constancy"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#aperture-problem","text":"Idea: To get more equations \u2192 Spatial coherence constraint Assume the pixel's neighbors have the same motion \\([u, v]\\) . If we use an \\(n\\times n\\) window, \\[ \\begin{bmatrix} I_x(\\textbf{p}_\\textbf{1}) & I_y(\\textbf{p}_\\textbf{1}) \\\\ \\vdots & \\vdots\\\\ I_x(\\textbf{p}_\\textbf{1}) & I_y(\\textbf{p}_{\\textbf{n}^\\textbf{2}}) \\\\ \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = - \\begin{bmatrix} I_t(\\textbf{p}_\\textbf{1}) \\\\ \\vdots \\\\ I_t(\\textbf{p}_{\\textbf{n}^\\textbf{2}}) \\end{bmatrix} \\Rightarrow Ad=b \\] More equations than variables. Thus we solve \\(\\min_d||Ad-b||^2\\) . Solution is given by \\((A^TA)d = A^Tb\\) , when solvable, which is \\(A^TA\\) should be invertible and well-conditioned (eigenvalues should not be too small, similarly with criteria for Harris corner detector). Thus it's easier to estimate the motion of a corner, then edge, then low texture region (flat).","title":"Aperture Problem"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#small-motion-assumption","text":"For taylor expansion, we want the motion as small as possbile. But probably not - at least larger than one pixel.","title":"Small Motion Assumption"},{"location":"Computer_Science_Courses/ICV/5_Image_Matching_and_Motion_Estimation/#solution","text":"Reduce the resolution Use the image pyramid","title":"Solution"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/","text":"Lecture 6 Image Stitching \u00b6 Image Warping \u00b6 Info Image filtering changes intensity of image. Image warping (\u7fd8\u66f2) changes shape of image. 2D Transformations \u00b6 Affine Transformations \u4eff\u5c04\u53d8\u6362 \u00b6 \\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\] \\[ \\begin{align} x' &= ax + by + c \\\\ y' &= dx + ey + f \\end{align} \\] \u81ea\u7531\u5ea6 (degree of freedom) \u4e3a 6 \uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u81f3\u5c11 3 \u7ec4\u70b9\uff0c\u4e5f\u5c31\u662f 6 \u4e2a\u65b9\u7a0b\uff0c\u4ee5\u80fd\u591f\u6c42\u89e3\u51fa \\(a, b, c, d, e, f\\) . Projective Transformation (Homography) \u5355\u5e94\u53d8\u6362 \u00b6 \\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} \\cong \\begin{bmatrix} h_{00} & h_{01} & h_{02} \\\\ h_{10} & h_{11} & h_{12} \\\\ h_{20} & h_{21} & h_{22} \\\\ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\] \\[ \\begin{align} x' = \\frac{h_{00}x + h_{01}y + h_{02}}{h_{20}x + h_{11}y + h_{12}} y' = \\frac{h_{10}x + h_{11}y + h_{12}}{h_{20}x + h_{11}y + h_{12}} \\end{align} \\] Constraint: \u4ee4 \\(||(h_{00}, h_{01}, \\dots, h_{22})||\\) (\u5411\u91cf\u8303\u6570) \u89c4\u5b9a\u4e3a \\(1\\) \uff0c\u6216\u8005\u4ee4 \\(h_{22}\\) \u4e3a \\(1\\) . \u8003\u8651\u5230\u4ee5\u4e0a\u7684\u9650\u5236\u6761\u4ef6 (constraint)\uff0c\u81ea\u7531\u5ea6 (degree of freedom) \u4e3a 8 \uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u81f3\u5c11 4 \u7ec4\u70b9\uff0c\u4e5f\u5c31\u662f 8 \u4e2a\u65b9\u7a0b\uff0c\u4ee5\u80fd\u591f\u6c42\u89e3\u51fa \\(h_{ij}\\) . Summary \u00b6 Implementation \u00b6 \u51fa\u4e8e\u65b9\u4fbf\u548c\u53ef\u64cd\u4f5c\u6027\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u8868\u5f81\u6574\u4f53\u7fd8\u66f2\u7684\u51fd\u6570 \\(T\\) \u800c\u4e0d\u662f\u5bf9\u6bcf\u4e2a\u50cf\u7d20\u70b9\u91c7\u53d6\u4e0d\u540c\u7684\u7fd8\u66f2\u7b56\u7565. Forward Warping (Not so good) \u00b6 \u5047\u8bbe\u56fe\u50cf\u51fd\u6570 \\(f:(x, y) \\rightarrow (r, g, b)\\) \u7ecf\u8fc7\u4e86\u53d8\u6362 \\(T: (x, y) \\rightarrow (x', y')\\) \u5f97\u5230\u65b0\u7684\u56fe\u50cf\u51fd\u6570 \\(g:(x', y') \\rightarrow (r, g, b)\\) . If the transformed pixed lands inside the pixels - Quite complicated to solve it. Inverse Warping \u00b6 Why not consider inversely? If the transformed pixed lands inside the pixels Solution 2D Interpolation! Image Stitching \u00b6 \\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} \\cong T \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\] Image Matching \u00b6 \u901a\u8fc7\u56fe\u50cf\u5339\u914d\u5f97\u5230\u5339\u914d\u70b9\uff0c\u6bcf\u4e2a\u5339\u914d\u70b9\u53ef\u4ee5\u63d0\u4f9b\u4e00\u4e2a\u4ee5\u4e0a\u5f62\u5f0f\u7684\u77e9\u9635\u4e58\u6cd5\u7b49\u5f0f. Find \\(T\\) for Image Wraping \u00b6 Affine Transformations \u00b6 \u5bf9\u4e8e\u6bcf\u4e2a\u5339\u914d\uff0c \\[ \\begin{align} x' = ax + by + c \\\\ y' = dx + ey + f \\end{align} \\] \u77e9\u9635\u5f62\u5f0f\u4e3a \\[ \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} x & y & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x & y & 1 \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix} \\] \u5bf9\u4e8e \\(n\\) \u4e2a\u5339\u914d\uff0c\u5f97\u5230\u4ee5\u4e0b\u65b9\u7a0b\u7ec4 \\[ \\begin{bmatrix} x_1 & y_1 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_1 & y_1 & 1 \\\\ x_2 & y_2 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_2 & y_2 & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_n & y_n & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_n & y_n & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix} = \\begin{bmatrix} x_1' \\\\ y_1' \\\\ x_2' \\\\ y_2' \\\\ \\vdots \\\\ x_n' \\\\ y_n' \\end{bmatrix} \\] \u4e5f\u5373 \\[ \\mathbf{A}_{2n \\times 6} \\times \\mathbf{t}_{6 \\times 1} = \\mathbf{b}_{2n \\times 1} \\] Note \u867d\u7136\u9009\u62e9 3 \u7ec4\u70b9\u662f\u80fd\u591f\u89e3\u51fa \\(a, b, c, d, e, f\\) \u7684\u6700\u5c11\u6570\u91cf\uff0c\u4f46\u662f\u5f88\u53ef\u80fd\u5b58\u5728\u90e8\u5206\u884c\u662f\u7ebf\u6027\u76f8\u5173\u4ece\u800c\u5bfc\u81f4\u5947\u5f02\u77e9\u9635\uff0c\u9000\u5316\u4e3a\u6ca1\u6709\u552f\u4e00\u89e3. \u6240\u4ee5\u4e3a\u4e86\u80fd\u591f\u5f97\u5230 \\(a, b, c, d, e, f\\) \u7684\u503c\uff0c\u91c7\u7528\u7684\u65b9\u5f0f\u662f\u53d6\u8f83\u591a\u7684\u70b9\u7136\u540e\u901a\u8fc7\u6700\u5c0f\u4e8c\u4e58\u6cd5\u8ba1\u7b97\uff0c\u4e5f\u5373\u8ba9 \\(\\mathbf{t}\\) \u6700\u5c0f\u5316 \\(||\\mathbf{A} \\mathbf{t} - \\mathbf{b}||\\) . Method of Least Square \\[ \\begin{align} \\mathbf{A^{T}At} &= \\mathbf{A^{T}b} \\\\ t &= \\mathbf{A^{T}A^{-1}A^{T}b} \\end{align} \\] Projective Transformations \u00b6 \u7c7b\u4f3c\u5730\uff0c\u5bf9\u4e8e\u6bcf\u4e2a\u5339\u914d\uff0c \\[ \\begin{align} x_i'(h_{20}x_i + h_{21} y_i + h_{22}) = h_{00}x_i + h_{01}y_i + h_{02} \\\\ y_i'(h_{20}x_i + h_{21} y_i + h_{22}) = h_{10}x_i + h_{11}y_i + h_{12} \\\\ \\end{align} \\] \u77e9\u9635\u5f62\u5f0f\u4e3a \\[ \\begin{bmatrix} x_i & y_i & 1 & 0 & 0 & 0 & -x_i'x_i & -x_i'y_i & -x_i'\\\\ 0 & 0 & 0 & x_i & y_i & 1 & -y_i'x_i & -y_i'y_i & -y_i' \\end{bmatrix} \\begin{bmatrix} h_{00} \\\\ h_{01} \\\\ h_{02} \\\\ h_{10} \\\\ h_{11} \\\\ h_{12} \\\\ h_{20} \\\\ h_{21} \\\\ h_{22} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\] \u5bf9\u4e8e \\(n\\) \u4e2a\u5339\u914d\uff0c\u5f97\u5230\u4ee5\u4e0b\u65b9\u7a0b\u7ec4 \\[ \\begin{bmatrix} x_1 & y_1 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_1 & y_1 & 1 \\\\ x_2 & y_2 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_2 & y_2 & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_n & y_n & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_n & y_n & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix} = \\begin{bmatrix} x_1' \\\\ y_1' \\\\ x_2' \\\\ y_2' \\\\ \\vdots \\\\ x_n' \\\\ y_n' \\end{bmatrix} \\] \u4e5f\u5373 \\[ \\mathbf{A}_{2n \\times 9} \\times \\mathbf{t}_{9 \\times 1} = \\mathbf{b}_{2n \\times 1} \\] \u540c\u6837\u5730\uff0c\u6211\u4eec\u8981\u6700\u5c0f\u5316 \\(||\\mathbf{Ah - 0}|| = \\mathbf{||Ah||}\\) . Contraint: \\(||\\mathbf{h}|| = 1\\) \u5982\u679c\u4f7f\u7528\u7684\u662f\u5411\u91cf\u8303\u6570\u4e3a \\(1\\) \u7684\u9650\u5236\uff0c\u90a3\u4e48\u6211\u4eec\u53ea\u9700\u8981\u786e\u5b9a \\(\\mathbf{h}\\) \u7684\u65b9\u5411 \\(\\mathbf{\\hat h}\\) . \u6570\u5b66\u4e0a\u53ef\u4ee5\u8bc1\u660e\u89e3 \\(\\mathbf{\\hat h}\\) \u662f \\(\\mathbf{A^{T}A}\\) \u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf. Robustness \u00b6 Outliers \u56fe\u50cf\u5339\u914d\u4e5f\u4e0d\u662f\u5b8c\u7f8e\u7684\uff0c\u53ef\u80fd\u6709\u90e8\u5206\u9519\u8bef\u7684\u5339\u914d\u70b9\u6210\u4e3a\u5b64\u7acb\u70b9 (outliner). Solution: RANSAC \u6bcf\u6b21\u968f\u673a\u53d6 \\(s\\) \u7ec4\u5339\u914d\u70b9. \u901a\u8fc7\u9009\u53d6\u7684\u5339\u914d\u70b9\u8ba1\u7b97\u51fa\u53d8\u6362\u77e9\u9635 \\(T\\) . \u5728\u6240\u6709\u7684\u5339\u914d\u70b9\u4e2d\uff0c\u7edf\u8ba1\u5728\u4e00\u5b9a\u8bef\u5dee\u8303\u56f4\u5185\u7b26\u5408\u53d8\u6362 \\(T\\) \u7684\u70b9 (inlier) \u7684\u6570\u91cf\u4f5c\u4e3a\u5f97\u5206. \u91cd\u590d \\(N\\) \u6b21\uff0c\u53d6\u5f97\u5206\u6700\u9ad8\u7684\u53d8\u6362 \\(T_0\\) . (Better Step) \u5bf9\u6240\u6709\u5927\u81f4\u7b26\u5408\u53d8\u6362 \\(T_0\\) \u7684\u5339\u914d\u70b9\u518d\u6b21\u8ba1\u7b97\u4e00\u4e2a\u5e73\u5747\u7684\u53d8\u6362\u77e9\u9635 \\(T\\) . Summary \u00b6 How to make image stitching? Input Images. Feature Matching. Compute transformation matrices \\(T\\) with RANSAC. Warp image 2 to image 1. Extension: Panaroma and Cylindrical Projection Problem: Drift Solution Small vertical errors accumulate over time. Apply correction s.t. the sum of drift is 0. \u4e5f\u5c31\u662f\u62cd\u6444\u5168\u666f\u56fe\u65f6\u5c3d\u53ef\u80fd\u51cf\u5c11\u4e0a\u4e0b\u6296\u52a8.","title":"Lecture 6"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#lecture-6-image-stitching","text":"","title":"Lecture 6 Image Stitching"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#image-warping","text":"Info Image filtering changes intensity of image. Image warping (\u7fd8\u66f2) changes shape of image.","title":"Image Warping"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#2d-transformations","text":"","title":"2D Transformations"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#affine-transformations","text":"\\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\] \\[ \\begin{align} x' &= ax + by + c \\\\ y' &= dx + ey + f \\end{align} \\] \u81ea\u7531\u5ea6 (degree of freedom) \u4e3a 6 \uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u81f3\u5c11 3 \u7ec4\u70b9\uff0c\u4e5f\u5c31\u662f 6 \u4e2a\u65b9\u7a0b\uff0c\u4ee5\u80fd\u591f\u6c42\u89e3\u51fa \\(a, b, c, d, e, f\\) .","title":"Affine Transformations \u4eff\u5c04\u53d8\u6362"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#projective-transformation-homography","text":"\\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} \\cong \\begin{bmatrix} h_{00} & h_{01} & h_{02} \\\\ h_{10} & h_{11} & h_{12} \\\\ h_{20} & h_{21} & h_{22} \\\\ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\] \\[ \\begin{align} x' = \\frac{h_{00}x + h_{01}y + h_{02}}{h_{20}x + h_{11}y + h_{12}} y' = \\frac{h_{10}x + h_{11}y + h_{12}}{h_{20}x + h_{11}y + h_{12}} \\end{align} \\] Constraint: \u4ee4 \\(||(h_{00}, h_{01}, \\dots, h_{22})||\\) (\u5411\u91cf\u8303\u6570) \u89c4\u5b9a\u4e3a \\(1\\) \uff0c\u6216\u8005\u4ee4 \\(h_{22}\\) \u4e3a \\(1\\) . \u8003\u8651\u5230\u4ee5\u4e0a\u7684\u9650\u5236\u6761\u4ef6 (constraint)\uff0c\u81ea\u7531\u5ea6 (degree of freedom) \u4e3a 8 \uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u81f3\u5c11 4 \u7ec4\u70b9\uff0c\u4e5f\u5c31\u662f 8 \u4e2a\u65b9\u7a0b\uff0c\u4ee5\u80fd\u591f\u6c42\u89e3\u51fa \\(h_{ij}\\) .","title":"Projective Transformation (Homography) \u5355\u5e94\u53d8\u6362"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#summary","text":"","title":"Summary"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#implementation","text":"\u51fa\u4e8e\u65b9\u4fbf\u548c\u53ef\u64cd\u4f5c\u6027\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u8868\u5f81\u6574\u4f53\u7fd8\u66f2\u7684\u51fd\u6570 \\(T\\) \u800c\u4e0d\u662f\u5bf9\u6bcf\u4e2a\u50cf\u7d20\u70b9\u91c7\u53d6\u4e0d\u540c\u7684\u7fd8\u66f2\u7b56\u7565.","title":"Implementation"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#forward-warping-not-so-good","text":"\u5047\u8bbe\u56fe\u50cf\u51fd\u6570 \\(f:(x, y) \\rightarrow (r, g, b)\\) \u7ecf\u8fc7\u4e86\u53d8\u6362 \\(T: (x, y) \\rightarrow (x', y')\\) \u5f97\u5230\u65b0\u7684\u56fe\u50cf\u51fd\u6570 \\(g:(x', y') \\rightarrow (r, g, b)\\) . If the transformed pixed lands inside the pixels - Quite complicated to solve it.","title":"Forward Warping (Not so good)"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#inverse-warping","text":"Why not consider inversely? If the transformed pixed lands inside the pixels Solution 2D Interpolation!","title":"Inverse Warping"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#image-stitching","text":"\\[ \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} \\cong T \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\]","title":"Image Stitching"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#image-matching","text":"\u901a\u8fc7\u56fe\u50cf\u5339\u914d\u5f97\u5230\u5339\u914d\u70b9\uff0c\u6bcf\u4e2a\u5339\u914d\u70b9\u53ef\u4ee5\u63d0\u4f9b\u4e00\u4e2a\u4ee5\u4e0a\u5f62\u5f0f\u7684\u77e9\u9635\u4e58\u6cd5\u7b49\u5f0f.","title":"Image Matching"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#find-t-for-image-wraping","text":"","title":"Find \\(T\\) for Image Wraping"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#affine-transformations_1","text":"\u5bf9\u4e8e\u6bcf\u4e2a\u5339\u914d\uff0c \\[ \\begin{align} x' = ax + by + c \\\\ y' = dx + ey + f \\end{align} \\] \u77e9\u9635\u5f62\u5f0f\u4e3a \\[ \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} x & y & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x & y & 1 \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix} \\] \u5bf9\u4e8e \\(n\\) \u4e2a\u5339\u914d\uff0c\u5f97\u5230\u4ee5\u4e0b\u65b9\u7a0b\u7ec4 \\[ \\begin{bmatrix} x_1 & y_1 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_1 & y_1 & 1 \\\\ x_2 & y_2 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_2 & y_2 & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_n & y_n & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_n & y_n & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix} = \\begin{bmatrix} x_1' \\\\ y_1' \\\\ x_2' \\\\ y_2' \\\\ \\vdots \\\\ x_n' \\\\ y_n' \\end{bmatrix} \\] \u4e5f\u5373 \\[ \\mathbf{A}_{2n \\times 6} \\times \\mathbf{t}_{6 \\times 1} = \\mathbf{b}_{2n \\times 1} \\] Note \u867d\u7136\u9009\u62e9 3 \u7ec4\u70b9\u662f\u80fd\u591f\u89e3\u51fa \\(a, b, c, d, e, f\\) \u7684\u6700\u5c11\u6570\u91cf\uff0c\u4f46\u662f\u5f88\u53ef\u80fd\u5b58\u5728\u90e8\u5206\u884c\u662f\u7ebf\u6027\u76f8\u5173\u4ece\u800c\u5bfc\u81f4\u5947\u5f02\u77e9\u9635\uff0c\u9000\u5316\u4e3a\u6ca1\u6709\u552f\u4e00\u89e3. \u6240\u4ee5\u4e3a\u4e86\u80fd\u591f\u5f97\u5230 \\(a, b, c, d, e, f\\) \u7684\u503c\uff0c\u91c7\u7528\u7684\u65b9\u5f0f\u662f\u53d6\u8f83\u591a\u7684\u70b9\u7136\u540e\u901a\u8fc7\u6700\u5c0f\u4e8c\u4e58\u6cd5\u8ba1\u7b97\uff0c\u4e5f\u5373\u8ba9 \\(\\mathbf{t}\\) \u6700\u5c0f\u5316 \\(||\\mathbf{A} \\mathbf{t} - \\mathbf{b}||\\) . Method of Least Square \\[ \\begin{align} \\mathbf{A^{T}At} &= \\mathbf{A^{T}b} \\\\ t &= \\mathbf{A^{T}A^{-1}A^{T}b} \\end{align} \\]","title":"Affine Transformations"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#projective-transformations","text":"\u7c7b\u4f3c\u5730\uff0c\u5bf9\u4e8e\u6bcf\u4e2a\u5339\u914d\uff0c \\[ \\begin{align} x_i'(h_{20}x_i + h_{21} y_i + h_{22}) = h_{00}x_i + h_{01}y_i + h_{02} \\\\ y_i'(h_{20}x_i + h_{21} y_i + h_{22}) = h_{10}x_i + h_{11}y_i + h_{12} \\\\ \\end{align} \\] \u77e9\u9635\u5f62\u5f0f\u4e3a \\[ \\begin{bmatrix} x_i & y_i & 1 & 0 & 0 & 0 & -x_i'x_i & -x_i'y_i & -x_i'\\\\ 0 & 0 & 0 & x_i & y_i & 1 & -y_i'x_i & -y_i'y_i & -y_i' \\end{bmatrix} \\begin{bmatrix} h_{00} \\\\ h_{01} \\\\ h_{02} \\\\ h_{10} \\\\ h_{11} \\\\ h_{12} \\\\ h_{20} \\\\ h_{21} \\\\ h_{22} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\] \u5bf9\u4e8e \\(n\\) \u4e2a\u5339\u914d\uff0c\u5f97\u5230\u4ee5\u4e0b\u65b9\u7a0b\u7ec4 \\[ \\begin{bmatrix} x_1 & y_1 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_1 & y_1 & 1 \\\\ x_2 & y_2 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_2 & y_2 & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_n & y_n & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & x_n & y_n & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix} = \\begin{bmatrix} x_1' \\\\ y_1' \\\\ x_2' \\\\ y_2' \\\\ \\vdots \\\\ x_n' \\\\ y_n' \\end{bmatrix} \\] \u4e5f\u5373 \\[ \\mathbf{A}_{2n \\times 9} \\times \\mathbf{t}_{9 \\times 1} = \\mathbf{b}_{2n \\times 1} \\] \u540c\u6837\u5730\uff0c\u6211\u4eec\u8981\u6700\u5c0f\u5316 \\(||\\mathbf{Ah - 0}|| = \\mathbf{||Ah||}\\) . Contraint: \\(||\\mathbf{h}|| = 1\\) \u5982\u679c\u4f7f\u7528\u7684\u662f\u5411\u91cf\u8303\u6570\u4e3a \\(1\\) \u7684\u9650\u5236\uff0c\u90a3\u4e48\u6211\u4eec\u53ea\u9700\u8981\u786e\u5b9a \\(\\mathbf{h}\\) \u7684\u65b9\u5411 \\(\\mathbf{\\hat h}\\) . \u6570\u5b66\u4e0a\u53ef\u4ee5\u8bc1\u660e\u89e3 \\(\\mathbf{\\hat h}\\) \u662f \\(\\mathbf{A^{T}A}\\) \u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf.","title":"Projective Transformations"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#robustness","text":"Outliers \u56fe\u50cf\u5339\u914d\u4e5f\u4e0d\u662f\u5b8c\u7f8e\u7684\uff0c\u53ef\u80fd\u6709\u90e8\u5206\u9519\u8bef\u7684\u5339\u914d\u70b9\u6210\u4e3a\u5b64\u7acb\u70b9 (outliner). Solution: RANSAC \u6bcf\u6b21\u968f\u673a\u53d6 \\(s\\) \u7ec4\u5339\u914d\u70b9. \u901a\u8fc7\u9009\u53d6\u7684\u5339\u914d\u70b9\u8ba1\u7b97\u51fa\u53d8\u6362\u77e9\u9635 \\(T\\) . \u5728\u6240\u6709\u7684\u5339\u914d\u70b9\u4e2d\uff0c\u7edf\u8ba1\u5728\u4e00\u5b9a\u8bef\u5dee\u8303\u56f4\u5185\u7b26\u5408\u53d8\u6362 \\(T\\) \u7684\u70b9 (inlier) \u7684\u6570\u91cf\u4f5c\u4e3a\u5f97\u5206. \u91cd\u590d \\(N\\) \u6b21\uff0c\u53d6\u5f97\u5206\u6700\u9ad8\u7684\u53d8\u6362 \\(T_0\\) . (Better Step) \u5bf9\u6240\u6709\u5927\u81f4\u7b26\u5408\u53d8\u6362 \\(T_0\\) \u7684\u5339\u914d\u70b9\u518d\u6b21\u8ba1\u7b97\u4e00\u4e2a\u5e73\u5747\u7684\u53d8\u6362\u77e9\u9635 \\(T\\) .","title":"Robustness"},{"location":"Computer_Science_Courses/ICV/6_Image_Stitching/#summary_1","text":"How to make image stitching? Input Images. Feature Matching. Compute transformation matrices \\(T\\) with RANSAC. Warp image 2 to image 1. Extension: Panaroma and Cylindrical Projection Problem: Drift Solution Small vertical errors accumulate over time. Apply correction s.t. the sum of drift is 0. \u4e5f\u5c31\u662f\u62cd\u6444\u5168\u666f\u56fe\u65f6\u5c3d\u53ef\u80fd\u51cf\u5c11\u4e0a\u4e0b\u6296\u52a8.","title":"Summary"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/","text":"Lecture 7 Structure from Motion (SfM) \u00b6 Camera Calibration \u76f8\u673a\u6807\u5b9a \u00b6 \u76f8\u673a\u6807\u5b9a\u7684\u8fc7\u7a0b\uff0c\u5c31\u662f\u4ece\u4e16\u754c\u5750\u6807\u7cfb\u5411\u50cf\u7d20\u5750\u6807\u7cfb\u8f6c\u6362\u7684\u8fc7\u7a0b. \u9700\u8981\u6ce8\u610f\u7684\u56db\u4e2a\u5750\u6807\u7cfb\uff1a \u4e16\u754c\u5750\u6807\u7cfb (World Coordinates) \u76f8\u673a\u5750\u6807\u7cfb (Camera Coordinates) \u56fe\u50cf\u5750\u6807\u7cfb (Image Plane) \u50cf\u7d20\u5750\u6807\u7cfb (Image Sensor) \u5176\u4e2d\uff0c\u524d\u4e24\u4e2a\u5750\u6807\u7cfb\u7684\u8f6c\u6362\u77e9\u9635\u53c2\u6570\u88ab\u79f0\u4e3a \u76f8\u673a\u5916\u53c2 \uff1b\u540e\u4e09\u4e2a\u5750\u6807\u7cfb\u7684\u8f6c\u6362\u77e9\u9635\u53c2\u6570\u88ab\u79f0\u4e3a \u76f8\u673a\u5185\u53c2 . Principle Analysis \u539f\u7406\u5206\u6790 \u00b6 Coordinate Transformation \u00b6 \u4ece\u4e16\u754c\u5750\u6807\u7cfb\u5230\u76f8\u673a\u5750\u6807\u7cfb\u7684\u53d8\u6362. \u7279\u70b9 \u00b6 \u521a\u4f53\u53d8\u6362\uff0c\u53ea\u6709\u65cb\u8f6c\u548c\u5e73\u79fb\uff0c\u5bf9\u5e94\u65cb\u8f6c\u77e9\u9635 \\(R\\) \uff08 \\(R\\) \u662f \u5355\u4f4d\u6b63\u4ea4\u77e9\u9635 (Orthonormal) ) \u548c\u5e73\u79fb\u5411\u91cf \\(c_w\\) . \\[ \\mathbf{x}_c = R(\\mathbf{x}_w - \\mathbf{c}_w) = R \\mathbf{x}_w - R \\mathbf{c}_w \\overset{\\Delta}{=\\!=} R \\mathbf{x}_w + \\mathbf{t}, \\text{ where } \\mathbf{t } = -R \\mathbf{c}_w. \\] \u6545\u8fd9\u4e2a\u53d8\u6362\u4e5f\u53ef\u4ee5\u7b49\u4ef7\u5730\u7528 \\(R\\) \u548c \\(\\mathbf{t}\\) \u6765\u8868\u5f81. \\[ \\mathbf{x}_c = \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\\\ \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} \\] \u540c\u6837\u8003\u8651\u9f50\u6b21\u5750\u6807\u7cfb \\[ \\mathbf{\\tilde x}_c = \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1\\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix} \\] Extrinsic Matrix (\u5916\u53c2\u77e9\u9635) \\[ M_{ext} = \\begin{bmatrix} R_{3\\times 3} & \\mathbf{t} \\\\ \\mathbf{0}_{1\\times 3} & 1 \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\] \u4e5f\u53ef\u4ee5\u5199\u4f5c \\[ M_{ext} = \\begin{bmatrix} R_{3\\times 3} & \\mathbf{t} \\\\ \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ \\end{bmatrix} \\] Perspective Projection \u00b6 \u4ece\u76f8\u673a\u5750\u6807\u7cfb\u5230\u56fe\u50cf\u5750\u6807\u7cfb\u7684\u53d8\u6362. \u524d\u9762\u51e0\u4e2a Lecture \u4e2d\u5df2\u7ecf\u8ba8\u8bba\u8fc7\u8fd9\u4e00\u90e8\u5206 \u5185\u5bb9 . \u6b64\u5904\u4e0d\u518d\u8d58\u8ff0. \\[ \\begin{bmatrix} x_i \\\\ y_i \\\\ 1 \\end{bmatrix} \\cong \\begin{bmatrix} f & 0 & 0 & 0 \\\\ 0 & f & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1 \\end{bmatrix} \\] Image Plane to Image Sensor Mapping \u00b6 \u4ece\u56fe\u50cf\u5750\u6807\u7cfb\u5230\u50cf\u7d20\u5750\u6807\u7cfb\u7684\u53d8\u6362. \u50cf\u7d20\u70b9 pixel \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u7531\u4e8e\u76f8\u673a\u81ea\u8eab\u7684\u5236\u4f5c\u5de5\u827a\u548c\u8981\u6c42\uff0c\u50cf\u7d20\u70b9\u5e76\u4e0d\u4e00\u5b9a\u662f\u6b63\u65b9\u5f62\uff0c\u800c\u662f\u77e9\u5f62\uff0c\u4e0e\u539f\u56fe\u50cf\u76f8\u6bd4\u5177\u6709\u4e00\u5b9a\u7684 \u7f29\u653e . \u8fd9\u4e00\u70b9\u5728\u89c6\u9891\u7684\u5236\u4f5c\u4e2d\u4e5f\u6709\u6240\u4f53\u73b0. \u6b64\u5916\uff0c\u7531\u4e8e\u56fe\u50cf\u5750\u6807\u7cfb\u662f\u4ece\u4e2d\u5fc3\u4e3a\u539f\u70b9\uff0c\u800c\u50cf\u7d20\u5750\u6807\u7cfb\u662f\u4ee5\u5de6\u4e0a\u89d2\u4e3a\u539f\u70b9\u7684\uff0c\u5750\u6807\u53d8\u6362\u8fd8\u9700\u8981\u4e00\u5b9a\u7684* \u5e73\u79fb . \\[ \\begin{align} u &= m_x f \\frac{x_c}{z_c} + c_x \\\\ v &= m_y f \\frac{y_c}{z_c} + c_y \\end{align} \\] \u7ed3\u5408\u4ee5\u4e0a\u4e24\u4e2a\u53d8\u6362\uff0c\u8bb0 \\(f_x = m_x f\\) , \\(f_y = m_y f\\) . \u5219\u4ece\u76f8\u673a\u5750\u6807\u7cfb\u5230\u50cf\u7d20\u5750\u6807\u7cfb\u7684\u53d8\u6362\u4e3a \\[ \\mathbf{\\tilde{u}} = \\begin{bmatrix} u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1 \\end{bmatrix} \\] Intrinsic Matrix (\u5185\u53c2\u77e9\u9635) \\[ M_{int} = \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\] \u4e5f\u53ef\u4ee5\u5199\u4f5c \\[ K = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\] Projection Matrix \\(P\\) World to Camera \\[ \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1\\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix} \\] \\[ \\mathbf{\\tilde u} = M_{int}\\mathbf{\\tilde x}_c \\] Camera to Pixel \\[ \\begin{bmatrix} u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1 \\end{bmatrix} \\] \\[ \\mathbf{\\tilde x}_c = M_{ext}\\mathbf{\\tilde x}_w \\] \u6545 \\[ \\mathbf{\\tilde u} = M_{int}M_{ext} \\mathbf{\\tilde x}_w = P \\mathbf{\\tilde x} \\] \u5176\u4e2d\uff0c \\(P\\) \u88ab\u79f0\u4e3a\u6295\u5f71\u77e9\u9635 (Projection Matrix). \\[ \\begin{bmatrix} u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\\\ p_{21} & p_{22} & p_{23} & p_{24} \\\\ p_{31} & p_{32} & p_{33} & p_{34} \\\\ p_{41} & p_{42} & p_{43} & p_{44} \\\\ \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix} \\] Implemetation \u00b6 Step 1. Capture an image of an object with known geometry. \u00b6 Step 2. Identify correspondences (Image Matching \u56fe\u50cf\u5339\u914d) between 3D scene points and image points. \u00b6 Step 3. For each corresponding point \\(i\\) , we get \u00b6 \\[ \\begin{bmatrix} u^{(i)} \\\\ v^{(i)} \\\\ w^{(i)} \\end{bmatrix} \\cong \\begin{bmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\\\ p_{21} & p_{22} & p_{23} & p_{24} \\\\ p_{31} & p_{32} & p_{33} & p_{34} \\\\ p_{41} & p_{42} & p_{43} & p_{44} \\\\ \\end{bmatrix} \\begin{bmatrix} x_w^{(i)} \\\\ y_w^{(i)} \\\\ z_w^{(i)} \\\\ 1 \\end{bmatrix} \\] \\[ \\begin{align} u^{(i)} &= \\frac{p_{11}x_w^{(i)} + p_{12}y_w^{(i)} + p_{13}z_w^{(i)} + p_{14}}{p_{31}x_w^{(i)} + p_{32}y_w^{(i)} + p_{33}z_w^{(i)} + p_{34}} \\\\ v^{(i)} &= \\frac{p_{21}x_w^{(i)} + p_{22}y_w^{(i)} + p_{23}z_w^{(i)} + p_{24}}{p_{31}x_w^{(i)} + p_{32}y_w^{(i)} + p_{33}z_w^{(i)} + p_{34}} \\\\ \\end{align} \\] Step 4. Rearrange. \u00b6 \\[ \\scriptsize \\begin{bmatrix} x_w^{(1)} & y_w^{(1)} & z_w^{(1)} & 1 & 0 & 0 & 0 & 0 & -u_1x_w^{(1)} & -u_1y_w^{(1)} & -u_1z_w^{(1)} & -u_1 \\\\ 0 & 0 & 0 & 0 & x_w^{(1)} & y_w^{(1)} & z_w^{(1)} & 1 & -v_1x_w^{(1)} & -v_1y_w^{(1)} & -v_1z_w^{(1)} & -v_1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_w^{(i)} & y_w^{(i)} & z_w^{(i)} & 1 & 0 & 0 & 0 & 0 & -u_ix_w^{(i)} & -u_iy_w^{(i)} & -u_iz_w^{(i)} & -u_i \\\\ 0 & 0 & 0 & 0 & x_w^{(i)} & y_w^{(i)} & z_w^{(i)} & 1 & -v_ix_w^{(i)} & -v_iy_w^{(i)} & -v_iz_w^{(i)} & -v_i \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_w^{(n)} & y_w^{(n)} & z_w^{(n)} & 1 & 0 & 0 & 0 & 0 & -u_nx_w^{(n)} & -u_ny_w^{(n)} & -u_nz_w^{(n)} & -u_n \\\\ 0 & 0 & 0 & 0 & x_w^{(n)} & y_w^{(n)} & z_w^{(n)} & 1 & -v_nx_w^{(n)} & -v_ny_w^{(n)} & -v_nz_w^{(n)} & -v_n \\\\ \\end{bmatrix} \\begin{bmatrix} p_{11} \\\\ p_{12} \\\\ p_{13} \\\\ p_{14} \\\\ p_{21} \\\\ p_{22} \\\\ p_{23} \\\\ p_{24} \\\\ p_{31} \\\\ p_{32} \\\\ p_{33} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\] \\[ i.e.\\ A \\mathbf{p} = 0 \\] Step 5 Solve for \\(P\\) . \u00b6 \u7c7b\u4f3c\u5730\uff0c\u6211\u4eec\u53ef\u4ee5\u7ed9\u51fa\u4e00\u5b9a\u7684\u9650\u5236 constraint. Option 1. \u4ee4 \\(p_{34} = 1\\) . Option 2. \u4ee4 \\(||p|| = 1\\) . \u5f97\u5230\u81ea\u7531\u5ea6\u4e3a \\(11\\) \uff0c\u81f3\u5c11\u9700\u8981 6 \u7ec4\u70b9. \u5e76\u540c\u6837\u8981\u6700\u5c0f\u5316 \\(||\\mathbf{Ap - 0}|| = \\mathbf{||Ap||}\\) . \u540c\u6837\u5982\u679c\u4f7f\u7528\u7684\u662f\u5411\u91cf\u8303\u6570\u4e3a \\(1\\) \u7684\u9650\u5236\uff0c\u90a3\u4e48\u6211\u4eec\u53ea\u9700\u8981\u786e\u5b9a \\(\\mathbf{p}\\) \u7684\u65b9\u5411 \\(\\mathbf{\\hat p}\\) . \u4e14 \\(\\mathbf{\\hat p}\\) \u662f \\(\\mathbf{A^{T}A}\\) \u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf. Step 6 From \\(P\\) to Answers \u00b6 \u7531 \\[ \\begin{bmatrix} p_{11} \\\\ p_{12} \\\\ p_{13} \\\\ p_{14} \\\\ p_{21} \\\\ p_{22} \\\\ p_{23} \\\\ p_{24} \\\\ p_{31} \\\\ p_{32} \\\\ p_{33} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\] \u6ce8\u610f\u5230 \\[ \\begin{bmatrix} p_{11} \\\\ p_{12} \\\\ p_{13} \\\\ p_{21} \\\\ p_{22} \\\\ p_{23} \\\\ p_{31} \\\\ p_{32} \\\\ p_{33} \\end{bmatrix} = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\\\ \\end{bmatrix} = KR \\] \u5176\u4e2d \\(K\\) \u662f\u4e0a\u4e09\u89d2\u77e9\u9635 (Upper Right Triangular Matrix)\uff0c \\(R\\) \u662f\u5355\u4f4d\u6b63\u4ea4\u77e9\u9635 (Orthonormal Matrix). \u7531 QR \u5206\u89e3 (QR Factorization) \u53ef\u77e5\uff0c \\(K\\) \u4e0e \\(R\\) \u7684\u89e3\u662f\u552f\u4e00\u7684. \u518d\u7531 \\[ \\begin{bmatrix} p_{14} \\\\ p_{24} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} = K \\mathbf{t} \\] \u6545\u53ef\u89e3\u5f97 \\[ \\mathbf{t} = K^{-1} \\begin{bmatrix} p_{14} \\\\ p_{24} \\\\ p_{34} \\end{bmatrix} \\] Problem: Distortion \u5b9e\u9645\u4e0a\u76f8\u673a\u81ea\u8eab\u4f1a\u5177\u6709\u4e00\u5b9a\u7684\u626d\u66f2\u7cfb\u6570\uff0c\u6211\u4eec\u5728\u6b64 \u5ffd\u7565 \u5b83\u7684\u5f71\u54cd. Perspective-n-Point Problem (PnP \u95ee\u9898) \u5f31\u5316\u4e00\u4e0b\u6761\u4ef6\uff0c\u5047\u8bbe\u5df2\u77e5\u76f8\u673a\u7684\u5185\u53c2\uff08\u5373\u77e9\u9635 \\(K\\) \uff09\uff0c\u6c42\u76f8\u673a\u7684\u5916\u53c2\uff08\u5373\u77e9\u9635 \\(R\\) \u548c \\(\\mathbf{t}\\) \uff09\uff0c\u4e5f\u5373\u76f8\u673a\u59ff\u6001 (camera pose). Direct Linear Transform (DLT) \u00b6 \u5373\u4ee5\u4e0a\u5728\u76f8\u673a\u6807\u5b9a\u4e2d\u63d0\u5230\u7684\u65b9\u6cd5\uff0c\u5148\u6c42\u89e3\u77e9\u9635 \\(P\\) \uff0c\u518d\u901a\u8fc7 \\(K\\) \u5f97\u5230 \\(R\\) \u548c \\(\\mathbf{t}\\) . P3P \u00b6 \u5373\u5df2\u77e5 \\(a, b, c, A, B, C\\) \u70b9\u5750\u6807\uff0c\u6c42 \\(O\\) \u5750\u6807. \u81f3\u5c11\u9700\u8981 \\(3\\) \u7ec4\u70b9 Intermediates. \u7531\u4f59\u5f26\u5b9a\u7406 \\[ \\begin{align} OA^2 + OB^2 - 2 OA \\cdot OB \\cos \\angle AOB &= AB^2 \\\\ OA^2 + OC^2 - 2 OB \\cdot OC \\cos \\angle BOC &= BC^2 \\\\ OA^2 + OC^2 - 2 OA \\cdot OC \\cos \\angle AOC &= AC^2 \\end{align} \\] \u4ee4 \\(x = \\frac{OA}{OC}, y = \\frac{OB}{OC}, v = \\frac{AB^2}{OC^2}, u = \\frac{BC^2}{AB^2}, w = \\frac{AC^2}{AB^2}\\) \uff0c\u5316\u7b80\u5f97 \\[ \\begin{align} (1 - u)y^2 - ux^2 - cos \\angle BOC y + 2uxy \\cos \\angle AOB + 1 &= 0 \\\\ (1 - w)x^2 - wy^2 - cos \\angle AOC y + 2wxy \\cos \\angle AOB + 1 &= 0 \\end{align} \\] \u4ee5\u4e0a\u65b9\u7a0b\u7ec4\u662f\u4e8c\u5143\u4e8c\u6b21\u65b9\u7a0b\u7ec4 (Binary Quadratic Equation)\uff0c\u6709\u56db\u7ec4\u89e3. \u5176\u4e2d\u6709\u4e24\u7ec4\u89e3\u7684 \\(O\\) \u5750\u6807\u5728 \u9762 \\(ABC\\) \u4e0e \u9762 \\(abc\\) \u4e4b\u95f4\uff0c\u820d\u53bb. \u8fd8\u9700\u8981\u4e00\u7ec4\u989d\u5916\u7684\u70b9\u6765\u786e\u5b9a\u552f\u4e00\u89e3\uff0c\u6545\u4e00\u5171\u9700\u8981 \\(4\\) \u7ec4\u70b9. PnP \u00b6 \u6700\u5c0f\u5316 \u91cd\u6295\u5f71\u8bef\u5dee (Reprojection Error) \\[ \\mathop{\\arg\\min\\limits_{R, \\mathbf{t}}} \\sum\\limits_i ||(p_i, K(RP_i + \\mathbf{t})||^2 \\] \u5176\u4e2d\uff0c \\(p_i\\) \u662f\u6295\u5f71\u9762\u4e0a\u7684\u4e8c\u7ef4\u5750\u6807\uff0c \\(P_i\\) \u662f\u7a7a\u95f4\u5185\u7684\u4e09\u7ef4\u5750\u6807. \u901a\u8fc7 P3P \u521d\u59cb\u5316\uff0cGauss-Newton \u6cd5\u4f18\u5316. EPnP \u00b6 one of the most popluar methods. time complexity \\(O(N)\\) . high accuracy. \u5927\u81f4\u65b9\u5f0f\u662f\u901a\u8fc7\u56db\u7ec4\u63a7\u5236\u70b9\u7684\u7ebf\u6027\u7ec4\u5408\u6c42\u89e3. Two-frame Structure from Motion \u00b6 Stereo vision \u7acb\u4f53\u89c6\u89c9 Compute 3D structure of the scene and camera poses from views (images). Two-frame SfM \u610f\u5728\u89e3\u51b3\u8fd9\u6837\u4e00\u4e2a\u95ee\u9898\uff1a\u5df2\u77e5\u4e24\u5f20\u56fe\u50cf\u548c\u76f8\u673a\u5185\u53c2\u77e9\u9635 ( \\(K_l(3 \\times 3)\\) \u4e0e \\(K_r(3 \\times 3)\\) )\uff0c\u6c42\u76f8\u673a\u7684\u4f4d\u7f6e\u4ee5\u53ca\u6240\u62cd\u6444\u4e3b\u4f53\u7684\u4e09\u7ef4\u4f4d\u7f6e\u5750\u6807. Principle Induction \u539f\u7406\u63a8\u5bfc \u00b6 Epipolar Geometry \u5bf9\u6781\u51e0\u4f55 \u5bf9\u6781\u51e0\u4f55\u63cf\u8ff0\u4e86\u4e00\u4e2a\u4e09\u7ef4\u70b9 \\(P\\) \u5728\u4e24\u4e2a\u89c6\u89d2\u7684\u4e8c\u7ef4\u6295\u5f71 \\(u_l, u_r\\) \u4e4b\u95f4\u7684\u51e0\u4f55\u5173\u7cfb. \u5e76\u901a\u8fc7\u8fd9\u4e2a\u51e0\u4f55\u5173\u7cfb\u5efa\u7acb\u4e24\u4e2a\u6444\u50cf\u673a\u4e4b\u95f4\u7684\u53d8\u6362\u5173\u7cfb\uff08\u5373\u7ed9\u51fa\u77e9\u9635 \\(R\\) \u548c \\(\\mathbf{t}\\) \uff09. Definition Epipole \u5bf9\u6781\u70b9 \uff1a\u4e00\u53f0\u6444\u50cf\u673a\u6295\u5f71\u5728\u53e6\u4e00\u53f0\u6444\u50cf\u673a\u4e2d\u7684\u70b9. \u5982\u56fe\u4e2d\u7684 \\(e_l\\) \u548c \\(e_r\\) . \u5bf9\u7ed9\u5b9a\u7684\u4e24\u53f0\u6444\u50cf\u673a\uff0c \\(e_l\\) \u548c \\(e_r\\) \u662f\u552f\u4e00\u7684. Epipolar Plane of Scene Point P \u5173\u4e8e P \u70b9\u7684\u5bf9\u6781\u9762 \uff1a\u7531 Scene Point \\(P\\) \u70b9\u548c\u4e24\u4e2a\u6444\u50cf\u673a \\(O_l\\) \u548c \\(O_r\\) \u5f62\u6210\u7684\u5e73\u9762. \u6bcf\u4e2a scene point \u6709\u552f\u4e00\u5bf9\u5e94\u7684\u5bf9\u6781\u9762. \u5bf9\u6781\u70b9\u5728\u5bf9\u6781\u9762\u4e0a. Essential Matrix \\(E\\) \u57fa\u672c\u77e9\u9635 \u00b6 \u5bf9\u4e8e\u4e00\u4e2a\u5bf9\u6781\u9762\uff0c\u4ee5\u4e0b\u7b49\u5f0f\u5173\u7cfb\u6210\u7acb\uff1a \\[ \\begin{align} \\mathbf{n} = \\mathbf{t} \\times \\mathbf{x}_l \\\\ \\\\ \\mathbf{x}_l \\cdot (\\mathbf{t} \\times \\mathbf{x}_l) = 0 \\end{align} \\] \u5176\u4e2d \\(\\mathbf{n}\\) \u88ab\u79f0\u4e3a Normal Vector. \u5047\u8bbe\u53f3\u8fb9\u7684\u6444\u50cf\u673a\u76f8\u5bf9\u5de6\u8fb9\u7684\u6444\u50cf\u673a\u8fdb\u884c\u521a\u4f53\u53d8\u6362 + \u5e73\u79fb\uff0c\u5373\uff1a \\[ \\begin{align} \\mathbf{x}_l &= R \\mathbf{x}_r + \\mathbf{t} \\\\ \\\\ \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\end{bmatrix} &= \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ y_r \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} \\end{align} \\] \u8fdb\u4e00\u6b65\u6f14\u7ece\u63a8\u7406\uff0c\u5f97\u5230\uff1a \\[ \\begin{align} \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\begin{bmatrix} t_yz_l - t_zy_l \\\\ t_zx_l - t_xz_l \\\\ t_xy_l - t_yx_l \\end{bmatrix} = 0 & \\text{, Cross-product definition} \\\\ \\\\ \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\underbrace{ \\begin{bmatrix} 0 & -t_z & t_y \\\\ t_z & 0 & -t_x \\\\ -t_y & t_x & 0 \\end{bmatrix} }_{T_\\times} \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\end{bmatrix} = 0 & \\text{, Matrix-vector form} \\end{align} \\] \u4ee3\u5165\u53d8\u6362\u77e9\u9635 \\[ \\small \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\left( \\underbrace{ \\begin{bmatrix} 0 & -t_z & t_y \\\\ t_z & 0 & -t_x \\\\ -t_y & t_x & 0 \\end{bmatrix} }_{T_\\times} \\underbrace{ \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\end{bmatrix} }_{R} \\begin{bmatrix} x_r \\\\ y_r \\\\ y_r \\end{bmatrix} + \\underbrace{ \\begin{bmatrix} 0 & -t_z & t_y \\\\ t_z & 0 & -t_x \\\\ -t_y & t_x & 0 \\end{bmatrix} \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} }_{\\mathbf{t} \\times \\mathbf{t} = \\mathbf{0}} \\right) = 0 \\] \\[ \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\underbrace{ \\begin{bmatrix} e_{11} & e_{12} & e_{13} \\\\ e_{21} & e_{22} & e_{23} \\\\ e_{31} & e_{32} & e_{33} \\end{bmatrix} }_{\\text{Essential Matrix } E} \\begin{bmatrix} x_r \\\\ y_r \\\\ y_r \\end{bmatrix} \\text{, where } E = T_\\times R \\] \u6ce8\u610f\u5230 \\(T_\\times\\) \u662f\u53cd\u5bf9\u79f0\u77e9\u9635 (skew-symmetric matrix) \u800c \\(R\\) \u662f\u5355\u4f4d\u6b63\u4ea4\u77e9\u9635 (orthonomal matrix)\uff0c\u7531 SVD \u5206\u89e3 (Singule Value Decomposition)\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece \\(E\\) \u552f\u4e00\u5206\u89e3\u5f97\u5230 \\(T_\\times\\) \u548c \\(R\\) . Fundemental Matrix \\(F\\) \u672c\u771f\u77e9\u9635 \u00b6 \u7531\u6295\u5f71\u5173\u7cfb\u4ee5\u53ca\u5185\u53c2\u77e9\u9635 \\(K\\) \uff0c\u5f97\u5230\uff1a \\[ \\small \\begin{align} \\text{Left Camera} && \\text{Right Camera} \\\\ z_l \\begin{bmatrix} u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &= \\underbrace{ \\begin{bmatrix} f_x^{(l)} & 0 & o_x^{(l)} \\\\ 0 & f_y^{(l)} & o_y^{(l)} \\\\ 0 & 0 & 1 \\end{bmatrix} }_{K_l} \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\end{bmatrix} & z_r \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} &= \\underbrace{ \\begin{bmatrix} f_x^{(r)} & 0 & o_x^{(r)} \\\\ 0 & f_y^{(r)} & o_y^{(r)} \\\\ 0 & 0 & 1 \\end{bmatrix} }_{K_r} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\end{bmatrix} \\\\ \\mathbf{x}_l^T &= \\begin{bmatrix} u_l & v_l & 1 \\end{bmatrix} z_l (K_l^{-1})^T & \\mathbf{x}_r &= K_r^{-1} z_r \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} \\end{align} \\] \u4ee3\u5165\u4ee5\u4e0a\u5f97\u5230\u7684\uff1a \\[ \\mathbf{x}^T_l E \\mathbf{x}_r = 0 \\] \u6709 \\[ \\require{canel} \\begin{bmatrix} u_l & v_l & 1 \\end{bmatrix} \\cancel{z_l} (K_l^{-1})^T E K_r^{-1} \\cancel{z_r} \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = 0 \\] \u5373 \\[ \\begin{bmatrix} u_l & v_l & 1 \\end{bmatrix} F \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = 0 \\text{, where } E = K_l^T F K_r \\] Implementation \u5b9e\u73b0 \u00b6 Step 1. Find a set ( \\(m\\) pairs) of corresponding features. \u627e\u5230\u4e00\u7ec4\uff08 \\(m\\) \u5bf9\uff09\u5339\u914d\u70b9. \u00b6 Least Number of Points At least 8 pairs: \\((u_l^{(i)}, v_l^{(i)})\\) \u2194 \\((u_r^{(i)}, v_r^{(i)})\\) NOTE that one pair only corresponds one equation. (not the same as before) Step 2. Build linear systems and solve for \\(F\\) . \u5efa\u7acb\u65b9\u7a0b\u7ec4\u5e76\u6c42\u89e3\u77e9\u9635 \\(F\\) . \u00b6 \u5bf9\u4e8e\u6bcf\u7ec4\u5339\u914d \\(i\\) \uff0c \\[ \\begin{bmatrix} u_l^{(i)} & v_l^{(i)} & 1 \\end{bmatrix} \\begin{bmatrix} f_{11} & f_{12} & f_{13} \\\\ f_{21} & f_{22} & f_{23} \\\\ f_{31} & f_{32} & f_{33} \\end{bmatrix} \\begin{bmatrix} u_r^{(i)} \\\\ v_r^{(i)} \\\\ 1 \\end{bmatrix} = 0 \\] \u5c55\u5f00\u5f97\uff0c \\[ \\small \\left( f_{11} u_r^{(i)} + f_{12} v_r^{(i)} + f_13 \\right)u_l^{(i)} + \\left( f_{21} u_r^{(i)} + f_{22} v_r^{(i)} + f_23 \\right)v_l^{(i)} + f_{31} u_r^{(i)} + f_{32} v_r^{(i)} + f_33 = 0 \\] \u5bf9\u6240\u6709\u9009\u62e9\u7684\u70b9\uff0c\u6709 \\[ \\small \\begin{bmatrix} u_l^{(1)}u_r^{(1)} & u_l^{(1)}v_r^{(1)} & u_l^{(1)} & v_l^{(1)}u_r^{(1)} & v_l^{(1)}v_r^{(1)} & v_l^{(1)} & u_r^{(1)} & v_r^{(1)} & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ u_l^{(i)}u_r^{(i)} & u_l^{(i)}v_r^{(i)} & u_l^{(i)} & v_l^{(i)}u_r^{(i)} & v_l^{(i)}v_r^{(i)} & v_l^{(i)} & u_r^{(i)} & v_r^{(i)} & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ u_l^{(m)}u_r^{(m)} & u_l^{(m)}v_r^{(m)} & u_l^{(m)} & v_l^{(m)}u_r^{(m)} & v_l^{(m)}v_r^{(m)} & v_l^{(m)} & u_r^{(m)} & v_r^{(m)} & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} f_{11} \\\\ f_{12} \\\\ f_{13} \\\\ f_{21} \\\\ f_{22} \\\\ f_{23} \\\\ f_{31} \\\\ f_{32} \\\\ f_{33} \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\] \\[ A \\mathbf{f} = \\mathbf{0} \\] \u540c\u6837\u5730 \uff0c\u6211\u4eec\u53ef\u4ee5\u7ed9\u51fa\u4e00\u5b9a\u7684\u9650\u5236 constraint. Option 1. \u4ee4 \\(f_{33} = 1\\) . Option 2. \u4ee4 \\(||f|| = 1\\) . \u5f97\u5230\u81ea\u7531\u5ea6\u4e3a \\(8\\) \uff0c\u81f3\u5c11\u9700\u8981 8 \u7ec4\u70b9. \u5e76\u540c\u6837\u8981\u6700\u5c0f\u5316 \\(||\\mathbf{Af - 0}|| = \\mathbf{||Af||}\\) . \u540c\u6837\u5982\u679c\u4f7f\u7528\u7684\u662f\u5411\u91cf\u8303\u6570\u4e3a \\(1\\) \u7684\u9650\u5236\uff0c\u90a3\u4e48\u89e3\u5c31\u662f \\(\\mathbf{A^{T}A}\\) \u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf. Step 3. Find \\(E\\) and Extract \\(R, \\mathbf{t}\\) \u00b6 \\[ E = K_l^T F K_r \\] \u901a\u8fc7 SVD \u5206\u89e3\uff0c \\(E = T_\\times R\\) \u5f97\u5230 \\(T_\\times\\) \u548c \\(R\\) . \\(R\\) \u5373\u4e3a\u65cb\u8f6c\u77e9\u9635\uff0c \\(\\mathbf{t}\\) \u53ef\u4ee5\u901a\u8fc7 \\(T_\\times\\) \u76f4\u63a5\u5f97\u5230. Step 4. Find 3D Position of Scene Points \u00b6 \u73b0\u5728\u5df2\u7ecf\u6709\u4ee5\u4e0b\u7b49\u5f0f \\[ \\small \\begin{align} \\text{Left Camera} \\\\ \\begin{bmatrix} u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} f_x^{(l)} & 0 & o_x^{(l)} & 0 \\\\ 0 & f_y^{(l)} & o_y^{(l)} & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\\\ 1 \\end{bmatrix} ,\\ \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y \\\\ r_{31} & r_{32} & r_{33} & t_z \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\\\ \\begin{bmatrix} u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} f_x^{(l)} & 0 & o_x^{(l)} & 0 \\\\ 0 & f_y^{(l)} & o_y^{(l)} & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y \\\\ r_{31} & r_{32} & r_{33} & t_z \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\\\ \\\\ \\normalsize \\mathbf{\\tilde u}_l &= \\normalsize P_l \\mathbf{\\tilde x}_r \\\\ \\\\ \\small \\text{Right Camera} \\\\ \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} f_x^{(r)} & 0 & o_x^{(r)} & 0 \\\\ 0 & f_y^{(r)} & o_y^{(r)} & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\\\ \\\\ \\normalsize \\mathbf{\\tilde u}_l &= \\normalsize M_{int_r} \\mathbf{\\tilde x}_r \\end{align} \\] \u7531 \\[ \\small \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} m_{11} & m_{12} & m_{13} & m_{14} \\\\ m_{21} & m_{22} & m_{23} & m_{24} \\\\ m_{31} & m_{32} & m_{33} & m_{34} \\\\ \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} ,\\ \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\\\ p_{21} & p_{22} & p_{23} & p_{24} \\\\ p_{31} & p_{32} & p_{33} & p_{34} \\\\ \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\] \u6574\u7406\u5f97 \\[ \\underbrace{ \\begin{bmatrix} u_r m_{31} - m_{11} & u_r m_{32} - m_{12} & u_r m_{33} - m_{13} \\\\ v_r m_{31} - m_{21} & v_r m_{32} - m_{22} & v_r m_{33} - m_{23} \\\\ u_l p_{31} - p_{11} & u_l p_{32} - p_{12} & u_l p_{33} - p_{13} \\\\ v_l p_{31} - p_{21} & v_l p_{32} - p_{22} & v_l p_{33} - p_{23} \\end{bmatrix} }_{A_{4\\times 3}} \\underbrace{ \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\end{bmatrix} }_{\\mathbf{x}_r} = \\underbrace{ \\begin{bmatrix} m_{14} - m_{34} \\\\ m_{24} - m_{34} \\\\ p_{14} - p_{34} \\\\ p_{24} - p_{34} \\end{bmatrix} }_{\\mathbf{b}} \\] \u7531\u5206\u6790\u4e0a\u89e3\u7684\u552f\u4e00\u6027\uff0c\u7406\u8bba\u4e0a\u6709\u4e24\u884c\u662f\u7ebf\u6027\u76f8\u5173\u7684. \u4f46\u662f\u5728\u5b9e\u9645\u53d6\u70b9\u4e2d\u53ef\u80fd\u4f1a\u5b58\u5728\u4e00\u5b9a\u7684\u8bef\u5dee\uff0c\u6240\u4ee5\u4e3a\u4e86\u6700\u5927\u5316\u5229\u7528\u6570\u636e\u91cf\uff0c\u8fd8\u662f\u91c7\u7528\u6700\u5c0f\u4e8c\u4e58\u6cd5. \u7531\u6700\u5c0f\u4e8c\u4e58\u6cd5\u5f97\uff0c \\[ \\mathbf{x}_r = (A^TA)^{-1}A^T \\mathbf{b} \\] Non-linear Solution \u4e0a\u9762\u7684\u662f\u901a\u8fc7\u89e3\u7ebf\u6027\u7cfb\u7edf\u7684\u65b9\u6cd5\uff0c\u53e6\u4e00\u79cd\u60f3\u6cd5\u662f\u6700\u5c0f\u5316 \u91cd\u6620\u5c04\u8bef\u5dee (reprojection error) . \\[ \\text{cost}(P) = \\text{dist}(\\mathbf{u}_l, \\mathbf{\\hat u}_l)^2 + \\text{dist}(\\mathbf{u}_r, \\mathbf{\\hat u}_r)^2 \\] \u53e6\u5916\uff0c\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\u6211\u4eec\u4e5f\u53ef\u4ee5\u4f18\u5316 \\(R\\) \u548c \\(\\mathbf{t}\\) . Multi-frame Structure from Motion \u00b6 \u4e0e Two-frame SfM \u7c7b\u4f3c\uff0c\u5df2\u77e5 \\(m\\) \u5f20\u56fe\u50cf\uff0c \\(n\\) \u4e2a\u4e09\u7ef4\u70b9\u4ee5\u53ca\u76f8\u673a\u5185\u53c2\u77e9\u9635\uff0c\u6c42\u76f8\u673a\u7684\u4f4d\u7f6e\u4ee5\u53ca\u6240\u62cd\u6444\u4e3b\u4f53\u7684\u4e09\u7ef4\u4f4d\u7f6e\u5750\u6807. \u4e5f\u5c31\u662f\u5bf9\u4e8e\u4ee5\u4e0b\u7b49\u5f0f\uff1a \\[ \\mathbf{u}_j^{(i)} = P_{proj}^{(i)} \\mathbf{P}_j \\text{, where } i = 1, \\dots, m, j = 1, \\dots, n \\] \u7531 \\(mn\\) \u4e2a\u6295\u5f71\u7684\u4e8c\u7ef4\u70b9 \\(\\mathbf{u}_j^{(i)}\\) \u6c42\u89e3 \\(m\\) \u4e2a\u6295\u5f71\u77e9\u9635 \\(P_{proj}^{(i)}\\) \u4e0e \\(n\\) \u4e2a\u4e09\u7ef4\u70b9\u5750\u6807 \\(P_j\\) . Solution: Sequential Structrue from Motion \u00b6 Step 1. Initialize camera motion and scene structure. \u521d\u59cb\u5316 \u00b6 \u9996\u5148\u9009\u4e24\u5f20\u56fe\u505a\u4e00\u6b21 Two-frame SfM. Step 2. Deal with an addition view. \u5904\u7406\u4e0b\u4e00\u5f20\u56fe \u00b6 \u5bf9\u4e8e\u6bcf\u65b0\u589e\u7684\u4e00\u5f20\u56fe\uff0c \u5bf9\u4e8e\u5df2\u7ecf\u5efa\u7acb\u4e09\u7ef4\u5750\u6807\u7684\u70b9\uff1a PnP \u95ee\u9898 . \u5bf9\u4e8e\u5c1a\u672a\u5efa\u7acb\u4e09\u7ef4\u5750\u6807\u7684\u70b9\uff1a\u505a Two-frame SfM. Step 3. Refine structure and motion: Bundle Adjustment. \u96c6\u675f\u8c03\u6574 \u00b6 \u5904\u7406\u5b8c\u6240\u6709\u7684\u56fe\u540e\uff0c\u518d\u901a\u8fc7\u4e00\u6b21 \u96c6\u675f\u8c03\u6574 \uff08\u5177\u4f53\u5b9e\u73b0\u662f LM \u7b97\u6cd5 \uff09\u5bf9\u4e09\u7ef4\u70b9\u5750\u6807\u548c\u76f8\u673a\u53c2\u6570\u505a\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u5373\u6700\u5c0f\u5316 \u91cd\u6620\u5c04\u8bef\u5dee \uff08\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\uff09\uff1a \\[ E(P_{proj}, \\mathbf{P}) = \\sum\\limits_{i=1}^m \\sum\\limits_{j=1}^n \\text{dist} \\left( u_j^{(i)}, P_{proj}^{(i)} \\mathbf{P}_j \\right)^2 \\] COLMAP \u00b6 COLMAP is a general-purpose Structure-from-Motion (SfM) and Multi-View Stereo (MVS) pipeline with a graphical and commandline interface. It offers a wide range of features for reconstruction of ordered and unordered image collections. Pipeline","title":"Lecture 7"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#lecture-7-structure-from-motion-sfm","text":"","title":"Lecture 7 Structure from Motion (SfM)"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#camera-calibration","text":"\u76f8\u673a\u6807\u5b9a\u7684\u8fc7\u7a0b\uff0c\u5c31\u662f\u4ece\u4e16\u754c\u5750\u6807\u7cfb\u5411\u50cf\u7d20\u5750\u6807\u7cfb\u8f6c\u6362\u7684\u8fc7\u7a0b. \u9700\u8981\u6ce8\u610f\u7684\u56db\u4e2a\u5750\u6807\u7cfb\uff1a \u4e16\u754c\u5750\u6807\u7cfb (World Coordinates) \u76f8\u673a\u5750\u6807\u7cfb (Camera Coordinates) \u56fe\u50cf\u5750\u6807\u7cfb (Image Plane) \u50cf\u7d20\u5750\u6807\u7cfb (Image Sensor) \u5176\u4e2d\uff0c\u524d\u4e24\u4e2a\u5750\u6807\u7cfb\u7684\u8f6c\u6362\u77e9\u9635\u53c2\u6570\u88ab\u79f0\u4e3a \u76f8\u673a\u5916\u53c2 \uff1b\u540e\u4e09\u4e2a\u5750\u6807\u7cfb\u7684\u8f6c\u6362\u77e9\u9635\u53c2\u6570\u88ab\u79f0\u4e3a \u76f8\u673a\u5185\u53c2 .","title":"Camera Calibration \u76f8\u673a\u6807\u5b9a"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#principle-analysis","text":"","title":"Principle Analysis \u539f\u7406\u5206\u6790"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#coordinate-transformation","text":"\u4ece\u4e16\u754c\u5750\u6807\u7cfb\u5230\u76f8\u673a\u5750\u6807\u7cfb\u7684\u53d8\u6362.","title":"Coordinate Transformation"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#_1","text":"\u521a\u4f53\u53d8\u6362\uff0c\u53ea\u6709\u65cb\u8f6c\u548c\u5e73\u79fb\uff0c\u5bf9\u5e94\u65cb\u8f6c\u77e9\u9635 \\(R\\) \uff08 \\(R\\) \u662f \u5355\u4f4d\u6b63\u4ea4\u77e9\u9635 (Orthonormal) ) \u548c\u5e73\u79fb\u5411\u91cf \\(c_w\\) . \\[ \\mathbf{x}_c = R(\\mathbf{x}_w - \\mathbf{c}_w) = R \\mathbf{x}_w - R \\mathbf{c}_w \\overset{\\Delta}{=\\!=} R \\mathbf{x}_w + \\mathbf{t}, \\text{ where } \\mathbf{t } = -R \\mathbf{c}_w. \\] \u6545\u8fd9\u4e2a\u53d8\u6362\u4e5f\u53ef\u4ee5\u7b49\u4ef7\u5730\u7528 \\(R\\) \u548c \\(\\mathbf{t}\\) \u6765\u8868\u5f81. \\[ \\mathbf{x}_c = \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\\\ \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} \\] \u540c\u6837\u8003\u8651\u9f50\u6b21\u5750\u6807\u7cfb \\[ \\mathbf{\\tilde x}_c = \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1\\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix} \\] Extrinsic Matrix (\u5916\u53c2\u77e9\u9635) \\[ M_{ext} = \\begin{bmatrix} R_{3\\times 3} & \\mathbf{t} \\\\ \\mathbf{0}_{1\\times 3} & 1 \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\] \u4e5f\u53ef\u4ee5\u5199\u4f5c \\[ M_{ext} = \\begin{bmatrix} R_{3\\times 3} & \\mathbf{t} \\\\ \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ \\end{bmatrix} \\]","title":"\u7279\u70b9"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#perspective-projection","text":"\u4ece\u76f8\u673a\u5750\u6807\u7cfb\u5230\u56fe\u50cf\u5750\u6807\u7cfb\u7684\u53d8\u6362. \u524d\u9762\u51e0\u4e2a Lecture \u4e2d\u5df2\u7ecf\u8ba8\u8bba\u8fc7\u8fd9\u4e00\u90e8\u5206 \u5185\u5bb9 . \u6b64\u5904\u4e0d\u518d\u8d58\u8ff0. \\[ \\begin{bmatrix} x_i \\\\ y_i \\\\ 1 \\end{bmatrix} \\cong \\begin{bmatrix} f & 0 & 0 & 0 \\\\ 0 & f & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1 \\end{bmatrix} \\]","title":"Perspective Projection"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#image-plane-to-image-sensor-mapping","text":"\u4ece\u56fe\u50cf\u5750\u6807\u7cfb\u5230\u50cf\u7d20\u5750\u6807\u7cfb\u7684\u53d8\u6362. \u50cf\u7d20\u70b9 pixel \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u7531\u4e8e\u76f8\u673a\u81ea\u8eab\u7684\u5236\u4f5c\u5de5\u827a\u548c\u8981\u6c42\uff0c\u50cf\u7d20\u70b9\u5e76\u4e0d\u4e00\u5b9a\u662f\u6b63\u65b9\u5f62\uff0c\u800c\u662f\u77e9\u5f62\uff0c\u4e0e\u539f\u56fe\u50cf\u76f8\u6bd4\u5177\u6709\u4e00\u5b9a\u7684 \u7f29\u653e . \u8fd9\u4e00\u70b9\u5728\u89c6\u9891\u7684\u5236\u4f5c\u4e2d\u4e5f\u6709\u6240\u4f53\u73b0. \u6b64\u5916\uff0c\u7531\u4e8e\u56fe\u50cf\u5750\u6807\u7cfb\u662f\u4ece\u4e2d\u5fc3\u4e3a\u539f\u70b9\uff0c\u800c\u50cf\u7d20\u5750\u6807\u7cfb\u662f\u4ee5\u5de6\u4e0a\u89d2\u4e3a\u539f\u70b9\u7684\uff0c\u5750\u6807\u53d8\u6362\u8fd8\u9700\u8981\u4e00\u5b9a\u7684* \u5e73\u79fb . \\[ \\begin{align} u &= m_x f \\frac{x_c}{z_c} + c_x \\\\ v &= m_y f \\frac{y_c}{z_c} + c_y \\end{align} \\] \u7ed3\u5408\u4ee5\u4e0a\u4e24\u4e2a\u53d8\u6362\uff0c\u8bb0 \\(f_x = m_x f\\) , \\(f_y = m_y f\\) . \u5219\u4ece\u76f8\u673a\u5750\u6807\u7cfb\u5230\u50cf\u7d20\u5750\u6807\u7cfb\u7684\u53d8\u6362\u4e3a \\[ \\mathbf{\\tilde{u}} = \\begin{bmatrix} u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1 \\end{bmatrix} \\] Intrinsic Matrix (\u5185\u53c2\u77e9\u9635) \\[ M_{int} = \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\] \u4e5f\u53ef\u4ee5\u5199\u4f5c \\[ K = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\] Projection Matrix \\(P\\) World to Camera \\[ \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1\\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix} \\] \\[ \\mathbf{\\tilde u} = M_{int}\\mathbf{\\tilde x}_c \\] Camera to Pixel \\[ \\begin{bmatrix} u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1 \\end{bmatrix} \\] \\[ \\mathbf{\\tilde x}_c = M_{ext}\\mathbf{\\tilde x}_w \\] \u6545 \\[ \\mathbf{\\tilde u} = M_{int}M_{ext} \\mathbf{\\tilde x}_w = P \\mathbf{\\tilde x} \\] \u5176\u4e2d\uff0c \\(P\\) \u88ab\u79f0\u4e3a\u6295\u5f71\u77e9\u9635 (Projection Matrix). \\[ \\begin{bmatrix} u \\\\ v \\\\ w \\end{bmatrix} \\cong \\begin{bmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\\\ p_{21} & p_{22} & p_{23} & p_{24} \\\\ p_{31} & p_{32} & p_{33} & p_{34} \\\\ p_{41} & p_{42} & p_{43} & p_{44} \\\\ \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix} \\]","title":"Image Plane to Image Sensor Mapping"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#implemetation","text":"","title":"Implemetation"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-1-capture-an-image-of-an-object-with-known-geometry","text":"","title":"Step 1. Capture an image of an object with known geometry."},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-2-identify-correspondences-image-matching-between-3d-scene-points-and-image-points","text":"","title":"Step 2. Identify correspondences (Image Matching \u56fe\u50cf\u5339\u914d) between 3D scene points and image points."},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-3-for-each-corresponding-point-i-we-get","text":"\\[ \\begin{bmatrix} u^{(i)} \\\\ v^{(i)} \\\\ w^{(i)} \\end{bmatrix} \\cong \\begin{bmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\\\ p_{21} & p_{22} & p_{23} & p_{24} \\\\ p_{31} & p_{32} & p_{33} & p_{34} \\\\ p_{41} & p_{42} & p_{43} & p_{44} \\\\ \\end{bmatrix} \\begin{bmatrix} x_w^{(i)} \\\\ y_w^{(i)} \\\\ z_w^{(i)} \\\\ 1 \\end{bmatrix} \\] \\[ \\begin{align} u^{(i)} &= \\frac{p_{11}x_w^{(i)} + p_{12}y_w^{(i)} + p_{13}z_w^{(i)} + p_{14}}{p_{31}x_w^{(i)} + p_{32}y_w^{(i)} + p_{33}z_w^{(i)} + p_{34}} \\\\ v^{(i)} &= \\frac{p_{21}x_w^{(i)} + p_{22}y_w^{(i)} + p_{23}z_w^{(i)} + p_{24}}{p_{31}x_w^{(i)} + p_{32}y_w^{(i)} + p_{33}z_w^{(i)} + p_{34}} \\\\ \\end{align} \\]","title":"Step 3. For each corresponding point \\(i\\),  we get"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-4-rearrange","text":"\\[ \\scriptsize \\begin{bmatrix} x_w^{(1)} & y_w^{(1)} & z_w^{(1)} & 1 & 0 & 0 & 0 & 0 & -u_1x_w^{(1)} & -u_1y_w^{(1)} & -u_1z_w^{(1)} & -u_1 \\\\ 0 & 0 & 0 & 0 & x_w^{(1)} & y_w^{(1)} & z_w^{(1)} & 1 & -v_1x_w^{(1)} & -v_1y_w^{(1)} & -v_1z_w^{(1)} & -v_1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_w^{(i)} & y_w^{(i)} & z_w^{(i)} & 1 & 0 & 0 & 0 & 0 & -u_ix_w^{(i)} & -u_iy_w^{(i)} & -u_iz_w^{(i)} & -u_i \\\\ 0 & 0 & 0 & 0 & x_w^{(i)} & y_w^{(i)} & z_w^{(i)} & 1 & -v_ix_w^{(i)} & -v_iy_w^{(i)} & -v_iz_w^{(i)} & -v_i \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_w^{(n)} & y_w^{(n)} & z_w^{(n)} & 1 & 0 & 0 & 0 & 0 & -u_nx_w^{(n)} & -u_ny_w^{(n)} & -u_nz_w^{(n)} & -u_n \\\\ 0 & 0 & 0 & 0 & x_w^{(n)} & y_w^{(n)} & z_w^{(n)} & 1 & -v_nx_w^{(n)} & -v_ny_w^{(n)} & -v_nz_w^{(n)} & -v_n \\\\ \\end{bmatrix} \\begin{bmatrix} p_{11} \\\\ p_{12} \\\\ p_{13} \\\\ p_{14} \\\\ p_{21} \\\\ p_{22} \\\\ p_{23} \\\\ p_{24} \\\\ p_{31} \\\\ p_{32} \\\\ p_{33} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\] \\[ i.e.\\ A \\mathbf{p} = 0 \\]","title":"Step 4. Rearrange."},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-5-solve-for-p","text":"\u7c7b\u4f3c\u5730\uff0c\u6211\u4eec\u53ef\u4ee5\u7ed9\u51fa\u4e00\u5b9a\u7684\u9650\u5236 constraint. Option 1. \u4ee4 \\(p_{34} = 1\\) . Option 2. \u4ee4 \\(||p|| = 1\\) . \u5f97\u5230\u81ea\u7531\u5ea6\u4e3a \\(11\\) \uff0c\u81f3\u5c11\u9700\u8981 6 \u7ec4\u70b9. \u5e76\u540c\u6837\u8981\u6700\u5c0f\u5316 \\(||\\mathbf{Ap - 0}|| = \\mathbf{||Ap||}\\) . \u540c\u6837\u5982\u679c\u4f7f\u7528\u7684\u662f\u5411\u91cf\u8303\u6570\u4e3a \\(1\\) \u7684\u9650\u5236\uff0c\u90a3\u4e48\u6211\u4eec\u53ea\u9700\u8981\u786e\u5b9a \\(\\mathbf{p}\\) \u7684\u65b9\u5411 \\(\\mathbf{\\hat p}\\) . \u4e14 \\(\\mathbf{\\hat p}\\) \u662f \\(\\mathbf{A^{T}A}\\) \u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf.","title":"Step 5 Solve for \\(P\\)."},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-6-from-p-to-answers","text":"\u7531 \\[ \\begin{bmatrix} p_{11} \\\\ p_{12} \\\\ p_{13} \\\\ p_{14} \\\\ p_{21} \\\\ p_{22} \\\\ p_{23} \\\\ p_{24} \\\\ p_{31} \\\\ p_{32} \\\\ p_{33} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix} f_x & 0 & c_x & 0 \\\\ 0 & f_y & c_y & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y\\\\ r_{31} & r_{32} & r_{33} & t_z\\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\] \u6ce8\u610f\u5230 \\[ \\begin{bmatrix} p_{11} \\\\ p_{12} \\\\ p_{13} \\\\ p_{21} \\\\ p_{22} \\\\ p_{23} \\\\ p_{31} \\\\ p_{32} \\\\ p_{33} \\end{bmatrix} = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\\\ \\end{bmatrix} = KR \\] \u5176\u4e2d \\(K\\) \u662f\u4e0a\u4e09\u89d2\u77e9\u9635 (Upper Right Triangular Matrix)\uff0c \\(R\\) \u662f\u5355\u4f4d\u6b63\u4ea4\u77e9\u9635 (Orthonormal Matrix). \u7531 QR \u5206\u89e3 (QR Factorization) \u53ef\u77e5\uff0c \\(K\\) \u4e0e \\(R\\) \u7684\u89e3\u662f\u552f\u4e00\u7684. \u518d\u7531 \\[ \\begin{bmatrix} p_{14} \\\\ p_{24} \\\\ p_{34} \\end{bmatrix} = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} = K \\mathbf{t} \\] \u6545\u53ef\u89e3\u5f97 \\[ \\mathbf{t} = K^{-1} \\begin{bmatrix} p_{14} \\\\ p_{24} \\\\ p_{34} \\end{bmatrix} \\] Problem: Distortion \u5b9e\u9645\u4e0a\u76f8\u673a\u81ea\u8eab\u4f1a\u5177\u6709\u4e00\u5b9a\u7684\u626d\u66f2\u7cfb\u6570\uff0c\u6211\u4eec\u5728\u6b64 \u5ffd\u7565 \u5b83\u7684\u5f71\u54cd. Perspective-n-Point Problem (PnP \u95ee\u9898) \u5f31\u5316\u4e00\u4e0b\u6761\u4ef6\uff0c\u5047\u8bbe\u5df2\u77e5\u76f8\u673a\u7684\u5185\u53c2\uff08\u5373\u77e9\u9635 \\(K\\) \uff09\uff0c\u6c42\u76f8\u673a\u7684\u5916\u53c2\uff08\u5373\u77e9\u9635 \\(R\\) \u548c \\(\\mathbf{t}\\) \uff09\uff0c\u4e5f\u5373\u76f8\u673a\u59ff\u6001 (camera pose).","title":"Step 6 From \\(P\\) to Answers"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#direct-linear-transform-dlt","text":"\u5373\u4ee5\u4e0a\u5728\u76f8\u673a\u6807\u5b9a\u4e2d\u63d0\u5230\u7684\u65b9\u6cd5\uff0c\u5148\u6c42\u89e3\u77e9\u9635 \\(P\\) \uff0c\u518d\u901a\u8fc7 \\(K\\) \u5f97\u5230 \\(R\\) \u548c \\(\\mathbf{t}\\) .","title":"Direct Linear Transform (DLT)"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#p3p","text":"\u5373\u5df2\u77e5 \\(a, b, c, A, B, C\\) \u70b9\u5750\u6807\uff0c\u6c42 \\(O\\) \u5750\u6807. \u81f3\u5c11\u9700\u8981 \\(3\\) \u7ec4\u70b9 Intermediates. \u7531\u4f59\u5f26\u5b9a\u7406 \\[ \\begin{align} OA^2 + OB^2 - 2 OA \\cdot OB \\cos \\angle AOB &= AB^2 \\\\ OA^2 + OC^2 - 2 OB \\cdot OC \\cos \\angle BOC &= BC^2 \\\\ OA^2 + OC^2 - 2 OA \\cdot OC \\cos \\angle AOC &= AC^2 \\end{align} \\] \u4ee4 \\(x = \\frac{OA}{OC}, y = \\frac{OB}{OC}, v = \\frac{AB^2}{OC^2}, u = \\frac{BC^2}{AB^2}, w = \\frac{AC^2}{AB^2}\\) \uff0c\u5316\u7b80\u5f97 \\[ \\begin{align} (1 - u)y^2 - ux^2 - cos \\angle BOC y + 2uxy \\cos \\angle AOB + 1 &= 0 \\\\ (1 - w)x^2 - wy^2 - cos \\angle AOC y + 2wxy \\cos \\angle AOB + 1 &= 0 \\end{align} \\] \u4ee5\u4e0a\u65b9\u7a0b\u7ec4\u662f\u4e8c\u5143\u4e8c\u6b21\u65b9\u7a0b\u7ec4 (Binary Quadratic Equation)\uff0c\u6709\u56db\u7ec4\u89e3. \u5176\u4e2d\u6709\u4e24\u7ec4\u89e3\u7684 \\(O\\) \u5750\u6807\u5728 \u9762 \\(ABC\\) \u4e0e \u9762 \\(abc\\) \u4e4b\u95f4\uff0c\u820d\u53bb. \u8fd8\u9700\u8981\u4e00\u7ec4\u989d\u5916\u7684\u70b9\u6765\u786e\u5b9a\u552f\u4e00\u89e3\uff0c\u6545\u4e00\u5171\u9700\u8981 \\(4\\) \u7ec4\u70b9.","title":"P3P"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#pnp","text":"\u6700\u5c0f\u5316 \u91cd\u6295\u5f71\u8bef\u5dee (Reprojection Error) \\[ \\mathop{\\arg\\min\\limits_{R, \\mathbf{t}}} \\sum\\limits_i ||(p_i, K(RP_i + \\mathbf{t})||^2 \\] \u5176\u4e2d\uff0c \\(p_i\\) \u662f\u6295\u5f71\u9762\u4e0a\u7684\u4e8c\u7ef4\u5750\u6807\uff0c \\(P_i\\) \u662f\u7a7a\u95f4\u5185\u7684\u4e09\u7ef4\u5750\u6807. \u901a\u8fc7 P3P \u521d\u59cb\u5316\uff0cGauss-Newton \u6cd5\u4f18\u5316.","title":"PnP"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#epnp","text":"one of the most popluar methods. time complexity \\(O(N)\\) . high accuracy. \u5927\u81f4\u65b9\u5f0f\u662f\u901a\u8fc7\u56db\u7ec4\u63a7\u5236\u70b9\u7684\u7ebf\u6027\u7ec4\u5408\u6c42\u89e3.","title":"EPnP"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#two-frame-structure-from-motion","text":"Stereo vision \u7acb\u4f53\u89c6\u89c9 Compute 3D structure of the scene and camera poses from views (images). Two-frame SfM \u610f\u5728\u89e3\u51b3\u8fd9\u6837\u4e00\u4e2a\u95ee\u9898\uff1a\u5df2\u77e5\u4e24\u5f20\u56fe\u50cf\u548c\u76f8\u673a\u5185\u53c2\u77e9\u9635 ( \\(K_l(3 \\times 3)\\) \u4e0e \\(K_r(3 \\times 3)\\) )\uff0c\u6c42\u76f8\u673a\u7684\u4f4d\u7f6e\u4ee5\u53ca\u6240\u62cd\u6444\u4e3b\u4f53\u7684\u4e09\u7ef4\u4f4d\u7f6e\u5750\u6807.","title":"Two-frame Structure from Motion"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#principle-induction","text":"Epipolar Geometry \u5bf9\u6781\u51e0\u4f55 \u5bf9\u6781\u51e0\u4f55\u63cf\u8ff0\u4e86\u4e00\u4e2a\u4e09\u7ef4\u70b9 \\(P\\) \u5728\u4e24\u4e2a\u89c6\u89d2\u7684\u4e8c\u7ef4\u6295\u5f71 \\(u_l, u_r\\) \u4e4b\u95f4\u7684\u51e0\u4f55\u5173\u7cfb. \u5e76\u901a\u8fc7\u8fd9\u4e2a\u51e0\u4f55\u5173\u7cfb\u5efa\u7acb\u4e24\u4e2a\u6444\u50cf\u673a\u4e4b\u95f4\u7684\u53d8\u6362\u5173\u7cfb\uff08\u5373\u7ed9\u51fa\u77e9\u9635 \\(R\\) \u548c \\(\\mathbf{t}\\) \uff09. Definition Epipole \u5bf9\u6781\u70b9 \uff1a\u4e00\u53f0\u6444\u50cf\u673a\u6295\u5f71\u5728\u53e6\u4e00\u53f0\u6444\u50cf\u673a\u4e2d\u7684\u70b9. \u5982\u56fe\u4e2d\u7684 \\(e_l\\) \u548c \\(e_r\\) . \u5bf9\u7ed9\u5b9a\u7684\u4e24\u53f0\u6444\u50cf\u673a\uff0c \\(e_l\\) \u548c \\(e_r\\) \u662f\u552f\u4e00\u7684. Epipolar Plane of Scene Point P \u5173\u4e8e P \u70b9\u7684\u5bf9\u6781\u9762 \uff1a\u7531 Scene Point \\(P\\) \u70b9\u548c\u4e24\u4e2a\u6444\u50cf\u673a \\(O_l\\) \u548c \\(O_r\\) \u5f62\u6210\u7684\u5e73\u9762. \u6bcf\u4e2a scene point \u6709\u552f\u4e00\u5bf9\u5e94\u7684\u5bf9\u6781\u9762. \u5bf9\u6781\u70b9\u5728\u5bf9\u6781\u9762\u4e0a.","title":"Principle Induction \u539f\u7406\u63a8\u5bfc"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#essential-matrix-e","text":"\u5bf9\u4e8e\u4e00\u4e2a\u5bf9\u6781\u9762\uff0c\u4ee5\u4e0b\u7b49\u5f0f\u5173\u7cfb\u6210\u7acb\uff1a \\[ \\begin{align} \\mathbf{n} = \\mathbf{t} \\times \\mathbf{x}_l \\\\ \\\\ \\mathbf{x}_l \\cdot (\\mathbf{t} \\times \\mathbf{x}_l) = 0 \\end{align} \\] \u5176\u4e2d \\(\\mathbf{n}\\) \u88ab\u79f0\u4e3a Normal Vector. \u5047\u8bbe\u53f3\u8fb9\u7684\u6444\u50cf\u673a\u76f8\u5bf9\u5de6\u8fb9\u7684\u6444\u50cf\u673a\u8fdb\u884c\u521a\u4f53\u53d8\u6362 + \u5e73\u79fb\uff0c\u5373\uff1a \\[ \\begin{align} \\mathbf{x}_l &= R \\mathbf{x}_r + \\mathbf{t} \\\\ \\\\ \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\end{bmatrix} &= \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ y_r \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} \\end{align} \\] \u8fdb\u4e00\u6b65\u6f14\u7ece\u63a8\u7406\uff0c\u5f97\u5230\uff1a \\[ \\begin{align} \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\begin{bmatrix} t_yz_l - t_zy_l \\\\ t_zx_l - t_xz_l \\\\ t_xy_l - t_yx_l \\end{bmatrix} = 0 & \\text{, Cross-product definition} \\\\ \\\\ \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\underbrace{ \\begin{bmatrix} 0 & -t_z & t_y \\\\ t_z & 0 & -t_x \\\\ -t_y & t_x & 0 \\end{bmatrix} }_{T_\\times} \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\end{bmatrix} = 0 & \\text{, Matrix-vector form} \\end{align} \\] \u4ee3\u5165\u53d8\u6362\u77e9\u9635 \\[ \\small \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\left( \\underbrace{ \\begin{bmatrix} 0 & -t_z & t_y \\\\ t_z & 0 & -t_x \\\\ -t_y & t_x & 0 \\end{bmatrix} }_{T_\\times} \\underbrace{ \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\end{bmatrix} }_{R} \\begin{bmatrix} x_r \\\\ y_r \\\\ y_r \\end{bmatrix} + \\underbrace{ \\begin{bmatrix} 0 & -t_z & t_y \\\\ t_z & 0 & -t_x \\\\ -t_y & t_x & 0 \\end{bmatrix} \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} }_{\\mathbf{t} \\times \\mathbf{t} = \\mathbf{0}} \\right) = 0 \\] \\[ \\begin{bmatrix} x_l & y_l & z_l \\end{bmatrix} \\underbrace{ \\begin{bmatrix} e_{11} & e_{12} & e_{13} \\\\ e_{21} & e_{22} & e_{23} \\\\ e_{31} & e_{32} & e_{33} \\end{bmatrix} }_{\\text{Essential Matrix } E} \\begin{bmatrix} x_r \\\\ y_r \\\\ y_r \\end{bmatrix} \\text{, where } E = T_\\times R \\] \u6ce8\u610f\u5230 \\(T_\\times\\) \u662f\u53cd\u5bf9\u79f0\u77e9\u9635 (skew-symmetric matrix) \u800c \\(R\\) \u662f\u5355\u4f4d\u6b63\u4ea4\u77e9\u9635 (orthonomal matrix)\uff0c\u7531 SVD \u5206\u89e3 (Singule Value Decomposition)\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece \\(E\\) \u552f\u4e00\u5206\u89e3\u5f97\u5230 \\(T_\\times\\) \u548c \\(R\\) .","title":"Essential Matrix \\(E\\) \u57fa\u672c\u77e9\u9635"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#fundemental-matrix-f","text":"\u7531\u6295\u5f71\u5173\u7cfb\u4ee5\u53ca\u5185\u53c2\u77e9\u9635 \\(K\\) \uff0c\u5f97\u5230\uff1a \\[ \\small \\begin{align} \\text{Left Camera} && \\text{Right Camera} \\\\ z_l \\begin{bmatrix} u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &= \\underbrace{ \\begin{bmatrix} f_x^{(l)} & 0 & o_x^{(l)} \\\\ 0 & f_y^{(l)} & o_y^{(l)} \\\\ 0 & 0 & 1 \\end{bmatrix} }_{K_l} \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\end{bmatrix} & z_r \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} &= \\underbrace{ \\begin{bmatrix} f_x^{(r)} & 0 & o_x^{(r)} \\\\ 0 & f_y^{(r)} & o_y^{(r)} \\\\ 0 & 0 & 1 \\end{bmatrix} }_{K_r} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\end{bmatrix} \\\\ \\mathbf{x}_l^T &= \\begin{bmatrix} u_l & v_l & 1 \\end{bmatrix} z_l (K_l^{-1})^T & \\mathbf{x}_r &= K_r^{-1} z_r \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} \\end{align} \\] \u4ee3\u5165\u4ee5\u4e0a\u5f97\u5230\u7684\uff1a \\[ \\mathbf{x}^T_l E \\mathbf{x}_r = 0 \\] \u6709 \\[ \\require{canel} \\begin{bmatrix} u_l & v_l & 1 \\end{bmatrix} \\cancel{z_l} (K_l^{-1})^T E K_r^{-1} \\cancel{z_r} \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = 0 \\] \u5373 \\[ \\begin{bmatrix} u_l & v_l & 1 \\end{bmatrix} F \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = 0 \\text{, where } E = K_l^T F K_r \\]","title":"Fundemental Matrix \\(F\\) \u672c\u771f\u77e9\u9635"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#implementation","text":"","title":"Implementation \u5b9e\u73b0"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-1-find-a-set-m-pairs-of-corresponding-features-m","text":"Least Number of Points At least 8 pairs: \\((u_l^{(i)}, v_l^{(i)})\\) \u2194 \\((u_r^{(i)}, v_r^{(i)})\\) NOTE that one pair only corresponds one equation. (not the same as before)","title":"Step 1. Find a set (\\(m\\) pairs) of corresponding features. \u627e\u5230\u4e00\u7ec4\uff08\\(m\\) \u5bf9\uff09\u5339\u914d\u70b9."},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-2-build-linear-systems-and-solve-for-f-f","text":"\u5bf9\u4e8e\u6bcf\u7ec4\u5339\u914d \\(i\\) \uff0c \\[ \\begin{bmatrix} u_l^{(i)} & v_l^{(i)} & 1 \\end{bmatrix} \\begin{bmatrix} f_{11} & f_{12} & f_{13} \\\\ f_{21} & f_{22} & f_{23} \\\\ f_{31} & f_{32} & f_{33} \\end{bmatrix} \\begin{bmatrix} u_r^{(i)} \\\\ v_r^{(i)} \\\\ 1 \\end{bmatrix} = 0 \\] \u5c55\u5f00\u5f97\uff0c \\[ \\small \\left( f_{11} u_r^{(i)} + f_{12} v_r^{(i)} + f_13 \\right)u_l^{(i)} + \\left( f_{21} u_r^{(i)} + f_{22} v_r^{(i)} + f_23 \\right)v_l^{(i)} + f_{31} u_r^{(i)} + f_{32} v_r^{(i)} + f_33 = 0 \\] \u5bf9\u6240\u6709\u9009\u62e9\u7684\u70b9\uff0c\u6709 \\[ \\small \\begin{bmatrix} u_l^{(1)}u_r^{(1)} & u_l^{(1)}v_r^{(1)} & u_l^{(1)} & v_l^{(1)}u_r^{(1)} & v_l^{(1)}v_r^{(1)} & v_l^{(1)} & u_r^{(1)} & v_r^{(1)} & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ u_l^{(i)}u_r^{(i)} & u_l^{(i)}v_r^{(i)} & u_l^{(i)} & v_l^{(i)}u_r^{(i)} & v_l^{(i)}v_r^{(i)} & v_l^{(i)} & u_r^{(i)} & v_r^{(i)} & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ u_l^{(m)}u_r^{(m)} & u_l^{(m)}v_r^{(m)} & u_l^{(m)} & v_l^{(m)}u_r^{(m)} & v_l^{(m)}v_r^{(m)} & v_l^{(m)} & u_r^{(m)} & v_r^{(m)} & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} f_{11} \\\\ f_{12} \\\\ f_{13} \\\\ f_{21} \\\\ f_{22} \\\\ f_{23} \\\\ f_{31} \\\\ f_{32} \\\\ f_{33} \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\] \\[ A \\mathbf{f} = \\mathbf{0} \\] \u540c\u6837\u5730 \uff0c\u6211\u4eec\u53ef\u4ee5\u7ed9\u51fa\u4e00\u5b9a\u7684\u9650\u5236 constraint. Option 1. \u4ee4 \\(f_{33} = 1\\) . Option 2. \u4ee4 \\(||f|| = 1\\) . \u5f97\u5230\u81ea\u7531\u5ea6\u4e3a \\(8\\) \uff0c\u81f3\u5c11\u9700\u8981 8 \u7ec4\u70b9. \u5e76\u540c\u6837\u8981\u6700\u5c0f\u5316 \\(||\\mathbf{Af - 0}|| = \\mathbf{||Af||}\\) . \u540c\u6837\u5982\u679c\u4f7f\u7528\u7684\u662f\u5411\u91cf\u8303\u6570\u4e3a \\(1\\) \u7684\u9650\u5236\uff0c\u90a3\u4e48\u89e3\u5c31\u662f \\(\\mathbf{A^{T}A}\\) \u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf.","title":"Step 2. Build linear systems and solve for \\(F\\). \u5efa\u7acb\u65b9\u7a0b\u7ec4\u5e76\u6c42\u89e3\u77e9\u9635 \\(F\\)."},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-3-find-e-and-extract-r-mathbft","text":"\\[ E = K_l^T F K_r \\] \u901a\u8fc7 SVD \u5206\u89e3\uff0c \\(E = T_\\times R\\) \u5f97\u5230 \\(T_\\times\\) \u548c \\(R\\) . \\(R\\) \u5373\u4e3a\u65cb\u8f6c\u77e9\u9635\uff0c \\(\\mathbf{t}\\) \u53ef\u4ee5\u901a\u8fc7 \\(T_\\times\\) \u76f4\u63a5\u5f97\u5230.","title":"Step 3. Find \\(E\\) and Extract \\(R, \\mathbf{t}\\)"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-4-find-3d-position-of-scene-points","text":"\u73b0\u5728\u5df2\u7ecf\u6709\u4ee5\u4e0b\u7b49\u5f0f \\[ \\small \\begin{align} \\text{Left Camera} \\\\ \\begin{bmatrix} u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} f_x^{(l)} & 0 & o_x^{(l)} & 0 \\\\ 0 & f_y^{(l)} & o_y^{(l)} & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\\\ 1 \\end{bmatrix} ,\\ \\begin{bmatrix} x_l \\\\ y_l \\\\ z_l \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y \\\\ r_{31} & r_{32} & r_{33} & t_z \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\\\ \\begin{bmatrix} u_l \\\\ v_l \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} f_x^{(l)} & 0 & o_x^{(l)} & 0 \\\\ 0 & f_y^{(l)} & o_y^{(l)} & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y \\\\ r_{31} & r_{32} & r_{33} & t_z \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\\\ \\\\ \\normalsize \\mathbf{\\tilde u}_l &= \\normalsize P_l \\mathbf{\\tilde x}_r \\\\ \\\\ \\small \\text{Right Camera} \\\\ \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} f_x^{(r)} & 0 & o_x^{(r)} & 0 \\\\ 0 & f_y^{(r)} & o_y^{(r)} & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\\\ \\\\ \\normalsize \\mathbf{\\tilde u}_l &= \\normalsize M_{int_r} \\mathbf{\\tilde x}_r \\end{align} \\] \u7531 \\[ \\small \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} m_{11} & m_{12} & m_{13} & m_{14} \\\\ m_{21} & m_{22} & m_{23} & m_{24} \\\\ m_{31} & m_{32} & m_{33} & m_{34} \\\\ \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} ,\\ \\begin{bmatrix} u_r \\\\ v_r \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\\\ p_{21} & p_{22} & p_{23} & p_{24} \\\\ p_{31} & p_{32} & p_{33} & p_{34} \\\\ \\end{bmatrix} \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\\\ 1 \\end{bmatrix} \\] \u6574\u7406\u5f97 \\[ \\underbrace{ \\begin{bmatrix} u_r m_{31} - m_{11} & u_r m_{32} - m_{12} & u_r m_{33} - m_{13} \\\\ v_r m_{31} - m_{21} & v_r m_{32} - m_{22} & v_r m_{33} - m_{23} \\\\ u_l p_{31} - p_{11} & u_l p_{32} - p_{12} & u_l p_{33} - p_{13} \\\\ v_l p_{31} - p_{21} & v_l p_{32} - p_{22} & v_l p_{33} - p_{23} \\end{bmatrix} }_{A_{4\\times 3}} \\underbrace{ \\begin{bmatrix} x_r \\\\ y_r \\\\ z_r \\end{bmatrix} }_{\\mathbf{x}_r} = \\underbrace{ \\begin{bmatrix} m_{14} - m_{34} \\\\ m_{24} - m_{34} \\\\ p_{14} - p_{34} \\\\ p_{24} - p_{34} \\end{bmatrix} }_{\\mathbf{b}} \\] \u7531\u5206\u6790\u4e0a\u89e3\u7684\u552f\u4e00\u6027\uff0c\u7406\u8bba\u4e0a\u6709\u4e24\u884c\u662f\u7ebf\u6027\u76f8\u5173\u7684. \u4f46\u662f\u5728\u5b9e\u9645\u53d6\u70b9\u4e2d\u53ef\u80fd\u4f1a\u5b58\u5728\u4e00\u5b9a\u7684\u8bef\u5dee\uff0c\u6240\u4ee5\u4e3a\u4e86\u6700\u5927\u5316\u5229\u7528\u6570\u636e\u91cf\uff0c\u8fd8\u662f\u91c7\u7528\u6700\u5c0f\u4e8c\u4e58\u6cd5. \u7531\u6700\u5c0f\u4e8c\u4e58\u6cd5\u5f97\uff0c \\[ \\mathbf{x}_r = (A^TA)^{-1}A^T \\mathbf{b} \\] Non-linear Solution \u4e0a\u9762\u7684\u662f\u901a\u8fc7\u89e3\u7ebf\u6027\u7cfb\u7edf\u7684\u65b9\u6cd5\uff0c\u53e6\u4e00\u79cd\u60f3\u6cd5\u662f\u6700\u5c0f\u5316 \u91cd\u6620\u5c04\u8bef\u5dee (reprojection error) . \\[ \\text{cost}(P) = \\text{dist}(\\mathbf{u}_l, \\mathbf{\\hat u}_l)^2 + \\text{dist}(\\mathbf{u}_r, \\mathbf{\\hat u}_r)^2 \\] \u53e6\u5916\uff0c\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\u6211\u4eec\u4e5f\u53ef\u4ee5\u4f18\u5316 \\(R\\) \u548c \\(\\mathbf{t}\\) .","title":"Step 4. Find 3D Position of Scene Points"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#multi-frame-structure-from-motion","text":"\u4e0e Two-frame SfM \u7c7b\u4f3c\uff0c\u5df2\u77e5 \\(m\\) \u5f20\u56fe\u50cf\uff0c \\(n\\) \u4e2a\u4e09\u7ef4\u70b9\u4ee5\u53ca\u76f8\u673a\u5185\u53c2\u77e9\u9635\uff0c\u6c42\u76f8\u673a\u7684\u4f4d\u7f6e\u4ee5\u53ca\u6240\u62cd\u6444\u4e3b\u4f53\u7684\u4e09\u7ef4\u4f4d\u7f6e\u5750\u6807. \u4e5f\u5c31\u662f\u5bf9\u4e8e\u4ee5\u4e0b\u7b49\u5f0f\uff1a \\[ \\mathbf{u}_j^{(i)} = P_{proj}^{(i)} \\mathbf{P}_j \\text{, where } i = 1, \\dots, m, j = 1, \\dots, n \\] \u7531 \\(mn\\) \u4e2a\u6295\u5f71\u7684\u4e8c\u7ef4\u70b9 \\(\\mathbf{u}_j^{(i)}\\) \u6c42\u89e3 \\(m\\) \u4e2a\u6295\u5f71\u77e9\u9635 \\(P_{proj}^{(i)}\\) \u4e0e \\(n\\) \u4e2a\u4e09\u7ef4\u70b9\u5750\u6807 \\(P_j\\) .","title":"Multi-frame Structure from Motion"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#solution-sequential-structrue-from-motion","text":"","title":"Solution: Sequential Structrue from Motion"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-1-initialize-camera-motion-and-scene-structure","text":"\u9996\u5148\u9009\u4e24\u5f20\u56fe\u505a\u4e00\u6b21 Two-frame SfM.","title":"Step 1. Initialize camera motion and scene structure. \u521d\u59cb\u5316"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-2-deal-with-an-addition-view","text":"\u5bf9\u4e8e\u6bcf\u65b0\u589e\u7684\u4e00\u5f20\u56fe\uff0c \u5bf9\u4e8e\u5df2\u7ecf\u5efa\u7acb\u4e09\u7ef4\u5750\u6807\u7684\u70b9\uff1a PnP \u95ee\u9898 . \u5bf9\u4e8e\u5c1a\u672a\u5efa\u7acb\u4e09\u7ef4\u5750\u6807\u7684\u70b9\uff1a\u505a Two-frame SfM.","title":"Step 2. Deal with an addition view. \u5904\u7406\u4e0b\u4e00\u5f20\u56fe"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#step-3-refine-structure-and-motion-bundle-adjustment","text":"\u5904\u7406\u5b8c\u6240\u6709\u7684\u56fe\u540e\uff0c\u518d\u901a\u8fc7\u4e00\u6b21 \u96c6\u675f\u8c03\u6574 \uff08\u5177\u4f53\u5b9e\u73b0\u662f LM \u7b97\u6cd5 \uff09\u5bf9\u4e09\u7ef4\u70b9\u5750\u6807\u548c\u76f8\u673a\u53c2\u6570\u505a\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u5373\u6700\u5c0f\u5316 \u91cd\u6620\u5c04\u8bef\u5dee \uff08\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\uff09\uff1a \\[ E(P_{proj}, \\mathbf{P}) = \\sum\\limits_{i=1}^m \\sum\\limits_{j=1}^n \\text{dist} \\left( u_j^{(i)}, P_{proj}^{(i)} \\mathbf{P}_j \\right)^2 \\]","title":"Step 3. Refine structure and motion: Bundle Adjustment. \u96c6\u675f\u8c03\u6574"},{"location":"Computer_Science_Courses/ICV/7_Structure_from_Motion/#colmap","text":"COLMAP is a general-purpose Structure-from-Motion (SfM) and Multi-View Stereo (MVS) pipeline with a graphical and commandline interface. It offers a wide range of features for reconstruction of ordered and unordered image collections. Pipeline","title":"COLMAP"},{"location":"Computer_Science_Courses/ICV/8_Depth_Estimation_and_3D_Reconstruction/","text":"","title":"Lecture 8"},{"location":"Computer_Science_Courses/ICV/9_Deep_Learning/","text":"","title":"Lecture 9"},{"location":"Mathematics_Basis/","text":"Title Page \u00b6 Abstract This section stores the notes of mathematics basis. Algrebra, analysis, and any other courses or topics concerned about mathematics. Welcome to point out anything wrong and careless mistakes!","title":"Title Page"},{"location":"Mathematics_Basis/#title-page","text":"Abstract This section stores the notes of mathematics basis. Algrebra, analysis, and any other courses or topics concerned about mathematics. Welcome to point out anything wrong and careless mistakes!","title":"Title Page"},{"location":"Mathematics_Basis/NA/Chap_1/","text":"Chapter 1 | Mathematical Preliminaries \u00b6 Error \u00b6 Truncation Error \u00b6 the error involved using a truncated or finite summation. Roundoff Error \u00b6 the error produced when performing real number calculations. It occurs because the arithmetic performed in a machine involved numbers with a finite number of digits. Chopping & Rounding Given a real number \\(y = 0.d_1d_2\\dots d_kd_{k+1}\\dots \\times 10^N\\) , the floating-point representation of \\(y\\) is \\(fl(y)\\) : \\[ fl(y) = \\left\\{ \\begin{align} & 0.d_1d_2\\dots d_k \\times 10^N, & {\\tt Chopping} \\\\ & {\\tt chop}\\left(y + 5 \\times 10^{n - (k + 1)}\\right). & {\\tt Rounding} \\end{align} \\right. \\] Definition If \\(p^*\\) is an approximation to \\(p\\) , then absolute error is \\(|p - p*|\\) and relative error is \\(\\frac{|p - p^*|}{|p|}\\) . The number \\(p^*\\) is said to be approximate to \\(p\\) to \\(t\\) * significant digits if \\(t\\) is the largest nonnegative integer for which \\[ \\frac{|p - p^*|}{|p|} < 5 \\times 10^{-t} \\] Example For the floating-point representation of \\(y\\) , the relative error is \\[ \\left|\\frac{y - fl(y)}{y}\\right|. \\] for chopping representation, \\[ \\left|\\frac{y - fl(y)}{y}\\right| = \\left|\\frac{0.d_{k + 1}d_{k + 2}\\dots}{0.d_1d_2\\dots}\\right| \\times 10^{-k} \\le \\frac{1}{0.1} \\times 10^{-k} = 10^{-k + 1}. \\] for rounding representation, \\[ \\left|\\frac{y - fl(y)}{y}\\right| \\le \\frac{0.5}{0.1} \\times 10^{-k} = 0.5 \\times 10^{-k + 1}. \\] Effect of Error \u00b6 Subtraction may reduce significant digits. e.g. 0.1234 - 0.1233 = 0.001 Division by small number of multiplication by large number magnify the abosolute error without modifying the relative error. Some Solutions to Reduce Error \u00b6 Quadratic Formula the roots of \\(ax^2 + bx + c = 0\\) is \\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}. \\] Sometimes \\(b\\) is closer to \\(\\sqrt{b^2 - 4ac}\\) , which may cause the subtraction to reduce significant digits. An alternate way is to modify the formula to \\[ x = \\frac{-2c}{b \\pm \\sqrt{b^2 - 4ac}}. \\] But it may cause the division by small number. So it's a tradeoff to use one of the two formulae above. Horner's Method \u79e6\u4e5d\u97f6\u7b97\u6cd5 \\[ \\begin{align} f(x) &= a_nx^n + a_{n-1}x^{n-1} + \\dots + a_1x + a_0 \\\\ &= (\\dots((a_nx+a_{n-1})x+a_{n-2})x+\\dots+a_1)x+a_0 \\end{align} \\] Stable Algorithms and Convergence \u00b6 Definition An algorithm that satisfies that small changes in the initial data produce correspondingly small changes in the final results is called stable ; otherwise it is unstable . An algorithm is called conditionally stable if it is stable only for certain choices of initial data. Suppose \\(E_0 > 0\\) denotes an initial errors, \\(E_n\\) denotes the magnitude of an error after \\(n\\) subsequent operations Linear growth of errors \\(E_n \\approx C n E_0\\) unavoidable and acceptable Exponential growth of errors \\(E_n \\approx C^n E_0\\) unacceptable Example the recursive equation \\(p_n = \\frac{10}{3}p_{n - 1} - p_{n - 2}\\) has the solution \\[ p_n = c_1 \\left(\\frac13\\right)^n + c_23^n. \\] If \\(p_0 = 1, p_1 = \\frac13\\) , then the solution is \\[ p_n = \\left(\\frac13\\right)^n. \\] Suppose we have five-digit rounding representation, then \\(\\hat p_0 = 1.0000\\) , \\(\\hat p_1 = 0.33333\\) , and the solution is \\[ \\hat p_n = 1.0000 \\left(\\frac13\\right)^n - 0.12500 \\times 10^{-5} \\cdot 3^n. \\] Then \\[ p_n - \\hat p_n = 0.12500 \\times 10^{-5} \\cdot 3^n \\] grow exponentially with \\(n\\) . On the other hand, the recursive equation \\(p_n = 2p_{n - 1} - p_{n - 2}\\) has the solution \\[ p_n = c_1 + c_2n \\] If \\(p_0 = 1, p_1 = \\frac13\\) , then the solution is \\[ p_n = 1 - \\frac23 n. \\] Suppose we have five-digit rounding representation, then \\(\\hat p_0 = 1.0000\\) , \\(\\hat p_1 = 0.33333\\) , and the solution is \\[ \\hat p_n = 1.0000 - 0.66667 n. \\] Then \\[ p_n - \\hat p_n = \\left(0.66667 - \\frac23\\right) n \\] grow linearly with \\(n\\) . Error of floating-point number (IEEE 754 standard) Range of normal representation \u00b6 Single-Precision Smallest \\(\\text{0/1\\ 00000001\\ 00\\dots00}\\) \\(\\pm 1.0 \\times 2^{-126} \\approx \\pm 1.2 \\times 10^{-38}\\) Largest \\(\\text{0/1\\ 11111110\\ 11\\dots11}\\) \\(\\pm 2.0 \\times 2^{127} \\approx \\pm 3.4 \\times 10^{38}\\) For Double-Precision, smallest \\(\\pm 1.0 \\times 2^{-1022} \\approx \\pm 2.2 \\times 10^{-308}\\) , largest \\(\\pm 2.0 \\times 2^{1023} \\approx \\pm 1.8 \\times 10^{308}\\) Relative precision \u00b6 Single \\(2^{-23}\\) Equivalent to \\(23 \\times \\log_{10}2 \\approx 6\\) decimal digits of precision\uff086 \u4f4d\u6709\u6548\u6570\u5b57\uff09 Double \\(2^{-52}\\) Equivalent to \\(52 \\times \\log_{10}2 \\approx 16\\) decimal digits of precision\uff0816 \u4f4d\u6709\u6548\u6570\u5b57\uff09","title":"Chap 1"},{"location":"Mathematics_Basis/NA/Chap_1/#chapter-1-mathematical-preliminaries","text":"","title":"Chapter 1 | Mathematical Preliminaries"},{"location":"Mathematics_Basis/NA/Chap_1/#error","text":"","title":"Error"},{"location":"Mathematics_Basis/NA/Chap_1/#truncation-error","text":"the error involved using a truncated or finite summation.","title":"Truncation Error"},{"location":"Mathematics_Basis/NA/Chap_1/#roundoff-error","text":"the error produced when performing real number calculations. It occurs because the arithmetic performed in a machine involved numbers with a finite number of digits. Chopping & Rounding Given a real number \\(y = 0.d_1d_2\\dots d_kd_{k+1}\\dots \\times 10^N\\) , the floating-point representation of \\(y\\) is \\(fl(y)\\) : \\[ fl(y) = \\left\\{ \\begin{align} & 0.d_1d_2\\dots d_k \\times 10^N, & {\\tt Chopping} \\\\ & {\\tt chop}\\left(y + 5 \\times 10^{n - (k + 1)}\\right). & {\\tt Rounding} \\end{align} \\right. \\] Definition If \\(p^*\\) is an approximation to \\(p\\) , then absolute error is \\(|p - p*|\\) and relative error is \\(\\frac{|p - p^*|}{|p|}\\) . The number \\(p^*\\) is said to be approximate to \\(p\\) to \\(t\\) * significant digits if \\(t\\) is the largest nonnegative integer for which \\[ \\frac{|p - p^*|}{|p|} < 5 \\times 10^{-t} \\] Example For the floating-point representation of \\(y\\) , the relative error is \\[ \\left|\\frac{y - fl(y)}{y}\\right|. \\] for chopping representation, \\[ \\left|\\frac{y - fl(y)}{y}\\right| = \\left|\\frac{0.d_{k + 1}d_{k + 2}\\dots}{0.d_1d_2\\dots}\\right| \\times 10^{-k} \\le \\frac{1}{0.1} \\times 10^{-k} = 10^{-k + 1}. \\] for rounding representation, \\[ \\left|\\frac{y - fl(y)}{y}\\right| \\le \\frac{0.5}{0.1} \\times 10^{-k} = 0.5 \\times 10^{-k + 1}. \\]","title":"Roundoff Error"},{"location":"Mathematics_Basis/NA/Chap_1/#effect-of-error","text":"Subtraction may reduce significant digits. e.g. 0.1234 - 0.1233 = 0.001 Division by small number of multiplication by large number magnify the abosolute error without modifying the relative error.","title":"Effect of Error"},{"location":"Mathematics_Basis/NA/Chap_1/#some-solutions-to-reduce-error","text":"Quadratic Formula the roots of \\(ax^2 + bx + c = 0\\) is \\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}. \\] Sometimes \\(b\\) is closer to \\(\\sqrt{b^2 - 4ac}\\) , which may cause the subtraction to reduce significant digits. An alternate way is to modify the formula to \\[ x = \\frac{-2c}{b \\pm \\sqrt{b^2 - 4ac}}. \\] But it may cause the division by small number. So it's a tradeoff to use one of the two formulae above. Horner's Method \u79e6\u4e5d\u97f6\u7b97\u6cd5 \\[ \\begin{align} f(x) &= a_nx^n + a_{n-1}x^{n-1} + \\dots + a_1x + a_0 \\\\ &= (\\dots((a_nx+a_{n-1})x+a_{n-2})x+\\dots+a_1)x+a_0 \\end{align} \\]","title":"Some Solutions to Reduce Error"},{"location":"Mathematics_Basis/NA/Chap_1/#stable-algorithms-and-convergence","text":"Definition An algorithm that satisfies that small changes in the initial data produce correspondingly small changes in the final results is called stable ; otherwise it is unstable . An algorithm is called conditionally stable if it is stable only for certain choices of initial data. Suppose \\(E_0 > 0\\) denotes an initial errors, \\(E_n\\) denotes the magnitude of an error after \\(n\\) subsequent operations Linear growth of errors \\(E_n \\approx C n E_0\\) unavoidable and acceptable Exponential growth of errors \\(E_n \\approx C^n E_0\\) unacceptable Example the recursive equation \\(p_n = \\frac{10}{3}p_{n - 1} - p_{n - 2}\\) has the solution \\[ p_n = c_1 \\left(\\frac13\\right)^n + c_23^n. \\] If \\(p_0 = 1, p_1 = \\frac13\\) , then the solution is \\[ p_n = \\left(\\frac13\\right)^n. \\] Suppose we have five-digit rounding representation, then \\(\\hat p_0 = 1.0000\\) , \\(\\hat p_1 = 0.33333\\) , and the solution is \\[ \\hat p_n = 1.0000 \\left(\\frac13\\right)^n - 0.12500 \\times 10^{-5} \\cdot 3^n. \\] Then \\[ p_n - \\hat p_n = 0.12500 \\times 10^{-5} \\cdot 3^n \\] grow exponentially with \\(n\\) . On the other hand, the recursive equation \\(p_n = 2p_{n - 1} - p_{n - 2}\\) has the solution \\[ p_n = c_1 + c_2n \\] If \\(p_0 = 1, p_1 = \\frac13\\) , then the solution is \\[ p_n = 1 - \\frac23 n. \\] Suppose we have five-digit rounding representation, then \\(\\hat p_0 = 1.0000\\) , \\(\\hat p_1 = 0.33333\\) , and the solution is \\[ \\hat p_n = 1.0000 - 0.66667 n. \\] Then \\[ p_n - \\hat p_n = \\left(0.66667 - \\frac23\\right) n \\] grow linearly with \\(n\\) . Error of floating-point number (IEEE 754 standard)","title":"Stable Algorithms and Convergence"},{"location":"Mathematics_Basis/NA/Chap_1/#range-of-normal-representation","text":"Single-Precision Smallest \\(\\text{0/1\\ 00000001\\ 00\\dots00}\\) \\(\\pm 1.0 \\times 2^{-126} \\approx \\pm 1.2 \\times 10^{-38}\\) Largest \\(\\text{0/1\\ 11111110\\ 11\\dots11}\\) \\(\\pm 2.0 \\times 2^{127} \\approx \\pm 3.4 \\times 10^{38}\\) For Double-Precision, smallest \\(\\pm 1.0 \\times 2^{-1022} \\approx \\pm 2.2 \\times 10^{-308}\\) , largest \\(\\pm 2.0 \\times 2^{1023} \\approx \\pm 1.8 \\times 10^{308}\\)","title":"Range of normal representation"},{"location":"Mathematics_Basis/NA/Chap_1/#relative-precision","text":"Single \\(2^{-23}\\) Equivalent to \\(23 \\times \\log_{10}2 \\approx 6\\) decimal digits of precision\uff086 \u4f4d\u6709\u6548\u6570\u5b57\uff09 Double \\(2^{-52}\\) Equivalent to \\(52 \\times \\log_{10}2 \\approx 16\\) decimal digits of precision\uff0816 \u4f4d\u6709\u6548\u6570\u5b57\uff09","title":"Relative precision"},{"location":"Mathematics_Basis/NA/Chap_2/","text":"Chapter 2 | Solution of Equations in One Variable \u00b6 Bisection Method \u00b6 Theorem Suppose that \\(f \\in C[a, b]\\) and \\(f(a)\\cdot f(b) \\lt 0\\) . The Bisection method generates a sequence \\(\\{p_n\\}(n = 0, 1, 2,\\dots)\\) approximating a zero \\(p\\) of \\(f\\) with \\[ |p_n-p| \\le \\frac{b - a}{2^n}, \\text{when }n \\ge 1 \\] Key Points for Algorithm Implementation \u00b6 \\(mid = a + (b - a)\\ /\\ 2, \\text{but not } mid = (a + b)\\ /\\ 2\\) , for accuracy and not exceeding the limit of range. \\(sign(FA)\\cdot sign(FM)\\gt 0, \\text{but not }FA\\cdot FM \\gt 0\\) , for saving time. Pros & Cons \u00b6 Simple premise, only requires a continuous \\(f\\) . Always converges to a solution. Slow to converge, and a good intermediate approximation can be inadvertently discarded. Cannot find multiple roots and complex roots. Fixed-Point Iteration \u00b6 \\[ f(x) = 0 \\leftrightarrow x = g(x) \\] Fixed-Point Theorem Let \\(g \\in C[a,b]\\) be such that \\(g(x)\\in[a,b], \\forall x \\in [a, b]\\) . Suppose \\(g'\\) exists on \\((a,b)\\) and that a constant \\(k\\ (0\\lt k \\lt 1)\\) exists, s.t. \\[ \\forall\\ x \\in (a,b), |g'(x)| \\le k. \\] Then \\(\\forall\\ p_0 \\in[a,b]\\) , the sequence \\(\\{p_n\\}\\) defined by \\[ p_n=g(p_{n-1}) \\] converges to the unique fixed-point \\(p\\in [a,b]\\) . . Corollary \\[ \\begin{align} |p_n - p| &\\le k^n \\max\\{p_0 = a, b - p_0\\} \\\\ |p_n - p| &\\le \\frac{k^n}{1 - k}|p_1 - p_0| \\end{align} \\] \\(\\Rightarrow\\) The smaller the \\(k\\) is, the faster it converges. Newton's Method (Newton-Raphson Method) \u00b6 Newton's method is an improvement of common fixed-point iteration method above. Theorem Let \\(f\\in C^2[a,b]\\) and \\(\\exists\\ p\\in[a,b], s.t. f(p) = 0\\) and \\(f'(p) \\ne 0\\) , then \\(\\exists\\ \\delta \\gt 0, \\forall\\ p_0 \\in [p - \\epsilon, p + \\epsilon]\\) , s.t. the sequence \\(\\{p_n\\}_{n = 1}^\\infty\\) defined by \\[ p_n = p_{n-1} - \\frac{f(p_{n-1})}{f'(p_{n-1})} \\] converges to \\(p\\) . Error Analysis for Iterative Methods \u00b6 Definition Suppose \\(\\{p_n\\}\\) is a sequence that converges to \\(p\\) , and \\(\\forall n, p_n \\ne p\\) . If positive constants \\(\\alpha\\) and \\(\\lambda\\) exist with \\[ \\lim_{n \\rightarrow \\infty} \\frac{|p_{n + 1} - p|}{|p_n - p|^\\alpha} = \\lambda \\] then \\(\\{p_n\\}\\) conveges to \\(p\\) of order \\(\\alpha\\) , with asymptotic error constant \\(\\lambda\\) . Specially, If \\(\\alpha = 1\\) , the sequence is linearly convergent. If \\(\\alpha = 2\\) , the sequence is quadratically convergent. Theorem The common fixed-point iteration method ( \\(g'(p) \\ne 0\\) ) with the premise in Fixed-Point Theorem is linearly convergent. Proof \\[ \\lim_{n\\rightarrow \\infty}\\frac{|p_{n+1} - p|}{|p_n - p|} = \\lim_{n\\rightarrow \\infty}\\frac{g'(\\xi)|p_n - p|}{|p_n - p|} = |g'(p)| \\] Theorem The Newton's method \\((g'(p)=0)\\) is at least quadratically convergent. Proof \\[ \\begin{align} & 0 = f(p) = f(p_n) + f'(p_n)(p - p_n) + \\frac{f''(\\xi _n)}{2!}(p - p_n)^2 \\\\ & \\Rightarrow p = \\underbrace{p_n - \\frac{f(p_n)}{f\"(p_n)}}_{p_{n+1}} - \\frac{f''(\\xi _n)}{2!f'(p_n)}(p - p_n)^2 \\\\ & \\Rightarrow \\frac{|p_{n+1} - p|}{|p_n - p|^2} = \\frac{f''(\\xi _n)}{2f'(p_n)} \\\\ & \\lim_{n\\rightarrow \\infty}\\frac{|p_{n+1} - p|}{|p_n - p|^2} = \\frac{f''(p_n)}{2f'(p_n)} \\end{align} \\] More commonly, Theorem Let \\(p\\) be a fixed point of \\(g(x)\\) . If there exists some constant \\(\\alpha\\ge 2\\) such that \\(g\\in C^\\alpha [p-\\delta, p+\\delta],g'(p)=\\dots=g^{(\\alpha-1)}(p)=0, g^{(\\alpha)}(p)=0\\) . Then the iterations with \\(p_n = g(p_{n-1}),n\\ge 1\\) , is of order \\(\\alpha\\) . Multiple Roots Situation \u00b6 Notice that from the proof above, Newton's method is quadratically convergent if \\(f'(p_n) \\ne 0\\) . If \\(f'(p) = 0\\) , then the equation has multiple roots at \\(p\\) . If \\(f(x) = (x - p)^mq(x)\\) and \\(q(p)\\ne 0\\) , for Newton's Method, \\[ g'(p) = \\frac{f(p)f''(p)}{f'(p)^2} = \\left.\\frac{f(x)f''(x)}{f'(x)^2}\\right|_{x=p} = \\dots =1 - \\frac{1}{m} \\lt 1 \\] It is still convegent, but not quadratically . A way to speed it up Let \\[ \\mu (x) = \\frac{f(x)}{f'(x)}, \\] then \\(\\mu (x)\\) has the same root as \\(f(x)\\) but no multiple roots anymore. And \\[ g(x) = x - \\frac{\\mu (x)}{\\mu'(x)} = x - \\frac{f(x)f'(x)}{f'(x)^2 - f(x)f''(x)} \\] Newton's method can be used there again. Pros & Cons \u00b6 Quadratic convegence Requires additional calculation of \\(f''(x)\\) The denominator consists of the difference of the two numbers both close to 0. Accelerating Convergence \u00b6 Aitken's \\(\\Delta^2\\) Method \u00b6 Definition Forward Difference \\(\\Delta p_n = p_{n+1} - p_n\\) . Similarly \\(\\Delta^kp_n = \\Delta(\\Delta^{k-1}p_n)\\) Representing Aitken's \\(\\Delta^2\\) Method by forward difference, we have \\[ \\hat{p_n} = p_n - \\frac{(\\Delta p_n)^2}{\\Delta^2p_n}. \\] Theorem Suppose that \\(\\{p_n\\}\\) is a sequence, \\(\\lim_{n\\rightarrow \\infty}p_n = p\\) , \\(\\exists N\\) , s.t. \\(\\forall n > N, (p_n - p)(p_{n+1} -p) \\gt 0\\) . Then the sequence \\(\\{\\hat{p_n}\\}\\) converges to \\(p\\) faster than \\(\\{p_n\\}\\) in the sense that \\[ \\lim_{n\\rightarrow \\infty}\\frac{\\hat{p_n} - p}{p_n - p} = 0. \\] The algorithm to implement it is called Steffensen\u2019s Acceleration .","title":"Chap 2"},{"location":"Mathematics_Basis/NA/Chap_2/#chapter-2-solution-of-equations-in-one-variable","text":"","title":"Chapter 2 | Solution of Equations in One Variable"},{"location":"Mathematics_Basis/NA/Chap_2/#bisection-method","text":"Theorem Suppose that \\(f \\in C[a, b]\\) and \\(f(a)\\cdot f(b) \\lt 0\\) . The Bisection method generates a sequence \\(\\{p_n\\}(n = 0, 1, 2,\\dots)\\) approximating a zero \\(p\\) of \\(f\\) with \\[ |p_n-p| \\le \\frac{b - a}{2^n}, \\text{when }n \\ge 1 \\]","title":"Bisection Method"},{"location":"Mathematics_Basis/NA/Chap_2/#key-points-for-algorithm-implementation","text":"\\(mid = a + (b - a)\\ /\\ 2, \\text{but not } mid = (a + b)\\ /\\ 2\\) , for accuracy and not exceeding the limit of range. \\(sign(FA)\\cdot sign(FM)\\gt 0, \\text{but not }FA\\cdot FM \\gt 0\\) , for saving time.","title":"Key Points for Algorithm Implementation"},{"location":"Mathematics_Basis/NA/Chap_2/#pros-cons","text":"Simple premise, only requires a continuous \\(f\\) . Always converges to a solution. Slow to converge, and a good intermediate approximation can be inadvertently discarded. Cannot find multiple roots and complex roots.","title":"Pros &amp; Cons"},{"location":"Mathematics_Basis/NA/Chap_2/#fixed-point-iteration","text":"\\[ f(x) = 0 \\leftrightarrow x = g(x) \\] Fixed-Point Theorem Let \\(g \\in C[a,b]\\) be such that \\(g(x)\\in[a,b], \\forall x \\in [a, b]\\) . Suppose \\(g'\\) exists on \\((a,b)\\) and that a constant \\(k\\ (0\\lt k \\lt 1)\\) exists, s.t. \\[ \\forall\\ x \\in (a,b), |g'(x)| \\le k. \\] Then \\(\\forall\\ p_0 \\in[a,b]\\) , the sequence \\(\\{p_n\\}\\) defined by \\[ p_n=g(p_{n-1}) \\] converges to the unique fixed-point \\(p\\in [a,b]\\) . . Corollary \\[ \\begin{align} |p_n - p| &\\le k^n \\max\\{p_0 = a, b - p_0\\} \\\\ |p_n - p| &\\le \\frac{k^n}{1 - k}|p_1 - p_0| \\end{align} \\] \\(\\Rightarrow\\) The smaller the \\(k\\) is, the faster it converges.","title":"Fixed-Point Iteration"},{"location":"Mathematics_Basis/NA/Chap_2/#newtons-method-newton-raphson-method","text":"Newton's method is an improvement of common fixed-point iteration method above. Theorem Let \\(f\\in C^2[a,b]\\) and \\(\\exists\\ p\\in[a,b], s.t. f(p) = 0\\) and \\(f'(p) \\ne 0\\) , then \\(\\exists\\ \\delta \\gt 0, \\forall\\ p_0 \\in [p - \\epsilon, p + \\epsilon]\\) , s.t. the sequence \\(\\{p_n\\}_{n = 1}^\\infty\\) defined by \\[ p_n = p_{n-1} - \\frac{f(p_{n-1})}{f'(p_{n-1})} \\] converges to \\(p\\) .","title":"Newton's Method (Newton-Raphson Method)"},{"location":"Mathematics_Basis/NA/Chap_2/#error-analysis-for-iterative-methods","text":"Definition Suppose \\(\\{p_n\\}\\) is a sequence that converges to \\(p\\) , and \\(\\forall n, p_n \\ne p\\) . If positive constants \\(\\alpha\\) and \\(\\lambda\\) exist with \\[ \\lim_{n \\rightarrow \\infty} \\frac{|p_{n + 1} - p|}{|p_n - p|^\\alpha} = \\lambda \\] then \\(\\{p_n\\}\\) conveges to \\(p\\) of order \\(\\alpha\\) , with asymptotic error constant \\(\\lambda\\) . Specially, If \\(\\alpha = 1\\) , the sequence is linearly convergent. If \\(\\alpha = 2\\) , the sequence is quadratically convergent. Theorem The common fixed-point iteration method ( \\(g'(p) \\ne 0\\) ) with the premise in Fixed-Point Theorem is linearly convergent. Proof \\[ \\lim_{n\\rightarrow \\infty}\\frac{|p_{n+1} - p|}{|p_n - p|} = \\lim_{n\\rightarrow \\infty}\\frac{g'(\\xi)|p_n - p|}{|p_n - p|} = |g'(p)| \\] Theorem The Newton's method \\((g'(p)=0)\\) is at least quadratically convergent. Proof \\[ \\begin{align} & 0 = f(p) = f(p_n) + f'(p_n)(p - p_n) + \\frac{f''(\\xi _n)}{2!}(p - p_n)^2 \\\\ & \\Rightarrow p = \\underbrace{p_n - \\frac{f(p_n)}{f\"(p_n)}}_{p_{n+1}} - \\frac{f''(\\xi _n)}{2!f'(p_n)}(p - p_n)^2 \\\\ & \\Rightarrow \\frac{|p_{n+1} - p|}{|p_n - p|^2} = \\frac{f''(\\xi _n)}{2f'(p_n)} \\\\ & \\lim_{n\\rightarrow \\infty}\\frac{|p_{n+1} - p|}{|p_n - p|^2} = \\frac{f''(p_n)}{2f'(p_n)} \\end{align} \\] More commonly, Theorem Let \\(p\\) be a fixed point of \\(g(x)\\) . If there exists some constant \\(\\alpha\\ge 2\\) such that \\(g\\in C^\\alpha [p-\\delta, p+\\delta],g'(p)=\\dots=g^{(\\alpha-1)}(p)=0, g^{(\\alpha)}(p)=0\\) . Then the iterations with \\(p_n = g(p_{n-1}),n\\ge 1\\) , is of order \\(\\alpha\\) .","title":"Error Analysis for Iterative Methods"},{"location":"Mathematics_Basis/NA/Chap_2/#multiple-roots-situation","text":"Notice that from the proof above, Newton's method is quadratically convergent if \\(f'(p_n) \\ne 0\\) . If \\(f'(p) = 0\\) , then the equation has multiple roots at \\(p\\) . If \\(f(x) = (x - p)^mq(x)\\) and \\(q(p)\\ne 0\\) , for Newton's Method, \\[ g'(p) = \\frac{f(p)f''(p)}{f'(p)^2} = \\left.\\frac{f(x)f''(x)}{f'(x)^2}\\right|_{x=p} = \\dots =1 - \\frac{1}{m} \\lt 1 \\] It is still convegent, but not quadratically . A way to speed it up Let \\[ \\mu (x) = \\frac{f(x)}{f'(x)}, \\] then \\(\\mu (x)\\) has the same root as \\(f(x)\\) but no multiple roots anymore. And \\[ g(x) = x - \\frac{\\mu (x)}{\\mu'(x)} = x - \\frac{f(x)f'(x)}{f'(x)^2 - f(x)f''(x)} \\] Newton's method can be used there again.","title":"Multiple Roots Situation"},{"location":"Mathematics_Basis/NA/Chap_2/#pros-cons_1","text":"Quadratic convegence Requires additional calculation of \\(f''(x)\\) The denominator consists of the difference of the two numbers both close to 0.","title":"Pros &amp; Cons"},{"location":"Mathematics_Basis/NA/Chap_2/#accelerating-convergence","text":"","title":"Accelerating Convergence"},{"location":"Mathematics_Basis/NA/Chap_2/#aitkens-delta2-method","text":"Definition Forward Difference \\(\\Delta p_n = p_{n+1} - p_n\\) . Similarly \\(\\Delta^kp_n = \\Delta(\\Delta^{k-1}p_n)\\) Representing Aitken's \\(\\Delta^2\\) Method by forward difference, we have \\[ \\hat{p_n} = p_n - \\frac{(\\Delta p_n)^2}{\\Delta^2p_n}. \\] Theorem Suppose that \\(\\{p_n\\}\\) is a sequence, \\(\\lim_{n\\rightarrow \\infty}p_n = p\\) , \\(\\exists N\\) , s.t. \\(\\forall n > N, (p_n - p)(p_{n+1} -p) \\gt 0\\) . Then the sequence \\(\\{\\hat{p_n}\\}\\) converges to \\(p\\) faster than \\(\\{p_n\\}\\) in the sense that \\[ \\lim_{n\\rightarrow \\infty}\\frac{\\hat{p_n} - p}{p_n - p} = 0. \\] The algorithm to implement it is called Steffensen\u2019s Acceleration .","title":"Aitken's \\(\\Delta^2\\) Method"},{"location":"Mathematics_Basis/NA/Chap_3/","text":"Chapter 3 | Interpolation and Polynomial Approximation \u00b6 Lagrange Interpolation \u00b6 Suppose we have function \\(y = f(x)\\) with the given points \\((x_0, y_0), (x_1, y_1), \\dots, (x_n, y_n)\\) , and then construct a relatively simple approximating function \\(g(x) \\approx f(x)\\) . Theorem 3.1 If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers and \\(f\\) is a function with given values \\(f(x_0), \\dots, f(x_n)\\) , then a unique polynomial \\(P(x)\\) of degree at most \\(n\\) exists with \\[ P(x_k) = f(x_k), \\text{ for each } k = 0, 1, \\dots, n. \\] and \\[ P(x) = \\sum\\limits_{k = 0}^n f(x_k)L_{n, k}(x), \\] where, for each \\(k = 0, 1, \\dots, n\\) , \\[ L_{n, k}(x) = \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n \\frac{(x - x_i)}{(x_k - x_i)}. \\] \\(L_{n, k}(x)\\) is called the n th Lagrange interpolating polynomial . Proof First we prove for the structure of function \\(L_{n, k}(x)\\) . From the definition of \\(P(x)\\) , \\(L_{n, k}(x)\\) has the following properties \\(L_{n, k}(x_i) = 0\\) when \\(i \\ne k\\) . \\(L_{n, k}(x_k) = 1\\) . To satisfy the first property, the numerator of \\(L_{n, k}(x)\\) contains the term \\[ (x - x_0)(x - x_1) \\cdots (x - x_{k - 1})(x - x_{k + 1}) \\cdots (x - x_n) = \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n (x - x_i). \\] To satisfy the second property, the denominator of \\(L_{n, k}(x)\\) must be equal to the numerator at \\(x = x_k\\) , thus \\[ \\begin{align} L_{n, k}(x) &= \\frac{(x - x_0)(x - x_1) \\cdots (x - x_{k - 1})(x - x_{k + 1}) \\cdots (x - x_n)}{(x_k - x_0)(x - x_1) \\cdots (x_k - x_{k - 1})(x_k - x_{k + 1}) \\cdots (x_k - x_n)} \\\\ &= \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n \\frac{x - x_i}{x_k - x_i}. \\end{align} \\] For uniqueness, we prove by contradition. If not, suppose \\(P(x)\\) and \\(Q(x)\\) both satisfying the conditions, then \\(D(x) = P(x) - Q(x)\\) is a polynomial of degree \\(\\text{deg}(D(x)) \\le n\\) , but \\(D(x)\\) has \\(n + 1\\) distinct roots \\(x_0, x_1, \\dots, x_n\\) , which leads to a contradiction. Theorem 3.2 If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers in the interval \\([a, b]\\) and \\(f \\in C^{n + 1}[a, b]\\) . \\(\\forall x \\in [a, b], \\exists \\xi \\in [a, b], s.t.\\) \\[ f(x) = P(x) + \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] where \\(P(x)\\) is the Lagrange interpolating polynomial. And \\[ R(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] is the truncation error . Proof Since \\(R(x) = f(x) - P(x)\\) has at least \\(n + 1\\) roots, thus \\[ R(x) = K(x)\\prod\\limits_{i = 0}^n (x - x_i). \\] For a fixed \\(x \\ne x_k\\) , define \\(g(t)\\) in \\([a, b]\\) by \\[ g(t) = R(t) - K(x)\\prod\\limits_{i = 0}^n (t - x_i). \\] Since \\(g(t)\\) has \\(n + 2\\) distinct roots \\(x, x_0, \\dots, x_n\\) , by Generalized Rolle's Theorem, \\[ \\exists\\ \\xi \\in [a, b],\\ s.t.\\ g^{(n + 1)}(\\xi) = 0. \\] Namely, \\[ f^{(n + 1)}(\\xi) - \\underbrace{P^{n + 1}(\\xi)}_{0} - K(x)(n + 1)! = 0. \\] Thus \\[ K(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!},\\ R(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] Example Suppose a table is to be prepared for the function \\(f(x) = e^x\\) for \\(x\\) in \\([0, 1]\\) . Assume that each entry of the table is accurate up to 8 decimal places and the step size is \\(h\\) . What should \\(h\\) be for linear interpolation to give an absolute error of at most \\(10^{-6}\\) ? Solution. \\[ \\begin{align} |f(x) - P(x)| &= \\left|\\frac{f^{(2)}(\\xi)}{2!}(x - x_j)(x - x_{j + 1})\\right| \\\\ &= \\left|\\frac{e^\\xi}{2}(x - kh)(x - (k + 1)h)\\right| \\le \\frac{e}{2} \\cdot \\frac{h^2}{4} = \\frac{eh^2}{8}. \\end{align} \\] Thus let \\(\\frac{eh^2}{8} \\le 10^{-6}\\) , we have \\(h \\le 1.72 \\times 10^{-3}\\) . To make \\(N = (1 - 0) / h\\) an integer, we can simply choose \\(h = 0.001\\) . Extrapolation Suppose \\(a = \\min\\limits_i \\{x_i\\}\\) , \\(b = \\max\\limits_i \\{x_i\\}\\) . Interpolation estimates value \\(P(x)\\) , \\(x \\in [a, b]\\) , while Extrapolation estimates value \\(P(x)\\) , \\(x \\notin [a, b]\\) . In genernal, interpolation is better than extrapolation. Neville's Method \u00b6 Motivation: When we have more interpolating points, the original Lagrange interpolating method should re-calculate all \\(L_{n, k}\\) , which is not efficient. Definition Let \\(f\\) be a function defined at \\(x_0, x_1, \\dots, x_n\\) , and suppose that \\(m_1, \\dots, m_k\\) are \\(k\\) distinct integers with \\(0 \\le m_i \\le n\\) for each \\(i\\) . The Lagrange polynomial that agrees with \\(f(x)\\) at the \\(k\\) points denoted by \\(P_{m_1, m_2, \\dots, m_k}(x)\\) . Thus \\(P(x) = P_{0, 1, \\dots, n}(x)\\) , where \\(P(x)\\) is the n th Lagrange polynomials that interpolate \\(f\\) at \\(k + 1\\) points \\(x_0, \\dots, x_k\\) . Theorem 3.3 Let \\(f\\) be a function defined at \\(x_0, x_1, \\dots, x_n\\) , and \\(x_i \\ne x_j\\) , then \\[ P(x) = \\frac{(x - x_j)P_{0, 1, \\dots, j - 1, j + 1, \\dots k}(x) - (x - x_i)P_{0, 1, \\dots, i - 1, i + 1, \\dots k}(x)}{(x_i - x_j)}. \\] Denote that \\[ Q_{i, j} = P_{i - j, i - j + 1, \\dots, i - 1, i}, \\] then from the theorem above, the interpolating polynomials can be generated recursively . Newton Interpolation \u00b6 Differing from Langrange polynomials, we try to represent \\(P(x)\\) by the following form: \\[ \\begin{align} N(x) = &\\ a_0 + a_1(x - x_0) + a_2(x - x_0)(x - x_1) + \\cdots \\\\ & + a_n(x - x_0)(x - x_1) \\cdots (x - x_{n - 1}). \\end{align} \\] Definition \\(f[x_i] = f(x_i)\\) is the zeroth divided difference w.r.t. \\(x_i\\) . The k th divided difference w.r.t. \\(x_i, x_{i + 1}, x_{i + k}\\) is defined recursively by \\[ f[x_i, x_{i + 1}, \\dots, x_{i + k - 1}, x_{i + k}] = \\frac{f[x_{i + 1}, x_{i + 2}, \\dots, x_{i + k}] - f[x_i, x_{i + 1}, \\dots, x_{i + k - 1}]}{x_{i + k} - x_i} \\] Then we derive the Newton's interpolatory divided-difference formula : \\[ N(x) = f[x_0] + \\sum\\limits_{k = 1}^n f[x_0, x_1, \\dots, x_k]\\prod\\limits_{i = 0}^{k - 1}(x - x_i). \\] And the divided difference can be generated as below, which is similar to Neville's Method. Theorem 3.4 If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers in the interval \\([a, b]\\) and \\(f \\in C^{n + 1}[a, b]\\) . \\(\\forall x \\in [a, b], \\exists \\xi \\in [a, b], s.t.\\) \\[ f(x) = N(x) + f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i). \\] where \\(N(x)\\) is the Newton's interpolatory divided-difference formula. And \\[ R(x) = f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i). \\] is the truncation error . Proof By definition of divided difference, we have \\[ \\left\\{ \\begin{align} f(x) &= f[x_0] + (x - x_0)f[x, x_0], & \\textbf{Eq.1} \\\\ f[x, x_0] &= f[x_0, x_1] + (x - x_1)f[x, x_0, x_1], & \\textbf{Eq.2} \\\\ & \\vdots \\\\ f[x, x_0, \\dots, x_{n - 1}] &= f[x_0, \\dots, x_n] + (x - x_n)f[x, x_0, \\dots, x_n]. & \\textbf{Eq.n - 1} \\end{align} \\right. \\] then compute \\[ \\textbf{Eq.1} + (x - x_0) \\times \\textbf{Eq.2} + \\cdots + (x - x_0) \\cdots (x - x_{n - 1}) \\times \\textbf{Eq.n - 1}. \\] i.e. \\[ f(x) = N(x) + \\underbrace{f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i)}_{R(x)}. \\] Note Since the uniqueness of n -th interpolating polynomial, \\(N(x) \\equiv P(x)\\) . They have the same truncation error, which is \\[ f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] Theorem 3.5 Suppose that \\(f \\in C^n[a, b]\\) and \\(x_0, x_1, \\dots, x_n\\) are distinct numbers in \\([a, b]\\) . Then \\(\\exists\\ \\xi \\in (a, b)\\) , s.t. \\[ f[x_0, x_1, \\dots, x_n] = \\frac{f^{(n)}(\\xi)}{n!}. \\] Special Case: Equal Spacing \u00b6 Definition Forward Difference \\[ \\begin{align} \\Delta f_i &= f_{i + 1} - f_i \\\\ \\Delta^k f_i &= \\Delta\\left(\\Delta^{k - 1} f_i \\right) = \\Delta^{k - 1}f_{i + 1} - \\Delta^{k - 1}f_i. \\end{align} \\] Backward Difference \\[ \\begin{align} \\nabla f_i &= f_i - f_{i - 1} \\\\ \\nabla^k f_i &= \\nabla\\left(\\nabla^{k - 1} f_i \\right) = \\nabla^{k - 1}f_i - \\nabla^{k - 1}f_{i - 1}. \\end{align} \\] Property Linearity: \\(\\Delta (af(x) + bg(x)) = a \\Delta f + b \\Delta g\\) . If \\(\\text{deg}(f(x)) = m\\) , then \\[ \\text{deg}\\left(\\Delta^kf(x)\\right) = \\left\\{ \\begin{align} & m - k, & 0 \\le k \\le m \\\\ & 0, & k > m \\end{align} \\right. \\] Decompose the recursive definition, \\[ \\begin{align} \\Delta^n f_k &= \\sum\\limits_{j = 0}^n (-1)^j \\binom{n}{j} f_{n + k - j} \\\\ \\nabla^n f_k &= \\sum\\limits_{j = 0}^n (-1)^{n - j} \\binom{n}{j} f_{j + k - n} \\end{align} \\] Vice versa, \\[ f_{n + k} = \\sum\\limits_{j = 0}^n \\binom{n}{j} \\Delta^j f_k \\] Suppose \\(x_0, x_1, \\dots x_n\\) are equally spaced, namely \\(x_i = x_0 + ih\\) . And let \\(x = x_0 + sh\\) , then \\(x - x_i = (s - i)h\\) . Thus \\[ \\begin{align} N(x) &= f[x_0] + \\sum\\limits_{k = 1}^n s(s - 1) \\cdots (s - k + 1) h^k f[x_0, x_1, \\dots, x_k] \\\\ &= f[x_0] + \\sum\\limits_{k = 1}^n \\binom{s}{k} k! h^k f[x_0, x_1, \\dots, x_k] \\end{align} \\] is called Newton forward divided-difference formula . From mathematical induction, we can derive that \\[ f[x_0, x_1, \\dots, x_k] = \\frac{1}{k!h^k}\\Delta^k f(x_0). \\] Thus we get the Newton Forward-Difference Formula . \\[ N(x) = f[x_0] + \\sum\\limits_{k = 1}^n \\binom{s}{k} \\Delta^k f(x_0). \\] Inversely, \\(x = x_n + sh\\) , then \\(x - x_i = (s + n - i)h\\) , Thus \\[ \\begin{align} N(x) &= f[x_n] + \\sum\\limits_{k = 1}^n s(s + 1) \\cdots (s + k - 1) h^k f[x_n, x_{n - 1}, \\dots, x_{n - k}] \\\\ &= f[x_n] + \\sum\\limits_{k = 1}^n (-1)^k \\binom{-s}{k} k! h^k f[x_n, x_{n - 1}, \\dots, x_{n - k}]. \\end{align} \\] is called Newton backward divided-difference formula . From mathematical induction, we can derive that \\[ f[x_n, x_{n - 1}, \\dots, x_0] = \\frac{1}{k!h^k}\\nabla^k f(x_n). \\] Thus we get the Newton Backward-Difference Formula . \\[ N(x) = f[x_0] + \\sum\\limits_{k = 1}^n (-1)^k \\binom{-s}{k} \\nabla^k f(x_n). \\] Hermit Interpolation \u00b6 Definition Let \\(x_0, x_1, \\dots, x_n\\) be \\(n + 1\\) distinct numbers in \\([a, b]\\) and \\(m_i \\in \\mathbb{N}\\) . Suppose that \\(f \\in C^m[a, b]\\) , where \\(m = \\max \\{m_i\\}\\) . The osculating polynomial approximating \\(f\\) is the polynomial \\(P(x)\\) of least degree such that \\[ \\frac{d^k P(x_i)}{dx^k} = \\frac{d^k f(x_i)}{dx^k}, \\text{ for each } i = 0, 1, \\dots, n \\text{ and } k = 0, 1, \\dots, m_i. \\] From definition above, we know that when \\(m_i = 0\\) for each \\(i\\) , it's the n -th Lagrange polynomial. And the cases that \\(m_i = 1\\) for each \\(i\\) , then it's Hermit Polynomials . Theorem 3.6 If \\(f \\in C^1[a, b]\\) and \\(x_0, \\dots, x_n \\in [a, b]\\) are distinct, the unique polynomial of least degree agreeing with \\(f\\) and \\(f'\\) at \\(x_0, \\dots, x_n\\) is the Hermit Polynomial of degree at most 2n + 1 defined by \\[ H_{2n + 1}(x) = \\sum\\limits_{j = 0}^n f(x_j) H_{n ,j}(x) + \\sum\\limits_{j = 0}^n f'(x_j) \\hat H_{n, j}(x), \\] where \\[ H_{n, j}(x) = [1 - 2(x - x_j)L'_{n, j}(x_j)]L^2_{n, j}(x) \\] and \\[ \\hat H_{n, j}(x) = (x - x_j)L^2_{n, j}(x). \\] Moreover, if \\(f \\in C^{2n + 2}[a, b]\\) , then \\(\\exists\\ \\xi \\in (a, b)\\) , s.t. \\[ f(x) = H_{2n + 1}(x) + \\underbrace{\\frac{1}{(2n + 2)!}\\prod\\limits_{i = 0}^n (x - x_i)^2 f^{2n + 2}(\\xi)}_{R(x)}. \\] The theorem above gives a complete description of Hermit interpolation. But in pratice, to compute \\(H_{2n + 1}(x)\\) through the formula above is tedious . To make it compute easier, we introduce a method that is similar to Newton's interpolation. Define the sequence \\(\\{z_k\\}_{0}^{2n + 1}\\) by \\[ z_{2i} = z_{2i + 1} = x_i. \\] Based on the Theorem 3.5 , we redefine that \\[ f[z_{2i}, z_{2i + 1}] = f'(z_{2i}) = f'(x_i). \\] Then Hermite polynomial can be represented by \\[ H_{2n + 1}(x) = f[z_0] + \\sum\\limits_{k = 1}^{2n + 1} f[z_0, \\dots, z_k]\\prod\\limits_{i = 0}^{k - 1}(x - z_i). \\] Cubic Spline Interpolation \u00b6 Motivation: For osculating polynomial approximation, we can let \\(m_i\\) be bigger to get high-degree polynomials. It can somehow be better but higher degree tends to causes a fluctuation or say overfitting . An alternative approach is to divide the interval into subintervals and approximate them respectively, which is called piecewise-polynomial approximation . The most common piecewise-polynomial approximation uses cubic polynomials called cubic spline approximation . Definition Given a function \\(f\\) defined on \\([a, b]\\) and a set of nodes \\(a = x_0 < x_1 < \\cdots < x_n = b\\) , a cubic spline interpolant \\(S\\) for \\(f\\) is a function that satisfies the following conditions: \\(S(x)\\) is a cubic polynomial, denoted \\(S_j(x)\\) , on the subinterval \\([x_j, x_j + 1]\\) , for each \\(j = 0, 1, \\dots, n - 1\\) ; \\(S(x_j) = f(x_j)\\) for each \\(j = 0, 1, \\dots, n\\) ; \\(S_{j + 1}(x_{j + 1}) = S_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\) ; \\(S'_{j + 1}(x_{j + 1}) = S'_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\) ; \\(S''_{j + 1}(x_{j + 1}) = S''_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\) ; One of the following sets of boundary conditions: \\(S''(x_0) = S''(x_n) = 0\\) ( free or natural boundary ); \\(S'(x_0) = f'(x_0)\\) and \\(S'(x_n) = f'(x_n)\\) ( clamped boundary ). The spline of natural boundary is called natural spline . Theorem 3.7 The cubic spline interpolation of either natural boundary or clamped boundary is unique . (Since the coefficient matrix \\(A\\) is strictly diagonally dominant.) Suppose interpolation function in each subinterval is \\[ S_j(x) = a_j + b_j(x - x_j) + c_j(x - x_j)^2 + d_j(x - x_j)^3. \\] From the conditions in the definition above, by some algebraic process, we can derive the solution with the following equations, \\[ \\begin{align} h_j &= x_{j + 1} - x_j; \\\\ a_j &= f(x_j); \\\\ b_j &= \\frac{1}{h_j}(a_{j + 1}) - \\frac{h_j}{3}(2c_j + c_{j + 1}); \\\\ d_j &= \\frac{1}{3h_j}{c_{j + 1} - c_j}. \\end{align} \\] While \\(c_j\\) is given by solving the following linear system, \\[ A\\mathbf{x} = \\mathbf{b}, \\] where \\[ \\small A = \\begin{bmatrix} 1 & 0 & 0 & \\cdots & \\cdots & 0 \\\\ h_0 & 2(h_0 + h_1) & h_1 & \\cdots & \\cdots & \\vdots \\\\ 0 & h_1 & 2(h_1 + h_2) & h_2 & \\ddots & \\vdots \\\\ \\vdots & \\ddots & \\ddots & \\ddots & \\ddots & 0 \\\\ \\vdots & \\ddots & \\ddots & h_{n - 2} & 2(h_{n - 2} + h_{n - 1}) & h_{n - 1} \\\\ 0 & \\cdots & \\cdots & 0 & 0 & 1 \\\\ \\end{bmatrix} , \\mathbf{x} = \\begin{bmatrix} c_0 \\\\ c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\] For natural boundary, \\[ \\small \\mathbf{b} = \\begin{bmatrix} 0 \\\\ \\frac{3}{h_1}(a_2 - a_1) - \\frac{3}{h_0}(a_1 - a_0) \\\\ \\vdots \\\\ \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) - \\frac{3}{h_{n - 2}}\\ (a_{n - 1} - a_{n - 2}) \\\\ 0 \\end{bmatrix} \\] For clamped boundary, \\[ \\small \\mathbf{b} = \\begin{bmatrix} \\frac{3}{h_0}(a_1 - a_0) - 3f'(a) \\\\ \\frac{3}{h_1}(a_2 - a_1) - \\frac{3}{h_0}(a_1 - a_0) \\\\ \\vdots \\\\ \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) - \\frac{3}{h_{n - 2}}\\ (a_{n - 1} - a_{n - 2}) \\\\ 3f'(b) - \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) \\\\ \\end{bmatrix} \\] Note If \\(f \\in C[a, b]\\) and \\(\\frac{\\max h_i}{\\min h_i} \\le C < \\infty\\) . Then \\(S(x)\\ \\overset{\\text{uniform}}{\\longrightarrow}\\ f(x)\\) when \\(\\max h_i \\rightarrow 0\\) . That is the accuracy of approximation can be improved by adding more nodes without increasing the degree of the splines. Curves \u00b6 We've discussed the interpolation of functions above, but we may encounter the case to interpolate a curve. Straightforward Technique \u00b6 For given points \\((x_0, y_0), (x_1, y_1), \\dots, (x_n, y_n)\\) , we can construct two approximation functions with \\[ x_i = x(t_i),\\ y_i = y(t_i). \\] The interpolation method can be Lagrange, Hermite and Cubic spline, whatever. Bezier Curve \u00b6 In nature, it's piecewise cubic Hermite polynomial , and the curve is call Bezier curve . Similarly, suppose two function \\(x(t)\\) and \\(y(t)\\) at each interval. We have the following condtions. \\[ x(0) = x_0,\\ x(1) = x_1,\\ x'(0) = \\alpha_0,\\ x'(1) = \\alpha_1. \\] \\[ y(0) = y_0,\\ y(1) = y_1,\\ y'(0) = \\beta_0,\\ y'(1) = \\beta_1. \\] The solution is \\[ \\begin{align} x(t) &= [2(x_0 - x_1) + (\\alpha_0 + \\alpha_1)]t^3 + [3(x_1 - x_0) - (\\alpha_1 + 2\\alpha_0)]t^2 + \\alpha_0 t + x_0. \\\\ y(t) &= [2(y_0 - y_1) + (\\beta_0 + \\beta_1)]t^3 + [3(y_1 - y_0) - (\\beta_1 + 2\\beta_0)]t^2 + \\alpha_0 t + y_0. \\end{align} \\]","title":"Chap 3"},{"location":"Mathematics_Basis/NA/Chap_3/#chapter-3-interpolation-and-polynomial-approximation","text":"","title":"Chapter 3 | Interpolation and Polynomial Approximation"},{"location":"Mathematics_Basis/NA/Chap_3/#lagrange-interpolation","text":"Suppose we have function \\(y = f(x)\\) with the given points \\((x_0, y_0), (x_1, y_1), \\dots, (x_n, y_n)\\) , and then construct a relatively simple approximating function \\(g(x) \\approx f(x)\\) . Theorem 3.1 If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers and \\(f\\) is a function with given values \\(f(x_0), \\dots, f(x_n)\\) , then a unique polynomial \\(P(x)\\) of degree at most \\(n\\) exists with \\[ P(x_k) = f(x_k), \\text{ for each } k = 0, 1, \\dots, n. \\] and \\[ P(x) = \\sum\\limits_{k = 0}^n f(x_k)L_{n, k}(x), \\] where, for each \\(k = 0, 1, \\dots, n\\) , \\[ L_{n, k}(x) = \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n \\frac{(x - x_i)}{(x_k - x_i)}. \\] \\(L_{n, k}(x)\\) is called the n th Lagrange interpolating polynomial . Proof First we prove for the structure of function \\(L_{n, k}(x)\\) . From the definition of \\(P(x)\\) , \\(L_{n, k}(x)\\) has the following properties \\(L_{n, k}(x_i) = 0\\) when \\(i \\ne k\\) . \\(L_{n, k}(x_k) = 1\\) . To satisfy the first property, the numerator of \\(L_{n, k}(x)\\) contains the term \\[ (x - x_0)(x - x_1) \\cdots (x - x_{k - 1})(x - x_{k + 1}) \\cdots (x - x_n) = \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n (x - x_i). \\] To satisfy the second property, the denominator of \\(L_{n, k}(x)\\) must be equal to the numerator at \\(x = x_k\\) , thus \\[ \\begin{align} L_{n, k}(x) &= \\frac{(x - x_0)(x - x_1) \\cdots (x - x_{k - 1})(x - x_{k + 1}) \\cdots (x - x_n)}{(x_k - x_0)(x - x_1) \\cdots (x_k - x_{k - 1})(x_k - x_{k + 1}) \\cdots (x_k - x_n)} \\\\ &= \\prod\\limits_{\\substack{i = 0 \\\\ i \\ne k}}^n \\frac{x - x_i}{x_k - x_i}. \\end{align} \\] For uniqueness, we prove by contradition. If not, suppose \\(P(x)\\) and \\(Q(x)\\) both satisfying the conditions, then \\(D(x) = P(x) - Q(x)\\) is a polynomial of degree \\(\\text{deg}(D(x)) \\le n\\) , but \\(D(x)\\) has \\(n + 1\\) distinct roots \\(x_0, x_1, \\dots, x_n\\) , which leads to a contradiction. Theorem 3.2 If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers in the interval \\([a, b]\\) and \\(f \\in C^{n + 1}[a, b]\\) . \\(\\forall x \\in [a, b], \\exists \\xi \\in [a, b], s.t.\\) \\[ f(x) = P(x) + \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] where \\(P(x)\\) is the Lagrange interpolating polynomial. And \\[ R(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] is the truncation error . Proof Since \\(R(x) = f(x) - P(x)\\) has at least \\(n + 1\\) roots, thus \\[ R(x) = K(x)\\prod\\limits_{i = 0}^n (x - x_i). \\] For a fixed \\(x \\ne x_k\\) , define \\(g(t)\\) in \\([a, b]\\) by \\[ g(t) = R(t) - K(x)\\prod\\limits_{i = 0}^n (t - x_i). \\] Since \\(g(t)\\) has \\(n + 2\\) distinct roots \\(x, x_0, \\dots, x_n\\) , by Generalized Rolle's Theorem, \\[ \\exists\\ \\xi \\in [a, b],\\ s.t.\\ g^{(n + 1)}(\\xi) = 0. \\] Namely, \\[ f^{(n + 1)}(\\xi) - \\underbrace{P^{n + 1}(\\xi)}_{0} - K(x)(n + 1)! = 0. \\] Thus \\[ K(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!},\\ R(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] Example Suppose a table is to be prepared for the function \\(f(x) = e^x\\) for \\(x\\) in \\([0, 1]\\) . Assume that each entry of the table is accurate up to 8 decimal places and the step size is \\(h\\) . What should \\(h\\) be for linear interpolation to give an absolute error of at most \\(10^{-6}\\) ? Solution. \\[ \\begin{align} |f(x) - P(x)| &= \\left|\\frac{f^{(2)}(\\xi)}{2!}(x - x_j)(x - x_{j + 1})\\right| \\\\ &= \\left|\\frac{e^\\xi}{2}(x - kh)(x - (k + 1)h)\\right| \\le \\frac{e}{2} \\cdot \\frac{h^2}{4} = \\frac{eh^2}{8}. \\end{align} \\] Thus let \\(\\frac{eh^2}{8} \\le 10^{-6}\\) , we have \\(h \\le 1.72 \\times 10^{-3}\\) . To make \\(N = (1 - 0) / h\\) an integer, we can simply choose \\(h = 0.001\\) . Extrapolation Suppose \\(a = \\min\\limits_i \\{x_i\\}\\) , \\(b = \\max\\limits_i \\{x_i\\}\\) . Interpolation estimates value \\(P(x)\\) , \\(x \\in [a, b]\\) , while Extrapolation estimates value \\(P(x)\\) , \\(x \\notin [a, b]\\) . In genernal, interpolation is better than extrapolation.","title":"Lagrange Interpolation"},{"location":"Mathematics_Basis/NA/Chap_3/#nevilles-method","text":"Motivation: When we have more interpolating points, the original Lagrange interpolating method should re-calculate all \\(L_{n, k}\\) , which is not efficient. Definition Let \\(f\\) be a function defined at \\(x_0, x_1, \\dots, x_n\\) , and suppose that \\(m_1, \\dots, m_k\\) are \\(k\\) distinct integers with \\(0 \\le m_i \\le n\\) for each \\(i\\) . The Lagrange polynomial that agrees with \\(f(x)\\) at the \\(k\\) points denoted by \\(P_{m_1, m_2, \\dots, m_k}(x)\\) . Thus \\(P(x) = P_{0, 1, \\dots, n}(x)\\) , where \\(P(x)\\) is the n th Lagrange polynomials that interpolate \\(f\\) at \\(k + 1\\) points \\(x_0, \\dots, x_k\\) . Theorem 3.3 Let \\(f\\) be a function defined at \\(x_0, x_1, \\dots, x_n\\) , and \\(x_i \\ne x_j\\) , then \\[ P(x) = \\frac{(x - x_j)P_{0, 1, \\dots, j - 1, j + 1, \\dots k}(x) - (x - x_i)P_{0, 1, \\dots, i - 1, i + 1, \\dots k}(x)}{(x_i - x_j)}. \\] Denote that \\[ Q_{i, j} = P_{i - j, i - j + 1, \\dots, i - 1, i}, \\] then from the theorem above, the interpolating polynomials can be generated recursively .","title":"Neville's Method"},{"location":"Mathematics_Basis/NA/Chap_3/#newton-interpolation","text":"Differing from Langrange polynomials, we try to represent \\(P(x)\\) by the following form: \\[ \\begin{align} N(x) = &\\ a_0 + a_1(x - x_0) + a_2(x - x_0)(x - x_1) + \\cdots \\\\ & + a_n(x - x_0)(x - x_1) \\cdots (x - x_{n - 1}). \\end{align} \\] Definition \\(f[x_i] = f(x_i)\\) is the zeroth divided difference w.r.t. \\(x_i\\) . The k th divided difference w.r.t. \\(x_i, x_{i + 1}, x_{i + k}\\) is defined recursively by \\[ f[x_i, x_{i + 1}, \\dots, x_{i + k - 1}, x_{i + k}] = \\frac{f[x_{i + 1}, x_{i + 2}, \\dots, x_{i + k}] - f[x_i, x_{i + 1}, \\dots, x_{i + k - 1}]}{x_{i + k} - x_i} \\] Then we derive the Newton's interpolatory divided-difference formula : \\[ N(x) = f[x_0] + \\sum\\limits_{k = 1}^n f[x_0, x_1, \\dots, x_k]\\prod\\limits_{i = 0}^{k - 1}(x - x_i). \\] And the divided difference can be generated as below, which is similar to Neville's Method. Theorem 3.4 If \\(x_0, x_1, \\dots, x_n\\) are \\(n + 1\\) distinct numbers in the interval \\([a, b]\\) and \\(f \\in C^{n + 1}[a, b]\\) . \\(\\forall x \\in [a, b], \\exists \\xi \\in [a, b], s.t.\\) \\[ f(x) = N(x) + f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i). \\] where \\(N(x)\\) is the Newton's interpolatory divided-difference formula. And \\[ R(x) = f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i). \\] is the truncation error . Proof By definition of divided difference, we have \\[ \\left\\{ \\begin{align} f(x) &= f[x_0] + (x - x_0)f[x, x_0], & \\textbf{Eq.1} \\\\ f[x, x_0] &= f[x_0, x_1] + (x - x_1)f[x, x_0, x_1], & \\textbf{Eq.2} \\\\ & \\vdots \\\\ f[x, x_0, \\dots, x_{n - 1}] &= f[x_0, \\dots, x_n] + (x - x_n)f[x, x_0, \\dots, x_n]. & \\textbf{Eq.n - 1} \\end{align} \\right. \\] then compute \\[ \\textbf{Eq.1} + (x - x_0) \\times \\textbf{Eq.2} + \\cdots + (x - x_0) \\cdots (x - x_{n - 1}) \\times \\textbf{Eq.n - 1}. \\] i.e. \\[ f(x) = N(x) + \\underbrace{f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i)}_{R(x)}. \\] Note Since the uniqueness of n -th interpolating polynomial, \\(N(x) \\equiv P(x)\\) . They have the same truncation error, which is \\[ f[x, x_0, \\dots, x_n]\\prod\\limits_{i = 0}^n (x - x_i) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^n (x - x_i). \\] Theorem 3.5 Suppose that \\(f \\in C^n[a, b]\\) and \\(x_0, x_1, \\dots, x_n\\) are distinct numbers in \\([a, b]\\) . Then \\(\\exists\\ \\xi \\in (a, b)\\) , s.t. \\[ f[x_0, x_1, \\dots, x_n] = \\frac{f^{(n)}(\\xi)}{n!}. \\]","title":"Newton Interpolation"},{"location":"Mathematics_Basis/NA/Chap_3/#special-case-equal-spacing","text":"Definition Forward Difference \\[ \\begin{align} \\Delta f_i &= f_{i + 1} - f_i \\\\ \\Delta^k f_i &= \\Delta\\left(\\Delta^{k - 1} f_i \\right) = \\Delta^{k - 1}f_{i + 1} - \\Delta^{k - 1}f_i. \\end{align} \\] Backward Difference \\[ \\begin{align} \\nabla f_i &= f_i - f_{i - 1} \\\\ \\nabla^k f_i &= \\nabla\\left(\\nabla^{k - 1} f_i \\right) = \\nabla^{k - 1}f_i - \\nabla^{k - 1}f_{i - 1}. \\end{align} \\] Property Linearity: \\(\\Delta (af(x) + bg(x)) = a \\Delta f + b \\Delta g\\) . If \\(\\text{deg}(f(x)) = m\\) , then \\[ \\text{deg}\\left(\\Delta^kf(x)\\right) = \\left\\{ \\begin{align} & m - k, & 0 \\le k \\le m \\\\ & 0, & k > m \\end{align} \\right. \\] Decompose the recursive definition, \\[ \\begin{align} \\Delta^n f_k &= \\sum\\limits_{j = 0}^n (-1)^j \\binom{n}{j} f_{n + k - j} \\\\ \\nabla^n f_k &= \\sum\\limits_{j = 0}^n (-1)^{n - j} \\binom{n}{j} f_{j + k - n} \\end{align} \\] Vice versa, \\[ f_{n + k} = \\sum\\limits_{j = 0}^n \\binom{n}{j} \\Delta^j f_k \\] Suppose \\(x_0, x_1, \\dots x_n\\) are equally spaced, namely \\(x_i = x_0 + ih\\) . And let \\(x = x_0 + sh\\) , then \\(x - x_i = (s - i)h\\) . Thus \\[ \\begin{align} N(x) &= f[x_0] + \\sum\\limits_{k = 1}^n s(s - 1) \\cdots (s - k + 1) h^k f[x_0, x_1, \\dots, x_k] \\\\ &= f[x_0] + \\sum\\limits_{k = 1}^n \\binom{s}{k} k! h^k f[x_0, x_1, \\dots, x_k] \\end{align} \\] is called Newton forward divided-difference formula . From mathematical induction, we can derive that \\[ f[x_0, x_1, \\dots, x_k] = \\frac{1}{k!h^k}\\Delta^k f(x_0). \\] Thus we get the Newton Forward-Difference Formula . \\[ N(x) = f[x_0] + \\sum\\limits_{k = 1}^n \\binom{s}{k} \\Delta^k f(x_0). \\] Inversely, \\(x = x_n + sh\\) , then \\(x - x_i = (s + n - i)h\\) , Thus \\[ \\begin{align} N(x) &= f[x_n] + \\sum\\limits_{k = 1}^n s(s + 1) \\cdots (s + k - 1) h^k f[x_n, x_{n - 1}, \\dots, x_{n - k}] \\\\ &= f[x_n] + \\sum\\limits_{k = 1}^n (-1)^k \\binom{-s}{k} k! h^k f[x_n, x_{n - 1}, \\dots, x_{n - k}]. \\end{align} \\] is called Newton backward divided-difference formula . From mathematical induction, we can derive that \\[ f[x_n, x_{n - 1}, \\dots, x_0] = \\frac{1}{k!h^k}\\nabla^k f(x_n). \\] Thus we get the Newton Backward-Difference Formula . \\[ N(x) = f[x_0] + \\sum\\limits_{k = 1}^n (-1)^k \\binom{-s}{k} \\nabla^k f(x_n). \\]","title":"Special Case: Equal Spacing"},{"location":"Mathematics_Basis/NA/Chap_3/#hermit-interpolation","text":"Definition Let \\(x_0, x_1, \\dots, x_n\\) be \\(n + 1\\) distinct numbers in \\([a, b]\\) and \\(m_i \\in \\mathbb{N}\\) . Suppose that \\(f \\in C^m[a, b]\\) , where \\(m = \\max \\{m_i\\}\\) . The osculating polynomial approximating \\(f\\) is the polynomial \\(P(x)\\) of least degree such that \\[ \\frac{d^k P(x_i)}{dx^k} = \\frac{d^k f(x_i)}{dx^k}, \\text{ for each } i = 0, 1, \\dots, n \\text{ and } k = 0, 1, \\dots, m_i. \\] From definition above, we know that when \\(m_i = 0\\) for each \\(i\\) , it's the n -th Lagrange polynomial. And the cases that \\(m_i = 1\\) for each \\(i\\) , then it's Hermit Polynomials . Theorem 3.6 If \\(f \\in C^1[a, b]\\) and \\(x_0, \\dots, x_n \\in [a, b]\\) are distinct, the unique polynomial of least degree agreeing with \\(f\\) and \\(f'\\) at \\(x_0, \\dots, x_n\\) is the Hermit Polynomial of degree at most 2n + 1 defined by \\[ H_{2n + 1}(x) = \\sum\\limits_{j = 0}^n f(x_j) H_{n ,j}(x) + \\sum\\limits_{j = 0}^n f'(x_j) \\hat H_{n, j}(x), \\] where \\[ H_{n, j}(x) = [1 - 2(x - x_j)L'_{n, j}(x_j)]L^2_{n, j}(x) \\] and \\[ \\hat H_{n, j}(x) = (x - x_j)L^2_{n, j}(x). \\] Moreover, if \\(f \\in C^{2n + 2}[a, b]\\) , then \\(\\exists\\ \\xi \\in (a, b)\\) , s.t. \\[ f(x) = H_{2n + 1}(x) + \\underbrace{\\frac{1}{(2n + 2)!}\\prod\\limits_{i = 0}^n (x - x_i)^2 f^{2n + 2}(\\xi)}_{R(x)}. \\] The theorem above gives a complete description of Hermit interpolation. But in pratice, to compute \\(H_{2n + 1}(x)\\) through the formula above is tedious . To make it compute easier, we introduce a method that is similar to Newton's interpolation. Define the sequence \\(\\{z_k\\}_{0}^{2n + 1}\\) by \\[ z_{2i} = z_{2i + 1} = x_i. \\] Based on the Theorem 3.5 , we redefine that \\[ f[z_{2i}, z_{2i + 1}] = f'(z_{2i}) = f'(x_i). \\] Then Hermite polynomial can be represented by \\[ H_{2n + 1}(x) = f[z_0] + \\sum\\limits_{k = 1}^{2n + 1} f[z_0, \\dots, z_k]\\prod\\limits_{i = 0}^{k - 1}(x - z_i). \\]","title":"Hermit Interpolation"},{"location":"Mathematics_Basis/NA/Chap_3/#cubic-spline-interpolation","text":"Motivation: For osculating polynomial approximation, we can let \\(m_i\\) be bigger to get high-degree polynomials. It can somehow be better but higher degree tends to causes a fluctuation or say overfitting . An alternative approach is to divide the interval into subintervals and approximate them respectively, which is called piecewise-polynomial approximation . The most common piecewise-polynomial approximation uses cubic polynomials called cubic spline approximation . Definition Given a function \\(f\\) defined on \\([a, b]\\) and a set of nodes \\(a = x_0 < x_1 < \\cdots < x_n = b\\) , a cubic spline interpolant \\(S\\) for \\(f\\) is a function that satisfies the following conditions: \\(S(x)\\) is a cubic polynomial, denoted \\(S_j(x)\\) , on the subinterval \\([x_j, x_j + 1]\\) , for each \\(j = 0, 1, \\dots, n - 1\\) ; \\(S(x_j) = f(x_j)\\) for each \\(j = 0, 1, \\dots, n\\) ; \\(S_{j + 1}(x_{j + 1}) = S_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\) ; \\(S'_{j + 1}(x_{j + 1}) = S'_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\) ; \\(S''_{j + 1}(x_{j + 1}) = S''_j(x_{j + 1})\\) for each \\(j = 0, 1, \\dots, n - 2\\) ; One of the following sets of boundary conditions: \\(S''(x_0) = S''(x_n) = 0\\) ( free or natural boundary ); \\(S'(x_0) = f'(x_0)\\) and \\(S'(x_n) = f'(x_n)\\) ( clamped boundary ). The spline of natural boundary is called natural spline . Theorem 3.7 The cubic spline interpolation of either natural boundary or clamped boundary is unique . (Since the coefficient matrix \\(A\\) is strictly diagonally dominant.) Suppose interpolation function in each subinterval is \\[ S_j(x) = a_j + b_j(x - x_j) + c_j(x - x_j)^2 + d_j(x - x_j)^3. \\] From the conditions in the definition above, by some algebraic process, we can derive the solution with the following equations, \\[ \\begin{align} h_j &= x_{j + 1} - x_j; \\\\ a_j &= f(x_j); \\\\ b_j &= \\frac{1}{h_j}(a_{j + 1}) - \\frac{h_j}{3}(2c_j + c_{j + 1}); \\\\ d_j &= \\frac{1}{3h_j}{c_{j + 1} - c_j}. \\end{align} \\] While \\(c_j\\) is given by solving the following linear system, \\[ A\\mathbf{x} = \\mathbf{b}, \\] where \\[ \\small A = \\begin{bmatrix} 1 & 0 & 0 & \\cdots & \\cdots & 0 \\\\ h_0 & 2(h_0 + h_1) & h_1 & \\cdots & \\cdots & \\vdots \\\\ 0 & h_1 & 2(h_1 + h_2) & h_2 & \\ddots & \\vdots \\\\ \\vdots & \\ddots & \\ddots & \\ddots & \\ddots & 0 \\\\ \\vdots & \\ddots & \\ddots & h_{n - 2} & 2(h_{n - 2} + h_{n - 1}) & h_{n - 1} \\\\ 0 & \\cdots & \\cdots & 0 & 0 & 1 \\\\ \\end{bmatrix} , \\mathbf{x} = \\begin{bmatrix} c_0 \\\\ c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\] For natural boundary, \\[ \\small \\mathbf{b} = \\begin{bmatrix} 0 \\\\ \\frac{3}{h_1}(a_2 - a_1) - \\frac{3}{h_0}(a_1 - a_0) \\\\ \\vdots \\\\ \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) - \\frac{3}{h_{n - 2}}\\ (a_{n - 1} - a_{n - 2}) \\\\ 0 \\end{bmatrix} \\] For clamped boundary, \\[ \\small \\mathbf{b} = \\begin{bmatrix} \\frac{3}{h_0}(a_1 - a_0) - 3f'(a) \\\\ \\frac{3}{h_1}(a_2 - a_1) - \\frac{3}{h_0}(a_1 - a_0) \\\\ \\vdots \\\\ \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) - \\frac{3}{h_{n - 2}}\\ (a_{n - 1} - a_{n - 2}) \\\\ 3f'(b) - \\frac{3}{h_{n - 1}}\\ (a_n - a_{n - 1}) \\\\ \\end{bmatrix} \\] Note If \\(f \\in C[a, b]\\) and \\(\\frac{\\max h_i}{\\min h_i} \\le C < \\infty\\) . Then \\(S(x)\\ \\overset{\\text{uniform}}{\\longrightarrow}\\ f(x)\\) when \\(\\max h_i \\rightarrow 0\\) . That is the accuracy of approximation can be improved by adding more nodes without increasing the degree of the splines.","title":"Cubic Spline Interpolation"},{"location":"Mathematics_Basis/NA/Chap_3/#curves","text":"We've discussed the interpolation of functions above, but we may encounter the case to interpolate a curve.","title":"Curves"},{"location":"Mathematics_Basis/NA/Chap_3/#straightforward-technique","text":"For given points \\((x_0, y_0), (x_1, y_1), \\dots, (x_n, y_n)\\) , we can construct two approximation functions with \\[ x_i = x(t_i),\\ y_i = y(t_i). \\] The interpolation method can be Lagrange, Hermite and Cubic spline, whatever.","title":"Straightforward Technique"},{"location":"Mathematics_Basis/NA/Chap_3/#bezier-curve","text":"In nature, it's piecewise cubic Hermite polynomial , and the curve is call Bezier curve . Similarly, suppose two function \\(x(t)\\) and \\(y(t)\\) at each interval. We have the following condtions. \\[ x(0) = x_0,\\ x(1) = x_1,\\ x'(0) = \\alpha_0,\\ x'(1) = \\alpha_1. \\] \\[ y(0) = y_0,\\ y(1) = y_1,\\ y'(0) = \\beta_0,\\ y'(1) = \\beta_1. \\] The solution is \\[ \\begin{align} x(t) &= [2(x_0 - x_1) + (\\alpha_0 + \\alpha_1)]t^3 + [3(x_1 - x_0) - (\\alpha_1 + 2\\alpha_0)]t^2 + \\alpha_0 t + x_0. \\\\ y(t) &= [2(y_0 - y_1) + (\\beta_0 + \\beta_1)]t^3 + [3(y_1 - y_0) - (\\beta_1 + 2\\beta_0)]t^2 + \\alpha_0 t + y_0. \\end{align} \\]","title":"Bezier Curve"},{"location":"Mathematics_Basis/NA/Chap_4/","text":"Chapter 4 | Numerical Differentiation and Integration \u00b6 Before everything start, it's necessary to introduce an approach to reduce truncation error: Richardson's extrapolation. Richardson's Extrapolation \u00b6 Suppose for each \\(h\\) , we have a formula \\(N(h)\\) to approximate an unknown value \\(M\\) . And suppose the truncation error have the form \\[ M - N(h) = K_1 h + K_2 h^2 + K_3 h^3 + \\cdots, \\] for some unknown constants \\(K_1\\) , \\(K_2\\) , \\(K_3\\) , \\(\\dots\\) . This has \\(O(h)\\) approximation. First, we try to make some transformation to reduce the \\(K_1 h\\) term. \\[ \\begin{align} M &= N(h) + K_1 h + K_2 h^2 + K_3 h^3 + \\cdots, \\\\ M &= N\\left(\\frac{h}{2}\\right) + K_1 \\frac{h}{2} + K_2 \\frac{h^2}{4} + K_3 \\frac{h^3}{8} + \\cdots, \\end{align} \\] Eliminate \\(K_1 h\\) , we have \\[ \\small M = \\left[N\\left(\\frac{h}{2}\\right) + \\left(N\\left(\\frac{h}{2}\\right) - N(h)\\right)\\right] + K_2\\left(\\frac{h^2}{2} - h^2\\right) + K_3 \\left(\\frac{h^3}{4} - h^3\\right) + \\cdots \\] Define \\(N_1(h) \\equiv N(h)\\) and \\[ N_2(h) = N_1 \\left(\\frac{h}{2}\\right) + \\left(N_1\\left(\\frac{h}{2}\\right) - N_1(h)\\right). \\] Then we have \\(O(h^2)\\) approximation formula for \\(M\\) : \\[ M = N_2(h) - \\frac{K_2}{2}h^2 - \\frac{3K_3}{4}h^3 - \\cdots. \\] Repeat this process, eliminate \\(K_2h^2\\) , we have \\[ M = N_3(h) + \\frac{K_3}{8}h^3 + \\cdots. \\] where \\[ N_3(h) = N_2 \\left(\\frac{h}{2}\\right) + \\frac{1}{3}\\left(N_2\\left(\\frac{h}{2}\\right) - N_2(h)\\right). \\] We can repeat this process recursively, and finally we get the following conclusion. Richardson's Extrapolation If \\(M\\) is in the form \\[ M = N(h) + \\sum\\limits_{j = 1}^{m - 1}K_jh^j + O(h^m), \\] then for each \\(j = 2, 3, \\dots, m\\) , we have an \\(O(h^j)\\) approximation of the form \\[ N_j(h) = N_{j - 1}\\left(\\frac{h}{2}\\right) + \\frac{1}{2^{j - 1} - 1}\\left(N_{j - 1}\\left(\\frac{h}{2}\\right) - N_{j - 1}(h)\\right). \\] Also it can be used if the truncation error has the form \\[ \\sum\\limits_{j = 1}^{m - 1}K_jh^{\\alpha_j} + O(h^{\\alpha_m}). \\] Numerical Differentiation \u00b6 First Order Differentiation \u00b6 Suppose \\(\\{x_0, x_1, \\dots, x_n\\}\\) are distinct in some interval \\(I\\) and \\(f \\in C^{n + 1}(I)\\) , then we have the Lagrange interpolating polynomials, \\[ f(x) = \\sum\\limits_{k = 0}^{n}f(x_k)L_k(x) + \\frac{f^{(n + 1)}(\\xi(x))}{(n + 1)!}\\prod\\limits_{i = 0}^{n}(x - x_i). \\] Differentiate \\(f(x)\\) and substitute \\(x_j\\) , \\[ f'(x_j) = \\sum\\limits_{k = 0}^{n}f(x_k)L_k'(x) + \\frac{f^{(n + 1)}(\\xi(x_j))}{(n + 1)!} \\prod\\limits_{\\substack{k = 0 \\\\ k \\ne j}}^n(x_j - x_k), \\] which is called an (n + 1)-point formula to approximate \\(f'(x_j)\\) . For convenience, we only discuss the equally spaced situation. Suppose the interval is \\([a, b]\\) divided in \\(n\\) parts, and \\(h = (b - a) / n\\) , \\(x_i = a + ih\\) . When \\(n = 1\\) , we simply get the two-point formula, \\[ f'(x_0) = \\frac{f(x_0 + h) - f(x_0)}{h} - \\frac{h}{2}f''(\\xi), \\] which is known as forward-difference formula . Inversely, change \\(h\\) to \\(-h\\) , \\[ f'(x_0) = \\frac{f(x_0) - f(x_0 - h)}{h} + \\frac{h}{2}f''(\\xi), \\] is known as backward-difference formula . When \\(n = 3\\) , we get the three-point formulae . Due to symmetry, there are only two. \\[ \\begin{align} f'(x_0) &= \\frac{1}{2h}[-3(f(x_0)) + 4f(x_0 + h) - f(x_0 + 2h)] + \\frac{h^2}{3}f^{(3)}(\\xi), \\\\ & \\text{where } \\xi \\in [x_0, x_0 + 2h], \\\\ f'(x_0) &= \\frac{1}{2h}[f(x_0 + h) + f(x_0 - h)] - \\frac{h^2}{6}f^{(3)}(\\xi), \\\\ & \\text{where } \\xi \\in [x_0 - h, x_0 + h]. \\end{align} \\] When \\(n = 5\\) , we get the five-point formulae . The following are the useful two of them. \\[ \\begin{align} f'(x_0) &= \\frac{1}{12h}[-25f(x_0) + 48f(x_0 + h) - 36f(x_0 + 2h) \\\\ & \\ \\ \\ \\ + 16f(x_0 + 3h) - 3f(x_0 + 4h)] + \\frac{h^4}{5}f^{(5)}(\\xi), \\\\ & \\text{where } \\xi \\in [x_0, x_0 + 4h], \\\\ f'(x_0) &= \\frac{1}{12h}[f(x_0 - 2h) - 8f(x_0 - h)] + 8f(x_0 + h) - f(x_0 + 2h)] \\\\ & \\ \\ \\ \\ + \\frac{h^4}{30}f^{(5)}(\\xi), \\\\ & \\text{where } \\xi \\in [x_0 - 2h, x_0 + 2h]. \\end{align} \\] Differentiation with Richardson's Extrapolation Consider the three-point formula, \\[ f'(x_0) = \\frac{1}{2h}[f(x_0 + h) + f(x_0 - h)] - \\frac{h^2}{6}f^{(3)}(\\xi) - \\frac{h^4}{120}f^{(5)}(\\xi) - \\cdots. \\] In this case, considering Richardson's extrapolation, we have \\[ N_1(h) = \\frac{1}{2h}[f(x_0 + h) + f(x_0 - h)]. \\] Eliminate \\(h^2\\) , we have \\[ f'(x_0) = N_2(h) + \\frac{h^4}{480}f^{(5)}(x_0) + \\cdots, \\] where \\[ N_2(h) = N_1 \\left(\\frac{h}{2}\\right) + \\frac{1}{3}\\left(N_1\\left(\\frac{h}{2}\\right) - N_1(h)\\right). \\] Continue this procedure we have, \\[ N_j(h) = N_{j - 1}\\left(\\frac{h}{2}\\right) + \\frac{1}{4^{j - 1} - 1}\\left(N_{j - 1}\\left(\\frac{h}{2}\\right) - N_{j - 1}(h)\\right). \\] Example Suppose \\(x_0 = 2.0\\) , \\(h = 0.2\\) , \\(f(x) = xe^x\\) , the exact value of \\(f'(x_0)\\) is 22.167168. While the extrapolation process is shown below. Higher Differentiation \u00b6 Take second order differentiation as an example. From Taylor polynomial about point \\(x_0\\) , \\[ \\begin{align} f(x_0 + h) &= f(x_0) + f'(x_0)h + \\frac{1}{2}f''(x_0)h^2 + \\frac{1}{6}f'''(x_0)h^3 + \\frac{1}{24}f^{(4)}(\\xi_1)h^4, \\\\ f(x_0 - h) &= f(x_0) - f'(x_0)h + \\frac{1}{2}f''(x_0)h^2 - \\frac{1}{6}f'''(x_0)h^3 + \\frac{1}{24}f^{(4)}(\\xi_{-1})h^4, \\\\ \\end{align} \\] where \\(x_0 - h < \\xi_{-1} < x_0 < \\xi_1 < x_0 + h\\) . Add these equations and take some transformations, we get \\[ f''(x_0) = \\frac{1}{h^2}[f(x_0 - h) - 2f(x_0) + f(x_0 + h)] - \\frac{h^2}{12}f^{(4)}(\\xi), \\] where \\(x_0 - h < \\xi < x_0 + h\\) , and \\(f^{(4)}(\\xi) = \\frac{1}{2}(f^{(4)}(\\xi_1) + f^{(4)}(\\xi_{-1}))\\) . Error Analysis \u00b6 Now we examine the formula below, \\[ f'(x_0) = \\frac{1}{2h}[f(x_0 + h) + f(x_0 - h)] - \\frac{h^2}{6}f^{(3)}(\\xi). \\] Suppose that in evaluation \\(f(x_0 + h)\\) and \\(f(x_0 - h)\\) we encounter roundoff errors \\(e(x_0 + h)\\) and \\(e(x_0 - h)\\) . Then our computed value \\(\\tilde f(x_0 + h)\\) and \\(\\tilde f(x_0 - h)\\) satisfy the following formulae, \\[ \\begin{align} f(x_0 + h) &= \\tilde f(x_0 + h) + e(x_0 + h), \\\\ f(x_0 - h) &= \\tilde f(x_0 - h) + e(x_0 - h). \\end{align} \\] Thus the total error is \\[ R = f'(x_0) - \\frac{\\tilde f(x_0 + h) - \\tilde f(x_0 - h)}{2h} = \\frac{e(x_0 + h) - e(x_0 - h)}{2h} - \\frac{h^2}{6}f^{(3)}(\\xi). \\] Moreover, suppose the roundoff error \\(e\\) are bounded by some number \\(\\varepsilon > 0\\) and \\(f^{(3)}(x)\\) is bounded by some number \\(M > 0\\) , then \\[ |R| \\le \\frac{\\varepsilon}{h} + \\frac{h^2}{6}M. \\] Thus theoretically the best choice of \\(h\\) is \\(\\sqrt[3]{3\\varepsilon / M}\\) . But in reality we cannot compute such an \\(h\\) since we know nothing about \\(f^{(3)}(x)\\) . In all, we should aware that the step size \\(h\\) cannot be too large or too small . Numerical Integration \u00b6 Numerical Quadrature \u00b6 Similarly to the differentiation case, suppose \\(\\{x_0, x_1, \\dots, x_n\\}\\) are distinct in some interval \\(I\\) and \\(f \\in C^{n + 1}(I)\\) , then we have the Lagrange interpolating polynomials, \\[ f(x) = \\sum\\limits_{k = 0}^{n}f(x_k)L_k(x) + \\frac{f^{(n + 1)}(\\xi(x))}{(n + 1)!}\\prod\\limits_{i = 0}^{n}(x - x_i). \\] Integrate \\(f(x)\\) and we get \\[ \\begin{align} \\int_a^b f(x) dx &= \\int_a^b \\sum\\limits_{k = 0}^{n}f(x_k)L_k'(x) dx + \\int_a^b \\frac{f^{(n + 1)}(\\xi(x))}{(n + 1)!} \\prod\\limits_{k = 0}^n (x - x_k)dx, \\\\ &= \\sum\\limits_{i = 0}^n A_i f(x_i) + \\underbrace{\\frac{1}{(n + 1)!} \\int_a^b \\prod_{i = 0}^n(x - x_i)f^{(n + 1)}(\\xi(x))dx}_{E(f)}. \\end{align} \\] where \\[ A_i = \\int_a^b L_i(x)dx. \\] Definition The degree of accuracy , or say precision , of a quadrature formula is the largest positive integer \\(n\\) such that the formula is exact for \\(x^k\\) , for each \\(k = 0, 1, \\dots, n\\) . Similarly, we suppose equally spaced situation here again. Theorem | (n + 1)-point closed Newton-Cotes formulae Suppose \\(x_0 = a\\) , \\(x_n = b\\) , and \\(h = (b - a) / n\\) , then \\(\\exists \\xi \\in (a, b)\\) , s.t. \\[ \\int_a^b f(x)dx = \\sum\\limits_{i = 0}^n A_i f(x_i) + \\frac{h^{n + 3}f^{(n + 2)}(\\xi)}{(n + 2)!}\\int_0^n t^2(t - 1)\\cdots(t - n)dt, \\] if \\(n\\) is even and \\(f \\in C^{n + 2}[a, b]\\) , and, \\[ \\int_a^b f(x)dx = \\sum\\limits_{i = 0}^n A_i f(x_i) + \\frac{h^{n + 2}f^{(n + 1)}(\\xi)}{(n + 1)!}\\int_0^n t(t - 1)\\cdots(t - n)dt, \\] if \\(n\\) is odd and \\(f \\in C^{n + 1}[a, b]\\) , where \\[ A_i = \\int_a^b L_i(x) dx = \\int_a^b \\prod\\limits_{\\substack{j = 0 \\\\ j \\ne i}}^n \\frac{(x - x_j)}{(x_i - x_j)} dx. \\] (n + 1)-point closed Newton-Cotes formulae \\(n = 1\\) Trapezoidal rule \\[ \\int_{x_0}^{x_1}f(x)dx = \\frac{h}{2}[f(x_0) + f(x_1)] - \\frac{h^3}{12}f''(\\xi) \\text{, where } x_0 < \\xi < x_1. \\] \\(n = 2\\) Simpson's rule \\[ \\int_{x_0}^{x_2}f(x)dx = \\frac{h}{3}[f(x_0) + 4f(x_1) + f(x_2)] - \\frac{h^5}{90}f^{(4)}(\\xi) \\text{, where } x_0 < \\xi < x_2. \\] \\(n = 3\\) Simpson's Three-Eighths rule \\[ \\int_{x_0}^{x_3}f(x)dx = \\frac{3h}{8}[f(x_0) + 3f(x_1) + 3f(x_2) + f(x_3)] - \\frac{3h^5}{80}f^{(4)}(\\xi), \\\\ \\text{where } x_0 < \\xi < x_3. \\] Trapezoidal rule Simpson's rule Theorem | (n + 1)-point open Newton-Cotes formulae Suppose \\(x_{-1} = a\\) , \\(x_{n + 1} = b\\) , and \\(h = (b - a) / (n + 2)\\) , then \\(\\exists \\xi \\in (a, b)\\) , s.t. \\[ \\int_a^b f(x)dx = \\sum\\limits_{i = 0}^n A_i f(x_i) + \\frac{h^{n + 3}f^{(n + 2)}(\\xi)}{(n + 2)!}\\int_{-1}^{n + 1} t^2(t - 1)\\cdots(t - n)dt, \\] if \\(n\\) is even and \\(f \\in C^{n + 2}[a, b]\\) , and, \\[ \\int_a^b f(x)dx = \\sum\\limits_{i = 0}^n A_i f(x_i) + \\frac{h^{n + 2}f^{(n + 1)}(\\xi)}{(n + 1)!}\\int_{-1}^{n + 1} t(t - 1)\\cdots(t - n)dt, \\] if \\(n\\) is odd and \\(f \\in C^{n + 1}[a, b]\\) , where \\[ A_i = \\int_a^b L_i(x) dx = \\int_a^b \\prod\\limits_{\\substack{j = 0 \\\\ j \\ne i}}^n \\frac{(x - x_j)}{(x_i - x_j)} dx. \\] (n + 1)-point open Newton-Cotes formulae \\(n = 0\\) Midpoint rule \\[ \\int_{x_{-1}}^{x_1}f(x)dx = 2hf(x_0) + \\frac{h^2}{3}f''(\\xi) \\text{, where } x_{-1} < \\xi < x_1. \\] Composite Numerical Integration \u00b6 Motivation: Although the Newton-Cotes gives a better and better approximation as \\(n\\) increases, since it's based on interpolating polynomials, it also owns the oscillatory nature of high-degree polynomials. Similarly, we discuss a piecewise approach to numerical integration with low-order Newton-Cotes formulae. These are the techniques most often applied . Theorem | Composite Trapezoidal rule \\(f \\in C^2[a, b]\\) , \\(h = (b - a) /n\\) , and \\(x_j = a + jh\\) , \\(j = 0, 1, \\dots, n\\) . Then \\(\\exists \\mu \\in (a, b)\\) , s.t. \\[ \\int_a^b f(x)dx = \\frac{h}{2}\\left[f(a) + 2 \\sum\\limits_{j = 1}^{n - 1}f(x_j) + f(b)\\right] - \\frac{b - a}{12}h^2 f''(\\mu). \\] Theorem | Composite Simpson's rule \\(f \\in C^2[a, b]\\) , \\(h = (b - a) /n\\) , and \\(x_j = a + jh\\) , \\(j = 0, 1, \\dots, n\\) . Then \\(\\exists \\mu \\in (a, b)\\) , s.t. \\[ \\small \\int_a^b f(x)dx = \\frac{h}{3}\\left[f(a) + 2 \\sum\\limits_{j = 1}^{(n / 2) - 1}f(x_{2j}) + 4 \\sum\\limits_{j = 1}^{n / 2}f(x_{2j - 1}) + f(b)\\right] - \\frac{b - a}{180}h^4 f^{(4)}(\\mu). \\] Theorem | Composite Midpoint rule \\(f \\in C^2[a, b]\\) , \\(h = (b - a) / (n + 2)\\) , and \\(x_j = a + (j + 1)h\\) , \\(j = -1, 0, \\dots, n + 1\\) . Then \\(\\exists \\mu \\in (a, b)\\) , s.t. \\[ \\int_a^b f(x)dx = 2h \\sum\\limits_{j = 0}^{n / 2}f(x_{2j}) + \\frac{b - a}{6} h^2 f''(\\mu). \\] Stability Composite integration techniques are all stable w.r.t roundoff error. Example: Composite Simpson's rule Suppose \\(f(x_i)\\) is apporximated by \\(\\tilde f(x_i)\\) with \\[ f(x_i) = \\tilde f(x_i) + e_i. \\] Then the accumulated error, \\[ e(h) = \\left|\\frac{h}{3}\\left[e_0 + 2\\sum\\limits_{j = 1}^{(n / 2) - 1}e_{2j} + 4\\sum\\limits_{j = 1}^{n / 2}e_{2j-1} + e_n\\right]\\right| \\] Suppose \\(e_i\\) are uniformly bounded by \\(\\varepsilon\\) , then \\[ e(h) \\le \\frac{h}{3}\\left[\\varepsilon + 2 \\left(\\frac{n}{2} - 1\\right) + 4 \\frac{n}{2} \\varepsilon + \\varepsilon \\right] = nh\\varepsilon = (b - a)\\varepsilon. \\] That means even though divide an interval to more parts, the roundoff error will not increase, which is quite stable. Romberg Integration \u00b6 Romberg integration combine the Composite Trapezoidal rule and Richardson's extrapolation to derive a more useful approximation. Suppose we divide the interval \\([a, b]\\) into \\(m_1 = 1\\) , \\(m_2 = 2\\) , \\(\\dots\\) , and \\(m_n = 2^{n - 1}\\) subintervals respectively. For each division, then the step size \\(h_k\\) is \\((b - a) / m_k = (b - a) / 2^{k - 1}\\) . Then we use \\(R_{k, 1}\\) to denote the composite trapezoidal rule, \\[ \\small R_{k, 1} = \\int_a^b f(x) dx = \\frac{h_k}{2} \\left[f(a) + f(b) + 2 \\left(\\sum\\limits_{i = 1}^{2^{k - 1} - 1} f(a + ih_k)\\right)\\right] - \\frac{(b - a)}{12}h^2_kf''(\\mu_k). \\] Mathematically, we have the recursive formula, \\[ R_{k, 1} = \\frac{1}{2}\\left[R_{k - 1, 1} + h_{k - 1}\\sum\\limits_{i = 1}^{2^{k - 2}}f(a+(2i-1)h_k)\\right]. \\] Theorem the Composite Trapezoidal rule can represented by an alternative error term in the form \\[ \\int_a^b f(x) dx - R_{k, 1} = \\sum\\limits_{i = 1}^{\\infty} K_i h^{2i}_k, \\] where \\(K_i\\) depends only on \\(f^{(2i-1)}(a)\\) and \\(f^{(2i-1)}(b)\\) . This nice theorem makes Richardson's extrapolation available to reduce the truncation error! Similar to Differentiation with Richardson's Extrapolation , we have the following formula, Romberg Integration \\[ R_{k, j} = R_{k, j - 1} + \\frac{R_{k, j - 1} - R_{k - 1, j - 1}}{4^{j - 1} - 1}, \\] with an \\(O(h^{2j}_k)\\) approximation. Adaptive Quadrature Methods \u00b6 Motivation: On the premise of equal spacing, in some cases, the left half of the interval is well approximated, and maybe we only need to subdivide the right half to approximate better. Here we introduce the Adaptive quadrature methods based on the Composite Simpson's rule. First, we want to derive, if we apply Simpson's rule in two subinterval and add them up, how much precision does it improve compared to only applying Simpson's rule just in the whole interval. From Simpson's rule, we have \\[ \\int_a^b f(x) dx = S(a, b) - \\frac{h^5}{90}f^{(4)}(\\mu), \\] where \\[ S(a, b) = \\frac{h}{3}[f(a) + 4f(a + h) + f(b)]. \\] If we divide \\([a, b]\\) into two subintervals, applying Simpson's rule respectively (namely apply Composite Simpson's rule with \\(n = 4\\) and step size \\(h / 2\\) ), we have \\[ \\int_a^b f(x) dx = S\\left(a, \\frac{a + b}{2}\\right) + S\\left(\\frac{a + b}{2}, b\\right) - \\frac{1}{16}\\left(\\frac{h^5}{90}\\right)f^{(4)}(\\tilde \\mu). \\] Moreover, assume \\(f^{(4)}(\\mu) \\approx f^{(4)}(\\tilde \\mu)\\) , then we have \\[ S\\left(a, \\frac{a + b}{2}\\right) + S\\left(\\frac{a + b}{2}, b\\right) - \\frac{1}{16}\\left(\\frac{h^5}{90}\\right)f^{(4)}(\\tilde \\mu) \\approx S(a, b) - \\frac{h^5}{90}f^{(4)}(\\mu), \\] so \\[ \\frac{h^5}{90}f^{(4)}(\\mu) \\approx \\frac{16}{15}\\left[S(a, b) - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right]. \\] Then, \\[ \\begin{align} \\left|\\int_a^b f(x) dx - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right| = \\left|\\frac{1}{16}\\left(\\frac{h^5}{90}\\right)f^{(4)}(\\tilde \\mu)\\right| \\\\ \\approx \\frac{1}{15} \\left|S(a, b) - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right| \\end{align} \\] This result means that the subdivision approximates \\(\\int_a^b f(x) dx\\) about 15 times better than it agree with \\(S(a, b)\\) . Thus suppose we have a tolerance \\(\\varepsilon\\) across the interval \\([a, b]\\) , if \\[ \\left|S(a, b) - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right| < 15\\varepsilon, \\] then we expect to have \\[ \\left|\\int_a^bf(x)dx - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right| < \\varepsilon, \\] and the subdivision is thought to be a better approximation to \\(\\int_a^bf(x)dx\\) . Conclusion Suppose we have a tolerance \\(\\varepsilon\\) on \\([a, b]\\) , we expect the tolerance is uniform. Thus at the subinterval \\([p, q] \\subseteq [a, b]\\) , with \\(q - p = k(b - a)\\) , we expect the tolerance as \\(k\\varepsilon\\) . Moreover, suppose the approximation of Simpson's rule on \\([p, q]\\) is \\(S\\) while the approxiamtion of Simpson's rule on \\([p, (p + q) / 2]\\) and \\([(p + q) / 2, q]\\) are \\(S_1\\) and \\(S_2\\) respectively. Then the criterion to subdivide is that \\[ |S_1 + S_2 - S| < M \\cdot k \\varepsilon. \\] where \\(M\\) is often taken as 10 but not 15, which we derive above, since it also consider the error between \\(f^{(4)}(\\mu)\\) and \\(f^{(4)}(\\tilde \\mu)\\) . Gaussian Quadrature \u00b6 Instead of equal spacing in the Newton-Cotes formulae, the selection of the points \\(x_i\\) also become variables. Gaussian Quadrature is aimed to construct a formula \\(\\sum\\limits_{k = 0}^n A_kf(x_k)\\) to approximate \\(\\int_a^b w(x)f(x)dx\\) of precision degree \\(2n + 1\\) with \\(n + 1\\) points, where \\(w(x)\\) is a weight function. (Compare to the equally spaced strategy only have a precision of around \\(n\\) ). That means, to determine \\(x_i\\) and \\(A_i\\) (totally \\(2n + 2\\) unknowns) such that the formula is accurate for \\(f(x) = 1, x, \\dots, x^{2n + 1}\\) (totally \\(2n + 2\\) equations). The selected points \\(x_i\\) are called Gaussian points . Problem Theoretically, since we have \\(2n + 2\\) unknowns and \\(2n + 2\\) eqautions, we can solve out \\(x_i\\) and \\(A_i\\) . But the equations are not linear ! Thus we give the following theorem to find Gaussian points without solving the nonlinear eqautions. Recap the definition of weight function and orthogonality of polynomials, and the construction of the set of orthogonal polynomials . Theorem \\(x_0, \\dots, x_n\\) are Gaussian point iff \\(W(x) = \\prod\\limits_{k = 0}^n (x - x_k)\\) is orthogonal to all the polynomials of degree no greater than \\(n\\) on interval \\([a,b]\\) w.r.t the weight function \\(w(x)\\) . Proof \\(\\Rightarrow\\) if \\(x_0, \\dots x_n\\) are Gaussian points, then the degree of precision of the formula \\(\\int_a^b w(x)f(x)dx \\approx \\sum\\limits_{k = 0}^n A_k f(x_k)\\) is at least \\(2n + 1\\) . Then \\(\\forall P(x) \\in \\Pi_n\\) , \\(\\text{deg}(P(x)W(x)) \\le 2n + 1\\) . Thus \\[ \\int_a^b w(x)P(x)W(x)dx = \\sum\\limits_{k = 0}^n A_k P(x_k)\\underbrace{W(x_k)}_{0} = 0. \\] \\(\\Leftarrow\\) \\(\\forall P(x) \\in \\Pi_{2n + 1}\\) , let \\(P(x) = W(x)q(x) + r(x), q(x), r(x) \\in \\Pi_{n}\\) , then \\[ \\begin{align} \\int_a^b w(x)P(x) dx &= \\int_a^b w(x)W(x)q(x)dx + \\int_a^b w(x)r(x)dx \\\\ &= \\sum\\limits_{k = 0}^n A_k r(x_k) = \\sum\\limits_{k = 0}^n A_k P(x_k). \\end{align} \\] Recap that the set of orthogonal polynomials \\(\\{\\varphi_0, \\dots, \\varphi_n, \\dots\\}\\) is linearly independent and \\(\\varphi_{n + 1}\\) is orthogonal to any polynomials of degree no greater than \\(n\\) . Thus we can take \\(\\varphi_{n + 1}(x)\\) to be \\(W(x)\\) and the roots of \\(\\varphi_{n + 1}(x)\\) are the Gaussian points. Genernal Solution Problem: Assume \\[ \\int_a^b w(x)f(x)dx \\approx \\sum\\limits_{i = 0}^n A_k f(x_k). \\] Step.1 Construct the set of orthogonal polynomial on the interval \\([a, b]\\) by Gram-Schimidt Process from \\(\\varphi_0(x) \\equiv 1\\) to \\(\\varphi_{n}(x)\\) . Step.2 Find the roots of \\(\\varphi_{n}(x)\\) , which are the Gaussian points \\(x_0, \\dots, x_n\\) . Step.3 Solve the linear systems of the equation given by the precision for \\(f(x) = 1, x, \\dots, x^{2n + 1}\\) , and obtain \\(A_0, \\dots A_n\\) . Although the method above is theoretically available, but it's tedious. But we have some special solutions which have been calculated. With some transformations, we can make them to solve the general problem too. Legendre Polynomials \u00b6 A typical set of orthogonal functions is Legrendre Polynomials. Legrendre Polynomials \\[ P_k(x) = \\frac{1}{2^k k!} \\frac{d^k}{dx^k}(x^2 - 1)^k, \\] or equally defined recursively by \\[ \\begin{align} & P_0(x) = 1,\\ P_1(x) = x, \\\\ & P_{k + 1}(x) = \\frac{1}{k + 1}\\left((2k + 1)x P_k(x) - k P_{k - 1}(x)\\right). \\end{align} \\] Property \\(\\{P_n(x)\\}\\) are orthogonal on \\([-1, 1]\\) , w.r.t the weight function \\[ w(x) \\equiv 1. \\] \\[ (P_j, P_k) = \\left\\{ \\begin{align} & 0, && k \\ne l, \\\\ & \\frac{2}{2k + 1}, && k = l. \\end{align} \\right. \\] \\(P_n(x)\\) is a monic polynomial of degree \\(n\\) . \\(P_n(x)\\) is symmetric w.r.t the origin. The \\(n\\) roots of \\(P_n(x)\\) are all on \\([-1, 1]\\) . The first few of them are \\[ \\begin{align} P_0(x) &= 1, \\\\ P_1(x) &= x, \\\\ P_2(x) &= x^2 - \\frac13, \\\\ P_3(x) &= x^3 - \\frac35 x, \\\\ P_4(x) &= x^4 - \\frac67 x^2 + \\frac{3}{35}. \\end{align} \\] The following table gives the pre-calculated values. And from interval \\([-1, 1]\\) to \\([a, b]\\) , we have a linear map \\[ t = \\frac{2x - a - b}{b - a} \\Leftrightarrow x = \\frac12 [(b - a)t + a + b]. \\] Thus we have \\[ \\int_a^b f(x) dx = \\int_{-1}^1 f\\left(\\frac{(b - a)t + (b + a)}{2}\\right)\\frac{b - a}{2}dt. \\] The formula using the roots of \\(P_{n + 1}(x)\\) is called the Gauss-Legendre quadrature formula. Example Approxiamte \\(\\int_1^{1.5} e^{-x^2}dx\\) (exact value to 7 decimal places is 0.1093643). Solution. \\[ \\int_1^{1.5} e^{-x^2} = \\frac14 \\int_{-1}^1 e^{-(t + 5)^2 / 16}dt, \\] For \\(n = 2\\) , \\[ \\int_1^{1.5} e^{-x^2} \\approx \\frac14 [e^{-(0.57735 + 5)^2 / 16} + e^{-(-57735 + 5)^2 / 16}] = 0.1094003. \\] For \\(n = 3\\) , \\[ \\begin{align} \\int_1^{1.5} e^{-x^2} &\\approx \\frac14 [0.55556e^{-(0.77460 + 5)^2 / 16} + 0.88889e^{-(5)^2 / 16} \\\\ & \\ \\ \\ \\ + 0.55556e^{-(-0.77460 + 5)^2 / 16}] \\\\ & = 0.1093642. \\end{align} \\] Chebyshev Polynomials \u00b6 Also, Chebyshev polynomials are typical set of orthogonal polynomials too. We don't discuss much about it there. The formula using the roots of \\(T_{n + 1}(x)\\) is called the Gauss-Chebyshev quadrature formula.","title":"Chap 4"},{"location":"Mathematics_Basis/NA/Chap_4/#chapter-4-numerical-differentiation-and-integration","text":"Before everything start, it's necessary to introduce an approach to reduce truncation error: Richardson's extrapolation.","title":"Chapter 4 | Numerical Differentiation and Integration"},{"location":"Mathematics_Basis/NA/Chap_4/#richardsons-extrapolation","text":"Suppose for each \\(h\\) , we have a formula \\(N(h)\\) to approximate an unknown value \\(M\\) . And suppose the truncation error have the form \\[ M - N(h) = K_1 h + K_2 h^2 + K_3 h^3 + \\cdots, \\] for some unknown constants \\(K_1\\) , \\(K_2\\) , \\(K_3\\) , \\(\\dots\\) . This has \\(O(h)\\) approximation. First, we try to make some transformation to reduce the \\(K_1 h\\) term. \\[ \\begin{align} M &= N(h) + K_1 h + K_2 h^2 + K_3 h^3 + \\cdots, \\\\ M &= N\\left(\\frac{h}{2}\\right) + K_1 \\frac{h}{2} + K_2 \\frac{h^2}{4} + K_3 \\frac{h^3}{8} + \\cdots, \\end{align} \\] Eliminate \\(K_1 h\\) , we have \\[ \\small M = \\left[N\\left(\\frac{h}{2}\\right) + \\left(N\\left(\\frac{h}{2}\\right) - N(h)\\right)\\right] + K_2\\left(\\frac{h^2}{2} - h^2\\right) + K_3 \\left(\\frac{h^3}{4} - h^3\\right) + \\cdots \\] Define \\(N_1(h) \\equiv N(h)\\) and \\[ N_2(h) = N_1 \\left(\\frac{h}{2}\\right) + \\left(N_1\\left(\\frac{h}{2}\\right) - N_1(h)\\right). \\] Then we have \\(O(h^2)\\) approximation formula for \\(M\\) : \\[ M = N_2(h) - \\frac{K_2}{2}h^2 - \\frac{3K_3}{4}h^3 - \\cdots. \\] Repeat this process, eliminate \\(K_2h^2\\) , we have \\[ M = N_3(h) + \\frac{K_3}{8}h^3 + \\cdots. \\] where \\[ N_3(h) = N_2 \\left(\\frac{h}{2}\\right) + \\frac{1}{3}\\left(N_2\\left(\\frac{h}{2}\\right) - N_2(h)\\right). \\] We can repeat this process recursively, and finally we get the following conclusion. Richardson's Extrapolation If \\(M\\) is in the form \\[ M = N(h) + \\sum\\limits_{j = 1}^{m - 1}K_jh^j + O(h^m), \\] then for each \\(j = 2, 3, \\dots, m\\) , we have an \\(O(h^j)\\) approximation of the form \\[ N_j(h) = N_{j - 1}\\left(\\frac{h}{2}\\right) + \\frac{1}{2^{j - 1} - 1}\\left(N_{j - 1}\\left(\\frac{h}{2}\\right) - N_{j - 1}(h)\\right). \\] Also it can be used if the truncation error has the form \\[ \\sum\\limits_{j = 1}^{m - 1}K_jh^{\\alpha_j} + O(h^{\\alpha_m}). \\]","title":"Richardson's Extrapolation"},{"location":"Mathematics_Basis/NA/Chap_4/#numerical-differentiation","text":"","title":"Numerical Differentiation"},{"location":"Mathematics_Basis/NA/Chap_4/#first-order-differentiation","text":"Suppose \\(\\{x_0, x_1, \\dots, x_n\\}\\) are distinct in some interval \\(I\\) and \\(f \\in C^{n + 1}(I)\\) , then we have the Lagrange interpolating polynomials, \\[ f(x) = \\sum\\limits_{k = 0}^{n}f(x_k)L_k(x) + \\frac{f^{(n + 1)}(\\xi(x))}{(n + 1)!}\\prod\\limits_{i = 0}^{n}(x - x_i). \\] Differentiate \\(f(x)\\) and substitute \\(x_j\\) , \\[ f'(x_j) = \\sum\\limits_{k = 0}^{n}f(x_k)L_k'(x) + \\frac{f^{(n + 1)}(\\xi(x_j))}{(n + 1)!} \\prod\\limits_{\\substack{k = 0 \\\\ k \\ne j}}^n(x_j - x_k), \\] which is called an (n + 1)-point formula to approximate \\(f'(x_j)\\) . For convenience, we only discuss the equally spaced situation. Suppose the interval is \\([a, b]\\) divided in \\(n\\) parts, and \\(h = (b - a) / n\\) , \\(x_i = a + ih\\) . When \\(n = 1\\) , we simply get the two-point formula, \\[ f'(x_0) = \\frac{f(x_0 + h) - f(x_0)}{h} - \\frac{h}{2}f''(\\xi), \\] which is known as forward-difference formula . Inversely, change \\(h\\) to \\(-h\\) , \\[ f'(x_0) = \\frac{f(x_0) - f(x_0 - h)}{h} + \\frac{h}{2}f''(\\xi), \\] is known as backward-difference formula . When \\(n = 3\\) , we get the three-point formulae . Due to symmetry, there are only two. \\[ \\begin{align} f'(x_0) &= \\frac{1}{2h}[-3(f(x_0)) + 4f(x_0 + h) - f(x_0 + 2h)] + \\frac{h^2}{3}f^{(3)}(\\xi), \\\\ & \\text{where } \\xi \\in [x_0, x_0 + 2h], \\\\ f'(x_0) &= \\frac{1}{2h}[f(x_0 + h) + f(x_0 - h)] - \\frac{h^2}{6}f^{(3)}(\\xi), \\\\ & \\text{where } \\xi \\in [x_0 - h, x_0 + h]. \\end{align} \\] When \\(n = 5\\) , we get the five-point formulae . The following are the useful two of them. \\[ \\begin{align} f'(x_0) &= \\frac{1}{12h}[-25f(x_0) + 48f(x_0 + h) - 36f(x_0 + 2h) \\\\ & \\ \\ \\ \\ + 16f(x_0 + 3h) - 3f(x_0 + 4h)] + \\frac{h^4}{5}f^{(5)}(\\xi), \\\\ & \\text{where } \\xi \\in [x_0, x_0 + 4h], \\\\ f'(x_0) &= \\frac{1}{12h}[f(x_0 - 2h) - 8f(x_0 - h)] + 8f(x_0 + h) - f(x_0 + 2h)] \\\\ & \\ \\ \\ \\ + \\frac{h^4}{30}f^{(5)}(\\xi), \\\\ & \\text{where } \\xi \\in [x_0 - 2h, x_0 + 2h]. \\end{align} \\] Differentiation with Richardson's Extrapolation Consider the three-point formula, \\[ f'(x_0) = \\frac{1}{2h}[f(x_0 + h) + f(x_0 - h)] - \\frac{h^2}{6}f^{(3)}(\\xi) - \\frac{h^4}{120}f^{(5)}(\\xi) - \\cdots. \\] In this case, considering Richardson's extrapolation, we have \\[ N_1(h) = \\frac{1}{2h}[f(x_0 + h) + f(x_0 - h)]. \\] Eliminate \\(h^2\\) , we have \\[ f'(x_0) = N_2(h) + \\frac{h^4}{480}f^{(5)}(x_0) + \\cdots, \\] where \\[ N_2(h) = N_1 \\left(\\frac{h}{2}\\right) + \\frac{1}{3}\\left(N_1\\left(\\frac{h}{2}\\right) - N_1(h)\\right). \\] Continue this procedure we have, \\[ N_j(h) = N_{j - 1}\\left(\\frac{h}{2}\\right) + \\frac{1}{4^{j - 1} - 1}\\left(N_{j - 1}\\left(\\frac{h}{2}\\right) - N_{j - 1}(h)\\right). \\] Example Suppose \\(x_0 = 2.0\\) , \\(h = 0.2\\) , \\(f(x) = xe^x\\) , the exact value of \\(f'(x_0)\\) is 22.167168. While the extrapolation process is shown below.","title":"First Order Differentiation"},{"location":"Mathematics_Basis/NA/Chap_4/#higher-differentiation","text":"Take second order differentiation as an example. From Taylor polynomial about point \\(x_0\\) , \\[ \\begin{align} f(x_0 + h) &= f(x_0) + f'(x_0)h + \\frac{1}{2}f''(x_0)h^2 + \\frac{1}{6}f'''(x_0)h^3 + \\frac{1}{24}f^{(4)}(\\xi_1)h^4, \\\\ f(x_0 - h) &= f(x_0) - f'(x_0)h + \\frac{1}{2}f''(x_0)h^2 - \\frac{1}{6}f'''(x_0)h^3 + \\frac{1}{24}f^{(4)}(\\xi_{-1})h^4, \\\\ \\end{align} \\] where \\(x_0 - h < \\xi_{-1} < x_0 < \\xi_1 < x_0 + h\\) . Add these equations and take some transformations, we get \\[ f''(x_0) = \\frac{1}{h^2}[f(x_0 - h) - 2f(x_0) + f(x_0 + h)] - \\frac{h^2}{12}f^{(4)}(\\xi), \\] where \\(x_0 - h < \\xi < x_0 + h\\) , and \\(f^{(4)}(\\xi) = \\frac{1}{2}(f^{(4)}(\\xi_1) + f^{(4)}(\\xi_{-1}))\\) .","title":"Higher Differentiation"},{"location":"Mathematics_Basis/NA/Chap_4/#error-analysis","text":"Now we examine the formula below, \\[ f'(x_0) = \\frac{1}{2h}[f(x_0 + h) + f(x_0 - h)] - \\frac{h^2}{6}f^{(3)}(\\xi). \\] Suppose that in evaluation \\(f(x_0 + h)\\) and \\(f(x_0 - h)\\) we encounter roundoff errors \\(e(x_0 + h)\\) and \\(e(x_0 - h)\\) . Then our computed value \\(\\tilde f(x_0 + h)\\) and \\(\\tilde f(x_0 - h)\\) satisfy the following formulae, \\[ \\begin{align} f(x_0 + h) &= \\tilde f(x_0 + h) + e(x_0 + h), \\\\ f(x_0 - h) &= \\tilde f(x_0 - h) + e(x_0 - h). \\end{align} \\] Thus the total error is \\[ R = f'(x_0) - \\frac{\\tilde f(x_0 + h) - \\tilde f(x_0 - h)}{2h} = \\frac{e(x_0 + h) - e(x_0 - h)}{2h} - \\frac{h^2}{6}f^{(3)}(\\xi). \\] Moreover, suppose the roundoff error \\(e\\) are bounded by some number \\(\\varepsilon > 0\\) and \\(f^{(3)}(x)\\) is bounded by some number \\(M > 0\\) , then \\[ |R| \\le \\frac{\\varepsilon}{h} + \\frac{h^2}{6}M. \\] Thus theoretically the best choice of \\(h\\) is \\(\\sqrt[3]{3\\varepsilon / M}\\) . But in reality we cannot compute such an \\(h\\) since we know nothing about \\(f^{(3)}(x)\\) . In all, we should aware that the step size \\(h\\) cannot be too large or too small .","title":"Error Analysis"},{"location":"Mathematics_Basis/NA/Chap_4/#numerical-integration","text":"","title":"Numerical Integration"},{"location":"Mathematics_Basis/NA/Chap_4/#numerical-quadrature","text":"Similarly to the differentiation case, suppose \\(\\{x_0, x_1, \\dots, x_n\\}\\) are distinct in some interval \\(I\\) and \\(f \\in C^{n + 1}(I)\\) , then we have the Lagrange interpolating polynomials, \\[ f(x) = \\sum\\limits_{k = 0}^{n}f(x_k)L_k(x) + \\frac{f^{(n + 1)}(\\xi(x))}{(n + 1)!}\\prod\\limits_{i = 0}^{n}(x - x_i). \\] Integrate \\(f(x)\\) and we get \\[ \\begin{align} \\int_a^b f(x) dx &= \\int_a^b \\sum\\limits_{k = 0}^{n}f(x_k)L_k'(x) dx + \\int_a^b \\frac{f^{(n + 1)}(\\xi(x))}{(n + 1)!} \\prod\\limits_{k = 0}^n (x - x_k)dx, \\\\ &= \\sum\\limits_{i = 0}^n A_i f(x_i) + \\underbrace{\\frac{1}{(n + 1)!} \\int_a^b \\prod_{i = 0}^n(x - x_i)f^{(n + 1)}(\\xi(x))dx}_{E(f)}. \\end{align} \\] where \\[ A_i = \\int_a^b L_i(x)dx. \\] Definition The degree of accuracy , or say precision , of a quadrature formula is the largest positive integer \\(n\\) such that the formula is exact for \\(x^k\\) , for each \\(k = 0, 1, \\dots, n\\) . Similarly, we suppose equally spaced situation here again. Theorem | (n + 1)-point closed Newton-Cotes formulae Suppose \\(x_0 = a\\) , \\(x_n = b\\) , and \\(h = (b - a) / n\\) , then \\(\\exists \\xi \\in (a, b)\\) , s.t. \\[ \\int_a^b f(x)dx = \\sum\\limits_{i = 0}^n A_i f(x_i) + \\frac{h^{n + 3}f^{(n + 2)}(\\xi)}{(n + 2)!}\\int_0^n t^2(t - 1)\\cdots(t - n)dt, \\] if \\(n\\) is even and \\(f \\in C^{n + 2}[a, b]\\) , and, \\[ \\int_a^b f(x)dx = \\sum\\limits_{i = 0}^n A_i f(x_i) + \\frac{h^{n + 2}f^{(n + 1)}(\\xi)}{(n + 1)!}\\int_0^n t(t - 1)\\cdots(t - n)dt, \\] if \\(n\\) is odd and \\(f \\in C^{n + 1}[a, b]\\) , where \\[ A_i = \\int_a^b L_i(x) dx = \\int_a^b \\prod\\limits_{\\substack{j = 0 \\\\ j \\ne i}}^n \\frac{(x - x_j)}{(x_i - x_j)} dx. \\] (n + 1)-point closed Newton-Cotes formulae \\(n = 1\\) Trapezoidal rule \\[ \\int_{x_0}^{x_1}f(x)dx = \\frac{h}{2}[f(x_0) + f(x_1)] - \\frac{h^3}{12}f''(\\xi) \\text{, where } x_0 < \\xi < x_1. \\] \\(n = 2\\) Simpson's rule \\[ \\int_{x_0}^{x_2}f(x)dx = \\frac{h}{3}[f(x_0) + 4f(x_1) + f(x_2)] - \\frac{h^5}{90}f^{(4)}(\\xi) \\text{, where } x_0 < \\xi < x_2. \\] \\(n = 3\\) Simpson's Three-Eighths rule \\[ \\int_{x_0}^{x_3}f(x)dx = \\frac{3h}{8}[f(x_0) + 3f(x_1) + 3f(x_2) + f(x_3)] - \\frac{3h^5}{80}f^{(4)}(\\xi), \\\\ \\text{where } x_0 < \\xi < x_3. \\] Trapezoidal rule Simpson's rule Theorem | (n + 1)-point open Newton-Cotes formulae Suppose \\(x_{-1} = a\\) , \\(x_{n + 1} = b\\) , and \\(h = (b - a) / (n + 2)\\) , then \\(\\exists \\xi \\in (a, b)\\) , s.t. \\[ \\int_a^b f(x)dx = \\sum\\limits_{i = 0}^n A_i f(x_i) + \\frac{h^{n + 3}f^{(n + 2)}(\\xi)}{(n + 2)!}\\int_{-1}^{n + 1} t^2(t - 1)\\cdots(t - n)dt, \\] if \\(n\\) is even and \\(f \\in C^{n + 2}[a, b]\\) , and, \\[ \\int_a^b f(x)dx = \\sum\\limits_{i = 0}^n A_i f(x_i) + \\frac{h^{n + 2}f^{(n + 1)}(\\xi)}{(n + 1)!}\\int_{-1}^{n + 1} t(t - 1)\\cdots(t - n)dt, \\] if \\(n\\) is odd and \\(f \\in C^{n + 1}[a, b]\\) , where \\[ A_i = \\int_a^b L_i(x) dx = \\int_a^b \\prod\\limits_{\\substack{j = 0 \\\\ j \\ne i}}^n \\frac{(x - x_j)}{(x_i - x_j)} dx. \\] (n + 1)-point open Newton-Cotes formulae \\(n = 0\\) Midpoint rule \\[ \\int_{x_{-1}}^{x_1}f(x)dx = 2hf(x_0) + \\frac{h^2}{3}f''(\\xi) \\text{, where } x_{-1} < \\xi < x_1. \\]","title":"Numerical Quadrature"},{"location":"Mathematics_Basis/NA/Chap_4/#composite-numerical-integration","text":"Motivation: Although the Newton-Cotes gives a better and better approximation as \\(n\\) increases, since it's based on interpolating polynomials, it also owns the oscillatory nature of high-degree polynomials. Similarly, we discuss a piecewise approach to numerical integration with low-order Newton-Cotes formulae. These are the techniques most often applied . Theorem | Composite Trapezoidal rule \\(f \\in C^2[a, b]\\) , \\(h = (b - a) /n\\) , and \\(x_j = a + jh\\) , \\(j = 0, 1, \\dots, n\\) . Then \\(\\exists \\mu \\in (a, b)\\) , s.t. \\[ \\int_a^b f(x)dx = \\frac{h}{2}\\left[f(a) + 2 \\sum\\limits_{j = 1}^{n - 1}f(x_j) + f(b)\\right] - \\frac{b - a}{12}h^2 f''(\\mu). \\] Theorem | Composite Simpson's rule \\(f \\in C^2[a, b]\\) , \\(h = (b - a) /n\\) , and \\(x_j = a + jh\\) , \\(j = 0, 1, \\dots, n\\) . Then \\(\\exists \\mu \\in (a, b)\\) , s.t. \\[ \\small \\int_a^b f(x)dx = \\frac{h}{3}\\left[f(a) + 2 \\sum\\limits_{j = 1}^{(n / 2) - 1}f(x_{2j}) + 4 \\sum\\limits_{j = 1}^{n / 2}f(x_{2j - 1}) + f(b)\\right] - \\frac{b - a}{180}h^4 f^{(4)}(\\mu). \\] Theorem | Composite Midpoint rule \\(f \\in C^2[a, b]\\) , \\(h = (b - a) / (n + 2)\\) , and \\(x_j = a + (j + 1)h\\) , \\(j = -1, 0, \\dots, n + 1\\) . Then \\(\\exists \\mu \\in (a, b)\\) , s.t. \\[ \\int_a^b f(x)dx = 2h \\sum\\limits_{j = 0}^{n / 2}f(x_{2j}) + \\frac{b - a}{6} h^2 f''(\\mu). \\] Stability Composite integration techniques are all stable w.r.t roundoff error. Example: Composite Simpson's rule Suppose \\(f(x_i)\\) is apporximated by \\(\\tilde f(x_i)\\) with \\[ f(x_i) = \\tilde f(x_i) + e_i. \\] Then the accumulated error, \\[ e(h) = \\left|\\frac{h}{3}\\left[e_0 + 2\\sum\\limits_{j = 1}^{(n / 2) - 1}e_{2j} + 4\\sum\\limits_{j = 1}^{n / 2}e_{2j-1} + e_n\\right]\\right| \\] Suppose \\(e_i\\) are uniformly bounded by \\(\\varepsilon\\) , then \\[ e(h) \\le \\frac{h}{3}\\left[\\varepsilon + 2 \\left(\\frac{n}{2} - 1\\right) + 4 \\frac{n}{2} \\varepsilon + \\varepsilon \\right] = nh\\varepsilon = (b - a)\\varepsilon. \\] That means even though divide an interval to more parts, the roundoff error will not increase, which is quite stable.","title":"Composite Numerical Integration"},{"location":"Mathematics_Basis/NA/Chap_4/#romberg-integration","text":"Romberg integration combine the Composite Trapezoidal rule and Richardson's extrapolation to derive a more useful approximation. Suppose we divide the interval \\([a, b]\\) into \\(m_1 = 1\\) , \\(m_2 = 2\\) , \\(\\dots\\) , and \\(m_n = 2^{n - 1}\\) subintervals respectively. For each division, then the step size \\(h_k\\) is \\((b - a) / m_k = (b - a) / 2^{k - 1}\\) . Then we use \\(R_{k, 1}\\) to denote the composite trapezoidal rule, \\[ \\small R_{k, 1} = \\int_a^b f(x) dx = \\frac{h_k}{2} \\left[f(a) + f(b) + 2 \\left(\\sum\\limits_{i = 1}^{2^{k - 1} - 1} f(a + ih_k)\\right)\\right] - \\frac{(b - a)}{12}h^2_kf''(\\mu_k). \\] Mathematically, we have the recursive formula, \\[ R_{k, 1} = \\frac{1}{2}\\left[R_{k - 1, 1} + h_{k - 1}\\sum\\limits_{i = 1}^{2^{k - 2}}f(a+(2i-1)h_k)\\right]. \\] Theorem the Composite Trapezoidal rule can represented by an alternative error term in the form \\[ \\int_a^b f(x) dx - R_{k, 1} = \\sum\\limits_{i = 1}^{\\infty} K_i h^{2i}_k, \\] where \\(K_i\\) depends only on \\(f^{(2i-1)}(a)\\) and \\(f^{(2i-1)}(b)\\) . This nice theorem makes Richardson's extrapolation available to reduce the truncation error! Similar to Differentiation with Richardson's Extrapolation , we have the following formula, Romberg Integration \\[ R_{k, j} = R_{k, j - 1} + \\frac{R_{k, j - 1} - R_{k - 1, j - 1}}{4^{j - 1} - 1}, \\] with an \\(O(h^{2j}_k)\\) approximation.","title":"Romberg Integration"},{"location":"Mathematics_Basis/NA/Chap_4/#adaptive-quadrature-methods","text":"Motivation: On the premise of equal spacing, in some cases, the left half of the interval is well approximated, and maybe we only need to subdivide the right half to approximate better. Here we introduce the Adaptive quadrature methods based on the Composite Simpson's rule. First, we want to derive, if we apply Simpson's rule in two subinterval and add them up, how much precision does it improve compared to only applying Simpson's rule just in the whole interval. From Simpson's rule, we have \\[ \\int_a^b f(x) dx = S(a, b) - \\frac{h^5}{90}f^{(4)}(\\mu), \\] where \\[ S(a, b) = \\frac{h}{3}[f(a) + 4f(a + h) + f(b)]. \\] If we divide \\([a, b]\\) into two subintervals, applying Simpson's rule respectively (namely apply Composite Simpson's rule with \\(n = 4\\) and step size \\(h / 2\\) ), we have \\[ \\int_a^b f(x) dx = S\\left(a, \\frac{a + b}{2}\\right) + S\\left(\\frac{a + b}{2}, b\\right) - \\frac{1}{16}\\left(\\frac{h^5}{90}\\right)f^{(4)}(\\tilde \\mu). \\] Moreover, assume \\(f^{(4)}(\\mu) \\approx f^{(4)}(\\tilde \\mu)\\) , then we have \\[ S\\left(a, \\frac{a + b}{2}\\right) + S\\left(\\frac{a + b}{2}, b\\right) - \\frac{1}{16}\\left(\\frac{h^5}{90}\\right)f^{(4)}(\\tilde \\mu) \\approx S(a, b) - \\frac{h^5}{90}f^{(4)}(\\mu), \\] so \\[ \\frac{h^5}{90}f^{(4)}(\\mu) \\approx \\frac{16}{15}\\left[S(a, b) - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right]. \\] Then, \\[ \\begin{align} \\left|\\int_a^b f(x) dx - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right| = \\left|\\frac{1}{16}\\left(\\frac{h^5}{90}\\right)f^{(4)}(\\tilde \\mu)\\right| \\\\ \\approx \\frac{1}{15} \\left|S(a, b) - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right| \\end{align} \\] This result means that the subdivision approximates \\(\\int_a^b f(x) dx\\) about 15 times better than it agree with \\(S(a, b)\\) . Thus suppose we have a tolerance \\(\\varepsilon\\) across the interval \\([a, b]\\) , if \\[ \\left|S(a, b) - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right| < 15\\varepsilon, \\] then we expect to have \\[ \\left|\\int_a^bf(x)dx - S\\left(a, \\frac{a + b}{2}\\right) - S\\left(\\frac{a + b}{2}, b\\right)\\right| < \\varepsilon, \\] and the subdivision is thought to be a better approximation to \\(\\int_a^bf(x)dx\\) . Conclusion Suppose we have a tolerance \\(\\varepsilon\\) on \\([a, b]\\) , we expect the tolerance is uniform. Thus at the subinterval \\([p, q] \\subseteq [a, b]\\) , with \\(q - p = k(b - a)\\) , we expect the tolerance as \\(k\\varepsilon\\) . Moreover, suppose the approximation of Simpson's rule on \\([p, q]\\) is \\(S\\) while the approxiamtion of Simpson's rule on \\([p, (p + q) / 2]\\) and \\([(p + q) / 2, q]\\) are \\(S_1\\) and \\(S_2\\) respectively. Then the criterion to subdivide is that \\[ |S_1 + S_2 - S| < M \\cdot k \\varepsilon. \\] where \\(M\\) is often taken as 10 but not 15, which we derive above, since it also consider the error between \\(f^{(4)}(\\mu)\\) and \\(f^{(4)}(\\tilde \\mu)\\) .","title":"Adaptive Quadrature Methods"},{"location":"Mathematics_Basis/NA/Chap_4/#gaussian-quadrature","text":"Instead of equal spacing in the Newton-Cotes formulae, the selection of the points \\(x_i\\) also become variables. Gaussian Quadrature is aimed to construct a formula \\(\\sum\\limits_{k = 0}^n A_kf(x_k)\\) to approximate \\(\\int_a^b w(x)f(x)dx\\) of precision degree \\(2n + 1\\) with \\(n + 1\\) points, where \\(w(x)\\) is a weight function. (Compare to the equally spaced strategy only have a precision of around \\(n\\) ). That means, to determine \\(x_i\\) and \\(A_i\\) (totally \\(2n + 2\\) unknowns) such that the formula is accurate for \\(f(x) = 1, x, \\dots, x^{2n + 1}\\) (totally \\(2n + 2\\) equations). The selected points \\(x_i\\) are called Gaussian points . Problem Theoretically, since we have \\(2n + 2\\) unknowns and \\(2n + 2\\) eqautions, we can solve out \\(x_i\\) and \\(A_i\\) . But the equations are not linear ! Thus we give the following theorem to find Gaussian points without solving the nonlinear eqautions. Recap the definition of weight function and orthogonality of polynomials, and the construction of the set of orthogonal polynomials . Theorem \\(x_0, \\dots, x_n\\) are Gaussian point iff \\(W(x) = \\prod\\limits_{k = 0}^n (x - x_k)\\) is orthogonal to all the polynomials of degree no greater than \\(n\\) on interval \\([a,b]\\) w.r.t the weight function \\(w(x)\\) . Proof \\(\\Rightarrow\\) if \\(x_0, \\dots x_n\\) are Gaussian points, then the degree of precision of the formula \\(\\int_a^b w(x)f(x)dx \\approx \\sum\\limits_{k = 0}^n A_k f(x_k)\\) is at least \\(2n + 1\\) . Then \\(\\forall P(x) \\in \\Pi_n\\) , \\(\\text{deg}(P(x)W(x)) \\le 2n + 1\\) . Thus \\[ \\int_a^b w(x)P(x)W(x)dx = \\sum\\limits_{k = 0}^n A_k P(x_k)\\underbrace{W(x_k)}_{0} = 0. \\] \\(\\Leftarrow\\) \\(\\forall P(x) \\in \\Pi_{2n + 1}\\) , let \\(P(x) = W(x)q(x) + r(x), q(x), r(x) \\in \\Pi_{n}\\) , then \\[ \\begin{align} \\int_a^b w(x)P(x) dx &= \\int_a^b w(x)W(x)q(x)dx + \\int_a^b w(x)r(x)dx \\\\ &= \\sum\\limits_{k = 0}^n A_k r(x_k) = \\sum\\limits_{k = 0}^n A_k P(x_k). \\end{align} \\] Recap that the set of orthogonal polynomials \\(\\{\\varphi_0, \\dots, \\varphi_n, \\dots\\}\\) is linearly independent and \\(\\varphi_{n + 1}\\) is orthogonal to any polynomials of degree no greater than \\(n\\) . Thus we can take \\(\\varphi_{n + 1}(x)\\) to be \\(W(x)\\) and the roots of \\(\\varphi_{n + 1}(x)\\) are the Gaussian points. Genernal Solution Problem: Assume \\[ \\int_a^b w(x)f(x)dx \\approx \\sum\\limits_{i = 0}^n A_k f(x_k). \\] Step.1 Construct the set of orthogonal polynomial on the interval \\([a, b]\\) by Gram-Schimidt Process from \\(\\varphi_0(x) \\equiv 1\\) to \\(\\varphi_{n}(x)\\) . Step.2 Find the roots of \\(\\varphi_{n}(x)\\) , which are the Gaussian points \\(x_0, \\dots, x_n\\) . Step.3 Solve the linear systems of the equation given by the precision for \\(f(x) = 1, x, \\dots, x^{2n + 1}\\) , and obtain \\(A_0, \\dots A_n\\) . Although the method above is theoretically available, but it's tedious. But we have some special solutions which have been calculated. With some transformations, we can make them to solve the general problem too.","title":"Gaussian Quadrature"},{"location":"Mathematics_Basis/NA/Chap_4/#legendre-polynomials","text":"A typical set of orthogonal functions is Legrendre Polynomials. Legrendre Polynomials \\[ P_k(x) = \\frac{1}{2^k k!} \\frac{d^k}{dx^k}(x^2 - 1)^k, \\] or equally defined recursively by \\[ \\begin{align} & P_0(x) = 1,\\ P_1(x) = x, \\\\ & P_{k + 1}(x) = \\frac{1}{k + 1}\\left((2k + 1)x P_k(x) - k P_{k - 1}(x)\\right). \\end{align} \\] Property \\(\\{P_n(x)\\}\\) are orthogonal on \\([-1, 1]\\) , w.r.t the weight function \\[ w(x) \\equiv 1. \\] \\[ (P_j, P_k) = \\left\\{ \\begin{align} & 0, && k \\ne l, \\\\ & \\frac{2}{2k + 1}, && k = l. \\end{align} \\right. \\] \\(P_n(x)\\) is a monic polynomial of degree \\(n\\) . \\(P_n(x)\\) is symmetric w.r.t the origin. The \\(n\\) roots of \\(P_n(x)\\) are all on \\([-1, 1]\\) . The first few of them are \\[ \\begin{align} P_0(x) &= 1, \\\\ P_1(x) &= x, \\\\ P_2(x) &= x^2 - \\frac13, \\\\ P_3(x) &= x^3 - \\frac35 x, \\\\ P_4(x) &= x^4 - \\frac67 x^2 + \\frac{3}{35}. \\end{align} \\] The following table gives the pre-calculated values. And from interval \\([-1, 1]\\) to \\([a, b]\\) , we have a linear map \\[ t = \\frac{2x - a - b}{b - a} \\Leftrightarrow x = \\frac12 [(b - a)t + a + b]. \\] Thus we have \\[ \\int_a^b f(x) dx = \\int_{-1}^1 f\\left(\\frac{(b - a)t + (b + a)}{2}\\right)\\frac{b - a}{2}dt. \\] The formula using the roots of \\(P_{n + 1}(x)\\) is called the Gauss-Legendre quadrature formula. Example Approxiamte \\(\\int_1^{1.5} e^{-x^2}dx\\) (exact value to 7 decimal places is 0.1093643). Solution. \\[ \\int_1^{1.5} e^{-x^2} = \\frac14 \\int_{-1}^1 e^{-(t + 5)^2 / 16}dt, \\] For \\(n = 2\\) , \\[ \\int_1^{1.5} e^{-x^2} \\approx \\frac14 [e^{-(0.57735 + 5)^2 / 16} + e^{-(-57735 + 5)^2 / 16}] = 0.1094003. \\] For \\(n = 3\\) , \\[ \\begin{align} \\int_1^{1.5} e^{-x^2} &\\approx \\frac14 [0.55556e^{-(0.77460 + 5)^2 / 16} + 0.88889e^{-(5)^2 / 16} \\\\ & \\ \\ \\ \\ + 0.55556e^{-(-0.77460 + 5)^2 / 16}] \\\\ & = 0.1093642. \\end{align} \\]","title":"Legendre Polynomials"},{"location":"Mathematics_Basis/NA/Chap_4/#chebyshev-polynomials","text":"Also, Chebyshev polynomials are typical set of orthogonal polynomials too. We don't discuss much about it there. The formula using the roots of \\(T_{n + 1}(x)\\) is called the Gauss-Chebyshev quadrature formula.","title":"Chebyshev Polynomials"},{"location":"Mathematics_Basis/NA/Chap_5/","text":"Chapter 5 | Initial-Value Problems for Ordinary Differential Equations \u00b6 Initial-Value Problem (IVP) Basic IVP (one variable first order) Approximate the solution \\(y(t)\\) to a problem of the form \\[ \\frac{dy}{dy} = f(t, y), t \\in [a, b], \\] subject to an initial condition \\[ y(a) = \\alpha. \\] Higher-Order Systems of First-Order IVP Moreover, add the number of unknowns, it becomes approximating \\(y_1(t), \\dots y_m(t)\\) to a problem of the form \\[ \\begin{align} \\frac{dy_1}{dt} &= f_1(t, y_1, y_2, \\dots, y_m), \\\\ \\frac{dy_2}{dt} &= f_2(t, y_1, y_2, \\dots, y_m), \\\\ & \\vdots \\\\ \\frac{dy_m}{dt} &= f_m(t, y_1, y_2, \\dots, y_m), \\end{align} \\] for \\(t \\in [a, b]\\) , subject to the initial conditions \\[ y_1(a) = \\alpha_1, y_2(a) = \\alpha_2, \\dots, y_m(a) = \\alpha_m. \\] Higher-Order IVP Or add the number of order, it becomes m th-order IVP of the form \\[ y^{(m)} = f(t, y, y', y'', \\dots, y^{(m - 1)}), \\] for \\(t \\in [a, b]\\) , subject to the inital conditions \\[ y(a) = \\alpha_1, y'(a) = \\alpha_2, \\dots, y^{(m)}(a) = \\alpha_m. \\] General Idea \u00b6 Of all the method we introduce below, we can only approximate some points, but not the whole function \\(y(t)\\) . The approximation points are called mesh points . Moreover, we will only approximate the equally spaced points. Suppose we apporximate the values at \\(N\\) points on \\([a, b]\\) , then the mesh points are \\[ t_i = a + ih, \\] where \\(h = (b - a) / N\\) is the step size . To get the value between mesh points, we can use interpolation method. Since we know the derivative value \\(f(t, y)\\) at the mesh point, it's nice to use Hermit interpolation or cubic spline interpolation . Availability and Uniqueness \u00b6 Before finding out the approximation, we need some conditions to guarantee its availability and uniqueness. Definition \\(f(t, y)\\) is said to satisfy a Lipschitz condition in the variable \\(y\\) on a set \\(D \\in \\mathbb{R}^2\\) if \\(\\exists L > 0\\) , s.t. \\[ |f(t, y_1) - f(t, y_2)| \\le L|y_1 - y_2|,\\ \\forall (t, y_1), (t, y_2) \\in D. \\] \\(L\\) is called a Lipschitz constant for \\(f\\) . Definition A set \\(D \\in \\mathbb{R}^2\\) is said to be convex if \\[ \\forall (t, y_1), (t, y_2) \\in D,\\ \\forall \\lambda \\in [0, 1],\\ ((1 - \\lambda)t_1 + \\lambda t_2, (1 - \\lambda)y_1 + \\lambda y_2) \\in D. \\] Theorem | Sufficient Condition \\(f(t, y)\\) is defined on a convex set \\(D \\in \\mathbb{R}^2\\) . If \\(\\exists L > 0\\) s.t. \\[ \\left|\\frac{\\partial f}{\\partial y}(t, y)\\right| \\le L,\\ \\forall (t, y) \\in D, \\] then \\(f\\) satisfies a Lipschitz condition on \\(D\\) in the variable \\(y\\) with Lipschitz constatnt \\(L\\) . Theorem | Unique Solution \\(f(t, y)\\) is continuous on \\(D = \\{(t, y) | a \\le t \\le b, -\\infty < y < \\infty\\}\\) ( \\(D\\) is convex). If \\(f\\) satisfies a Lipschitz condition on \\(D\\) in the variable \\(y\\) , then the IVP \\[ y'(t) = f(t, y),\\ a \\le t \\le b,\\ y(a) = \\alpha, \\] has a unique solution \\(y(t)\\) for \\(a \\le t \\le b\\) . Definition The IVP \\[ \\frac{dy}{dt} = f(t, y),\\ a \\le t \\le b,\\ y(a) = \\alpha, \\] is said to be a well-posed problem if A unique solution \\(y(t)\\) exists; \\(\\forall \\varepsilon > 0, \\exists k(\\varepsilon) > 0\\) , s.t. \\(\\forall |\\varepsilon_0| < \\varepsilon\\) , and \\(|\\delta(t)|\\) is continuous with \\(|\\delta(t)| < \\varepsilon\\) on \\([a, b]\\) , a unique solution \\(z(t)\\) to the IVP \\[ \\frac{dz}{dt} = f(t, y) + \\delta(t),\\ a \\le t \\le b,\\ y(a) = \\alpha, \\text{ (perturbed problem)} \\] exists with \\[ |z(t) - y(t)| < k(\\varepsilon)\\varepsilon,\\ \\forall t \\in [a, b]. \\] Numerical methods will always be concerned with solving a perturbed problem since roundoff error is unavoidable. Thus we want the problem to be well-posed , which means the perturbation will not affect the result approxiamtion a lot. Theorem \\(f(t, y)\\) is continuous on \\(D = \\{(t, y) | a \\le t \\le b, -\\infty < y < \\infty\\}\\) ( \\(D\\) is convex). If \\(f\\) satisfies a Lipschitz condition on \\(D\\) in the variable \\(y\\) , then the IVP \\[ y'(t) = f(t, y),\\ a \\le t \\le b,\\ y(a) = \\alpha, \\] is well-posed . Besides, we also need to discuss the sufficient condition of m th-order system of first-order IVP . Similarly, we have the definition of Lipschitz condition and it's sufficient for the uniqueness of the solution. Definition \\(f(t, y_1, \\dots, y_m)\\) define on the convex set \\[ D = \\{(t, y_1, \\dots, y_m | a \\le t \\le b, -\\infty < u_i < \\infty, \\text{ for each } i = 1, 2, \\dots, m\\} \\] is said to satisfy Lipschitz condition on \\(D\\) in the variables \\(y_1, \\dots, y_m\\) if \\(\\exists L > 0\\) , s.t \\[ \\begin{align} & \\forall (t, y_1, \\dots, y_n), (t, z_1, \\dots, z_n) \\in D, \\\\ & |f(t, y_1, \\dots, y_m) - f(t, z_1, \\dots, z_m)| \\le L \\sum\\limits_{j = 1}^m |y_j - z_j| \\end{align} \\] Theorem if \\(f(t, y_1, \\dots, y_m)\\) satisfy Lipschitz condition, then the m th order systems of first-order IVP has a unique solution \\(y_1(t), \\dots, y_n(t)\\) . Euler's Method \u00b6 To measure the error of Taylor methods (Euler's method is Taylor's method of order 1), we first give the definition of local truncation error , which somehow measure the truncation error at a specified step. Definition The difference method \\[ \\begin{align} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + h\\phi(t_i, w_i). \\end{align} \\] has the local truncation error \\[ \\tau_{i + 1}(h) = \\frac{y_{i + 1} - (y_i + h\\phi(t_i, y_i))}{h} = \\frac{y_{i + 1} - y_i}{h} - \\phi(t_i, y_i). \\] Forward / Explicit Euler's Method \u00b6 We use Taylor's Theorem to derive Euler's method. Suppose \\(y(t)\\) is the unique solution of IVP, for each mesh points, we have \\[ y(t_{i + 1}) = y(t_i) + hy'(t_i) + \\frac{h^2}{2}y''(\\xi_i), \\xi_i \\in [t_i, t_{i + 1}], \\] and since \\(y'(t_i) = f(t_i, y(t_i))\\) , we have \\[ y(t_{i + 1}) = y(t_i) + hf(t_i, y(t_i)) + \\frac{h^2}{2}y''(\\xi_i), \\xi_i \\in [t_i, t_{i + 1}], \\] If \\(w_i \\approx y(t_i)\\) and we delete the remainder term, we get the (Explicit) Euler's method , \\[ \\begin{align} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + hf(t_i, w_i). \\text{(difference equations)} \\end{align} \\] The local truncation error of Euler's Method is \\[ \\tau_{i + 1}(h) = \\frac{hy'(t_i) + \\frac{h^2}{2}y''(\\xi_i)}{h} - f(t_i, y_i) = \\frac{h}{2}y''(\\xi_i) = O(h). \\] Theorem The error bound of Euler's method, \\[ |y(t_i) - w_i| \\le \\frac{hM}{2L}\\left[e^{L(t_i - a)} - 1\\right], \\] where \\(M\\) is the upper bound of \\(|y''(t)|\\) . Although it has an exponential term (which is not such an accurate error bound), the most important intuitive is that the error bound is proportional to \\(h\\) . If we consider the roundoff error, we have the following theorem. Theorem Suppose there exists roundoff error and \\(u_i = w_i + \\delta_i, i = 0, 1, \\dots\\) , then \\[ |y(t_i) - u_i| \\le \\frac{1}{L}\\left(\\frac{hM}{2} + \\frac{\\delta}{h}\\right)\\left[e^{L(t_i - a)} - 1\\right] + |\\delta_0|e^{L(t_i - a)}, \\] where \\(\\delta\\) is the upper bound of \\(\\delta_i\\) . This comes to a different case, we can't take \\(h\\) to be too small, and the optimal choice of \\(h\\) is \\[ h = \\sqrt{\\frac{2\\delta}{M}}. \\] Backward/ Implicit Euler's Method \u00b6 Inversely, if we use Taylor's Theorem in the following way, \\[ y(t_i) = y(t_{i + 1}) - hy'(t_{i + 1}) + \\frac{h^2}{2}y''(\\xi_{i + 1}), \\xi_i \\in [t_i, t_{i + 1}]. \\] Thus we get the Implicit Euler's Method , \\[ \\begin{align} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + hf(t_{i + 1}, w_{i + 1}). \\end{align} \\] Since \\(w_{i + 1}\\) is on the both side, we may use some methods in Chapter 2 to solve out the equation. The local truncation error of implicit Euler's method is \\[ \\begin{align} \\tau_{i + 1}(h) &= \\frac{hy'(t_i) + \\frac{h^2}{2}y''(\\xi_i)}{h} - f(t_{i + 1}, y_{i + 1}) \\\\ &= y'(t_i) - y'(t_{i + 1}) + \\frac{h}{2}y''(\\xi_i) \\\\ &= y'(t_i) - y'(t_i) - hy''(\\xi'_i) + \\frac{h}{2}y''(\\xi_i) \\\\ &= -\\frac{h}{2}y''(\\eta_i) = O(h). \\end{align} \\] Trapezoidal Method \u00b6 Notice the local truncation errors of two Euler's method are \\[ \\tau_{i + 1}(h) = \\frac{h}{2}y''(\\xi'), \\tau_{i + 1}(h) = -\\frac{h}{2}y''(\\eta'), \\] If we combine these two, we then obtain a method of \\(O(h^2)\\) local truncation error, which is called Trapezoidal Method . \\[ \\begin{align} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + \\frac{h}{2}[f(t_i, w_i) + f(t_{i + 1}, w_{i + 1})]. \\end{align} \\] Higher-Order Taylor Method \u00b6 In Euler's methods, we only employ first order Taylor polynomials, if we expand it more to n th Taylor polynomials, we have \\[ \\begin{align} y(t_{i + 1}) &= y(t_i) + hy'(t_i) + \\frac{h^2}{2}y''(t_i) + \\cdots + \\frac{h^n}{n!}y^{(n)}(t_i) + \\frac{h^{n + 1}}{(n + 1)!} y^{(n + 1)}(\\xi_i), \\\\ & \\text{ where } \\xi_i \\in [t_i, t_{i + 1}], \\end{align} \\] and since \\(y^{(k)}(t) = f^{(k - 1)}(t, y(t))\\) , we have \\[ \\begin{align} y(t_{i + 1}) &= y(t_i) + hf(t_i, y(t_i)) + \\frac{h^2}{2}f'(t_i, y(t_i)) + \\cdots + \\frac{h^n}{n!}f^{(n - 1)}(t_i, y(t_i)) \\\\ & \\ \\ \\ \\ + \\frac{h^{n + 1}}{(n + 1)!} f^{(n)}(\\xi_i, y(\\xi_i)). \\end{align} \\] Taylor Method of order \\(n\\) \\[ \\begin{align} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + hT^{(n)}(t_i, w_i), \\end{align} \\] where \\[ T^{(n)}(t_i, w_i) = f(t_i, w_i) + \\frac{h}{2}f'(t_i, w_i) + \\cdots + \\frac{h^{n - 1}}{n!}f^{(n - 1)}(t_i, w_i). \\] Theorem The local truncation error of Taylor method of order \\(n\\) is \\(O(h^n)\\) . Runge-Kutta Method \u00b6 Although Taylor method gives more accuracy as \\(n\\) increase, but it's not an easy job to calculate n th derivative of \\(f\\) . Here we introduce a new method called Runge-Kutta Method , which has low local truncation error and doesn't need to compute derivatives. It's based on Taylor's Theorem in two variables . Theorem | Taylor's Theorem in two variables (Recap) \\(f(x, y)\\) has continous \\(n + 1\\) partial derivatives on the neigbourhood of \\(A(x_0, y_0)\\) , \\(U(A)\\) , then \\(\\forall (x, y) \\in U(A)\\) , denote \\(\\Delta x = x - x_0\\) , \\(\\Delta y = y - y_0\\) , \\(\\exists \\theta \\in (0, 1)\\) , s.t. \\[ f(x, y) = P_n(x, y) + R_n(x, y), \\] where \\[ \\small \\begin{align} P_n(x, y) &= f(x_0, y_0) + \\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)f(x_0, y_0) + \\frac{1}{2!}\\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)^2f(x_0, y_0) + \\\\ & \\ \\ \\ \\ \\ \\cdots + \\frac{1}{n!}\\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)^nf(x_0, y_0), \\end{align} \\] \\[ \\small R_n(x, y) = \\frac{1}{(n + 1)!}\\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)^{n + 1}f(x_0 + \\theta \\Delta x, y_0 + \\theta \\Delta y). \\] and \\[ \\small \\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)^{k}f(x, y) = \\sum\\limits_{i = 0}^k \\binom{k}{i} (\\Delta x)^i (\\Delta y)^{k - i} \\frac{\\partial^k f}{\\partial x^i \\partial y^{k - i}}(x, y). \\] Derivation of Midpoint Method To equate \\[ T^{(2)}(t, y) = f(t, y) + \\frac{h}{2}f'(t, y) = f(t, y) + \\frac{h}{2}\\frac{\\partial f}{\\partial t}(t,y) + \\frac{h}{2}\\frac{\\partial f}{\\partial y}(t,y)\\cdot f(t, y). \\] with \\[ \\begin{align} a_1f(t + \\alpha_1, y + \\beta_1) &= a_1f(t, y) + a_1 \\alpha_1 \\frac{\\partial f}{\\partial t}(t, y) \\\\ & \\ \\ \\ \\ + a_1 \\beta_1 \\frac{\\partial f}{\\partial y}(t, y) + a_1 \\cdot R_1(t + \\alpha_1, y + \\beta_1). \\end{align} \\] except for the last residual term \\(R_1\\) , we have \\[ a_1 = 1,\\ \\ \\alpha_1 = \\frac{h}{2},\\ \\ beta_1 = \\frac{h}{2}f(t, y). \\] Thus \\[ T^{(2)}(t, y) = f\\left(t + \\frac{h}{2}, y + \\frac{h}{2}f(t, y)\\right) - R_1. \\] Since the term \\(R_1\\) is only concerned with the 2 nd -order partial derivatives of \\(f\\) , if they are bounded, then \\(R_1\\) is \\(O(h^2)\\) , the order of the local truncation error. Replace \\(T^{(2)}(t,y)\\) by the formula above in the Taylor method of order 2, we have the Midpoint Method . Midpoint Method \\[ \\begin{align} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + hf\\left(t_i + \\frac{h}{2}, w_i + \\frac{h}{2}f(t_i, w_i)\\right). \\end{align} \\] Derivation of Modified Euler Method and Heun's Method Similarly, approximate \\[ T^{(3)}(t, y) = f(t, y) + \\frac{h}{2}f'(t, y) + \\frac{h^2}{6}f''(t,y), \\] with \\[ a_1f(t,y) + a_2f(t + \\alpha_2, y + \\delta_2f(t,y)). \\] But it doesn't have sufficient equation to determine the 4 unknowns \\(a_1, a_2, \\alpha_2, \\delta_2\\) above. But we have \\[ a_1 + a_2 = 1, \\ \\alpha_2 = \\delta_2\\ \\overset{\\Delta}{=\\!=}\\ ph. \\] Thus a number of \\(O(h^2)\\) methods can be derived, and they are generally in the form \\[ \\begin{align} w_{i + 1} &= w_i + h(a_1 K_1 + a_2 K_2) \\\\ K_1 &= f(t_i, w_i), \\\\ K_2 &= f(t_i + ph, y + ph K_1) \\end{align} \\] The most important are the following two. Modified Euler Method \\[ \\begin{align} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + \\frac{h}{2}[f(t_i, w_i) + f(t_i + h, w_i + hf(t_i, w_i))]. \\end{align} \\] Heun's Method \\[ \\begin{align} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + \\frac{h}{4}\\left[f(t_i, w_i) + 3f\\left(t_i + \\frac23 h, w_i + \\frac23 hf(t_i, w_i)\\right)\\right]. \\end{align} \\] In a similar manner, approximate \\(T^{(n)}(t, y)\\) with \\[ \\lambda_1 K_1 + \\lambda_2 K_2 + \\cdots + \\lambda_m K_m, \\] where \\[ \\begin{align} K_1 &= f(t, y), \\\\ K_2 &= f(t + \\alpha_2 h, y + \\beta_{21} h K_1), \\\\ & \\vdots \\\\ K_m &= f(t + \\alpha_m h, y + \\beta_{m1} h K_1 + \\beta_{m, m - 1} hK_{m - 1}). \\end{align} \\] with different \\(m\\) , we can derive quite a lot of Runge-Kutta methods. The most common Runge-Kutta method is given below. Runge-Kutta Order Four \\[ \\begin{align} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + \\frac{h}{6}(K_1 + 2K_2 + 2K_3 + K_4), \\\\ K_1 &= f(t_i, w_i), \\\\ K_2 &= f\\left(t_i + \\frac{h}{2}, w_i + \\frac{h}{2}K_1\\right), \\\\ K_3 &= f\\left(t_i + \\frac{h}{2}, w_i + \\frac{h}{2}K_2\\right), \\\\ K_4 &= f\\left(t_i + h, w_i + h K_3\\right). \\end{align} \\] The local trancation error of Runge-Kutta are given below, \\(n\\) is the number of evaluations per step. Property Compared to Taylor's method, evaluate \\(f(t, y)\\) the same times , Runge-Kutta gives the lowest error. Multistep Method \u00b6 In the previous section, the difference equation only relates \\(w_{i + 1}\\) with \\(w_{i}\\) . Multistep Method discuss the situation of relating more term of previous predicted \\(w\\) . Definition An m -step multistep method for solving the IVP \\[ y'(t) = f(t, y),\\ a \\le t \\le b,\\ y(a) = \\alpha, \\] has a difference equation \\[ \\begin{align} w_{i + 1} &= a_{m - 1}w_i + a_{m - 2}w_{i - 1} + \\cdots a_0 w_{i + 1 - m} + h[b_mf(t_{i + 1}, w_{i + 1}) \\\\ & \\ \\ \\ \\ + b_{m - 1}f(t_i, w_i) + \\cdots + b_0 f(t_{i + 1 - m}, w_{i + 1 - m})], \\end{align} \\] with the starting values \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\dots\\ , w_{m - 1} = \\alpha_{m - 1}. \\] If \\(b_m = 0\\) , it's called explicit , or open . Otherwise, it's called implicit , or closed . Similarly, we first define local truncation error for multistep method to measure the error at a specified step. Definition For a m -step multistep method, the local truncation error at \\((i + 1)\\) st step is \\[ \\begin{align} \\tau_{i + 1}(h) &= \\frac{y(t_{i + 1}) - a_{m - 1}y(t_i) - \\cdots - a_0 y(t_{i + 1 - m})}{h} \\\\ & \\ \\ \\ \\ - b_{m - 1}f(t_i, y(t_{i + 1})) + \\cdots + b_0 f(t_{i + 1 - m}, y(t_{i + 1 - m}))], \\end{align} \\] Adams Method \u00b6 In this section, we only consider that \\(a_{m - 1} = 1\\) and \\(a_i = 0, i \\ne m - 1\\) . To derive the Adams method, we start from a simple formula. \\[ y(t_{i + 1}) - y(t_i) = \\int_{t_i}^{t_{i + 1}}y'(t)dt = \\int_{t_i}^{t_{i + 1}}f(t, y(t))dt. \\] i.e. \\[ y(t_{i + 1}) = y(t_i) + \\int_{t_i}^{t_{i + 1}}f(t, y(t))dt. \\] We cannot integrate \\(f(t, y(t))\\) without knowing \\(y(t)\\) . Instead we use an interpolating polynomial \\(P(t)\\) by points \\((t_0, w_0), \\dots (t_i, w_i)\\) to approximate \\(f(t, y)\\) . Thus we approximate \\(y(t_{i + 1})\\) by \\[ y(t_{i + 1}) \\approx w_i + \\int_{t_i}^{t_{i + 1}} P(t) dt. \\] For convenience, we use Newton backward-difference formula to represent \\(P(t)\\) . Adams-Bashforth m -Step Explicit Method \u00b6 To derive an Adam-Bashforth explicit m -step technique , we form the interpolating polynomial \\(P_{m - 1}(t)\\) by \\[ (t_i, f(t_i, y(t_i))), \\dots (t_{i + 1 - m}, f(t_{i + 1 - m}, y(t_{i + 1 - m}))). \\] Then suppose \\(t = t_i + sh\\) , with \\(dt = hds\\) , we have \\[ \\begin{align} \\int_{t_i}^{t_{i + 1}} f(t, y(t)) dt &= \\int_{t_i}^{t_{i + 1}} \\sum\\limits_{k = 0}^{m - 1}(-1)^k \\binom{-s}{k}\\nabla^k f(t_i, y(t_i))dt \\\\ & \\ \\ \\ \\ + \\int_{t_i}^{t_{i + 1}} \\frac{f^{(m)}(\\xi_i, y(\\xi_i))}{m!}\\prod\\limits_{k = 0}^{m - 1}(t - t_{i - k})dt \\\\ \\left(\\text{Note that } dt = hds\\right) &= h\\sum\\limits_{k = 0}^{m - 1}\\nabla^k (-1)^k \\int_0^1 \\binom{-s}{k}ds \\\\ & \\ \\ \\ \\ + h^{m + 1}\\int_0^1 \\binom{-s}{m}f^{(m)}(\\xi_i, y(\\xi_i))ds. \\\\ \\left(\\substack{\\text{Weighted Mean Value} \\\\ \\text{Theorem for Integral}}\\right) &= h\\sum\\limits_{k = 0}^{m - 1}\\nabla^k (-1)^k \\int_0^1 \\binom{-s}{k}ds \\\\ & \\ \\ \\ \\ + h^{m + 1}f^{(m)}(\\mu_i, y(\\mu_i))\\int_0^1 \\binom{-s}{m}ds. \\end{align} \\] Since for given \\(k\\) , we have \\(k\\) \\(0\\) \\(1\\) \\(2\\) \\(3\\) \\(4\\) \\((-1)^k\\int_0^1 \\binom{-s}{k}ds\\) \\(1\\) \\(\\frac12\\) \\(\\frac{5}{12}\\) \\(\\frac38\\) \\(\\frac{251}{720}\\) Thus \\[ \\begin{align} y(t_{i + 1}) &= y(t_i) + h\\left[f(t_i, y(t_i)) + \\frac12 \\nabla f(t_i, y(t_i)) + \\frac{5}{12} \\nabla^2 f(t_i, y(t_i)) + \\cdots\\right] \\\\ & \\ \\ \\ \\ + h^{m + 1}f^{(m)}(\\mu_i, y(\\mu_i))\\int_0^1 \\binom{-s}{m}ds. \\end{align} \\] Adams-Bashforth m -Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1, \\dots, w_{m - 1} = \\alpha_{m - 1}, \\] \\[ w_{i + 1} = w_i + h\\sum\\limits_{k = 0}^{m - 1}\\nabla^k (-1)^k \\int_0^1 \\binom{-s}{k}ds, \\] with local truncation error \\[ \\tau_{i + 1}(h) = h^{m + 1}f^{(m)}(\\mu_i, y(\\mu_i))\\int_0^1 \\binom{-s}{m}ds. \\] Adams-Bashforth Two-Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1, \\] \\[ w_{i + 1} = w_i + \\frac{h}{2}[3f(t_i, w_i) - f(t_{i - 1}, w_{i - 1})], \\] with local truncation error \\[ \\tau_{i + 1}(h) = \\frac{5}{12}y'''(\\mu_i)h^2. \\] Adams-Bashforth Three-Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2, \\] \\[ w_{i + 1} = w_i + \\frac{h}{12}[23f(t_i, w_i) - 16f(t_{i - 1}, w_{i - 1}) + 5f(t_{i - 2}, w_{i - 2})], \\] with local truncation error \\[ \\tau_{i + 1}(h) = \\frac{3}{8}y^{(4)}(\\mu_i)h^3. \\] Adams-Bashforth Four-Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2,\\ \\ w_3 = \\alpha_3, \\] \\[ \\begin{align} w_{i + 1} = w_i + \\frac{h}{24}&[55f(t_i, w_i) - 59f(t_{i - 1}, w_{i - 1}) \\\\ & + 37f(t_{i - 2}, w_{i - 2}) - 9f(t_{i - 3}, w_{i - 3})], \\end{align} \\] with local truncation error \\[ \\tau_{i + 1}(h) = \\frac{251}{720}y^{(5)}(\\mu_i)h^4. \\] Adams-Moulton m -Step Implicit Method \u00b6 For implicit case, we use \\((t_{i + 1}, f(t_{i + 1}, y(t_{i + 1})))\\) instead of \\((t_{i + 1 - m}, f(t_{i + 1 - m}, y(t_{i + 1 - m})))\\) to constrcut the interpolating polynomials. Similarly we have the conclusions below. Adams-Moulton m -Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1, \\dots, w_{m - 1} = \\alpha_{m - 1}, \\] \\[ w_{i + 1} = w_i + h\\sum\\limits_{k = 0}^{m - 1}\\nabla^k (-1)^k \\int_0^1 \\binom{-s}{k}ds, \\] with local truncation error \\[ \\tau_{i + 1}(h) = h^{m + 1}f^{(m)}(\\mu_i, y(\\mu_i))\\int_0^1 \\binom{-s}{m}ds. \\] Adams-Moulton Two-Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1, \\] \\[ w_{i + 1} = w_i + \\frac{h}{12}[5f(t_{i + 1}, w_{i + 1}) + 8f(t_i, w_i) - f(t_{i - 1}, w_{i - 1})], \\] with local truncation error \\[ \\tau_{i + 1}(h) = -\\frac{1}{24}y^{(4)}(\\mu_i)h^3. \\] Adams-Moulton Three-Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2, \\] \\[ w_{i + 1} = w_i + \\frac{h}{24}[9f(t_{i + 1}, w_{i + 1}) + 19f(t_i, w_i) - 5f(t_{i - 1}, w_{i - 1}) + f(t_{i - 2}, w_{i - 2})], \\] with local truncation error \\[ \\tau_{i + 1}(h) = -\\frac{19}{720}y^{(5)}(\\mu_i)h^4. \\] Adams-Moulton Four-Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2,\\ \\ w_3 = \\alpha_3, \\] \\[ \\begin{align} w_{i + 1} = w_i + \\frac{h}{720}&[251f(t_{i + 1}, w_{i + 1}) + 646f(t_i, w_i) - 264f(t_{i - 1}, w_{i - 1}) \\\\ & + 106f(t_{i - 2}, w_{i - 2}) - 19f(t_{i - 3}, w_{i - 3})], \\end{align} \\] with local truncation error \\[ \\tau_{i + 1}(h) = -\\frac{3}{160}y^{(6)}(\\mu_i)h^5. \\] NOTE that comparing an m -step Adams-Bashforth explicit method and (m - 1) -step Adams-Moulton implicit method, they both involve m evaluation of \\(f\\) per step, and both have the local truncation error of the term \\(ky^{(m + 1)}(\\mu_i)h^m\\) . And the latter implicit method has the smaller coefficient \\(k\\) . This leads to greater stability and smaller roundoff errors for implicit methods. Predictor-Corrector Method \u00b6 Implicit method has some advantages but it's hard to calculate. We could use some iterative methods introduced in Chapter 2 to solve it but it complicates the process considerably. In practice, since explicit method is easy to calculate, we combine the explicit and implicit method to predictor-corrector method . Predictor-Corrector Method Step.1 Compute first \\(m\\) initial values by Runge-Kutta method. Step.2 Predict by Adams-Bashforth explicit method. Step.3 Correct by Adams-Moulton implicit method. NOTE: All the formulae used in the three steps have the same order of truncation error. Example Take \\(m = 4\\) which is the most common case as exmaple. Step.1 From initial value \\(w_0 = \\alpha\\) , we use Runge-Kutta method of order four to derive \\(w_1\\) , \\(w_2\\) and \\(w_3\\) . Set \\(i = 3\\) . Step.2 Use four-step Adams-Bashforth explicit method, we have \\[ \\small w_{i + 1}^{(0)} = w_i + \\frac{h}{24}[55f(t_i, w_i) - 59f(t_{i - 1}, w_{i - 1}) + 37f(t_{i - 2}, w_{i - 2}) - 9f(t_{i - 3}, w_{i - 3})], \\] Step.3 Use three-step Adams-Moulton \\[ \\small w_{i + 1}^{(1)} = w_i + \\frac{h}{24}[9f(t_{i + 1}, w_{i + 1}^{(1)}) + 19f(t_i, w_i) - 5f(t_{i - 1}, w_{i - 1}) + f(t_{i - 2}, w_{i - 2})], \\] Then we have the two options \\(i = i + 1\\) , and go back to Step. 2 , approximate the next point. Or, for higher accuracy, we can repeat Step.3 iteratively by \\[ \\small w_{i + 1}^{(k + 1)} = w_i + \\frac{h}{24}[9f(t_{i + 1}, w_{i + 1}^{(k)}) + 19f(t_i, w_i) - 5f(t_{i - 1}, w_{i - 1}) + f(t_{i - 2}, w_{i - 2})]. \\] Other Methods \u00b6 If we derive the multistep method by Taylor expansion , we can have more methods. We take an example to show it. Suppose we want to derive a difference equation in the form \\[ w_{i + 1} = a_0 w_{i - 1} + h(b_2 f(t_{i + 1}, w_{i + 1}) + b_1 f(t_i, w_i) + b_0 f(t_{i - 1}, w_{i - 1})). \\] We use \\(y_i\\) and \\(f_i\\) to denote \\(y(t_i)\\) and \\(f(t_i, y_i)\\) respectively. Expand \\(y_{i - 1}, f_{i + 1}, f_{i}, f_{i - 1}\\) , we have \\[ \\begin{align} y_{i - 1} &= y_i - hy_i' + \\frac12 h^2 y_i'' - \\frac16h^3y_i''' + O(h^4), \\\\ f_{i + 1} &= y_i' + hy_i'' + \\frac12 h^2 y_i''' + O(h^3), \\\\ f_i &= y_i', \\\\ f_{i - 1} &= y_i' - hy_i'' + \\frac12 h^2 y_i''' + O(h^3), \\\\ \\end{align} \\] and note that \\[ y_{i + 1} = y_i + hy_i' + \\frac12 h^2 y_i'' + \\frac16 h^3 y_i'''+ O(h^4). \\] Equate them with \\[ y_{i + 1} = a_0 y_{i - 1} + h(b_2 f_{i + 1} + b_1 f_i + b_0 f_{i - 1}). \\] We can solve out \\[ a_0 = 1,\\ \\ b_2 = \\frac13,\\ \\ b_1 = \\frac43,\\ \\ b_0 = \\frac13. \\] Thus \\[ w_{i + 1} = w_{i - 1} + \\frac{h}{3}(f(t_{i + 1}, w_{i + 1}) + 4f(t_i, w_i) + f(t_{i - 1}, w_{i - 1})). \\] Simpson Implicit Method \\[ w_0 = \\alpha,\\ w_1 = \\alpha_1, \\\\ w_{i + 1} = w_{i - 1} + \\frac{h}{3}(f(t_{i + 1}, w_{i + 1}) + 4f(t_i, w_i) + f(t_{i - 1}, w_{i - 1})). \\] with local truncation error \\[ \\tau_{i + 1}(h) = \\frac{h^4}{90}y^{(5)}(\\xi_i). \\] Note that it's an implicit method, and the most commonly used corresponding predictor is Milne's Method \\[ w_0 = \\alpha,\\ w_1 = \\alpha_1,\\ w_2 = \\alpha_2 \\\\ w_{i + 1} = w_{i - 3} + \\frac{4h}{3}(2f(t_i, w_i) + - f(t_{i - 1}, w_{i - 1}) + 2f(t_{i - 2}, w_{i - 2})). \\] with local truncation error \\[ \\tau_{i + 1}(h) = \\frac{14}{90}h^4 y^{(5)}(\\xi_i). \\] which can also derive by Taylor expansion in the similar manner. Higher-Order Systems of First-Order IVP \u00b6 Actually it just vectorize the method that we've mentioned above. Take Runge-Kutta Order Four as example. Runge-Kutta Method for Systems of Differential Equations \\[ \\begin{align} w_{1, 0} &= \\alpha_1, w_{2, 0} = \\alpha_2, \\dots, w_{m, 0} = \\alpha_m. \\\\ w_{i, j + 1} &= w_{i, j} + \\frac{h}{6}(K_{1, i} + 2K_{2, i} + 2K_{3, i} + K_{4, i}), \\\\ K_{1, i} &= f_i(t_j, w_{1, j}, w_{2, j}, \\dots, w_{m, j}), \\\\ K_{2, i} &= f_i\\left(t_j + \\frac{h}{2}, w_{1, j} + \\frac{h}{2}K_{1, 1}, w_{2, j} + \\frac{h}{2}K_{1, 2}, \\dots, w_{m, j} + \\frac{h}{2}K_{1, m}\\right), \\\\ K_{3, i} &= f_i\\left(t_j + \\frac{h}{2}, w_{1, j} + \\frac{h}{2}K_{2, 1}, w_{2, j} + \\frac{h}{2}K_{2, 2}, \\dots, w_{m, j} + \\frac{h}{2}K_{2, m}\\right), \\\\ K_{4, i} &= f_i\\left(t_j + h, w_{1, j} + h K_{3, 1}, w_{2, j} + h K_{3, 2}, \\dots, w_{m, j} + h K_{3. m} \\right). \\end{align} \\] Higher-Order IVP \u00b6 We can deal with higher-order IVP the same as higher-order systems of first-order IVP. The only transformation we need to do is let \\(u_1(t) = y(t)\\) , \\(u_2(t) = y'(t)\\) , \\(\\dots\\) , and \\(u_m(t) = y^{(m - 1)}(t)\\) . This produces the first-order system \\[ \\begin{align} \\frac{du_1}{dt} &= \\frac{dy}{dt} = u_2, \\\\ \\frac{du_2}{dt} &= \\frac{dy'}{dt} = u_3, \\\\ & \\vdots \\\\ \\frac{du_{m - 1}}{dt} &= \\frac{dy^{(m - 2)}}{dt} = u_m, \\\\ \\frac{du_m}{dt} &= \\frac{dy^{(m - 1)}}{dt} = y^{(m)} = f(t, y', y'', \\dots, y^{(m - 1)}) = f(t, u_1, \\dots, u_m). \\end{align} \\] with initial conditions \\[ u_1(a) = y(a) = \\alpha_1, u_2(a) = y'(a) = \\alpha_2, \\dots, u_m(a) = y^{(m - 1)}(a) = \\alpha_m. \\] Stability \u00b6 Definition A one-step difference-equation method with local truncation error \\(\\tau_i(h)\\) is consistent with the differential equation it approximates if \\[ \\lim\\limits_{h \\rightarrow 0} \\max\\limits_{1 \\le i \\le N}|\\tau_i(h)| = 0. \\] For m -step multistep methods, it's also required that \\[ \\lim\\limits_{h \\rightarrow 0} |\\alpha_i - y_i| = 0, i = 1, 2, \\dots, m - 1, \\] since at most cases, these \\(\\alpha_i\\) are derived by one-step methods. Definition A one-step / multistep difference-equation method is convergent w.r.t the differential equation it approximates if \\[ \\lim\\limits_{h \\rightarrow 0} \\max\\limits_{1 \\le i \\le N}|w_i - y(t_i)| = 0. \\] Definition A stable method is one whose results depent continously on the initial data, in the sense that small changes or perturbations in the intial conditions produce correspondingly small changes in the subsequent approximations. One Step Method \u00b6 It's relatively natural to think the stability is somewhat analogous to the discussion of well-posed, and thus it's not surprising Lipschitz condition appears. The following theorem gives the relation among consistency , convergence and stability . Theorem Suppose the IVP is approximated by a one-step method in the form \\[ \\begin{align} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + h \\phi(t_i, w_i, h). \\end{align} \\] If \\(\\exists h_0 > 0\\) and \\(\\phi(t, w, h)\\) is continuous and satisfies a Lipschitz condition in the varaible \\(w\\) with Lipschitz constant \\(L\\) on the set \\[ D = \\{(t, w, h) | a \\le t \\le b, -\\infty < w < \\infty, 0 \\le h \\le h_0\\}. \\] Then The method is stable . The difference method is convergent iff it's consistent , which is equivalent to \\[ \\phi(t, y, 0) = f(t, y),\\ t \\in [a, b]. \\] (Relation between Global Error and Local Truncation Error) If \\(\\exists \\tau(h)\\) s.t. \\(|\\tau_i(h)| \\le \\tau(h)\\) whenever \\(0 \\le h \\le h_0\\) , then \\[ |y(t_i) - w_i| \\le \\frac{\\tau(h)}{L}e^{L(t_i - a)}. \\] Multistep Method \u00b6 Theorem | Relation between Global Error and Local Truncation Error Suppose the IVP is approximated by a predictor-corrector method with an m -step Adams-Bashforth predictor equation \\[ w_{i + 1} = w_i + h[b_{m - 1}f(t_i, w_i) + \\cdots + b_0f(t_{i + 1 - m}, w_{i + 1 - m})], \\] with local truncation error \\(\\tau_{i + 1}(h)\\) , and an (m - 1) -step Adams-Moulton corrector eqation \\[ w_{i + 1} = w_i + h[\\tilde b_{m - 1}f(t_i, w_i) + \\cdots + \\tilde b_0f(t_{i + 1 - m}, w_{i + 1 - m})], \\] with local truncation error \\(\\tilde \\tau_{i + 1}(h)\\) . In addition, suppose \\(f(t, y)\\) and \\(f_y(t, y)\\) are continous on \\(D = \\{(t, y) | a \\le t \\le b, -\\infty < y < \\infty\\}\\) and \\(f_y\\) is bounded. Then the local truncation error \\(\\sigma_{i + 1}(h)\\) of the predictor-corrector method is \\[ \\small \\sigma_{i + 1}(h) = \\tilde \\tau_{i + 1}(h) + \\tau_{i + 1}(h) \\tilde b_{m - 1} \\frac{\\partial f}{\\partial y}(t_{i + 1}, \\theta_{i + 1}),\\ \\text{ for some } \\theta_{i + 1} \\in (0, h\\tau_{i + 1}(h)) \\] Moreover, \\(\\exists k_1, k_2\\) , s.t. \\[ |w_i - y(t_i)| \\le \\left[\\max\\limits_{0 \\le j \\le m - 1}|w_j - y(t_j)| + k_1\\sigma(h) \\right] e^{k_2(t_i - a)}, \\] where \\(\\sigma(h) = \\max\\limits_{m \\le j \\le N}|\\sigma_j(h)|\\) . For the difference equation of multistep method, we first introduce characteristc polynomial associated with the method, given by \\[ P(\\lambda) = \\lambda^m - a_{m - 1}\\lambda^{m - 1} - a_{m - 2}\\lambda^{m - 2} - \\cdots - a_1 \\lambda - a_0. \\] Definition Suppose the roots of the characteristic equation \\[ P(\\lambda) = \\lambda^m - a_{m - 1}\\lambda^{m - 1} - a_{m - 2}\\lambda^{m - 2} - \\cdots - a_1 \\lambda - a_0 = 0 \\] associated with the multistep difference method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\dots\\ , w_{m - 1} = \\alpha_{m - 1}, \\\\ w_{i + 1} = a_{m - 1}w_i + a_{m - 2}w_{i - 1} + \\cdots a_0 w_{i + 1 - m} + hF(t_i, h, w_{i + 1}, w_i \\dots, w_{i + 1 - m}). \\] If \\(|\\lambda_i| \\le 1\\) , for each \\(i = 1, 2, \\dots, m\\) and all roots with modulus value 1 are simple roots, then the difference method is said to satisfy the root condition . Methods that satisfy the root condition and have \\(\\lambda = 1\\) as the only root of the characteristic equation of magnitude one are called strongly stable . Methods that satisfy the root condition and have more than one disctinct root with magnitude one are called weakly stable . Methods that do not satisfy the root condition are called unstable . Theorem A multistep method of the form \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\dots\\ , w_{m - 1} = \\alpha_{m - 1}, \\\\ w_{i + 1} = a_{m - 1}w_i + a_{m - 2}w_{i - 1} + \\cdots a_0 w_{i + 1 - m} + hF(t_i, h, w_{i + 1}, w_i \\dots, w_{i + 1 - m}). \\] is stable iff it satisfies the root condition. Moreover, if the difference method is consistent with the differential equation, then the method is stable iff it is convergent . Stiff Equations \u00b6 However, a special type of solution is not easy to deal with, when the magnitude of derivative increases but the solution does not. They are call stiff equations . Stiff differential eqaution are characterized as those whose exact solution has a term of the form \\(e^{-ct}\\) , where \\(c\\) is a large positive constant. This is called the transient solution. Actaully there is another important part of solution called steady-state solution. Let's first define test equation to examine what happen with the stiff cases. Definition A test equation is the IVP \\[ y' = \\lambda y,\\ \\ y(0) = \\alpha,\\ \\ \\text{ where } \\text{Re}(\\lambda) < 0. \\] The solution is obviously \\(y(t) = \\alpha e^{\\lambda t}\\) , which is the transient solution. At this case, the steady-state solution is zero, thus the approximation characteristics of a method are easy to determine. Example of Euler's Method Take Euler's method as an example, and denote \\(H = h\\lambda\\) . We have \\[ w_0 = \\alpha, \\\\ w_{i + 1} = w_i + h(\\lambda w_i) = (1 + h\\lambda)w_i = (1 + H)w_i, \\] so \\[ w_{i + 1} = (1 + H)^{i + 1}w_0 = (1 + H)^{i + 1}\\alpha. \\] The absolute error is \\[ |y(t_i) - w_i| = |(e^H)^i - (1 + H)^i||\\alpha|. \\] When \\(H < 0\\) , \\((e^H)^i\\) decays to zero as \\(i\\) increases. Thus we want \\((1 + H)^i\\) to decay too, which implies that \\[ |1 + H| < 1. \\] From another perspective, consider the roundoff error of the initial value \\(\\alpha\\) by, \\[ w_0 = \\alpha + \\delta_0, \\] At the i th step the roundoff error is \\[ \\delta_i = (1 + H)^i \\delta_0. \\] To control the error, we also want \\(|1 + H| < 1\\) . In general, when applying to the test equation, we have the difference equation of the form \\[ w_{i + 1} = Q(H)w_i,\\ \\ H = h \\lambda. \\] To make \\(Q(H)\\) approximate \\(e^H\\) , we want at least \\(|Q(H)| < 1\\) to make it decay to 0. The inequality delimit a region in a complex plane. For a multistep method, the difference equation is in the form \\[ (1 - H b_m)w_{i + 1} - (a_{m - 1} + H b_{m - 1})w_i - \\cdots - (a_0 + H b_0)w_{i + 1 - m} = 0. \\] We can define a characteristic polynomial \\[ Q(z, H) = (1 - H b_m)z^m - (a_{m - 1} + H b_{m - 1})z^{m - 1} - \\cdots - (a_0 + H b_0). \\] Suppose \\(\\beta_i\\) are the distinct roots of \\(Q(z, H)\\) , then it can be proved that \\(\\exists c_i\\) , s.t. \\[ w_{i} = \\sum\\limits_{k = 1}^m c_k\\beta_k^i. \\] Again, to make \\(w_i\\) approximate \\((e^H)^i\\) , we want at least all \\(|\\beta_k| < 1\\) to make it decay to 0. It also delimit a region in a complex plane. Definition The Region R of abosolute stability for a one-step method is \\[ R = \\{H \\in \\mathbb{C} | |Q(H)| < 1\\}, \\] and for a multistep method, \\[ R = \\{H \\in \\mathbb{C} | |\\beta_k| < 1, for all zero point of Q(z, H)\\}. \\] A numerical method is said to be A-stable if its region \\(R\\) of abosolute stability contains left half-plane , which means \\(\\text{Re}(\\lambda) < 0\\) , stable with stiff equation . Method A is said to be more stable than method B if the region of absolute stability of A is larger than that of B. The region of Euler's method is like Thus it's only stable for some cases of stiff equation. When the negative \\(\\lambda\\) becomes smaller and get out of the region, it becomes not stable. Similarly, for Runge-Kutta Order Four method (explicit), applying test equation, we have \\[ w_{i + 1} = \\left(1 + H + \\frac{H^2}{2} + \\frac{H^3}{6} + \\frac{H^4}{24}\\right)w_i. \\] And the region is like Let's consider some implicit method. Say Euler's implicit method, we have \\[ w_{i + 1} = \\frac{1}{1 - H} w_i, \\] whose region is Thus it's A-stable . Also, implicit Trapezoidal method and 2 nd -order Runge-Kutta implict method are A-stable , both with the difference equation, \\[ w_{i + 1} = \\frac{2 + H}{2 - H}w_i. \\] whose region is just right covers the left half-plane. Thus an important intuitive is that implicit method is more stable than explicit method in the stiff discussion .","title":"Chap 5"},{"location":"Mathematics_Basis/NA/Chap_5/#chapter-5-initial-value-problems-for-ordinary-differential-equations","text":"Initial-Value Problem (IVP) Basic IVP (one variable first order) Approximate the solution \\(y(t)\\) to a problem of the form \\[ \\frac{dy}{dy} = f(t, y), t \\in [a, b], \\] subject to an initial condition \\[ y(a) = \\alpha. \\] Higher-Order Systems of First-Order IVP Moreover, add the number of unknowns, it becomes approximating \\(y_1(t), \\dots y_m(t)\\) to a problem of the form \\[ \\begin{align} \\frac{dy_1}{dt} &= f_1(t, y_1, y_2, \\dots, y_m), \\\\ \\frac{dy_2}{dt} &= f_2(t, y_1, y_2, \\dots, y_m), \\\\ & \\vdots \\\\ \\frac{dy_m}{dt} &= f_m(t, y_1, y_2, \\dots, y_m), \\end{align} \\] for \\(t \\in [a, b]\\) , subject to the initial conditions \\[ y_1(a) = \\alpha_1, y_2(a) = \\alpha_2, \\dots, y_m(a) = \\alpha_m. \\] Higher-Order IVP Or add the number of order, it becomes m th-order IVP of the form \\[ y^{(m)} = f(t, y, y', y'', \\dots, y^{(m - 1)}), \\] for \\(t \\in [a, b]\\) , subject to the inital conditions \\[ y(a) = \\alpha_1, y'(a) = \\alpha_2, \\dots, y^{(m)}(a) = \\alpha_m. \\]","title":"Chapter 5 | Initial-Value Problems for Ordinary Differential Equations"},{"location":"Mathematics_Basis/NA/Chap_5/#general-idea","text":"Of all the method we introduce below, we can only approximate some points, but not the whole function \\(y(t)\\) . The approximation points are called mesh points . Moreover, we will only approximate the equally spaced points. Suppose we apporximate the values at \\(N\\) points on \\([a, b]\\) , then the mesh points are \\[ t_i = a + ih, \\] where \\(h = (b - a) / N\\) is the step size . To get the value between mesh points, we can use interpolation method. Since we know the derivative value \\(f(t, y)\\) at the mesh point, it's nice to use Hermit interpolation or cubic spline interpolation .","title":"General Idea"},{"location":"Mathematics_Basis/NA/Chap_5/#availability-and-uniqueness","text":"Before finding out the approximation, we need some conditions to guarantee its availability and uniqueness. Definition \\(f(t, y)\\) is said to satisfy a Lipschitz condition in the variable \\(y\\) on a set \\(D \\in \\mathbb{R}^2\\) if \\(\\exists L > 0\\) , s.t. \\[ |f(t, y_1) - f(t, y_2)| \\le L|y_1 - y_2|,\\ \\forall (t, y_1), (t, y_2) \\in D. \\] \\(L\\) is called a Lipschitz constant for \\(f\\) . Definition A set \\(D \\in \\mathbb{R}^2\\) is said to be convex if \\[ \\forall (t, y_1), (t, y_2) \\in D,\\ \\forall \\lambda \\in [0, 1],\\ ((1 - \\lambda)t_1 + \\lambda t_2, (1 - \\lambda)y_1 + \\lambda y_2) \\in D. \\] Theorem | Sufficient Condition \\(f(t, y)\\) is defined on a convex set \\(D \\in \\mathbb{R}^2\\) . If \\(\\exists L > 0\\) s.t. \\[ \\left|\\frac{\\partial f}{\\partial y}(t, y)\\right| \\le L,\\ \\forall (t, y) \\in D, \\] then \\(f\\) satisfies a Lipschitz condition on \\(D\\) in the variable \\(y\\) with Lipschitz constatnt \\(L\\) . Theorem | Unique Solution \\(f(t, y)\\) is continuous on \\(D = \\{(t, y) | a \\le t \\le b, -\\infty < y < \\infty\\}\\) ( \\(D\\) is convex). If \\(f\\) satisfies a Lipschitz condition on \\(D\\) in the variable \\(y\\) , then the IVP \\[ y'(t) = f(t, y),\\ a \\le t \\le b,\\ y(a) = \\alpha, \\] has a unique solution \\(y(t)\\) for \\(a \\le t \\le b\\) . Definition The IVP \\[ \\frac{dy}{dt} = f(t, y),\\ a \\le t \\le b,\\ y(a) = \\alpha, \\] is said to be a well-posed problem if A unique solution \\(y(t)\\) exists; \\(\\forall \\varepsilon > 0, \\exists k(\\varepsilon) > 0\\) , s.t. \\(\\forall |\\varepsilon_0| < \\varepsilon\\) , and \\(|\\delta(t)|\\) is continuous with \\(|\\delta(t)| < \\varepsilon\\) on \\([a, b]\\) , a unique solution \\(z(t)\\) to the IVP \\[ \\frac{dz}{dt} = f(t, y) + \\delta(t),\\ a \\le t \\le b,\\ y(a) = \\alpha, \\text{ (perturbed problem)} \\] exists with \\[ |z(t) - y(t)| < k(\\varepsilon)\\varepsilon,\\ \\forall t \\in [a, b]. \\] Numerical methods will always be concerned with solving a perturbed problem since roundoff error is unavoidable. Thus we want the problem to be well-posed , which means the perturbation will not affect the result approxiamtion a lot. Theorem \\(f(t, y)\\) is continuous on \\(D = \\{(t, y) | a \\le t \\le b, -\\infty < y < \\infty\\}\\) ( \\(D\\) is convex). If \\(f\\) satisfies a Lipschitz condition on \\(D\\) in the variable \\(y\\) , then the IVP \\[ y'(t) = f(t, y),\\ a \\le t \\le b,\\ y(a) = \\alpha, \\] is well-posed . Besides, we also need to discuss the sufficient condition of m th-order system of first-order IVP . Similarly, we have the definition of Lipschitz condition and it's sufficient for the uniqueness of the solution. Definition \\(f(t, y_1, \\dots, y_m)\\) define on the convex set \\[ D = \\{(t, y_1, \\dots, y_m | a \\le t \\le b, -\\infty < u_i < \\infty, \\text{ for each } i = 1, 2, \\dots, m\\} \\] is said to satisfy Lipschitz condition on \\(D\\) in the variables \\(y_1, \\dots, y_m\\) if \\(\\exists L > 0\\) , s.t \\[ \\begin{align} & \\forall (t, y_1, \\dots, y_n), (t, z_1, \\dots, z_n) \\in D, \\\\ & |f(t, y_1, \\dots, y_m) - f(t, z_1, \\dots, z_m)| \\le L \\sum\\limits_{j = 1}^m |y_j - z_j| \\end{align} \\] Theorem if \\(f(t, y_1, \\dots, y_m)\\) satisfy Lipschitz condition, then the m th order systems of first-order IVP has a unique solution \\(y_1(t), \\dots, y_n(t)\\) .","title":"Availability and Uniqueness"},{"location":"Mathematics_Basis/NA/Chap_5/#eulers-method","text":"To measure the error of Taylor methods (Euler's method is Taylor's method of order 1), we first give the definition of local truncation error , which somehow measure the truncation error at a specified step. Definition The difference method \\[ \\begin{align} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + h\\phi(t_i, w_i). \\end{align} \\] has the local truncation error \\[ \\tau_{i + 1}(h) = \\frac{y_{i + 1} - (y_i + h\\phi(t_i, y_i))}{h} = \\frac{y_{i + 1} - y_i}{h} - \\phi(t_i, y_i). \\]","title":"Euler's Method"},{"location":"Mathematics_Basis/NA/Chap_5/#forward-explicit-eulers-method","text":"We use Taylor's Theorem to derive Euler's method. Suppose \\(y(t)\\) is the unique solution of IVP, for each mesh points, we have \\[ y(t_{i + 1}) = y(t_i) + hy'(t_i) + \\frac{h^2}{2}y''(\\xi_i), \\xi_i \\in [t_i, t_{i + 1}], \\] and since \\(y'(t_i) = f(t_i, y(t_i))\\) , we have \\[ y(t_{i + 1}) = y(t_i) + hf(t_i, y(t_i)) + \\frac{h^2}{2}y''(\\xi_i), \\xi_i \\in [t_i, t_{i + 1}], \\] If \\(w_i \\approx y(t_i)\\) and we delete the remainder term, we get the (Explicit) Euler's method , \\[ \\begin{align} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + hf(t_i, w_i). \\text{(difference equations)} \\end{align} \\] The local truncation error of Euler's Method is \\[ \\tau_{i + 1}(h) = \\frac{hy'(t_i) + \\frac{h^2}{2}y''(\\xi_i)}{h} - f(t_i, y_i) = \\frac{h}{2}y''(\\xi_i) = O(h). \\] Theorem The error bound of Euler's method, \\[ |y(t_i) - w_i| \\le \\frac{hM}{2L}\\left[e^{L(t_i - a)} - 1\\right], \\] where \\(M\\) is the upper bound of \\(|y''(t)|\\) . Although it has an exponential term (which is not such an accurate error bound), the most important intuitive is that the error bound is proportional to \\(h\\) . If we consider the roundoff error, we have the following theorem. Theorem Suppose there exists roundoff error and \\(u_i = w_i + \\delta_i, i = 0, 1, \\dots\\) , then \\[ |y(t_i) - u_i| \\le \\frac{1}{L}\\left(\\frac{hM}{2} + \\frac{\\delta}{h}\\right)\\left[e^{L(t_i - a)} - 1\\right] + |\\delta_0|e^{L(t_i - a)}, \\] where \\(\\delta\\) is the upper bound of \\(\\delta_i\\) . This comes to a different case, we can't take \\(h\\) to be too small, and the optimal choice of \\(h\\) is \\[ h = \\sqrt{\\frac{2\\delta}{M}}. \\]","title":"Forward / Explicit Euler's Method"},{"location":"Mathematics_Basis/NA/Chap_5/#backward-implicit-eulers-method","text":"Inversely, if we use Taylor's Theorem in the following way, \\[ y(t_i) = y(t_{i + 1}) - hy'(t_{i + 1}) + \\frac{h^2}{2}y''(\\xi_{i + 1}), \\xi_i \\in [t_i, t_{i + 1}]. \\] Thus we get the Implicit Euler's Method , \\[ \\begin{align} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + hf(t_{i + 1}, w_{i + 1}). \\end{align} \\] Since \\(w_{i + 1}\\) is on the both side, we may use some methods in Chapter 2 to solve out the equation. The local truncation error of implicit Euler's method is \\[ \\begin{align} \\tau_{i + 1}(h) &= \\frac{hy'(t_i) + \\frac{h^2}{2}y''(\\xi_i)}{h} - f(t_{i + 1}, y_{i + 1}) \\\\ &= y'(t_i) - y'(t_{i + 1}) + \\frac{h}{2}y''(\\xi_i) \\\\ &= y'(t_i) - y'(t_i) - hy''(\\xi'_i) + \\frac{h}{2}y''(\\xi_i) \\\\ &= -\\frac{h}{2}y''(\\eta_i) = O(h). \\end{align} \\]","title":"Backward/ Implicit Euler's Method"},{"location":"Mathematics_Basis/NA/Chap_5/#trapezoidal-method","text":"Notice the local truncation errors of two Euler's method are \\[ \\tau_{i + 1}(h) = \\frac{h}{2}y''(\\xi'), \\tau_{i + 1}(h) = -\\frac{h}{2}y''(\\eta'), \\] If we combine these two, we then obtain a method of \\(O(h^2)\\) local truncation error, which is called Trapezoidal Method . \\[ \\begin{align} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + \\frac{h}{2}[f(t_i, w_i) + f(t_{i + 1}, w_{i + 1})]. \\end{align} \\]","title":"Trapezoidal Method"},{"location":"Mathematics_Basis/NA/Chap_5/#higher-order-taylor-method","text":"In Euler's methods, we only employ first order Taylor polynomials, if we expand it more to n th Taylor polynomials, we have \\[ \\begin{align} y(t_{i + 1}) &= y(t_i) + hy'(t_i) + \\frac{h^2}{2}y''(t_i) + \\cdots + \\frac{h^n}{n!}y^{(n)}(t_i) + \\frac{h^{n + 1}}{(n + 1)!} y^{(n + 1)}(\\xi_i), \\\\ & \\text{ where } \\xi_i \\in [t_i, t_{i + 1}], \\end{align} \\] and since \\(y^{(k)}(t) = f^{(k - 1)}(t, y(t))\\) , we have \\[ \\begin{align} y(t_{i + 1}) &= y(t_i) + hf(t_i, y(t_i)) + \\frac{h^2}{2}f'(t_i, y(t_i)) + \\cdots + \\frac{h^n}{n!}f^{(n - 1)}(t_i, y(t_i)) \\\\ & \\ \\ \\ \\ + \\frac{h^{n + 1}}{(n + 1)!} f^{(n)}(\\xi_i, y(\\xi_i)). \\end{align} \\] Taylor Method of order \\(n\\) \\[ \\begin{align} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + hT^{(n)}(t_i, w_i), \\end{align} \\] where \\[ T^{(n)}(t_i, w_i) = f(t_i, w_i) + \\frac{h}{2}f'(t_i, w_i) + \\cdots + \\frac{h^{n - 1}}{n!}f^{(n - 1)}(t_i, w_i). \\] Theorem The local truncation error of Taylor method of order \\(n\\) is \\(O(h^n)\\) .","title":"Higher-Order Taylor Method"},{"location":"Mathematics_Basis/NA/Chap_5/#runge-kutta-method","text":"Although Taylor method gives more accuracy as \\(n\\) increase, but it's not an easy job to calculate n th derivative of \\(f\\) . Here we introduce a new method called Runge-Kutta Method , which has low local truncation error and doesn't need to compute derivatives. It's based on Taylor's Theorem in two variables . Theorem | Taylor's Theorem in two variables (Recap) \\(f(x, y)\\) has continous \\(n + 1\\) partial derivatives on the neigbourhood of \\(A(x_0, y_0)\\) , \\(U(A)\\) , then \\(\\forall (x, y) \\in U(A)\\) , denote \\(\\Delta x = x - x_0\\) , \\(\\Delta y = y - y_0\\) , \\(\\exists \\theta \\in (0, 1)\\) , s.t. \\[ f(x, y) = P_n(x, y) + R_n(x, y), \\] where \\[ \\small \\begin{align} P_n(x, y) &= f(x_0, y_0) + \\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)f(x_0, y_0) + \\frac{1}{2!}\\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)^2f(x_0, y_0) + \\\\ & \\ \\ \\ \\ \\ \\cdots + \\frac{1}{n!}\\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)^nf(x_0, y_0), \\end{align} \\] \\[ \\small R_n(x, y) = \\frac{1}{(n + 1)!}\\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)^{n + 1}f(x_0 + \\theta \\Delta x, y_0 + \\theta \\Delta y). \\] and \\[ \\small \\left(\\Delta x\\frac{\\partial}{\\partial x} + \\Delta y \\frac{\\partial}{\\partial y}\\right)^{k}f(x, y) = \\sum\\limits_{i = 0}^k \\binom{k}{i} (\\Delta x)^i (\\Delta y)^{k - i} \\frac{\\partial^k f}{\\partial x^i \\partial y^{k - i}}(x, y). \\] Derivation of Midpoint Method To equate \\[ T^{(2)}(t, y) = f(t, y) + \\frac{h}{2}f'(t, y) = f(t, y) + \\frac{h}{2}\\frac{\\partial f}{\\partial t}(t,y) + \\frac{h}{2}\\frac{\\partial f}{\\partial y}(t,y)\\cdot f(t, y). \\] with \\[ \\begin{align} a_1f(t + \\alpha_1, y + \\beta_1) &= a_1f(t, y) + a_1 \\alpha_1 \\frac{\\partial f}{\\partial t}(t, y) \\\\ & \\ \\ \\ \\ + a_1 \\beta_1 \\frac{\\partial f}{\\partial y}(t, y) + a_1 \\cdot R_1(t + \\alpha_1, y + \\beta_1). \\end{align} \\] except for the last residual term \\(R_1\\) , we have \\[ a_1 = 1,\\ \\ \\alpha_1 = \\frac{h}{2},\\ \\ beta_1 = \\frac{h}{2}f(t, y). \\] Thus \\[ T^{(2)}(t, y) = f\\left(t + \\frac{h}{2}, y + \\frac{h}{2}f(t, y)\\right) - R_1. \\] Since the term \\(R_1\\) is only concerned with the 2 nd -order partial derivatives of \\(f\\) , if they are bounded, then \\(R_1\\) is \\(O(h^2)\\) , the order of the local truncation error. Replace \\(T^{(2)}(t,y)\\) by the formula above in the Taylor method of order 2, we have the Midpoint Method . Midpoint Method \\[ \\begin{align} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + hf\\left(t_i + \\frac{h}{2}, w_i + \\frac{h}{2}f(t_i, w_i)\\right). \\end{align} \\] Derivation of Modified Euler Method and Heun's Method Similarly, approximate \\[ T^{(3)}(t, y) = f(t, y) + \\frac{h}{2}f'(t, y) + \\frac{h^2}{6}f''(t,y), \\] with \\[ a_1f(t,y) + a_2f(t + \\alpha_2, y + \\delta_2f(t,y)). \\] But it doesn't have sufficient equation to determine the 4 unknowns \\(a_1, a_2, \\alpha_2, \\delta_2\\) above. But we have \\[ a_1 + a_2 = 1, \\ \\alpha_2 = \\delta_2\\ \\overset{\\Delta}{=\\!=}\\ ph. \\] Thus a number of \\(O(h^2)\\) methods can be derived, and they are generally in the form \\[ \\begin{align} w_{i + 1} &= w_i + h(a_1 K_1 + a_2 K_2) \\\\ K_1 &= f(t_i, w_i), \\\\ K_2 &= f(t_i + ph, y + ph K_1) \\end{align} \\] The most important are the following two. Modified Euler Method \\[ \\begin{align} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + \\frac{h}{2}[f(t_i, w_i) + f(t_i + h, w_i + hf(t_i, w_i))]. \\end{align} \\] Heun's Method \\[ \\begin{align} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + \\frac{h}{4}\\left[f(t_i, w_i) + 3f\\left(t_i + \\frac23 h, w_i + \\frac23 hf(t_i, w_i)\\right)\\right]. \\end{align} \\] In a similar manner, approximate \\(T^{(n)}(t, y)\\) with \\[ \\lambda_1 K_1 + \\lambda_2 K_2 + \\cdots + \\lambda_m K_m, \\] where \\[ \\begin{align} K_1 &= f(t, y), \\\\ K_2 &= f(t + \\alpha_2 h, y + \\beta_{21} h K_1), \\\\ & \\vdots \\\\ K_m &= f(t + \\alpha_m h, y + \\beta_{m1} h K_1 + \\beta_{m, m - 1} hK_{m - 1}). \\end{align} \\] with different \\(m\\) , we can derive quite a lot of Runge-Kutta methods. The most common Runge-Kutta method is given below. Runge-Kutta Order Four \\[ \\begin{align} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + \\frac{h}{6}(K_1 + 2K_2 + 2K_3 + K_4), \\\\ K_1 &= f(t_i, w_i), \\\\ K_2 &= f\\left(t_i + \\frac{h}{2}, w_i + \\frac{h}{2}K_1\\right), \\\\ K_3 &= f\\left(t_i + \\frac{h}{2}, w_i + \\frac{h}{2}K_2\\right), \\\\ K_4 &= f\\left(t_i + h, w_i + h K_3\\right). \\end{align} \\] The local trancation error of Runge-Kutta are given below, \\(n\\) is the number of evaluations per step. Property Compared to Taylor's method, evaluate \\(f(t, y)\\) the same times , Runge-Kutta gives the lowest error.","title":"Runge-Kutta Method"},{"location":"Mathematics_Basis/NA/Chap_5/#multistep-method","text":"In the previous section, the difference equation only relates \\(w_{i + 1}\\) with \\(w_{i}\\) . Multistep Method discuss the situation of relating more term of previous predicted \\(w\\) . Definition An m -step multistep method for solving the IVP \\[ y'(t) = f(t, y),\\ a \\le t \\le b,\\ y(a) = \\alpha, \\] has a difference equation \\[ \\begin{align} w_{i + 1} &= a_{m - 1}w_i + a_{m - 2}w_{i - 1} + \\cdots a_0 w_{i + 1 - m} + h[b_mf(t_{i + 1}, w_{i + 1}) \\\\ & \\ \\ \\ \\ + b_{m - 1}f(t_i, w_i) + \\cdots + b_0 f(t_{i + 1 - m}, w_{i + 1 - m})], \\end{align} \\] with the starting values \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\dots\\ , w_{m - 1} = \\alpha_{m - 1}. \\] If \\(b_m = 0\\) , it's called explicit , or open . Otherwise, it's called implicit , or closed . Similarly, we first define local truncation error for multistep method to measure the error at a specified step. Definition For a m -step multistep method, the local truncation error at \\((i + 1)\\) st step is \\[ \\begin{align} \\tau_{i + 1}(h) &= \\frac{y(t_{i + 1}) - a_{m - 1}y(t_i) - \\cdots - a_0 y(t_{i + 1 - m})}{h} \\\\ & \\ \\ \\ \\ - b_{m - 1}f(t_i, y(t_{i + 1})) + \\cdots + b_0 f(t_{i + 1 - m}, y(t_{i + 1 - m}))], \\end{align} \\]","title":"Multistep Method"},{"location":"Mathematics_Basis/NA/Chap_5/#adams-method","text":"In this section, we only consider that \\(a_{m - 1} = 1\\) and \\(a_i = 0, i \\ne m - 1\\) . To derive the Adams method, we start from a simple formula. \\[ y(t_{i + 1}) - y(t_i) = \\int_{t_i}^{t_{i + 1}}y'(t)dt = \\int_{t_i}^{t_{i + 1}}f(t, y(t))dt. \\] i.e. \\[ y(t_{i + 1}) = y(t_i) + \\int_{t_i}^{t_{i + 1}}f(t, y(t))dt. \\] We cannot integrate \\(f(t, y(t))\\) without knowing \\(y(t)\\) . Instead we use an interpolating polynomial \\(P(t)\\) by points \\((t_0, w_0), \\dots (t_i, w_i)\\) to approximate \\(f(t, y)\\) . Thus we approximate \\(y(t_{i + 1})\\) by \\[ y(t_{i + 1}) \\approx w_i + \\int_{t_i}^{t_{i + 1}} P(t) dt. \\] For convenience, we use Newton backward-difference formula to represent \\(P(t)\\) .","title":"Adams Method"},{"location":"Mathematics_Basis/NA/Chap_5/#adams-bashforth-m-step-explicit-method","text":"To derive an Adam-Bashforth explicit m -step technique , we form the interpolating polynomial \\(P_{m - 1}(t)\\) by \\[ (t_i, f(t_i, y(t_i))), \\dots (t_{i + 1 - m}, f(t_{i + 1 - m}, y(t_{i + 1 - m}))). \\] Then suppose \\(t = t_i + sh\\) , with \\(dt = hds\\) , we have \\[ \\begin{align} \\int_{t_i}^{t_{i + 1}} f(t, y(t)) dt &= \\int_{t_i}^{t_{i + 1}} \\sum\\limits_{k = 0}^{m - 1}(-1)^k \\binom{-s}{k}\\nabla^k f(t_i, y(t_i))dt \\\\ & \\ \\ \\ \\ + \\int_{t_i}^{t_{i + 1}} \\frac{f^{(m)}(\\xi_i, y(\\xi_i))}{m!}\\prod\\limits_{k = 0}^{m - 1}(t - t_{i - k})dt \\\\ \\left(\\text{Note that } dt = hds\\right) &= h\\sum\\limits_{k = 0}^{m - 1}\\nabla^k (-1)^k \\int_0^1 \\binom{-s}{k}ds \\\\ & \\ \\ \\ \\ + h^{m + 1}\\int_0^1 \\binom{-s}{m}f^{(m)}(\\xi_i, y(\\xi_i))ds. \\\\ \\left(\\substack{\\text{Weighted Mean Value} \\\\ \\text{Theorem for Integral}}\\right) &= h\\sum\\limits_{k = 0}^{m - 1}\\nabla^k (-1)^k \\int_0^1 \\binom{-s}{k}ds \\\\ & \\ \\ \\ \\ + h^{m + 1}f^{(m)}(\\mu_i, y(\\mu_i))\\int_0^1 \\binom{-s}{m}ds. \\end{align} \\] Since for given \\(k\\) , we have \\(k\\) \\(0\\) \\(1\\) \\(2\\) \\(3\\) \\(4\\) \\((-1)^k\\int_0^1 \\binom{-s}{k}ds\\) \\(1\\) \\(\\frac12\\) \\(\\frac{5}{12}\\) \\(\\frac38\\) \\(\\frac{251}{720}\\) Thus \\[ \\begin{align} y(t_{i + 1}) &= y(t_i) + h\\left[f(t_i, y(t_i)) + \\frac12 \\nabla f(t_i, y(t_i)) + \\frac{5}{12} \\nabla^2 f(t_i, y(t_i)) + \\cdots\\right] \\\\ & \\ \\ \\ \\ + h^{m + 1}f^{(m)}(\\mu_i, y(\\mu_i))\\int_0^1 \\binom{-s}{m}ds. \\end{align} \\] Adams-Bashforth m -Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1, \\dots, w_{m - 1} = \\alpha_{m - 1}, \\] \\[ w_{i + 1} = w_i + h\\sum\\limits_{k = 0}^{m - 1}\\nabla^k (-1)^k \\int_0^1 \\binom{-s}{k}ds, \\] with local truncation error \\[ \\tau_{i + 1}(h) = h^{m + 1}f^{(m)}(\\mu_i, y(\\mu_i))\\int_0^1 \\binom{-s}{m}ds. \\] Adams-Bashforth Two-Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1, \\] \\[ w_{i + 1} = w_i + \\frac{h}{2}[3f(t_i, w_i) - f(t_{i - 1}, w_{i - 1})], \\] with local truncation error \\[ \\tau_{i + 1}(h) = \\frac{5}{12}y'''(\\mu_i)h^2. \\] Adams-Bashforth Three-Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2, \\] \\[ w_{i + 1} = w_i + \\frac{h}{12}[23f(t_i, w_i) - 16f(t_{i - 1}, w_{i - 1}) + 5f(t_{i - 2}, w_{i - 2})], \\] with local truncation error \\[ \\tau_{i + 1}(h) = \\frac{3}{8}y^{(4)}(\\mu_i)h^3. \\] Adams-Bashforth Four-Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2,\\ \\ w_3 = \\alpha_3, \\] \\[ \\begin{align} w_{i + 1} = w_i + \\frac{h}{24}&[55f(t_i, w_i) - 59f(t_{i - 1}, w_{i - 1}) \\\\ & + 37f(t_{i - 2}, w_{i - 2}) - 9f(t_{i - 3}, w_{i - 3})], \\end{align} \\] with local truncation error \\[ \\tau_{i + 1}(h) = \\frac{251}{720}y^{(5)}(\\mu_i)h^4. \\]","title":"Adams-Bashforth m-Step Explicit Method"},{"location":"Mathematics_Basis/NA/Chap_5/#adams-moulton-m-step-implicit-method","text":"For implicit case, we use \\((t_{i + 1}, f(t_{i + 1}, y(t_{i + 1})))\\) instead of \\((t_{i + 1 - m}, f(t_{i + 1 - m}, y(t_{i + 1 - m})))\\) to constrcut the interpolating polynomials. Similarly we have the conclusions below. Adams-Moulton m -Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1, \\dots, w_{m - 1} = \\alpha_{m - 1}, \\] \\[ w_{i + 1} = w_i + h\\sum\\limits_{k = 0}^{m - 1}\\nabla^k (-1)^k \\int_0^1 \\binom{-s}{k}ds, \\] with local truncation error \\[ \\tau_{i + 1}(h) = h^{m + 1}f^{(m)}(\\mu_i, y(\\mu_i))\\int_0^1 \\binom{-s}{m}ds. \\] Adams-Moulton Two-Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1, \\] \\[ w_{i + 1} = w_i + \\frac{h}{12}[5f(t_{i + 1}, w_{i + 1}) + 8f(t_i, w_i) - f(t_{i - 1}, w_{i - 1})], \\] with local truncation error \\[ \\tau_{i + 1}(h) = -\\frac{1}{24}y^{(4)}(\\mu_i)h^3. \\] Adams-Moulton Three-Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2, \\] \\[ w_{i + 1} = w_i + \\frac{h}{24}[9f(t_{i + 1}, w_{i + 1}) + 19f(t_i, w_i) - 5f(t_{i - 1}, w_{i - 1}) + f(t_{i - 2}, w_{i - 2})], \\] with local truncation error \\[ \\tau_{i + 1}(h) = -\\frac{19}{720}y^{(5)}(\\mu_i)h^4. \\] Adams-Moulton Four-Step Explicit Method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\ w_2 = \\alpha_2,\\ \\ w_3 = \\alpha_3, \\] \\[ \\begin{align} w_{i + 1} = w_i + \\frac{h}{720}&[251f(t_{i + 1}, w_{i + 1}) + 646f(t_i, w_i) - 264f(t_{i - 1}, w_{i - 1}) \\\\ & + 106f(t_{i - 2}, w_{i - 2}) - 19f(t_{i - 3}, w_{i - 3})], \\end{align} \\] with local truncation error \\[ \\tau_{i + 1}(h) = -\\frac{3}{160}y^{(6)}(\\mu_i)h^5. \\] NOTE that comparing an m -step Adams-Bashforth explicit method and (m - 1) -step Adams-Moulton implicit method, they both involve m evaluation of \\(f\\) per step, and both have the local truncation error of the term \\(ky^{(m + 1)}(\\mu_i)h^m\\) . And the latter implicit method has the smaller coefficient \\(k\\) . This leads to greater stability and smaller roundoff errors for implicit methods.","title":"Adams-Moulton m-Step Implicit Method"},{"location":"Mathematics_Basis/NA/Chap_5/#predictor-corrector-method","text":"Implicit method has some advantages but it's hard to calculate. We could use some iterative methods introduced in Chapter 2 to solve it but it complicates the process considerably. In practice, since explicit method is easy to calculate, we combine the explicit and implicit method to predictor-corrector method . Predictor-Corrector Method Step.1 Compute first \\(m\\) initial values by Runge-Kutta method. Step.2 Predict by Adams-Bashforth explicit method. Step.3 Correct by Adams-Moulton implicit method. NOTE: All the formulae used in the three steps have the same order of truncation error. Example Take \\(m = 4\\) which is the most common case as exmaple. Step.1 From initial value \\(w_0 = \\alpha\\) , we use Runge-Kutta method of order four to derive \\(w_1\\) , \\(w_2\\) and \\(w_3\\) . Set \\(i = 3\\) . Step.2 Use four-step Adams-Bashforth explicit method, we have \\[ \\small w_{i + 1}^{(0)} = w_i + \\frac{h}{24}[55f(t_i, w_i) - 59f(t_{i - 1}, w_{i - 1}) + 37f(t_{i - 2}, w_{i - 2}) - 9f(t_{i - 3}, w_{i - 3})], \\] Step.3 Use three-step Adams-Moulton \\[ \\small w_{i + 1}^{(1)} = w_i + \\frac{h}{24}[9f(t_{i + 1}, w_{i + 1}^{(1)}) + 19f(t_i, w_i) - 5f(t_{i - 1}, w_{i - 1}) + f(t_{i - 2}, w_{i - 2})], \\] Then we have the two options \\(i = i + 1\\) , and go back to Step. 2 , approximate the next point. Or, for higher accuracy, we can repeat Step.3 iteratively by \\[ \\small w_{i + 1}^{(k + 1)} = w_i + \\frac{h}{24}[9f(t_{i + 1}, w_{i + 1}^{(k)}) + 19f(t_i, w_i) - 5f(t_{i - 1}, w_{i - 1}) + f(t_{i - 2}, w_{i - 2})]. \\]","title":"Predictor-Corrector Method"},{"location":"Mathematics_Basis/NA/Chap_5/#other-methods","text":"If we derive the multistep method by Taylor expansion , we can have more methods. We take an example to show it. Suppose we want to derive a difference equation in the form \\[ w_{i + 1} = a_0 w_{i - 1} + h(b_2 f(t_{i + 1}, w_{i + 1}) + b_1 f(t_i, w_i) + b_0 f(t_{i - 1}, w_{i - 1})). \\] We use \\(y_i\\) and \\(f_i\\) to denote \\(y(t_i)\\) and \\(f(t_i, y_i)\\) respectively. Expand \\(y_{i - 1}, f_{i + 1}, f_{i}, f_{i - 1}\\) , we have \\[ \\begin{align} y_{i - 1} &= y_i - hy_i' + \\frac12 h^2 y_i'' - \\frac16h^3y_i''' + O(h^4), \\\\ f_{i + 1} &= y_i' + hy_i'' + \\frac12 h^2 y_i''' + O(h^3), \\\\ f_i &= y_i', \\\\ f_{i - 1} &= y_i' - hy_i'' + \\frac12 h^2 y_i''' + O(h^3), \\\\ \\end{align} \\] and note that \\[ y_{i + 1} = y_i + hy_i' + \\frac12 h^2 y_i'' + \\frac16 h^3 y_i'''+ O(h^4). \\] Equate them with \\[ y_{i + 1} = a_0 y_{i - 1} + h(b_2 f_{i + 1} + b_1 f_i + b_0 f_{i - 1}). \\] We can solve out \\[ a_0 = 1,\\ \\ b_2 = \\frac13,\\ \\ b_1 = \\frac43,\\ \\ b_0 = \\frac13. \\] Thus \\[ w_{i + 1} = w_{i - 1} + \\frac{h}{3}(f(t_{i + 1}, w_{i + 1}) + 4f(t_i, w_i) + f(t_{i - 1}, w_{i - 1})). \\] Simpson Implicit Method \\[ w_0 = \\alpha,\\ w_1 = \\alpha_1, \\\\ w_{i + 1} = w_{i - 1} + \\frac{h}{3}(f(t_{i + 1}, w_{i + 1}) + 4f(t_i, w_i) + f(t_{i - 1}, w_{i - 1})). \\] with local truncation error \\[ \\tau_{i + 1}(h) = \\frac{h^4}{90}y^{(5)}(\\xi_i). \\] Note that it's an implicit method, and the most commonly used corresponding predictor is Milne's Method \\[ w_0 = \\alpha,\\ w_1 = \\alpha_1,\\ w_2 = \\alpha_2 \\\\ w_{i + 1} = w_{i - 3} + \\frac{4h}{3}(2f(t_i, w_i) + - f(t_{i - 1}, w_{i - 1}) + 2f(t_{i - 2}, w_{i - 2})). \\] with local truncation error \\[ \\tau_{i + 1}(h) = \\frac{14}{90}h^4 y^{(5)}(\\xi_i). \\] which can also derive by Taylor expansion in the similar manner.","title":"Other Methods"},{"location":"Mathematics_Basis/NA/Chap_5/#higher-order-systems-of-first-order-ivp","text":"Actually it just vectorize the method that we've mentioned above. Take Runge-Kutta Order Four as example. Runge-Kutta Method for Systems of Differential Equations \\[ \\begin{align} w_{1, 0} &= \\alpha_1, w_{2, 0} = \\alpha_2, \\dots, w_{m, 0} = \\alpha_m. \\\\ w_{i, j + 1} &= w_{i, j} + \\frac{h}{6}(K_{1, i} + 2K_{2, i} + 2K_{3, i} + K_{4, i}), \\\\ K_{1, i} &= f_i(t_j, w_{1, j}, w_{2, j}, \\dots, w_{m, j}), \\\\ K_{2, i} &= f_i\\left(t_j + \\frac{h}{2}, w_{1, j} + \\frac{h}{2}K_{1, 1}, w_{2, j} + \\frac{h}{2}K_{1, 2}, \\dots, w_{m, j} + \\frac{h}{2}K_{1, m}\\right), \\\\ K_{3, i} &= f_i\\left(t_j + \\frac{h}{2}, w_{1, j} + \\frac{h}{2}K_{2, 1}, w_{2, j} + \\frac{h}{2}K_{2, 2}, \\dots, w_{m, j} + \\frac{h}{2}K_{2, m}\\right), \\\\ K_{4, i} &= f_i\\left(t_j + h, w_{1, j} + h K_{3, 1}, w_{2, j} + h K_{3, 2}, \\dots, w_{m, j} + h K_{3. m} \\right). \\end{align} \\]","title":"Higher-Order Systems of First-Order IVP"},{"location":"Mathematics_Basis/NA/Chap_5/#higher-order-ivp","text":"We can deal with higher-order IVP the same as higher-order systems of first-order IVP. The only transformation we need to do is let \\(u_1(t) = y(t)\\) , \\(u_2(t) = y'(t)\\) , \\(\\dots\\) , and \\(u_m(t) = y^{(m - 1)}(t)\\) . This produces the first-order system \\[ \\begin{align} \\frac{du_1}{dt} &= \\frac{dy}{dt} = u_2, \\\\ \\frac{du_2}{dt} &= \\frac{dy'}{dt} = u_3, \\\\ & \\vdots \\\\ \\frac{du_{m - 1}}{dt} &= \\frac{dy^{(m - 2)}}{dt} = u_m, \\\\ \\frac{du_m}{dt} &= \\frac{dy^{(m - 1)}}{dt} = y^{(m)} = f(t, y', y'', \\dots, y^{(m - 1)}) = f(t, u_1, \\dots, u_m). \\end{align} \\] with initial conditions \\[ u_1(a) = y(a) = \\alpha_1, u_2(a) = y'(a) = \\alpha_2, \\dots, u_m(a) = y^{(m - 1)}(a) = \\alpha_m. \\]","title":"Higher-Order IVP"},{"location":"Mathematics_Basis/NA/Chap_5/#stability","text":"Definition A one-step difference-equation method with local truncation error \\(\\tau_i(h)\\) is consistent with the differential equation it approximates if \\[ \\lim\\limits_{h \\rightarrow 0} \\max\\limits_{1 \\le i \\le N}|\\tau_i(h)| = 0. \\] For m -step multistep methods, it's also required that \\[ \\lim\\limits_{h \\rightarrow 0} |\\alpha_i - y_i| = 0, i = 1, 2, \\dots, m - 1, \\] since at most cases, these \\(\\alpha_i\\) are derived by one-step methods. Definition A one-step / multistep difference-equation method is convergent w.r.t the differential equation it approximates if \\[ \\lim\\limits_{h \\rightarrow 0} \\max\\limits_{1 \\le i \\le N}|w_i - y(t_i)| = 0. \\] Definition A stable method is one whose results depent continously on the initial data, in the sense that small changes or perturbations in the intial conditions produce correspondingly small changes in the subsequent approximations.","title":"Stability"},{"location":"Mathematics_Basis/NA/Chap_5/#one-step-method","text":"It's relatively natural to think the stability is somewhat analogous to the discussion of well-posed, and thus it's not surprising Lipschitz condition appears. The following theorem gives the relation among consistency , convergence and stability . Theorem Suppose the IVP is approximated by a one-step method in the form \\[ \\begin{align} w_0 &= \\alpha, \\\\ w_{i + 1} &= w_i + h \\phi(t_i, w_i, h). \\end{align} \\] If \\(\\exists h_0 > 0\\) and \\(\\phi(t, w, h)\\) is continuous and satisfies a Lipschitz condition in the varaible \\(w\\) with Lipschitz constant \\(L\\) on the set \\[ D = \\{(t, w, h) | a \\le t \\le b, -\\infty < w < \\infty, 0 \\le h \\le h_0\\}. \\] Then The method is stable . The difference method is convergent iff it's consistent , which is equivalent to \\[ \\phi(t, y, 0) = f(t, y),\\ t \\in [a, b]. \\] (Relation between Global Error and Local Truncation Error) If \\(\\exists \\tau(h)\\) s.t. \\(|\\tau_i(h)| \\le \\tau(h)\\) whenever \\(0 \\le h \\le h_0\\) , then \\[ |y(t_i) - w_i| \\le \\frac{\\tau(h)}{L}e^{L(t_i - a)}. \\]","title":"One Step Method"},{"location":"Mathematics_Basis/NA/Chap_5/#multistep-method_1","text":"Theorem | Relation between Global Error and Local Truncation Error Suppose the IVP is approximated by a predictor-corrector method with an m -step Adams-Bashforth predictor equation \\[ w_{i + 1} = w_i + h[b_{m - 1}f(t_i, w_i) + \\cdots + b_0f(t_{i + 1 - m}, w_{i + 1 - m})], \\] with local truncation error \\(\\tau_{i + 1}(h)\\) , and an (m - 1) -step Adams-Moulton corrector eqation \\[ w_{i + 1} = w_i + h[\\tilde b_{m - 1}f(t_i, w_i) + \\cdots + \\tilde b_0f(t_{i + 1 - m}, w_{i + 1 - m})], \\] with local truncation error \\(\\tilde \\tau_{i + 1}(h)\\) . In addition, suppose \\(f(t, y)\\) and \\(f_y(t, y)\\) are continous on \\(D = \\{(t, y) | a \\le t \\le b, -\\infty < y < \\infty\\}\\) and \\(f_y\\) is bounded. Then the local truncation error \\(\\sigma_{i + 1}(h)\\) of the predictor-corrector method is \\[ \\small \\sigma_{i + 1}(h) = \\tilde \\tau_{i + 1}(h) + \\tau_{i + 1}(h) \\tilde b_{m - 1} \\frac{\\partial f}{\\partial y}(t_{i + 1}, \\theta_{i + 1}),\\ \\text{ for some } \\theta_{i + 1} \\in (0, h\\tau_{i + 1}(h)) \\] Moreover, \\(\\exists k_1, k_2\\) , s.t. \\[ |w_i - y(t_i)| \\le \\left[\\max\\limits_{0 \\le j \\le m - 1}|w_j - y(t_j)| + k_1\\sigma(h) \\right] e^{k_2(t_i - a)}, \\] where \\(\\sigma(h) = \\max\\limits_{m \\le j \\le N}|\\sigma_j(h)|\\) . For the difference equation of multistep method, we first introduce characteristc polynomial associated with the method, given by \\[ P(\\lambda) = \\lambda^m - a_{m - 1}\\lambda^{m - 1} - a_{m - 2}\\lambda^{m - 2} - \\cdots - a_1 \\lambda - a_0. \\] Definition Suppose the roots of the characteristic equation \\[ P(\\lambda) = \\lambda^m - a_{m - 1}\\lambda^{m - 1} - a_{m - 2}\\lambda^{m - 2} - \\cdots - a_1 \\lambda - a_0 = 0 \\] associated with the multistep difference method \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\dots\\ , w_{m - 1} = \\alpha_{m - 1}, \\\\ w_{i + 1} = a_{m - 1}w_i + a_{m - 2}w_{i - 1} + \\cdots a_0 w_{i + 1 - m} + hF(t_i, h, w_{i + 1}, w_i \\dots, w_{i + 1 - m}). \\] If \\(|\\lambda_i| \\le 1\\) , for each \\(i = 1, 2, \\dots, m\\) and all roots with modulus value 1 are simple roots, then the difference method is said to satisfy the root condition . Methods that satisfy the root condition and have \\(\\lambda = 1\\) as the only root of the characteristic equation of magnitude one are called strongly stable . Methods that satisfy the root condition and have more than one disctinct root with magnitude one are called weakly stable . Methods that do not satisfy the root condition are called unstable . Theorem A multistep method of the form \\[ w_0 = \\alpha,\\ \\ w_1 = \\alpha_1,\\ \\dots\\ , w_{m - 1} = \\alpha_{m - 1}, \\\\ w_{i + 1} = a_{m - 1}w_i + a_{m - 2}w_{i - 1} + \\cdots a_0 w_{i + 1 - m} + hF(t_i, h, w_{i + 1}, w_i \\dots, w_{i + 1 - m}). \\] is stable iff it satisfies the root condition. Moreover, if the difference method is consistent with the differential equation, then the method is stable iff it is convergent .","title":"Multistep Method"},{"location":"Mathematics_Basis/NA/Chap_5/#stiff-equations","text":"However, a special type of solution is not easy to deal with, when the magnitude of derivative increases but the solution does not. They are call stiff equations . Stiff differential eqaution are characterized as those whose exact solution has a term of the form \\(e^{-ct}\\) , where \\(c\\) is a large positive constant. This is called the transient solution. Actaully there is another important part of solution called steady-state solution. Let's first define test equation to examine what happen with the stiff cases. Definition A test equation is the IVP \\[ y' = \\lambda y,\\ \\ y(0) = \\alpha,\\ \\ \\text{ where } \\text{Re}(\\lambda) < 0. \\] The solution is obviously \\(y(t) = \\alpha e^{\\lambda t}\\) , which is the transient solution. At this case, the steady-state solution is zero, thus the approximation characteristics of a method are easy to determine. Example of Euler's Method Take Euler's method as an example, and denote \\(H = h\\lambda\\) . We have \\[ w_0 = \\alpha, \\\\ w_{i + 1} = w_i + h(\\lambda w_i) = (1 + h\\lambda)w_i = (1 + H)w_i, \\] so \\[ w_{i + 1} = (1 + H)^{i + 1}w_0 = (1 + H)^{i + 1}\\alpha. \\] The absolute error is \\[ |y(t_i) - w_i| = |(e^H)^i - (1 + H)^i||\\alpha|. \\] When \\(H < 0\\) , \\((e^H)^i\\) decays to zero as \\(i\\) increases. Thus we want \\((1 + H)^i\\) to decay too, which implies that \\[ |1 + H| < 1. \\] From another perspective, consider the roundoff error of the initial value \\(\\alpha\\) by, \\[ w_0 = \\alpha + \\delta_0, \\] At the i th step the roundoff error is \\[ \\delta_i = (1 + H)^i \\delta_0. \\] To control the error, we also want \\(|1 + H| < 1\\) . In general, when applying to the test equation, we have the difference equation of the form \\[ w_{i + 1} = Q(H)w_i,\\ \\ H = h \\lambda. \\] To make \\(Q(H)\\) approximate \\(e^H\\) , we want at least \\(|Q(H)| < 1\\) to make it decay to 0. The inequality delimit a region in a complex plane. For a multistep method, the difference equation is in the form \\[ (1 - H b_m)w_{i + 1} - (a_{m - 1} + H b_{m - 1})w_i - \\cdots - (a_0 + H b_0)w_{i + 1 - m} = 0. \\] We can define a characteristic polynomial \\[ Q(z, H) = (1 - H b_m)z^m - (a_{m - 1} + H b_{m - 1})z^{m - 1} - \\cdots - (a_0 + H b_0). \\] Suppose \\(\\beta_i\\) are the distinct roots of \\(Q(z, H)\\) , then it can be proved that \\(\\exists c_i\\) , s.t. \\[ w_{i} = \\sum\\limits_{k = 1}^m c_k\\beta_k^i. \\] Again, to make \\(w_i\\) approximate \\((e^H)^i\\) , we want at least all \\(|\\beta_k| < 1\\) to make it decay to 0. It also delimit a region in a complex plane. Definition The Region R of abosolute stability for a one-step method is \\[ R = \\{H \\in \\mathbb{C} | |Q(H)| < 1\\}, \\] and for a multistep method, \\[ R = \\{H \\in \\mathbb{C} | |\\beta_k| < 1, for all zero point of Q(z, H)\\}. \\] A numerical method is said to be A-stable if its region \\(R\\) of abosolute stability contains left half-plane , which means \\(\\text{Re}(\\lambda) < 0\\) , stable with stiff equation . Method A is said to be more stable than method B if the region of absolute stability of A is larger than that of B. The region of Euler's method is like Thus it's only stable for some cases of stiff equation. When the negative \\(\\lambda\\) becomes smaller and get out of the region, it becomes not stable. Similarly, for Runge-Kutta Order Four method (explicit), applying test equation, we have \\[ w_{i + 1} = \\left(1 + H + \\frac{H^2}{2} + \\frac{H^3}{6} + \\frac{H^4}{24}\\right)w_i. \\] And the region is like Let's consider some implicit method. Say Euler's implicit method, we have \\[ w_{i + 1} = \\frac{1}{1 - H} w_i, \\] whose region is Thus it's A-stable . Also, implicit Trapezoidal method and 2 nd -order Runge-Kutta implict method are A-stable , both with the difference equation, \\[ w_{i + 1} = \\frac{2 + H}{2 - H}w_i. \\] whose region is just right covers the left half-plane. Thus an important intuitive is that implicit method is more stable than explicit method in the stiff discussion .","title":"Stiff Equations"},{"location":"Mathematics_Basis/NA/Chap_6/","text":"Chapter 6 | Direct Methods for Solving Linear Systems \u00b6 Linear Systems of Equations \u00b6 Gaussian Elimination with Backward Substitution \u00b6 \\[ A\\mathbf{x} = \\mathbf{b} \\] Let \\(A^{(1)} = A, \\mathbf{b}^{(1)} = \\mathbf{b}\\) , For Step \\(k\\ (1\\le k \\le n-1)\\) , if \\(a^{(k)}_{kk}\\ne0\\) ( pivot element ), compute \\(m_{ik} = \\frac{a^{(k)}_{ik}}{a^{(k)}_{kk}}\\) and \\[ \\left\\{ \\begin{align} a_{ij}^{(k+1)} = a_{ij}^{(k)} - m_{ik}a_{kj}^{(k)} \\\\ b_i^{(k+1)} = b_i^{(k)} - m_{ik}b_k^{(k)} \\end{align} \\text{, where }i, j = k+1, \\dots, n \\right. \\] Elimination \u00b6 After \\(n-1\\) steps, \\[ \\begin{bmatrix} a^{(1)}_{11} & a^{(1)}_{12} & \\cdots & a^{(1)}_{1n} \\\\ & a^{(2)}_{22} & \\cdots & a^{(2)}_{2n} \\\\ & & \\cdots & \\vdots \\\\ & & & a^{(n)}_{nn} \\\\ \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix} b^{(1)}_1 \\\\ b^{(2)}_2 \\\\ \\vdots \\\\ b^{(n)}_n \\end{bmatrix} \\] Backward Substitution \u00b6 Then, \\[ x_n = \\frac{b_n^{(n)}}{a^{(n)}_{nn}}, x_i = \\frac{1}{a_{ii}^{(i)}}\\left(b^{(i)}_i - \\sum_{j = i + 1}^n a^{(i)}_{ij} x_j\\right), i = n-1, \\dots, 1 \\] Complexity \u00b6 For Elimination, \\[ \\sum_{k=1}^{n-1}(n - k)(n - k + 2) = \\frac{n^3}{3} + \\frac{n^2}{2} - \\frac{5}{6}n. \\] For Backward-substitution, \\[ 1 + \\sum_{k=1}^{n-1}(n - k + 1) = \\frac{n^2}{2} + \\frac{n}{2}. \\] In total, \\[ \\frac{n^3}{3} + n^2 - \\frac{n}{3}. \\] Pivoting Strategies \u00b6 Motivation: For Gaussian Elimination with Backward Substituion, if the pivot element \\(a_{kk}^{(k)}\\) is small compared to \\(a_{ik}^{(k)}\\) , then \\(m_{ik}\\) is large with high roundoff error . Thus we need some transformation to improve the accuracy. Partial Pivoting (a.k.a Maximal Column Pivoting) \u00b6 Determine the smallest \\(p \\ge k\\) such that \\[ \\left|a_{pk}^{(k)}\\right| = \\max_{k \\le i \\le n} \\left|a_{ik}^{(k)}\\right| \\] and interchange row \\(p\\) and row \\(k\\) . Requires \\(O(N^2)\\) additional comparisons . Scaled Partial Pivoting (a.k.a Scaled-Column Pivoting) \u00b6 Determine the smallest \\(p \\ge k\\) such that \\[ \\frac{\\left|a_{pk}^{(k)}\\right|}{s_p} = \\max\\limits_{k \\le i \\le n} \\frac{\\left|a_{ik}^{(k)}\\right|}{s_i} \\] and interchange row \\(p\\) and row \\(k\\) , where \\(s_i = \\max\\limits_{1 \\le j \\le n} \\left|a_{ij}\\right|\\) . (Simply put, Place the element in the pivot position that is largest relative to the entries in its row.) Requires \\(O(N^2)\\) additional comparisons and \\(O(N^2)\\) divisions . Complete Pivoting (a.k.a Maximal Pivoting) \u00b6 Search all the entries \\(a_{ij}\\) for \\(i,j = k, \\dots,n\\) to find the entry with the largest magnitude. Both row and column interchanges are performed to bring this entry to the pivot position. Requires \\(O\\left(\\frac{1}{3}N^3\\right)\\) additional comparisons . Matrix Factorization (LU Factorization) \u00b6 Considering the matrix form of Gaussian Elimination, for total \\(n-1\\) steps, \\[ L_{n-1}L_{n-2}\\dots L_1[A\\ \\textbf{b}] = \\begin{bmatrix} a^{(1)}_{11} & a^{(1)}_{12} & \\cdots & a^{(1)}_{1n} & b_1^{(1)} \\\\ & a^{(2)}_{22} & \\cdots & a^{(2)}_{2n} & b_2^{(2)} \\\\ & & \\cdots & \\vdots & \\vdots \\\\ & & & a^{(n)}_{nn} & b_n^{(n)} \\\\ \\end{bmatrix}, \\] where \\[ L_k = \\begin{bmatrix} 1 & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\ 0 & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \\vdots & \\ddots & 1 & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots & -m_{k+1, k} & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots& \\vdots & \\ddots & \\ddots & 0 \\\\ 0 & \\cdots & -m_{n, k} & \\cdots & \\cdots & 1 \\end{bmatrix} \\] It's simple to compute that \\[ L_k^{-1} = \\begin{bmatrix} 1 & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\ 0 & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \\vdots & \\ddots & 1 & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots & m_{k+1, k} & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots& \\vdots & \\ddots & \\ddots & 0 \\\\ 0 & \\cdots & m_{n, k} & \\cdots & \\cdots & 1 \\end{bmatrix}. \\] Thus we let \\[ L_1^{-1}L_2^{-1}\\dots L_{n-1}^{-1} = \\begin{bmatrix} 1 & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\ m_{1,2} & 1 & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \\vdots & \\ddots & 1 & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots &\\ddots& \\ddots & 1 & 0\\\\ m_{n,1} & \\cdots & \\cdots & \\cdots & m_{n, n-1} & 1 \\end{bmatrix} = L, \\] and \\[ U = \\begin{bmatrix} a^{(1)}_{11} & a^{(1)}_{12} & \\cdots & a^{(1)}_{1n} \\\\ & a^{(2)}_{22} & \\cdots & a^{(2)}_{2n} \\\\ & & \\cdots & \\vdots \\\\ & & & a^{(n)}_{nn} \\\\ \\end{bmatrix}. \\] Then we get \\[ A = LU \\] Theorem If Gaussian elimination can be performed on the linear system \\(A\\mathbf{x} = \\mathbf{b}\\) without row interchanges, then the matrix \\(A\\) can be factored into the product of a lower-triangular matrix \\(L\\) and an upper-triangular matrix \\(U\\) . If \\(L\\) has to be unitary , then the factorization is unique . Special Types of Matrices \u00b6 Strictly Diagonally Dominant Matrix \u4e25\u683c\u4e3b\u5bf9\u89d2\u5360\u4f18\u77e9\u9635 \u00b6 Definition The \\(n \\times n\\) matrix \\(A\\) is said to be strictly diagnoally dominant when \\[ |a_{ii}| \\gt \\sum_{j=1,j\\ne i}^{n}|a_{ij}|,\\text{ for each } i = 1, \\dots, n \\] Theorem A strictly diagonally dominant matrix is nonsingular . And Gaussian elimination can be performed without row or column interchanges, and computations will be stable with respect to the growth of roundoff errors.\uff08\u6ee1\u79e9\u3001\u65e0\u9700\u4ea4\u6362\u884c\u5217\u3001\u8bef\u5dee\u7a33\u5b9a\uff09 Positive Definite Matrix \u6b63\u5b9a\u77e9\u9635 \u00b6 Definition (Recap) A matrix \\(A\\) is positive definite if it's symmetric and if \\(\\mathbf{x}^tA\\mathbf{x} > 0\\) for every \\(n\\) -dimensional vector \\(\\mathbf{x} \\ne \\mathbf{0}\\) . Theorem If \\(A\\) is an \\(n \\times n\\) positive definite matrix, then \\(A\\) is nonsingular; \\(a_{ii} > 0\\) , for each \\(i = 1, 2, \\dots, n\\) ; \\(\\max\\limits_{1 \\le k, j \\le n}|a_{kj}| \\le \\max\\limits_{1 \\le i \\le n}|a_{ii}|\\) ; \\((a_{ij})^2 < a_{ii}a_{jj}\\) , for each \\(i \\ne j\\) . Choleski's Method (LDLt factorization) \u00b6 Further decompose \\(U\\) to \\(D\\tilde U\\) . \\(A\\) is symmetric \\(\\Rightarrow\\) \\(L = \\tilde U^t\\) . Thus \\[ A = LU = LD\\tilde U = LDL^t.\\ \\ (\u4e0b\u4e09\u89d2\u77e9\u9635\u4e58\u5bf9\u89d2\u77e9\u9635\u518d\u4e58\u5176\u8f6c\u7f6e) \\] Let \\[ D^{1/2} = \\begin{bmatrix} \\sqrt{u_{11}} & & & \\\\ & \\sqrt{u_{22}} & & \\\\ & & \\ddots & \\\\ & & & \\sqrt{u_{nn}} \\end{bmatrix}, \\] and \\(\\widetilde{L} = LD^{1/2}\\) . Then \\[ A = \\tilde L \\tilde L^t.\\ \\ (\u4e0b\u4e09\u89d2\u77e9\u9635\u4e58\u5176\u8f6c\u7f6e) \\] Tridiagonal Linear System \u4e09\u5bf9\u89d2\u77e9\u9635 \u00b6 Definition An \\(n \\times n\\) matrix \\(A\\) is called a band matrix if \\(\\exists p, q (1 < p, q < n)\\) , s.t. whenever \\(i + p \\le j\\) or \\(j + q \\le i\\) , \\(a_{ij} = 0\\) . And \\(w = p + q - 1\\) is called the bandwidth . Specially, if \\(p = q = 2\\) , then \\(A\\) is called tridiagonal , with the following form, \\[ \\begin{bmatrix} b_1 & c_1 & & & \\\\ a_2 & b_2 & c_2 & & \\\\ & \\ddots & \\ddots & \\ddots \\\\ & & a_{n-1} & b_{n-1} & c_{n-1} \\\\ & & & a_n & b_n \\\\ \\end{bmatrix} \\] Crout Factorization \u00b6 \\[ A = LU = \\begin{bmatrix} l_{11} \\\\ l_{21} & l_{22} \\\\ & \\ddots & \\ddots \\\\ & & \\ddots & \\ddots \\\\ & & & l_{n, n- 1} & l_{n, n} \\end{bmatrix} \\begin{bmatrix} 1 & u_{12}\\\\ & 1 & u_{23} \\\\ & & \\ddots & \\ddots \\\\ & & & \\ddots & u_{n-1,n}\\\\ & & & & 1 \\end{bmatrix} \\] the time complexity is \\(O(N)\\) .","title":"Chap 6"},{"location":"Mathematics_Basis/NA/Chap_6/#chapter-6-direct-methods-for-solving-linear-systems","text":"","title":"Chapter 6 | Direct Methods for Solving Linear Systems"},{"location":"Mathematics_Basis/NA/Chap_6/#linear-systems-of-equations","text":"","title":"Linear Systems of Equations"},{"location":"Mathematics_Basis/NA/Chap_6/#gaussian-elimination-with-backward-substitution","text":"\\[ A\\mathbf{x} = \\mathbf{b} \\] Let \\(A^{(1)} = A, \\mathbf{b}^{(1)} = \\mathbf{b}\\) , For Step \\(k\\ (1\\le k \\le n-1)\\) , if \\(a^{(k)}_{kk}\\ne0\\) ( pivot element ), compute \\(m_{ik} = \\frac{a^{(k)}_{ik}}{a^{(k)}_{kk}}\\) and \\[ \\left\\{ \\begin{align} a_{ij}^{(k+1)} = a_{ij}^{(k)} - m_{ik}a_{kj}^{(k)} \\\\ b_i^{(k+1)} = b_i^{(k)} - m_{ik}b_k^{(k)} \\end{align} \\text{, where }i, j = k+1, \\dots, n \\right. \\]","title":"Gaussian Elimination with Backward Substitution"},{"location":"Mathematics_Basis/NA/Chap_6/#elimination","text":"After \\(n-1\\) steps, \\[ \\begin{bmatrix} a^{(1)}_{11} & a^{(1)}_{12} & \\cdots & a^{(1)}_{1n} \\\\ & a^{(2)}_{22} & \\cdots & a^{(2)}_{2n} \\\\ & & \\cdots & \\vdots \\\\ & & & a^{(n)}_{nn} \\\\ \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix} b^{(1)}_1 \\\\ b^{(2)}_2 \\\\ \\vdots \\\\ b^{(n)}_n \\end{bmatrix} \\]","title":"Elimination"},{"location":"Mathematics_Basis/NA/Chap_6/#backward-substitution","text":"Then, \\[ x_n = \\frac{b_n^{(n)}}{a^{(n)}_{nn}}, x_i = \\frac{1}{a_{ii}^{(i)}}\\left(b^{(i)}_i - \\sum_{j = i + 1}^n a^{(i)}_{ij} x_j\\right), i = n-1, \\dots, 1 \\]","title":"Backward Substitution"},{"location":"Mathematics_Basis/NA/Chap_6/#complexity","text":"For Elimination, \\[ \\sum_{k=1}^{n-1}(n - k)(n - k + 2) = \\frac{n^3}{3} + \\frac{n^2}{2} - \\frac{5}{6}n. \\] For Backward-substitution, \\[ 1 + \\sum_{k=1}^{n-1}(n - k + 1) = \\frac{n^2}{2} + \\frac{n}{2}. \\] In total, \\[ \\frac{n^3}{3} + n^2 - \\frac{n}{3}. \\]","title":"Complexity"},{"location":"Mathematics_Basis/NA/Chap_6/#pivoting-strategies","text":"Motivation: For Gaussian Elimination with Backward Substituion, if the pivot element \\(a_{kk}^{(k)}\\) is small compared to \\(a_{ik}^{(k)}\\) , then \\(m_{ik}\\) is large with high roundoff error . Thus we need some transformation to improve the accuracy.","title":"Pivoting Strategies"},{"location":"Mathematics_Basis/NA/Chap_6/#partial-pivoting-aka-maximal-column-pivoting","text":"Determine the smallest \\(p \\ge k\\) such that \\[ \\left|a_{pk}^{(k)}\\right| = \\max_{k \\le i \\le n} \\left|a_{ik}^{(k)}\\right| \\] and interchange row \\(p\\) and row \\(k\\) . Requires \\(O(N^2)\\) additional comparisons .","title":"Partial Pivoting (a.k.a Maximal Column Pivoting)"},{"location":"Mathematics_Basis/NA/Chap_6/#scaled-partial-pivoting-aka-scaled-column-pivoting","text":"Determine the smallest \\(p \\ge k\\) such that \\[ \\frac{\\left|a_{pk}^{(k)}\\right|}{s_p} = \\max\\limits_{k \\le i \\le n} \\frac{\\left|a_{ik}^{(k)}\\right|}{s_i} \\] and interchange row \\(p\\) and row \\(k\\) , where \\(s_i = \\max\\limits_{1 \\le j \\le n} \\left|a_{ij}\\right|\\) . (Simply put, Place the element in the pivot position that is largest relative to the entries in its row.) Requires \\(O(N^2)\\) additional comparisons and \\(O(N^2)\\) divisions .","title":"Scaled Partial Pivoting (a.k.a Scaled-Column Pivoting)"},{"location":"Mathematics_Basis/NA/Chap_6/#complete-pivoting-aka-maximal-pivoting","text":"Search all the entries \\(a_{ij}\\) for \\(i,j = k, \\dots,n\\) to find the entry with the largest magnitude. Both row and column interchanges are performed to bring this entry to the pivot position. Requires \\(O\\left(\\frac{1}{3}N^3\\right)\\) additional comparisons .","title":"Complete Pivoting (a.k.a Maximal Pivoting)"},{"location":"Mathematics_Basis/NA/Chap_6/#matrix-factorization-lu-factorization","text":"Considering the matrix form of Gaussian Elimination, for total \\(n-1\\) steps, \\[ L_{n-1}L_{n-2}\\dots L_1[A\\ \\textbf{b}] = \\begin{bmatrix} a^{(1)}_{11} & a^{(1)}_{12} & \\cdots & a^{(1)}_{1n} & b_1^{(1)} \\\\ & a^{(2)}_{22} & \\cdots & a^{(2)}_{2n} & b_2^{(2)} \\\\ & & \\cdots & \\vdots & \\vdots \\\\ & & & a^{(n)}_{nn} & b_n^{(n)} \\\\ \\end{bmatrix}, \\] where \\[ L_k = \\begin{bmatrix} 1 & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\ 0 & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \\vdots & \\ddots & 1 & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots & -m_{k+1, k} & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots& \\vdots & \\ddots & \\ddots & 0 \\\\ 0 & \\cdots & -m_{n, k} & \\cdots & \\cdots & 1 \\end{bmatrix} \\] It's simple to compute that \\[ L_k^{-1} = \\begin{bmatrix} 1 & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\ 0 & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \\vdots & \\ddots & 1 & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots & m_{k+1, k} & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots& \\vdots & \\ddots & \\ddots & 0 \\\\ 0 & \\cdots & m_{n, k} & \\cdots & \\cdots & 1 \\end{bmatrix}. \\] Thus we let \\[ L_1^{-1}L_2^{-1}\\dots L_{n-1}^{-1} = \\begin{bmatrix} 1 & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\ m_{1,2} & 1 & \\ddots & \\ddots & \\ddots & \\vdots \\\\ \\vdots & \\ddots & 1 & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots\\\\ \\vdots & \\ddots &\\ddots& \\ddots & 1 & 0\\\\ m_{n,1} & \\cdots & \\cdots & \\cdots & m_{n, n-1} & 1 \\end{bmatrix} = L, \\] and \\[ U = \\begin{bmatrix} a^{(1)}_{11} & a^{(1)}_{12} & \\cdots & a^{(1)}_{1n} \\\\ & a^{(2)}_{22} & \\cdots & a^{(2)}_{2n} \\\\ & & \\cdots & \\vdots \\\\ & & & a^{(n)}_{nn} \\\\ \\end{bmatrix}. \\] Then we get \\[ A = LU \\] Theorem If Gaussian elimination can be performed on the linear system \\(A\\mathbf{x} = \\mathbf{b}\\) without row interchanges, then the matrix \\(A\\) can be factored into the product of a lower-triangular matrix \\(L\\) and an upper-triangular matrix \\(U\\) . If \\(L\\) has to be unitary , then the factorization is unique .","title":"Matrix Factorization (LU Factorization)"},{"location":"Mathematics_Basis/NA/Chap_6/#special-types-of-matrices","text":"","title":"Special Types of Matrices"},{"location":"Mathematics_Basis/NA/Chap_6/#strictly-diagonally-dominant-matrix","text":"Definition The \\(n \\times n\\) matrix \\(A\\) is said to be strictly diagnoally dominant when \\[ |a_{ii}| \\gt \\sum_{j=1,j\\ne i}^{n}|a_{ij}|,\\text{ for each } i = 1, \\dots, n \\] Theorem A strictly diagonally dominant matrix is nonsingular . And Gaussian elimination can be performed without row or column interchanges, and computations will be stable with respect to the growth of roundoff errors.\uff08\u6ee1\u79e9\u3001\u65e0\u9700\u4ea4\u6362\u884c\u5217\u3001\u8bef\u5dee\u7a33\u5b9a\uff09","title":"Strictly Diagonally Dominant Matrix \u4e25\u683c\u4e3b\u5bf9\u89d2\u5360\u4f18\u77e9\u9635"},{"location":"Mathematics_Basis/NA/Chap_6/#positive-definite-matrix","text":"Definition (Recap) A matrix \\(A\\) is positive definite if it's symmetric and if \\(\\mathbf{x}^tA\\mathbf{x} > 0\\) for every \\(n\\) -dimensional vector \\(\\mathbf{x} \\ne \\mathbf{0}\\) . Theorem If \\(A\\) is an \\(n \\times n\\) positive definite matrix, then \\(A\\) is nonsingular; \\(a_{ii} > 0\\) , for each \\(i = 1, 2, \\dots, n\\) ; \\(\\max\\limits_{1 \\le k, j \\le n}|a_{kj}| \\le \\max\\limits_{1 \\le i \\le n}|a_{ii}|\\) ; \\((a_{ij})^2 < a_{ii}a_{jj}\\) , for each \\(i \\ne j\\) .","title":"Positive Definite Matrix \u6b63\u5b9a\u77e9\u9635"},{"location":"Mathematics_Basis/NA/Chap_6/#choleskis-method-ldlt-factorization","text":"Further decompose \\(U\\) to \\(D\\tilde U\\) . \\(A\\) is symmetric \\(\\Rightarrow\\) \\(L = \\tilde U^t\\) . Thus \\[ A = LU = LD\\tilde U = LDL^t.\\ \\ (\u4e0b\u4e09\u89d2\u77e9\u9635\u4e58\u5bf9\u89d2\u77e9\u9635\u518d\u4e58\u5176\u8f6c\u7f6e) \\] Let \\[ D^{1/2} = \\begin{bmatrix} \\sqrt{u_{11}} & & & \\\\ & \\sqrt{u_{22}} & & \\\\ & & \\ddots & \\\\ & & & \\sqrt{u_{nn}} \\end{bmatrix}, \\] and \\(\\widetilde{L} = LD^{1/2}\\) . Then \\[ A = \\tilde L \\tilde L^t.\\ \\ (\u4e0b\u4e09\u89d2\u77e9\u9635\u4e58\u5176\u8f6c\u7f6e) \\]","title":"Choleski's Method (LDLt factorization)"},{"location":"Mathematics_Basis/NA/Chap_6/#tridiagonal-linear-system","text":"Definition An \\(n \\times n\\) matrix \\(A\\) is called a band matrix if \\(\\exists p, q (1 < p, q < n)\\) , s.t. whenever \\(i + p \\le j\\) or \\(j + q \\le i\\) , \\(a_{ij} = 0\\) . And \\(w = p + q - 1\\) is called the bandwidth . Specially, if \\(p = q = 2\\) , then \\(A\\) is called tridiagonal , with the following form, \\[ \\begin{bmatrix} b_1 & c_1 & & & \\\\ a_2 & b_2 & c_2 & & \\\\ & \\ddots & \\ddots & \\ddots \\\\ & & a_{n-1} & b_{n-1} & c_{n-1} \\\\ & & & a_n & b_n \\\\ \\end{bmatrix} \\]","title":"Tridiagonal Linear System \u4e09\u5bf9\u89d2\u77e9\u9635"},{"location":"Mathematics_Basis/NA/Chap_6/#crout-factorization","text":"\\[ A = LU = \\begin{bmatrix} l_{11} \\\\ l_{21} & l_{22} \\\\ & \\ddots & \\ddots \\\\ & & \\ddots & \\ddots \\\\ & & & l_{n, n- 1} & l_{n, n} \\end{bmatrix} \\begin{bmatrix} 1 & u_{12}\\\\ & 1 & u_{23} \\\\ & & \\ddots & \\ddots \\\\ & & & \\ddots & u_{n-1,n}\\\\ & & & & 1 \\end{bmatrix} \\] the time complexity is \\(O(N)\\) .","title":"Crout Factorization"},{"location":"Mathematics_Basis/NA/Chap_7/","text":"Chapter 7 | Iterative Techniques in Matrix Algebra \u00b6 Norms of Vectors and Matrices \u00b6 Vector Norms \u00b6 Definition A vector norm on \\(\\mathbb{R}^n\\) is a function \\(||\\cdot||\\) , from \\(\\mathbb{R}^n\\) into \\(\\mathbb{R}\\) with the following properties. \\[ \\begin{align} \\forall \\mathbf{x, y} \\in \\mathbb{R}^n, \\alpha \\in \\mathbb{R}, & (1)\\ ||\\mathbf{x}|| \\ge 0;\\ ||\\mathbf{x}|| = 0 \\Leftrightarrow \\mathbf{x} = \\mathbf{0}. \\\\ & (2)\\ ||\\alpha \\mathbf{x}|| = |\\alpha| \\cdot ||\\mathbf{x}|| \\\\ & (3)\\ ||\\mathbf{x} + \\mathbf{y}|| \\le ||\\mathbf{x}|| + ||\\mathbf{y}|| \\\\ \\end{align} \\] Commonly used examples L1 Norm: \\(||\\mathbf{x}||_1 = \\sum\\limits_{i = 1}^n|x_i|\\) . L2 Norm / Euclidean Norm: \\(||\\mathbf{x}||_2 = \\sqrt{\\sum\\limits_{i = 1}^n|x_i|^2}\\) . p-Norm: \\(||\\mathbf{x}||_p = \\left(\\sum\\limits_{i = 1}^n|x_i|^p\\right)^{1/p}\\) . Infinity Norm: \\(||\\mathbf{x}||_\\infty = \\max\\limits_{1 \\le i \\le n} |x_i|\\) . Convergence of Vector \u00b6 Similarly with a scalar, a sequence of vectors \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) in \\(\\mathbb{R}^n\\) is said to converge to \\(\\mathbf{x}\\) with respect to norm \\(||\\cdot||\\) , if \\(\\forall \\epsilon \\gt 0,\\exists N \\in \\mathbb{N}\\) , s.t \\(\\forall k \\gt N, ||\\mathbf{x}^{(k)} - \\mathbf{x}|| \\lt \\epsilon\\) . Theorem \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) in \\(\\mathbb{R}^n\\) converges to \\(\\mathbf{x}\\) with respect to norm \\(||\\cdot||_\\infty\\) if and only if \\(\\forall i,\\lim_{k \\rightarrow \\infty}x_i^{(k)} = x_i\\) . Equivalence \u00b6 Definition If \\(\\exists C_1, C_2,\\ s.t.\\ C_1||\\mathbf{x}||_B \\le ||\\mathbf{x}||_A \\le C_2||\\mathbf{x}||_B\\) , then \\(||\\mathbf{x}||_A\\) and \\(||\\mathbf{x}||_B\\) are equivalent . Theorem All the vector norms on \\(\\mathbb{R}^n\\) are equivalent. Matrix Norms \u00b6 Definition A matrix norm on \\(\\mathbb{R}^n\\) is a function \\(||\\cdot||\\) , from \\(M_n(\\mathbb{R})\\) matrices into \\(\\mathbb{R}\\) with the following properties. \\[ \\begin{align} \\forall A, B \\in M_n(\\mathbb{R}), \\alpha \\in \\mathbb{R}, & (1)\\ ||A|| \\ge 0;\\ ||A|| = 0 \\Leftrightarrow A = \\mathbf{O}. \\\\ & (2)\\ ||\\alpha A|| = |\\alpha| \\cdot ||A|| \\\\ & (3)\\ ||A + B|| \\le ||A|| + ||B|| \\\\ & (4)\\ ||AB|| \\le ||A|| \\cdot ||B|| \\end{align} \\] Commonly used examples Frobenius Norm: \\(||A||_F = \\sqrt{\\sum\\limits_{i=1}^n\\sum_{j=1}^n|a_{ij}|^2}\\) . Natural Norm: \\(||A||_p = \\max\\limits_{\\mathbf{x}\\ne \\mathbf{0}} \\frac{||A\\mathbf{x}||_p}{||\\mathbf{x}||_p} = \\max\\limits_{||\\mathbf{x}_p|| = 1} ||A\\mathbf{x}||_p\\) , where \\(||\\cdot||_p\\) is the vector norm. \\(||A||_\\infty = \\max\\limits_{1\\le i \\le n}\\sum\\limits_{j=1}^n|a_{ij}|\\) . \\(||A||_1= \\max\\limits_{1\\le j \\le n}\\sum\\limits_{i=1}^n|a_{ij}|\\) . (Spectral Norm) \\(||A||_2= \\sqrt{\\lambda_{max}(A^TA)}\\) . Corollary For any vector \\(\\mathbf{x} \\ne 0\\) , matrix \\(A\\) , and any natural norm \\(||\\cdot||\\) , we have \\[ ||A\\mathbf{x}|| \\le ||A|| \\cdot ||\\mathbf{x}||. \\] Eigenvalues and Eigenvectors \u00b6 Definition (Recap) If \\(A\\) is a square matrix, the **characteristic polynomial$ of \\(A\\) is defined by \\[ p(\\lambda) = \\text{det}(A - \\lambda I). \\] The roots of \\(p\\) are eigenvalues . If \\(\\lambda\\) is an eigenvalue and \\(\\mathbf{x} \\ne 0\\) satisfies \\((A - \\lambda I)\\mathbf{x} = \\mathbf{0}\\) , then \\(\\mathbf{x}\\) is an eigenvector . Spectral Radius \u00b6 Definition The spectral radius of a matrix \\(A\\) is defined by \\[ \\rho(A) = \\max|\\lambda| \\text{, where $\\lambda$ is an eigenvalue of $A$}. \\] (Recap that for complex \\(\\lambda = \\alpha + \\beta i\\) , \\(|\\lambda| = \\sqrt{\\alpha^2 + \\beta^2}\\) .) Theorem \\(\\forall A \\in M_n(\\mathbb{R})\\) , \\(||A||_2 = \\sqrt{\\rho(A^tA)}\\) . \\(\\rho(A) \\le ||A||\\) , for any natural norm \\(||\\cdot||\\) . Proof A proof for the second property. Suppose \\(\\lambda\\) is an eigenvalue of \\(A\\) with eigenvector \\(\\mathbf{x}\\) and \\(||\\mathbf{x}|| = 1\\) , \\[ |\\lambda| = |\\lambda| \\cdot ||\\mathbf{x}|| = ||\\lambda \\mathbf{x}|| \\le ||A|| \\cdot ||\\mathbf{x}|| = ||A||. \\] Thus, \\[ \\rho(A) = \\max|\\lambda| \\le ||A||. \\] Convergence of Matrix \u00b6 Definition \\(A \\in M_n(\\mathbb{R}))\\) is convergent if \\[ \\lim_{k \\rightarrow \\infty}\\left(A^k\\right)_{ij} = 0 \\text{ , for each } i = 1, 2, \\dots, n \\text{ and } j = 1, 2, \\dots, n. \\] Theorem The following statements are equivalent. \\(A\\) is a convergent matrix. \\(\\lim\\limits_{n \\rightarrow \\infty} ||A^n|| = 0\\) , for some natural norm. \\(\\lim\\limits_{n \\rightarrow \\infty} ||A^n|| = 0\\) , for all natural norms. \\(\\rho(A) < 1\\) . \\(\\forall \\mathbf{x}, \\lim\\limits_{n \\rightarrow \\infty} ||A^n \\mathbf{x}|| = \\mathbf{0}\\) . Iterative Techniques for Solving Linear Systems \u00b6 \\[ A\\mathbf{x} = \\mathbf{b} \\Leftrightarrow (D - L - U)\\mathbf{x} = \\mathbf{b} \\Leftrightarrow D\\mathbf{x} = (L + U)\\mathbf{x} + \\mathbf{b} \\\\ \\] Thus, \\[ \\mathbf{x} = D^{-1}(L + U)\\mathbf{x} + D^{-1}\\mathbf{b} \\] Jacobi Iterative Method \u00b6 Let \\(T_j = D^{-1}(L+U)\\) and \\(\\mathbf{c}_\\mathbf{j} = D^{-1}\\mathbf{b}\\) , then \\[ \\mathbf{x}^{(k)} = T_j\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\mathbf{j}. \\] Gauss-Seidel Iterative Method \u00b6 \\[ \\small \\mathbf{x}^{(k)} = D^{-1}(L\\mathbf{x}^{(k)} + U\\mathbf{x}^{(k - 1)}) + D^{-1}\\mathbf{b} \\Leftrightarrow \\mathbf{x}^{(k)} = (D - L)^{-1}U\\mathbf{x}^{(k - 1)} + (D - L)^{-1}\\mathbf{b} \\] Let \\(T_g = (D - L)^{-1}U\\) and \\(\\mathbf{c}_\\mathbf{g} = (D - L)^{-1}\\mathbf{b}\\) , then \\[ \\mathbf{x}^{(k)} = T_g\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\mathbf{g}. \\] Convergence of Iterative Methods \u00b6 Consider the following formula \\[ \\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c} \\] where \\(\\mathbf{x}^{(0)}\\) is arbitrary. Lemma If \\(\\rho(T) \\lt 1\\) , then \\((I - T)^{-1}\\) exists and \\[ (I - T)^{-1} = \\sum\\limits_{j = 0}^\\infty T^j. \\] Proof Suppose \\(\\lambda\\) is an eigenvalue of \\(T\\) with eigenvector \\(\\mathbf{x}\\) , then since \\(T\\mathbf{x} = \\lambda \\mathbf{x} \\Leftrightarrow (I - T)\\mathbf{x} = (1 - \\lambda)\\mathbf{x}\\) , thus \\(1 - \\lambda\\) is an eigenvalue of \\(I - T\\) . Since \\(|\\lambda| \\le \\rho(T) < 1\\) , thus \\(\\lambda = 1\\) is not an eigenvalue of \\(T\\) and \\(0\\) is not an eigenvvalue of \\(I - T\\) . Hence, \\((I - T)^{-1}\\) exists. Let \\(S_m = I + T + T^2 + \\cdots + T^m\\) , then \\[ (I - T)S_m = (1 + T + \\cdots + T^m) - (T + T^2 + \\cdots + T^{m + 1}) = I - T^{m + 1}. \\] Since \\(T\\) is convergent, thus \\[ \\lim\\limits_{m \\rightarrow \\infty} (I - T)S_m = \\lim\\limits_{m \\rightarrow \\infty}(I - T^{m + 1}) = I. \\] Thus, $(I - T)^{-1} = \\lim\\limits_{m \\rightarrow \\infty}S_m = \\sum\\limits_{j = 0}^\\infty T^j. Theorem \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) defined by \\(\\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c}\\) converges to the unique solution of \\(\\mathbf{x} = T\\mathbf{x} + \\mathbf{c}\\) if and only if \\(\\rho(T) \\lt 1\\) . Proof \\(\\Rightarrow\\) : define error \\(\\mathbf{e}^{(k)} = \\mathbf{x} - \\mathbf{x}^{(k)}\\) , then \\[ \\mathbf{e}^{(k)} = (T\\mathbf{x} + c) - (T\\mathbf{x}^{(k - 1)} + c) = T(\\mathbf{x} - \\mathbf{x}^{(k - 1)})T\\mathbf{e}^{(k - 1)} \\Rightarrow \\mathbf{e}^{(k)} = T^k \\mathbf{e}^{(0)} \\] Since it converges, thus \\[ \\lim\\limits_{k \\rightarrow \\infty} \\mathbf{e}^{(k)} = 0 \\Rightarrow \\lim\\limits_{k \\rightarrow \\infty} T^k \\mathbf{e}^{(0)} = 0, \\forall \\mathbf{e^{(0)}} \\] \\[ \\Leftrightarrow \\rho(T) < 1 \\] \\(\\Leftarrow\\) : \\[ \\mathbf{x}^{(k)} = T^{(k)}\\mathbf{x}^(0) + (T^{k - 1} + \\cdots + T + I) \\mathbf{c}. \\] Since \\(\\rho(T) < 1\\) , \\(T\\) is convergent and \\[ \\lim\\limits_{k \\rightarrow \\infty} T^k x^{(0)} = \\mathbf{0}. \\] From the Lemma above, we have \\[ \\lim\\limits_{k \\rightarrow \\infty} \\mathbf{x}^{(k)} = \\lim\\limits_{k \\rightarrow \\infty} T^k\\mathbf{x}^{(0)} + \\left(\\sum\\limits_{j = 0}^\\infty T^j \\right)\\mathbf{c} = \\mathbf{0} + (I - T)^{-1}\\mathbf{c} = (I - T)^{-1}\\mathbf{c}. \\] Thus \\(\\{\\mathbf{x}^{(k)}\\}\\) converges to \\(\\mathbf{x} \\equiv (I - T)^{-1} \\Leftrightarrow \\mathbf{x} = T\\mathbf{x} + c\\) . Corollary If \\(||T||\\lt 1\\) for any matrix norm and \\(\\mathbf{c}\\) is a given vector, then for all \\(\\mathbf{x}^{(0)}\\in \\mathbb{R}^n\\) , \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) defined by \\(\\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c}\\) converges to \\(\\mathbf{x}\\) , and the following error bounds hold \\(||\\mathbf{x} - \\mathbf{x}^{(k)}|| \\le ||T||^k||\\mathbf{x}^{(0)} - \\mathbf{x}||\\) . \\(||\\mathbf{x} - \\mathbf{x}^{(k)}|| \\le \\frac{||T||^k}{1 - ||T||}||\\mathbf{x}^{(1)} - \\mathbf{x}||\\) . Theorem \\(\\forall A\\) is strictly diagonally dominant, \\(\\forall \\text{\\bf{x}}^{(0)}\\) , both Jacobi and Gauss-Seidel methods give \\(\\{\\mathbf{x}^{(k)}\\}_{k=0}^\\infty\\) that converge to the unique solution of \\(A\\mathbf{x} = \\mathbf{b}\\) . Relaxation Methods \u00b6 Definition Suppose \\(\\mathbf{\\tilde x}\\) is an approximation to the solution of \\(A\\mathbf{x} = \\mathbf{b}\\) , then the residual vector for \\(\\mathbf{\\tilde x}\\) w.r.t this linear system is \\(\\mathbf{r} = \\mathbf{b} - A\\mathbf{\\tilde x}\\) . Further examine Gauss-Seidel method, \\[ x_i^{(k)} = x_i^{(k - 1)} + \\frac{r_i^{(k)}}{a_{ii}}, \\text{ where } r_i^{(k)} = b_i - \\sum_{j \\lt i} a_{ij}x_j^{(k)} - \\sum_{j \\ge i} a_{ij}x_j^{(k - 1)} \\] Let \\(x_i^{(k)} = x_i^{(k - 1)} + \\omega\\frac{r_i^{(k)}}{a_{ii}}\\) , by modifying the value of \\(\\omega\\) , we can somehow get faster convergence. \\(0 \\lt \\omega \\le 1\\) Under-Relaxation Method \\(\\omega = 1\\) Gauss-Seidel Method \\(\\omega \\gt 1\\) Successive Over-Relaxation Method (SOR) In matrix form, \\[ \\mathbf{x}^{(k)} = (D - \\omega L)^{-1}[(1 - \\omega)D + \\omega U]\\mathbf{x}^{(k - 1)} + (D - \\omega L)^{-1}\\mathbf{b} \\] Let \\(T_\\omega = (D - \\omega L)^{-1}[(1 - \\omega)D + \\omega U]\\) and \\(\\mathbf{c}_\\omega = (D - \\omega L)^{-1}\\mathbf{b}\\) , then \\[ \\mathbf{x}^{(k)} = T_\\omega\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\omega. \\] Theorem (Kahan) If \\(a_{ii} \\ne 0\\) , then \\(\\rho(T_\\omega)\\ge |\\omega -1 |\\) , which implies \\(0 \\lt \\omega \\lt 2\\) . Theorem (Ostrowski-Reich) If \\(A\\) is positive definite and \\(0 \\lt \\omega \\lt 2\\) , the SOR method converges for any choice of initial approximation. Theorem If \\(A\\) is positive definite and tridiagonal, then \\(\\rho(T_g) = \\rho(T_j)^2\\lt1\\) , and the optimal choice of \\(\\omega\\) for the SOR method is \\[ \\omega = \\frac{2}{1 + \\sqrt{1 - \\rho(T_j)^2}}. \\] Error Bounds and Iterative Refinement \u00b6 Definition The conditional number of the nonsigular matrix \\(A\\) relative to a norm \\(||\\cdot||\\) $ is \\[ K(A) = ||A|| \\cdot ||A^{-1}||. \\] A matrix \\(A\\) is well-conditioned if \\(K(A)\\) is close to 1, and is ill-conditioned when \\(K(A)\\) is significantly greater than 1. Proposition If \\(A\\) is symmetric, then \\(K(A)_2 = \\frac{\\max|\\lambda|}{\\min|\\lambda|}\\) . \\(K(A)_2 = 1\\) if \\(A\\) is orthogonal. \\(\\forall \\text{ orthogonal matrix }R, K(RA)_2 = K(AR)_2 = K(A)_2.\\) \\(\\forall \\text{ natural norm } ||\\cdot||_p,\\ K(A)_p \\ge 1.\\) \\(K(\\alpha A) = K(A).\\) Theorem For any natural norm, \\[ ||\\mathbf{x} - \\mathbf{\\tilde x}|| \\le ||\\mathbf{r}|| \\cdot ||A^{-1}|| \\] and if \\(\\mathbf{x} \\ne \\mathbf{0}\\) and \\(\\mathbf{b} \\ne \\mathbf{0}\\) , \\[ \\frac{||\\mathbf{x} - \\mathbf{\\tilde x}||}{||\\mathbf{x}||} \\le ||A||\\cdot||A^{-1}|| \\frac{||\\mathbf{r}||}{||\\mathbf{b}||} = K(A)\\frac{||\\mathbf{r}||}{||\\mathbf{b}||}. \\] Iterative Refinement Step.1 Solve \\(A\\mathbf{x} = \\mathbf{b}\\) and get an approximation solution \\(\\mathbf{x}_{0}\\) . Let \\(i = 1\\) . Step.2 Let \\(\\mathbf{r} = \\mathbf{b} - A\\mathbf{x}_{i - 1}\\) . Step.3 Solve \\(A\\mathbf{d} = \\mathbf{r}\\) and get the solution \\(\\mathbf{d}\\) . Step.4 The better approximation is \\(\\mathbf{x}_{i} = \\mathbf{x}_{i - 1} + \\mathbf{d}.\\) Step.5 Judge whether it's precise enough. If not, let \\(i = i + 1\\) and then repeat from Step.2 . In reality, \\(A\\) and \\(\\mathbf{b}\\) may be perturbed by an amount \\(\\delta A\\) and \\(\\delta \\mathbf{b}\\) . For \\(A(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b} + \\delta\\mathbf{b}\\) \\[ \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le ||A|| \\cdot ||A^{-1}|| \\cdot \\frac{||\\delta \\mathbf{b}||}{||\\mathbf{b}||} \\] For \\((A + \\delta A)(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b}\\) \\[ \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le \\frac{||A^{-1}|| \\cdot ||\\delta A||}{1 - ||A^{-1}|| \\cdot ||\\delta A||} = \\frac{||A|| \\cdot ||A^{-1}|| \\cdot \\frac{||\\delta A||}{||A||}}{1 - ||A|| \\cdot ||A^{-1}|| \\cdot\\frac{||\\delta A||}{||A||}} \\] Theorem If \\(A\\) is nonsingular and \\[ ||\\delta A|| \\lt \\frac{1}{||A^{-1}||} \\] then \\((A + \\delta A)(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b} + \\delta\\mathbf{b}\\) with the error estimate \\[ \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le \\frac{K(A)}{1 - K(A)\\frac{||\\delta A||}{||A||}}\\left(\\frac{||\\delta A||}{||A||} + \\frac{||\\delta\\mathbf{b}||}{||\\mathbf{b}||}\\right) \\]","title":"Chap 7"},{"location":"Mathematics_Basis/NA/Chap_7/#chapter-7-iterative-techniques-in-matrix-algebra","text":"","title":"Chapter 7 | Iterative Techniques in Matrix Algebra"},{"location":"Mathematics_Basis/NA/Chap_7/#norms-of-vectors-and-matrices","text":"","title":"Norms of Vectors and Matrices"},{"location":"Mathematics_Basis/NA/Chap_7/#vector-norms","text":"Definition A vector norm on \\(\\mathbb{R}^n\\) is a function \\(||\\cdot||\\) , from \\(\\mathbb{R}^n\\) into \\(\\mathbb{R}\\) with the following properties. \\[ \\begin{align} \\forall \\mathbf{x, y} \\in \\mathbb{R}^n, \\alpha \\in \\mathbb{R}, & (1)\\ ||\\mathbf{x}|| \\ge 0;\\ ||\\mathbf{x}|| = 0 \\Leftrightarrow \\mathbf{x} = \\mathbf{0}. \\\\ & (2)\\ ||\\alpha \\mathbf{x}|| = |\\alpha| \\cdot ||\\mathbf{x}|| \\\\ & (3)\\ ||\\mathbf{x} + \\mathbf{y}|| \\le ||\\mathbf{x}|| + ||\\mathbf{y}|| \\\\ \\end{align} \\] Commonly used examples L1 Norm: \\(||\\mathbf{x}||_1 = \\sum\\limits_{i = 1}^n|x_i|\\) . L2 Norm / Euclidean Norm: \\(||\\mathbf{x}||_2 = \\sqrt{\\sum\\limits_{i = 1}^n|x_i|^2}\\) . p-Norm: \\(||\\mathbf{x}||_p = \\left(\\sum\\limits_{i = 1}^n|x_i|^p\\right)^{1/p}\\) . Infinity Norm: \\(||\\mathbf{x}||_\\infty = \\max\\limits_{1 \\le i \\le n} |x_i|\\) .","title":"Vector Norms"},{"location":"Mathematics_Basis/NA/Chap_7/#convergence-of-vector","text":"Similarly with a scalar, a sequence of vectors \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) in \\(\\mathbb{R}^n\\) is said to converge to \\(\\mathbf{x}\\) with respect to norm \\(||\\cdot||\\) , if \\(\\forall \\epsilon \\gt 0,\\exists N \\in \\mathbb{N}\\) , s.t \\(\\forall k \\gt N, ||\\mathbf{x}^{(k)} - \\mathbf{x}|| \\lt \\epsilon\\) . Theorem \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) in \\(\\mathbb{R}^n\\) converges to \\(\\mathbf{x}\\) with respect to norm \\(||\\cdot||_\\infty\\) if and only if \\(\\forall i,\\lim_{k \\rightarrow \\infty}x_i^{(k)} = x_i\\) .","title":"Convergence of Vector"},{"location":"Mathematics_Basis/NA/Chap_7/#equivalence","text":"Definition If \\(\\exists C_1, C_2,\\ s.t.\\ C_1||\\mathbf{x}||_B \\le ||\\mathbf{x}||_A \\le C_2||\\mathbf{x}||_B\\) , then \\(||\\mathbf{x}||_A\\) and \\(||\\mathbf{x}||_B\\) are equivalent . Theorem All the vector norms on \\(\\mathbb{R}^n\\) are equivalent.","title":"Equivalence"},{"location":"Mathematics_Basis/NA/Chap_7/#matrix-norms","text":"Definition A matrix norm on \\(\\mathbb{R}^n\\) is a function \\(||\\cdot||\\) , from \\(M_n(\\mathbb{R})\\) matrices into \\(\\mathbb{R}\\) with the following properties. \\[ \\begin{align} \\forall A, B \\in M_n(\\mathbb{R}), \\alpha \\in \\mathbb{R}, & (1)\\ ||A|| \\ge 0;\\ ||A|| = 0 \\Leftrightarrow A = \\mathbf{O}. \\\\ & (2)\\ ||\\alpha A|| = |\\alpha| \\cdot ||A|| \\\\ & (3)\\ ||A + B|| \\le ||A|| + ||B|| \\\\ & (4)\\ ||AB|| \\le ||A|| \\cdot ||B|| \\end{align} \\] Commonly used examples Frobenius Norm: \\(||A||_F = \\sqrt{\\sum\\limits_{i=1}^n\\sum_{j=1}^n|a_{ij}|^2}\\) . Natural Norm: \\(||A||_p = \\max\\limits_{\\mathbf{x}\\ne \\mathbf{0}} \\frac{||A\\mathbf{x}||_p}{||\\mathbf{x}||_p} = \\max\\limits_{||\\mathbf{x}_p|| = 1} ||A\\mathbf{x}||_p\\) , where \\(||\\cdot||_p\\) is the vector norm. \\(||A||_\\infty = \\max\\limits_{1\\le i \\le n}\\sum\\limits_{j=1}^n|a_{ij}|\\) . \\(||A||_1= \\max\\limits_{1\\le j \\le n}\\sum\\limits_{i=1}^n|a_{ij}|\\) . (Spectral Norm) \\(||A||_2= \\sqrt{\\lambda_{max}(A^TA)}\\) . Corollary For any vector \\(\\mathbf{x} \\ne 0\\) , matrix \\(A\\) , and any natural norm \\(||\\cdot||\\) , we have \\[ ||A\\mathbf{x}|| \\le ||A|| \\cdot ||\\mathbf{x}||. \\]","title":"Matrix Norms"},{"location":"Mathematics_Basis/NA/Chap_7/#eigenvalues-and-eigenvectors","text":"Definition (Recap) If \\(A\\) is a square matrix, the **characteristic polynomial$ of \\(A\\) is defined by \\[ p(\\lambda) = \\text{det}(A - \\lambda I). \\] The roots of \\(p\\) are eigenvalues . If \\(\\lambda\\) is an eigenvalue and \\(\\mathbf{x} \\ne 0\\) satisfies \\((A - \\lambda I)\\mathbf{x} = \\mathbf{0}\\) , then \\(\\mathbf{x}\\) is an eigenvector .","title":"Eigenvalues and Eigenvectors"},{"location":"Mathematics_Basis/NA/Chap_7/#spectral-radius","text":"Definition The spectral radius of a matrix \\(A\\) is defined by \\[ \\rho(A) = \\max|\\lambda| \\text{, where $\\lambda$ is an eigenvalue of $A$}. \\] (Recap that for complex \\(\\lambda = \\alpha + \\beta i\\) , \\(|\\lambda| = \\sqrt{\\alpha^2 + \\beta^2}\\) .) Theorem \\(\\forall A \\in M_n(\\mathbb{R})\\) , \\(||A||_2 = \\sqrt{\\rho(A^tA)}\\) . \\(\\rho(A) \\le ||A||\\) , for any natural norm \\(||\\cdot||\\) . Proof A proof for the second property. Suppose \\(\\lambda\\) is an eigenvalue of \\(A\\) with eigenvector \\(\\mathbf{x}\\) and \\(||\\mathbf{x}|| = 1\\) , \\[ |\\lambda| = |\\lambda| \\cdot ||\\mathbf{x}|| = ||\\lambda \\mathbf{x}|| \\le ||A|| \\cdot ||\\mathbf{x}|| = ||A||. \\] Thus, \\[ \\rho(A) = \\max|\\lambda| \\le ||A||. \\]","title":"Spectral Radius"},{"location":"Mathematics_Basis/NA/Chap_7/#convergence-of-matrix","text":"Definition \\(A \\in M_n(\\mathbb{R}))\\) is convergent if \\[ \\lim_{k \\rightarrow \\infty}\\left(A^k\\right)_{ij} = 0 \\text{ , for each } i = 1, 2, \\dots, n \\text{ and } j = 1, 2, \\dots, n. \\] Theorem The following statements are equivalent. \\(A\\) is a convergent matrix. \\(\\lim\\limits_{n \\rightarrow \\infty} ||A^n|| = 0\\) , for some natural norm. \\(\\lim\\limits_{n \\rightarrow \\infty} ||A^n|| = 0\\) , for all natural norms. \\(\\rho(A) < 1\\) . \\(\\forall \\mathbf{x}, \\lim\\limits_{n \\rightarrow \\infty} ||A^n \\mathbf{x}|| = \\mathbf{0}\\) .","title":"Convergence of Matrix"},{"location":"Mathematics_Basis/NA/Chap_7/#iterative-techniques-for-solving-linear-systems","text":"\\[ A\\mathbf{x} = \\mathbf{b} \\Leftrightarrow (D - L - U)\\mathbf{x} = \\mathbf{b} \\Leftrightarrow D\\mathbf{x} = (L + U)\\mathbf{x} + \\mathbf{b} \\\\ \\] Thus, \\[ \\mathbf{x} = D^{-1}(L + U)\\mathbf{x} + D^{-1}\\mathbf{b} \\]","title":"Iterative Techniques for Solving Linear Systems"},{"location":"Mathematics_Basis/NA/Chap_7/#jacobi-iterative-method","text":"Let \\(T_j = D^{-1}(L+U)\\) and \\(\\mathbf{c}_\\mathbf{j} = D^{-1}\\mathbf{b}\\) , then \\[ \\mathbf{x}^{(k)} = T_j\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\mathbf{j}. \\]","title":"Jacobi Iterative Method"},{"location":"Mathematics_Basis/NA/Chap_7/#gauss-seidel-iterative-method","text":"\\[ \\small \\mathbf{x}^{(k)} = D^{-1}(L\\mathbf{x}^{(k)} + U\\mathbf{x}^{(k - 1)}) + D^{-1}\\mathbf{b} \\Leftrightarrow \\mathbf{x}^{(k)} = (D - L)^{-1}U\\mathbf{x}^{(k - 1)} + (D - L)^{-1}\\mathbf{b} \\] Let \\(T_g = (D - L)^{-1}U\\) and \\(\\mathbf{c}_\\mathbf{g} = (D - L)^{-1}\\mathbf{b}\\) , then \\[ \\mathbf{x}^{(k)} = T_g\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\mathbf{g}. \\]","title":"Gauss-Seidel Iterative Method"},{"location":"Mathematics_Basis/NA/Chap_7/#convergence-of-iterative-methods","text":"Consider the following formula \\[ \\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c} \\] where \\(\\mathbf{x}^{(0)}\\) is arbitrary. Lemma If \\(\\rho(T) \\lt 1\\) , then \\((I - T)^{-1}\\) exists and \\[ (I - T)^{-1} = \\sum\\limits_{j = 0}^\\infty T^j. \\] Proof Suppose \\(\\lambda\\) is an eigenvalue of \\(T\\) with eigenvector \\(\\mathbf{x}\\) , then since \\(T\\mathbf{x} = \\lambda \\mathbf{x} \\Leftrightarrow (I - T)\\mathbf{x} = (1 - \\lambda)\\mathbf{x}\\) , thus \\(1 - \\lambda\\) is an eigenvalue of \\(I - T\\) . Since \\(|\\lambda| \\le \\rho(T) < 1\\) , thus \\(\\lambda = 1\\) is not an eigenvalue of \\(T\\) and \\(0\\) is not an eigenvvalue of \\(I - T\\) . Hence, \\((I - T)^{-1}\\) exists. Let \\(S_m = I + T + T^2 + \\cdots + T^m\\) , then \\[ (I - T)S_m = (1 + T + \\cdots + T^m) - (T + T^2 + \\cdots + T^{m + 1}) = I - T^{m + 1}. \\] Since \\(T\\) is convergent, thus \\[ \\lim\\limits_{m \\rightarrow \\infty} (I - T)S_m = \\lim\\limits_{m \\rightarrow \\infty}(I - T^{m + 1}) = I. \\] Thus, $(I - T)^{-1} = \\lim\\limits_{m \\rightarrow \\infty}S_m = \\sum\\limits_{j = 0}^\\infty T^j. Theorem \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) defined by \\(\\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c}\\) converges to the unique solution of \\(\\mathbf{x} = T\\mathbf{x} + \\mathbf{c}\\) if and only if \\(\\rho(T) \\lt 1\\) . Proof \\(\\Rightarrow\\) : define error \\(\\mathbf{e}^{(k)} = \\mathbf{x} - \\mathbf{x}^{(k)}\\) , then \\[ \\mathbf{e}^{(k)} = (T\\mathbf{x} + c) - (T\\mathbf{x}^{(k - 1)} + c) = T(\\mathbf{x} - \\mathbf{x}^{(k - 1)})T\\mathbf{e}^{(k - 1)} \\Rightarrow \\mathbf{e}^{(k)} = T^k \\mathbf{e}^{(0)} \\] Since it converges, thus \\[ \\lim\\limits_{k \\rightarrow \\infty} \\mathbf{e}^{(k)} = 0 \\Rightarrow \\lim\\limits_{k \\rightarrow \\infty} T^k \\mathbf{e}^{(0)} = 0, \\forall \\mathbf{e^{(0)}} \\] \\[ \\Leftrightarrow \\rho(T) < 1 \\] \\(\\Leftarrow\\) : \\[ \\mathbf{x}^{(k)} = T^{(k)}\\mathbf{x}^(0) + (T^{k - 1} + \\cdots + T + I) \\mathbf{c}. \\] Since \\(\\rho(T) < 1\\) , \\(T\\) is convergent and \\[ \\lim\\limits_{k \\rightarrow \\infty} T^k x^{(0)} = \\mathbf{0}. \\] From the Lemma above, we have \\[ \\lim\\limits_{k \\rightarrow \\infty} \\mathbf{x}^{(k)} = \\lim\\limits_{k \\rightarrow \\infty} T^k\\mathbf{x}^{(0)} + \\left(\\sum\\limits_{j = 0}^\\infty T^j \\right)\\mathbf{c} = \\mathbf{0} + (I - T)^{-1}\\mathbf{c} = (I - T)^{-1}\\mathbf{c}. \\] Thus \\(\\{\\mathbf{x}^{(k)}\\}\\) converges to \\(\\mathbf{x} \\equiv (I - T)^{-1} \\Leftrightarrow \\mathbf{x} = T\\mathbf{x} + c\\) . Corollary If \\(||T||\\lt 1\\) for any matrix norm and \\(\\mathbf{c}\\) is a given vector, then for all \\(\\mathbf{x}^{(0)}\\in \\mathbb{R}^n\\) , \\(\\{\\mathbf{x}^{(k)}\\}_{k=1}^\\infty\\) defined by \\(\\mathbf{x}^{(k)} = T\\mathbf{x}^{(k - 1)} + \\mathbf{c}\\) converges to \\(\\mathbf{x}\\) , and the following error bounds hold \\(||\\mathbf{x} - \\mathbf{x}^{(k)}|| \\le ||T||^k||\\mathbf{x}^{(0)} - \\mathbf{x}||\\) . \\(||\\mathbf{x} - \\mathbf{x}^{(k)}|| \\le \\frac{||T||^k}{1 - ||T||}||\\mathbf{x}^{(1)} - \\mathbf{x}||\\) . Theorem \\(\\forall A\\) is strictly diagonally dominant, \\(\\forall \\text{\\bf{x}}^{(0)}\\) , both Jacobi and Gauss-Seidel methods give \\(\\{\\mathbf{x}^{(k)}\\}_{k=0}^\\infty\\) that converge to the unique solution of \\(A\\mathbf{x} = \\mathbf{b}\\) .","title":"Convergence of Iterative Methods"},{"location":"Mathematics_Basis/NA/Chap_7/#relaxation-methods","text":"Definition Suppose \\(\\mathbf{\\tilde x}\\) is an approximation to the solution of \\(A\\mathbf{x} = \\mathbf{b}\\) , then the residual vector for \\(\\mathbf{\\tilde x}\\) w.r.t this linear system is \\(\\mathbf{r} = \\mathbf{b} - A\\mathbf{\\tilde x}\\) . Further examine Gauss-Seidel method, \\[ x_i^{(k)} = x_i^{(k - 1)} + \\frac{r_i^{(k)}}{a_{ii}}, \\text{ where } r_i^{(k)} = b_i - \\sum_{j \\lt i} a_{ij}x_j^{(k)} - \\sum_{j \\ge i} a_{ij}x_j^{(k - 1)} \\] Let \\(x_i^{(k)} = x_i^{(k - 1)} + \\omega\\frac{r_i^{(k)}}{a_{ii}}\\) , by modifying the value of \\(\\omega\\) , we can somehow get faster convergence. \\(0 \\lt \\omega \\le 1\\) Under-Relaxation Method \\(\\omega = 1\\) Gauss-Seidel Method \\(\\omega \\gt 1\\) Successive Over-Relaxation Method (SOR) In matrix form, \\[ \\mathbf{x}^{(k)} = (D - \\omega L)^{-1}[(1 - \\omega)D + \\omega U]\\mathbf{x}^{(k - 1)} + (D - \\omega L)^{-1}\\mathbf{b} \\] Let \\(T_\\omega = (D - \\omega L)^{-1}[(1 - \\omega)D + \\omega U]\\) and \\(\\mathbf{c}_\\omega = (D - \\omega L)^{-1}\\mathbf{b}\\) , then \\[ \\mathbf{x}^{(k)} = T_\\omega\\mathbf{x}^{(k - 1)} + \\mathbf{c}_\\omega. \\] Theorem (Kahan) If \\(a_{ii} \\ne 0\\) , then \\(\\rho(T_\\omega)\\ge |\\omega -1 |\\) , which implies \\(0 \\lt \\omega \\lt 2\\) . Theorem (Ostrowski-Reich) If \\(A\\) is positive definite and \\(0 \\lt \\omega \\lt 2\\) , the SOR method converges for any choice of initial approximation. Theorem If \\(A\\) is positive definite and tridiagonal, then \\(\\rho(T_g) = \\rho(T_j)^2\\lt1\\) , and the optimal choice of \\(\\omega\\) for the SOR method is \\[ \\omega = \\frac{2}{1 + \\sqrt{1 - \\rho(T_j)^2}}. \\]","title":"Relaxation Methods"},{"location":"Mathematics_Basis/NA/Chap_7/#error-bounds-and-iterative-refinement","text":"Definition The conditional number of the nonsigular matrix \\(A\\) relative to a norm \\(||\\cdot||\\) $ is \\[ K(A) = ||A|| \\cdot ||A^{-1}||. \\] A matrix \\(A\\) is well-conditioned if \\(K(A)\\) is close to 1, and is ill-conditioned when \\(K(A)\\) is significantly greater than 1. Proposition If \\(A\\) is symmetric, then \\(K(A)_2 = \\frac{\\max|\\lambda|}{\\min|\\lambda|}\\) . \\(K(A)_2 = 1\\) if \\(A\\) is orthogonal. \\(\\forall \\text{ orthogonal matrix }R, K(RA)_2 = K(AR)_2 = K(A)_2.\\) \\(\\forall \\text{ natural norm } ||\\cdot||_p,\\ K(A)_p \\ge 1.\\) \\(K(\\alpha A) = K(A).\\) Theorem For any natural norm, \\[ ||\\mathbf{x} - \\mathbf{\\tilde x}|| \\le ||\\mathbf{r}|| \\cdot ||A^{-1}|| \\] and if \\(\\mathbf{x} \\ne \\mathbf{0}\\) and \\(\\mathbf{b} \\ne \\mathbf{0}\\) , \\[ \\frac{||\\mathbf{x} - \\mathbf{\\tilde x}||}{||\\mathbf{x}||} \\le ||A||\\cdot||A^{-1}|| \\frac{||\\mathbf{r}||}{||\\mathbf{b}||} = K(A)\\frac{||\\mathbf{r}||}{||\\mathbf{b}||}. \\] Iterative Refinement Step.1 Solve \\(A\\mathbf{x} = \\mathbf{b}\\) and get an approximation solution \\(\\mathbf{x}_{0}\\) . Let \\(i = 1\\) . Step.2 Let \\(\\mathbf{r} = \\mathbf{b} - A\\mathbf{x}_{i - 1}\\) . Step.3 Solve \\(A\\mathbf{d} = \\mathbf{r}\\) and get the solution \\(\\mathbf{d}\\) . Step.4 The better approximation is \\(\\mathbf{x}_{i} = \\mathbf{x}_{i - 1} + \\mathbf{d}.\\) Step.5 Judge whether it's precise enough. If not, let \\(i = i + 1\\) and then repeat from Step.2 . In reality, \\(A\\) and \\(\\mathbf{b}\\) may be perturbed by an amount \\(\\delta A\\) and \\(\\delta \\mathbf{b}\\) . For \\(A(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b} + \\delta\\mathbf{b}\\) \\[ \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le ||A|| \\cdot ||A^{-1}|| \\cdot \\frac{||\\delta \\mathbf{b}||}{||\\mathbf{b}||} \\] For \\((A + \\delta A)(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b}\\) \\[ \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le \\frac{||A^{-1}|| \\cdot ||\\delta A||}{1 - ||A^{-1}|| \\cdot ||\\delta A||} = \\frac{||A|| \\cdot ||A^{-1}|| \\cdot \\frac{||\\delta A||}{||A||}}{1 - ||A|| \\cdot ||A^{-1}|| \\cdot\\frac{||\\delta A||}{||A||}} \\] Theorem If \\(A\\) is nonsingular and \\[ ||\\delta A|| \\lt \\frac{1}{||A^{-1}||} \\] then \\((A + \\delta A)(\\mathbf{x} + \\delta\\mathbf{x}) = \\mathbf{b} + \\delta\\mathbf{b}\\) with the error estimate \\[ \\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||} \\le \\frac{K(A)}{1 - K(A)\\frac{||\\delta A||}{||A||}}\\left(\\frac{||\\delta A||}{||A||} + \\frac{||\\delta\\mathbf{b}||}{||\\mathbf{b}||}\\right) \\]","title":"Error Bounds and Iterative Refinement"},{"location":"Mathematics_Basis/NA/Chap_8/","text":"Chapter 8 | Approximation Theory \u00b6 Abstract Target \u00b6 Given \\(x_1, \\dots, x_m\\) and \\(y_1, \\dots, y_m\\) sampled from a funciton \\(y = f(x)\\) , or the continuous function \\(f(x), x \\in [a, b]\\) itself, find a simpler function \\(P(x) \\approx f(x)\\) . Measurement of Error \u00b6 ( Minimax ) minimize (discrete) \\(E_\\infty = \\max\\limits_{1 \\le i \\le m}|P(x_i) - y_i|\\) . (continuous) \\(E_\\infty = \\max\\limits_{a \\le x \\le b}|P(x) - f(x)|\\) . ( Absolute Deviation ) minimize (discrete) \\(E_1 = \\sum\\limits_{i = 1}^{m}|P(x_i) - y_i|\\) . (continuous) \\(E_1 = \\int\\nolimits_{a}^{b}|P(x) - f(x)|dx\\) . ( Least Squares Method ) minimize (discrete) \\(E_2 = \\sum\\limits_{i = 1}^{m}|P(x_i) - y_i|^2\\) . (continuous) \\(E_2 = \\int\\nolimits_{a}^{b}|P(x) - f(x)|^2dx\\) . In this course, we only discuss the minimax and least square parts, and only make \\(P(x)\\) a polynomial function. General Least Squares Approximation \u00b6 Definition \\(w\\) is called a weight function if (discrete) $$ \\forall i, w_i > 0. $$ (continuous) \\(w\\) is an integrable function and on the interval \\(I\\) , \\[ \\forall x \\in I, w(x) \\ge 0, \\] \\[ \\forall I' \\subseteq I, w(x) \\not\\equiv 0. \\] Considering the weight function, the least square method can be more general as below, (discrete) \\(E_2 = \\sum\\limits_{i = 1}^{m}w_i|P(x_i) - y_i|^2\\) . (continuous) \\(E_2 = \\int\\nolimits_{a}^{b}w(x)|P(x) - f(x)|^2dx\\) . Discrete Least Squares Approximation \u00b6 Target \u00b6 Approximate a set of data \\(\\{(x_i, y_i) | i = 1, 2, \\dots, m\\}\\) , with an algebraic polynomial \\[ P_n(x) = a_nx^n + a_{n - 1}x^{n - 1} + \\cdots + a_1x + a_0, \\] of degree \\(n < m - 1\\) (in most case \\(n \\ll m\\) ), with the least squares measurement, w.r.t. and weight function \\(w_i \\equiv 1\\) . Solution \u00b6 \\[ \\begin{align} E_2 &= \\sum\\limits_{i = 1}^m (y_i - P_n(x_i))^2 \\\\ &= \\sum\\limits_{i = 1}^m y_i^2 - 2\\sum\\limits_{i = 1}^m P_n(x_i)y_i + \\sum\\limits_{i = 1}^m P_n^2(x_i) \\\\ &= \\sum\\limits_{i = 1}^m y_i^2 - 2\\sum\\limits_{i = 1}^m \\left(\\sum\\limits_{j = 0}^n a_jx_i^j\\right)y_i + \\sum\\limits_{i = 1}^m \\left(\\sum\\limits_{j = 0}^n a_jx_i^j \\right)^2 \\\\ &= \\sum\\limits_{i = 1}^m y_i^2 - 2\\sum\\limits_{j = 0}^n a_j \\left( \\sum\\limits_{i = 1}^m y_i x_i^j \\right) + \\sum\\limits_{j = 0}^n \\sum\\limits_{k = 0}^n a_ja_k \\left(\\sum\\limits_{i = 1}^m x_i^{j + k} \\right). \\end{align} \\] The necessary condition to minimize \\(E_2\\) is \\[ 0 = \\frac{\\partial E_2}{\\partial a_j} = -2 \\sum\\limits_{i = 1}^m y_i x_i^j + 2 \\sum\\limits_{k = 0}^n a_k \\sum\\limits_{i = 1}^m x_i^{j + k}. \\] Then we get the \\(n + 1\\) normal equations with \\(n + 1\\) unknown \\(a_j\\) , \\[ \\sum\\limits_{k = 0}^n a_k \\sum\\limits_{i = 1}^m x_i^{j + k} = \\sum\\limits_{i = 1}^m y_i x_i^j. \\] Let \\(b_k = \\sum\\limits_{i = 1}^m x_i^k\\) and \\(c_k = \\sum\\limits_{i = 1}^m y_i x_i^k\\) , we can represent the normal equations by \\[ \\begin{bmatrix} b_{0 + 0} & \\cdots & b_{0 + n} \\\\ \\vdots & \\ddots & \\vdots \\\\ b_{n + 0} & \\cdots & b_{n + n} \\end{bmatrix} \\begin{bmatrix} a_0 \\\\ \\vdots \\\\ a_n \\end{bmatrix} = \\begin{bmatrix} c_0 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\] Theorem Normal equations have a unique solution if \\(x_i\\) are distinct . Proof Suppose \\(X\\) is a \\(n + 1 \\times m\\) Vandermonde Matrix , which is \\[ X = \\begin{bmatrix} 1 & x_1 & x_1^2 & \\cdots & x_1^n \\\\ 1 & x_2 & x_2^2 & \\cdots & x_2^n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_m & x_m^2 & \\cdots & x_m^n \\\\ \\end{bmatrix} \\] and let \\(\\mathbf{y} = (y_1, y_1, \\cdots, y_m)^T\\) . Then the normal equations can be represented by (notice the dimension of matrices and vectors), \\[ X X^T \\mathbf{a} = X \\mathbf{y}. \\] Since \\(x_i\\) are distinct, \\(X\\) is a column full rank matrix, namely \\[ \\text{rank}(X) = n + 1. \\] Since \\(x_i \\in \\mathbb{R}\\) , thus \\[ \\text{rank}(X X^T) = \\text{rank}(X) = n + 1. \\] Hence the normal equations have a unique solution. Logarithmic Linear Least Squares \u00b6 To approximate by the function of the form \\[ y = be^{ax}, \\] or \\[ y = bx^a, \\] we can consider the logarithm of the equation by \\[ \\ln y = \\ln b + ax, \\] and \\[ \\ln y = \\ln b + a \\ln x. \\] Note It's a simple algrebric transformation. But we should point out that it minimize the logarithmic linear least squares, but not linear least squares. Just consider the arguments to minimize the following two errors, \\[ E = \\sum\\limits_{i = 1}^m (y_i - be^{ax_i})^2, \\] and \\[ E' = \\sum\\limits_{i = 1}^m (\\ln y - (\\ln b + ax))^2, \\] they are slightly different actually. Continuous Least Squares Approximation \u00b6 Now we consider the continuous function instead of discrete points. Target \u00b6 Approxiate function \\(f \\in C[a, b]\\) , with an algebraic polynomial \\[ P_n(x) = a_nx^n + a_{n - 1}x^{n - 1} + \\cdots + a_1x + a_0, \\] with the least squares measurement, w.r.t the weight function \\(w(x) \\equiv 1\\) . Solution \u00b6 Problem Similarly to the discrete situation, we can derive the normal equations by making \\[ 0 = \\frac{\\partial E}{\\partial a_j}, \\] and we get the normal equations \\[ \\sum\\limits_{k = 0}^n a_k \\int_{a}^{b} x^{j + k} dx = \\int_{a}^{b} x^j f(x)dx. \\] However , notice that \\[ \\int_{a}^{b} x^{j + k} dx = \\frac{b^{j + k + 1} - a^{j + k + 1}}{j + k + 1}. \\] Thus the coefficient of the linear system is a Hilbert matrix , which have a large conditional number . In actual numerical calculation, this gives a large roundoff error. Another disadvantage is that we can not easily get \\(P_{n + 1}(x)\\) from \\(P_{n}(x)\\) , similarly with the topic of Lagrange interpolation. Hence we introduce a different solution based on the concept of orthogonal polynomials . Definition The set of function \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is linearly independent on \\([a, b]\\) if, whenever \\[ \\forall\\ x \\in [a, b], c_0\\varphi_0(x) + c_1\\varphi_1(x) + \\cdots + c_n\\varphi_n(x) = 0 \\] we have \\(c_0 = c_1 = \\cdots = c_n = 0\\) . Otherwise it's linearly dependent . Theorem If \\(\\text{deg}(\\varphi_j(x)) = j\\) , then \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is linearly independent on any interval \\([a, b]\\) . Theorem \\(\\Pi_n\\) is the linear space spanned by \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) , where \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is linear independent, and \\(\\Pi_n\\) is the set of all polynomials of degree at most n . Definition For the linear independent set \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) , \\(\\forall P(x) \\in \\Pi_n\\) , \\(P(x) = \\sum\\limits_{j = 0}^n \\alpha_j \\varphi_j(x)\\) is called a generalized polynomial . Definition Inner product w.r.t the weight function \\(w\\) is defined and denoted by (discrete) \\((f, g) = \\sum\\limits_{i = 1}^{m} w_i f(x_i)g(x_i)\\) (continuous) \\((f, g) = \\int\\nolimits_{a}^{b} w(x) f(x) g(x) dx\\) . \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is an orthogonal set of functions for the interval \\([a,b]\\) w.r.t the weight function \\(w\\) if \\[ (\\varphi_j, \\varphi_k) = \\left\\{ \\begin{align} & 0, && j \\ne k, \\\\ & \\alpha_k > 0, && j = k. \\end{align} \\right. \\] In addition, if \\(\\alpha_k = 1\\) , then the set is orthonormal \uff08\u5355\u4f4d\u6b63\u4ea4\uff09 . Motivation of Orthogonality Considering \\(w(x)\\) , then the normal equations can be represented by \\[ \\int_a^b w(x)f(x)\\varphi_j(x)dx = \\sum\\limits_{k = 0}^n a_k \\int_a^b w(x)\\varphi_k(x)\\varphi_j(x)dx, \\] If we define the orthogonal set of functions as above, the equations reduce to \\[ \\int_a^b w(x)f(x)\\varphi_j(x)dx = a_j \\alpha_j. \\] Theorem \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is an orthogonal set of functions on the interval \\([a, b]\\) w.r.t. the weight function \\(w\\) , then the least squares approximation to \\(f\\) on \\([a, b]\\) w.r.t. \\(w\\) is \\[ P(x) = \\sum\\limits_{k = 0}^n a_k \\varphi_k(x), \\] where, for each \\(k = 0, 1, \\dots, n\\) , \\[ a_k = \\frac{\\int_a^b w(x) \\varphi_k(x)f(x)dx}{\\int_a^b w(x) \\varphi_k^2(x)dx} = \\frac{(\\varphi_k, f)}{(\\varphi_{k}, \\varphi_{k})} = \\frac{(\\varphi_k, f)}{\\alpha_k}. \\] Base on the Gram-Schmidt Process , we have the following theorem to construct the orthogonal polynomials on \\([a, b]\\) w.r.t a weight function \\(w\\) . Theorem The set of polynomial functions \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) defined in the following way is orthogonal on \\([a, b]\\) w.r.t. the weight function \\(w\\) . \\[ \\begin{align} & \\varphi_0(x) \\equiv 1,\\ \\varphi_1(x) = x - B_1, \\\\ & \\varphi_k(x) = (x - B_k) \\varphi_{k - 1}(x) - C_k \\varphi_{k - 2}(x), k \\ge 2. \\\\ \\end{align} \\] where \\[ B_k = \\frac{(x\\varphi_{k - 1}, \\varphi_{k - 1})}{\\varphi_{k - 1}, \\varphi_{k - 1}},\\ C_k = \\frac{(x\\varphi_{k - 1}, \\varphi_{k - 2})}{\\varphi_{k - 2}, \\varphi_{k - 2}}. \\] Minimax Approximation \u00b6 Here we introduce Chebyshev polynomials to deal with \\(E_\\infty\\) error, and by the way, we can use it to economize the power series. Chebyshev Polynomials \u00b6 Chebyshev polynomials are defined concisely as below, \\[ T_n(x) = \\cos(n \\arccos x) \\] or equally defined recursively by \\[ \\begin{align} & T_0(x) = 1,\\ T_1(x) = x, \\\\ & T_{n + 1}(x) = 2x T_{n}(x) - T_{n - 1}(x). \\end{align} \\] Property \\(\\{T_n(x)\\}\\) are orthogonal on \\((-1, 1)\\) w.r.t the weight function \\[ w(x) = \\frac{1}{\\sqrt{1 - x^2}}. \\] \\[ (T_n, T_m) = \\int_{-1}^1 \\frac{T_n(x)T_m(x)}{\\sqrt{1 - x^2}}dx = \\left\\{ \\begin{align} & 0, && n \\ne m \\\\ & \\pi, && n = m = 0 \\\\ & \\frac{\\pi}{2}, && n = m \\ne 0 \\end{align} \\right. \\] \\(T_n(x)\\) is a polynomial of degree \\(n\\) with the leading coefficient \\(2^{n - 1}\\) . \\(T_n(x)\\) has \\(n\\) zero points at \\[ \\bar{x}_k = \\cos \\left(\\frac{2k-1}{2n}\\pi\\right),\\ k = 1, 2, \\dots, n. \\] \\(T_n(x)\\) has extrema at \\[ \\bar{x}'_k = \\cos \\frac{k\\pi}n \\text{, with } T(\\bar{x}'_k) = (-1)^k,\\ k = 1, 2, \\dots, n. \\] Monic Chebyshev Polynomials \u00b6 Monic chebyshev polynomials are defined by \\[ \\tilde T_0(x) = 1,\\ \\tilde T_n(x) = \\frac{1}{2^{n - 1}} T_n(x). \\] The following is an important theorem for the position of Chebyshev polynomials. Theorem \\[ \\frac{1}{2^{n - 1}} = \\max\\limits_{x \\in [-1, 1]}|\\tilde T_n(x)| \\le \\max\\limits_{x \\in [-1, 1]}|P_n(x)|,\\ \\forall\\ P_n(x) \\in \\tilde \\Pi_n, \\] where \\(\\tilde \\Pi_n\\) denotes the set of all monic polynomials of degree n . From this theorem, we can answer where to place interpolating points to minimize the error in Lagrange interpolation. Recap that \\[ R(x) = f(x) - P(x) = \\frac{f^{n + 1}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^{n}(x - x_i). \\] To minimize \\(R(x)\\) , since there is no control over \\(\\xi\\) , so we only need to minimize \\[ |w_n(x)| = |\\prod\\limits_{i = 0}^{n}(x - x_i)|. \\] Since \\(w_n(x)\\) is a monic polynomial of degree \\((n + 1)\\) , we can obtain minimum when \\(w_n(x) = \\tilde T_{n + 1}(x)\\) . To make it equal, we can simply make their zero points equal, namely \\[ x_k = \\bar{x}_{k + 1} = \\cos \\frac{2k + 1}{2(n + 1)}\\pi. \\] Corollary \\[ \\max\\limits_{x \\in [-1, 1]} |f(x) - P(x)| \\le \\frac{\\max\\limits_{x \\in [-1, 1]}\\left|f^{(n + 1)}(x)\\right|}{2^n (n + 1)!}. \\] Solution of Minimax Approximation \u00b6 Now we come back to the minimax approximation. To minimize \\[ E_\\infty = \\max\\limits_{a \\le x \\le b}|P(x) - f(x)|, \\] with a polynomial of degree \\(n\\) on the interval \\([a, b]\\) , we need the following steps: Step.1 Find the roots of \\(T_{n + 1}(t)\\) on the interval \\([-1, 1]\\) , denoted by \\(t_0, \\dots, t_n\\) . Step.2 Extend it to the interval \\([a, b]\\) by \\[ x_i = \\frac12[(b - a)t_i + a + b]. \\] Step.3 Substitue \\(x_i\\) into \\(f(x)\\) to get \\(y_i\\) . Step.4 Compute the Langrange polynomial \\(P(x)\\) of the interpolating points \\((x_i, y_i)\\) . Ecomomization of Power Series \u00b6 Chebyshev polynomials can also be used to reduce the degree of an approximating polynomials with a minimal loss of accuracy . Consider approximating \\[ P_n(x) = a_nx^n + a_{n - 1}x^{n - 1} + \\cdots + a_1x + a_0 \\] on [-1, 1]. Then the target is to minimize \\[ \\max_{x\\in[-1,1]}|P_n(x) - P_{n - 1}(x)|. \\] Solution \u00b6 Since \\((P_n(x) - P_{n - 1}(x)) / a_n\\) is monic, thus \\[ \\max_{x\\in[-1,1]}\\left|\\frac{1}{a_n}((P_n(x) - P_{n - 1}(x))\\right| \\ge \\frac{1}{2^{n - 1}}. \\] Equality occurs when \\[ \\frac{1}{a_n}((P_n(x) - P_{n - 1}(x)) = \\tilde T_n(x). \\] Thus we can choose \\[ P_{n - 1}(x) = P_n(x) - a_n \\tilde T_n(x) \\] with the minimum error value of \\[ \\max_{x\\in[-1,1]}|P_n(x) - P_{n - 1}(x)| = |a_n| \\max_{x\\in[-1,1]}\\left|\\frac{1}{a_n}((P_n(x) - P_{n - 1}(x))\\right| = \\frac{|a_n|}{2^{n - 1}}. \\]","title":"Chap 8"},{"location":"Mathematics_Basis/NA/Chap_8/#chapter-8-approximation-theory","text":"Abstract","title":"Chapter 8 | Approximation Theory"},{"location":"Mathematics_Basis/NA/Chap_8/#target","text":"Given \\(x_1, \\dots, x_m\\) and \\(y_1, \\dots, y_m\\) sampled from a funciton \\(y = f(x)\\) , or the continuous function \\(f(x), x \\in [a, b]\\) itself, find a simpler function \\(P(x) \\approx f(x)\\) .","title":"Target"},{"location":"Mathematics_Basis/NA/Chap_8/#measurement-of-error","text":"( Minimax ) minimize (discrete) \\(E_\\infty = \\max\\limits_{1 \\le i \\le m}|P(x_i) - y_i|\\) . (continuous) \\(E_\\infty = \\max\\limits_{a \\le x \\le b}|P(x) - f(x)|\\) . ( Absolute Deviation ) minimize (discrete) \\(E_1 = \\sum\\limits_{i = 1}^{m}|P(x_i) - y_i|\\) . (continuous) \\(E_1 = \\int\\nolimits_{a}^{b}|P(x) - f(x)|dx\\) . ( Least Squares Method ) minimize (discrete) \\(E_2 = \\sum\\limits_{i = 1}^{m}|P(x_i) - y_i|^2\\) . (continuous) \\(E_2 = \\int\\nolimits_{a}^{b}|P(x) - f(x)|^2dx\\) . In this course, we only discuss the minimax and least square parts, and only make \\(P(x)\\) a polynomial function.","title":"Measurement of Error"},{"location":"Mathematics_Basis/NA/Chap_8/#general-least-squares-approximation","text":"Definition \\(w\\) is called a weight function if (discrete) $$ \\forall i, w_i > 0. $$ (continuous) \\(w\\) is an integrable function and on the interval \\(I\\) , \\[ \\forall x \\in I, w(x) \\ge 0, \\] \\[ \\forall I' \\subseteq I, w(x) \\not\\equiv 0. \\] Considering the weight function, the least square method can be more general as below, (discrete) \\(E_2 = \\sum\\limits_{i = 1}^{m}w_i|P(x_i) - y_i|^2\\) . (continuous) \\(E_2 = \\int\\nolimits_{a}^{b}w(x)|P(x) - f(x)|^2dx\\) .","title":"General Least Squares Approximation"},{"location":"Mathematics_Basis/NA/Chap_8/#discrete-least-squares-approximation","text":"","title":"Discrete Least Squares Approximation"},{"location":"Mathematics_Basis/NA/Chap_8/#target_1","text":"Approximate a set of data \\(\\{(x_i, y_i) | i = 1, 2, \\dots, m\\}\\) , with an algebraic polynomial \\[ P_n(x) = a_nx^n + a_{n - 1}x^{n - 1} + \\cdots + a_1x + a_0, \\] of degree \\(n < m - 1\\) (in most case \\(n \\ll m\\) ), with the least squares measurement, w.r.t. and weight function \\(w_i \\equiv 1\\) .","title":"Target"},{"location":"Mathematics_Basis/NA/Chap_8/#solution","text":"\\[ \\begin{align} E_2 &= \\sum\\limits_{i = 1}^m (y_i - P_n(x_i))^2 \\\\ &= \\sum\\limits_{i = 1}^m y_i^2 - 2\\sum\\limits_{i = 1}^m P_n(x_i)y_i + \\sum\\limits_{i = 1}^m P_n^2(x_i) \\\\ &= \\sum\\limits_{i = 1}^m y_i^2 - 2\\sum\\limits_{i = 1}^m \\left(\\sum\\limits_{j = 0}^n a_jx_i^j\\right)y_i + \\sum\\limits_{i = 1}^m \\left(\\sum\\limits_{j = 0}^n a_jx_i^j \\right)^2 \\\\ &= \\sum\\limits_{i = 1}^m y_i^2 - 2\\sum\\limits_{j = 0}^n a_j \\left( \\sum\\limits_{i = 1}^m y_i x_i^j \\right) + \\sum\\limits_{j = 0}^n \\sum\\limits_{k = 0}^n a_ja_k \\left(\\sum\\limits_{i = 1}^m x_i^{j + k} \\right). \\end{align} \\] The necessary condition to minimize \\(E_2\\) is \\[ 0 = \\frac{\\partial E_2}{\\partial a_j} = -2 \\sum\\limits_{i = 1}^m y_i x_i^j + 2 \\sum\\limits_{k = 0}^n a_k \\sum\\limits_{i = 1}^m x_i^{j + k}. \\] Then we get the \\(n + 1\\) normal equations with \\(n + 1\\) unknown \\(a_j\\) , \\[ \\sum\\limits_{k = 0}^n a_k \\sum\\limits_{i = 1}^m x_i^{j + k} = \\sum\\limits_{i = 1}^m y_i x_i^j. \\] Let \\(b_k = \\sum\\limits_{i = 1}^m x_i^k\\) and \\(c_k = \\sum\\limits_{i = 1}^m y_i x_i^k\\) , we can represent the normal equations by \\[ \\begin{bmatrix} b_{0 + 0} & \\cdots & b_{0 + n} \\\\ \\vdots & \\ddots & \\vdots \\\\ b_{n + 0} & \\cdots & b_{n + n} \\end{bmatrix} \\begin{bmatrix} a_0 \\\\ \\vdots \\\\ a_n \\end{bmatrix} = \\begin{bmatrix} c_0 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\] Theorem Normal equations have a unique solution if \\(x_i\\) are distinct . Proof Suppose \\(X\\) is a \\(n + 1 \\times m\\) Vandermonde Matrix , which is \\[ X = \\begin{bmatrix} 1 & x_1 & x_1^2 & \\cdots & x_1^n \\\\ 1 & x_2 & x_2^2 & \\cdots & x_2^n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_m & x_m^2 & \\cdots & x_m^n \\\\ \\end{bmatrix} \\] and let \\(\\mathbf{y} = (y_1, y_1, \\cdots, y_m)^T\\) . Then the normal equations can be represented by (notice the dimension of matrices and vectors), \\[ X X^T \\mathbf{a} = X \\mathbf{y}. \\] Since \\(x_i\\) are distinct, \\(X\\) is a column full rank matrix, namely \\[ \\text{rank}(X) = n + 1. \\] Since \\(x_i \\in \\mathbb{R}\\) , thus \\[ \\text{rank}(X X^T) = \\text{rank}(X) = n + 1. \\] Hence the normal equations have a unique solution.","title":"Solution"},{"location":"Mathematics_Basis/NA/Chap_8/#logarithmic-linear-least-squares","text":"To approximate by the function of the form \\[ y = be^{ax}, \\] or \\[ y = bx^a, \\] we can consider the logarithm of the equation by \\[ \\ln y = \\ln b + ax, \\] and \\[ \\ln y = \\ln b + a \\ln x. \\] Note It's a simple algrebric transformation. But we should point out that it minimize the logarithmic linear least squares, but not linear least squares. Just consider the arguments to minimize the following two errors, \\[ E = \\sum\\limits_{i = 1}^m (y_i - be^{ax_i})^2, \\] and \\[ E' = \\sum\\limits_{i = 1}^m (\\ln y - (\\ln b + ax))^2, \\] they are slightly different actually.","title":"Logarithmic Linear Least Squares"},{"location":"Mathematics_Basis/NA/Chap_8/#continuous-least-squares-approximation","text":"Now we consider the continuous function instead of discrete points.","title":"Continuous Least Squares Approximation"},{"location":"Mathematics_Basis/NA/Chap_8/#target_2","text":"Approxiate function \\(f \\in C[a, b]\\) , with an algebraic polynomial \\[ P_n(x) = a_nx^n + a_{n - 1}x^{n - 1} + \\cdots + a_1x + a_0, \\] with the least squares measurement, w.r.t the weight function \\(w(x) \\equiv 1\\) .","title":"Target"},{"location":"Mathematics_Basis/NA/Chap_8/#solution_1","text":"Problem Similarly to the discrete situation, we can derive the normal equations by making \\[ 0 = \\frac{\\partial E}{\\partial a_j}, \\] and we get the normal equations \\[ \\sum\\limits_{k = 0}^n a_k \\int_{a}^{b} x^{j + k} dx = \\int_{a}^{b} x^j f(x)dx. \\] However , notice that \\[ \\int_{a}^{b} x^{j + k} dx = \\frac{b^{j + k + 1} - a^{j + k + 1}}{j + k + 1}. \\] Thus the coefficient of the linear system is a Hilbert matrix , which have a large conditional number . In actual numerical calculation, this gives a large roundoff error. Another disadvantage is that we can not easily get \\(P_{n + 1}(x)\\) from \\(P_{n}(x)\\) , similarly with the topic of Lagrange interpolation. Hence we introduce a different solution based on the concept of orthogonal polynomials . Definition The set of function \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is linearly independent on \\([a, b]\\) if, whenever \\[ \\forall\\ x \\in [a, b], c_0\\varphi_0(x) + c_1\\varphi_1(x) + \\cdots + c_n\\varphi_n(x) = 0 \\] we have \\(c_0 = c_1 = \\cdots = c_n = 0\\) . Otherwise it's linearly dependent . Theorem If \\(\\text{deg}(\\varphi_j(x)) = j\\) , then \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is linearly independent on any interval \\([a, b]\\) . Theorem \\(\\Pi_n\\) is the linear space spanned by \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) , where \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is linear independent, and \\(\\Pi_n\\) is the set of all polynomials of degree at most n . Definition For the linear independent set \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) , \\(\\forall P(x) \\in \\Pi_n\\) , \\(P(x) = \\sum\\limits_{j = 0}^n \\alpha_j \\varphi_j(x)\\) is called a generalized polynomial . Definition Inner product w.r.t the weight function \\(w\\) is defined and denoted by (discrete) \\((f, g) = \\sum\\limits_{i = 1}^{m} w_i f(x_i)g(x_i)\\) (continuous) \\((f, g) = \\int\\nolimits_{a}^{b} w(x) f(x) g(x) dx\\) . \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is an orthogonal set of functions for the interval \\([a,b]\\) w.r.t the weight function \\(w\\) if \\[ (\\varphi_j, \\varphi_k) = \\left\\{ \\begin{align} & 0, && j \\ne k, \\\\ & \\alpha_k > 0, && j = k. \\end{align} \\right. \\] In addition, if \\(\\alpha_k = 1\\) , then the set is orthonormal \uff08\u5355\u4f4d\u6b63\u4ea4\uff09 . Motivation of Orthogonality Considering \\(w(x)\\) , then the normal equations can be represented by \\[ \\int_a^b w(x)f(x)\\varphi_j(x)dx = \\sum\\limits_{k = 0}^n a_k \\int_a^b w(x)\\varphi_k(x)\\varphi_j(x)dx, \\] If we define the orthogonal set of functions as above, the equations reduce to \\[ \\int_a^b w(x)f(x)\\varphi_j(x)dx = a_j \\alpha_j. \\] Theorem \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) is an orthogonal set of functions on the interval \\([a, b]\\) w.r.t. the weight function \\(w\\) , then the least squares approximation to \\(f\\) on \\([a, b]\\) w.r.t. \\(w\\) is \\[ P(x) = \\sum\\limits_{k = 0}^n a_k \\varphi_k(x), \\] where, for each \\(k = 0, 1, \\dots, n\\) , \\[ a_k = \\frac{\\int_a^b w(x) \\varphi_k(x)f(x)dx}{\\int_a^b w(x) \\varphi_k^2(x)dx} = \\frac{(\\varphi_k, f)}{(\\varphi_{k}, \\varphi_{k})} = \\frac{(\\varphi_k, f)}{\\alpha_k}. \\] Base on the Gram-Schmidt Process , we have the following theorem to construct the orthogonal polynomials on \\([a, b]\\) w.r.t a weight function \\(w\\) . Theorem The set of polynomial functions \\(\\{\\varphi_0(x), \\dots, \\varphi_n(x)\\}\\) defined in the following way is orthogonal on \\([a, b]\\) w.r.t. the weight function \\(w\\) . \\[ \\begin{align} & \\varphi_0(x) \\equiv 1,\\ \\varphi_1(x) = x - B_1, \\\\ & \\varphi_k(x) = (x - B_k) \\varphi_{k - 1}(x) - C_k \\varphi_{k - 2}(x), k \\ge 2. \\\\ \\end{align} \\] where \\[ B_k = \\frac{(x\\varphi_{k - 1}, \\varphi_{k - 1})}{\\varphi_{k - 1}, \\varphi_{k - 1}},\\ C_k = \\frac{(x\\varphi_{k - 1}, \\varphi_{k - 2})}{\\varphi_{k - 2}, \\varphi_{k - 2}}. \\]","title":"Solution"},{"location":"Mathematics_Basis/NA/Chap_8/#minimax-approximation","text":"Here we introduce Chebyshev polynomials to deal with \\(E_\\infty\\) error, and by the way, we can use it to economize the power series.","title":"Minimax Approximation"},{"location":"Mathematics_Basis/NA/Chap_8/#chebyshev-polynomials","text":"Chebyshev polynomials are defined concisely as below, \\[ T_n(x) = \\cos(n \\arccos x) \\] or equally defined recursively by \\[ \\begin{align} & T_0(x) = 1,\\ T_1(x) = x, \\\\ & T_{n + 1}(x) = 2x T_{n}(x) - T_{n - 1}(x). \\end{align} \\] Property \\(\\{T_n(x)\\}\\) are orthogonal on \\((-1, 1)\\) w.r.t the weight function \\[ w(x) = \\frac{1}{\\sqrt{1 - x^2}}. \\] \\[ (T_n, T_m) = \\int_{-1}^1 \\frac{T_n(x)T_m(x)}{\\sqrt{1 - x^2}}dx = \\left\\{ \\begin{align} & 0, && n \\ne m \\\\ & \\pi, && n = m = 0 \\\\ & \\frac{\\pi}{2}, && n = m \\ne 0 \\end{align} \\right. \\] \\(T_n(x)\\) is a polynomial of degree \\(n\\) with the leading coefficient \\(2^{n - 1}\\) . \\(T_n(x)\\) has \\(n\\) zero points at \\[ \\bar{x}_k = \\cos \\left(\\frac{2k-1}{2n}\\pi\\right),\\ k = 1, 2, \\dots, n. \\] \\(T_n(x)\\) has extrema at \\[ \\bar{x}'_k = \\cos \\frac{k\\pi}n \\text{, with } T(\\bar{x}'_k) = (-1)^k,\\ k = 1, 2, \\dots, n. \\]","title":"Chebyshev Polynomials"},{"location":"Mathematics_Basis/NA/Chap_8/#monic-chebyshev-polynomials","text":"Monic chebyshev polynomials are defined by \\[ \\tilde T_0(x) = 1,\\ \\tilde T_n(x) = \\frac{1}{2^{n - 1}} T_n(x). \\] The following is an important theorem for the position of Chebyshev polynomials. Theorem \\[ \\frac{1}{2^{n - 1}} = \\max\\limits_{x \\in [-1, 1]}|\\tilde T_n(x)| \\le \\max\\limits_{x \\in [-1, 1]}|P_n(x)|,\\ \\forall\\ P_n(x) \\in \\tilde \\Pi_n, \\] where \\(\\tilde \\Pi_n\\) denotes the set of all monic polynomials of degree n . From this theorem, we can answer where to place interpolating points to minimize the error in Lagrange interpolation. Recap that \\[ R(x) = f(x) - P(x) = \\frac{f^{n + 1}(\\xi)}{(n + 1)!}\\prod\\limits_{i = 0}^{n}(x - x_i). \\] To minimize \\(R(x)\\) , since there is no control over \\(\\xi\\) , so we only need to minimize \\[ |w_n(x)| = |\\prod\\limits_{i = 0}^{n}(x - x_i)|. \\] Since \\(w_n(x)\\) is a monic polynomial of degree \\((n + 1)\\) , we can obtain minimum when \\(w_n(x) = \\tilde T_{n + 1}(x)\\) . To make it equal, we can simply make their zero points equal, namely \\[ x_k = \\bar{x}_{k + 1} = \\cos \\frac{2k + 1}{2(n + 1)}\\pi. \\] Corollary \\[ \\max\\limits_{x \\in [-1, 1]} |f(x) - P(x)| \\le \\frac{\\max\\limits_{x \\in [-1, 1]}\\left|f^{(n + 1)}(x)\\right|}{2^n (n + 1)!}. \\]","title":"Monic Chebyshev Polynomials"},{"location":"Mathematics_Basis/NA/Chap_8/#solution-of-minimax-approximation","text":"Now we come back to the minimax approximation. To minimize \\[ E_\\infty = \\max\\limits_{a \\le x \\le b}|P(x) - f(x)|, \\] with a polynomial of degree \\(n\\) on the interval \\([a, b]\\) , we need the following steps: Step.1 Find the roots of \\(T_{n + 1}(t)\\) on the interval \\([-1, 1]\\) , denoted by \\(t_0, \\dots, t_n\\) . Step.2 Extend it to the interval \\([a, b]\\) by \\[ x_i = \\frac12[(b - a)t_i + a + b]. \\] Step.3 Substitue \\(x_i\\) into \\(f(x)\\) to get \\(y_i\\) . Step.4 Compute the Langrange polynomial \\(P(x)\\) of the interpolating points \\((x_i, y_i)\\) .","title":"Solution of Minimax Approximation"},{"location":"Mathematics_Basis/NA/Chap_8/#ecomomization-of-power-series","text":"Chebyshev polynomials can also be used to reduce the degree of an approximating polynomials with a minimal loss of accuracy . Consider approximating \\[ P_n(x) = a_nx^n + a_{n - 1}x^{n - 1} + \\cdots + a_1x + a_0 \\] on [-1, 1]. Then the target is to minimize \\[ \\max_{x\\in[-1,1]}|P_n(x) - P_{n - 1}(x)|. \\]","title":"Ecomomization of Power Series"},{"location":"Mathematics_Basis/NA/Chap_8/#solution_2","text":"Since \\((P_n(x) - P_{n - 1}(x)) / a_n\\) is monic, thus \\[ \\max_{x\\in[-1,1]}\\left|\\frac{1}{a_n}((P_n(x) - P_{n - 1}(x))\\right| \\ge \\frac{1}{2^{n - 1}}. \\] Equality occurs when \\[ \\frac{1}{a_n}((P_n(x) - P_{n - 1}(x)) = \\tilde T_n(x). \\] Thus we can choose \\[ P_{n - 1}(x) = P_n(x) - a_n \\tilde T_n(x) \\] with the minimum error value of \\[ \\max_{x\\in[-1,1]}|P_n(x) - P_{n - 1}(x)| = |a_n| \\max_{x\\in[-1,1]}\\left|\\frac{1}{a_n}((P_n(x) - P_{n - 1}(x))\\right| = \\frac{|a_n|}{2^{n - 1}}. \\]","title":"Solution"},{"location":"Mathematics_Basis/NA/Chap_9/","text":"Chapter 9 | Approximating Eigenvalues \u00b6 Power Method \u00b6 The Power Method is an iterative technique used to determine the dominant eigenvalue of a matrix (the eigenvalue with the largest magnitude). Suppose \\(A \\in M_n(\\mathbb{R})\\) with eigenvalues satisfying \\(|\\lambda_1| \\gt |\\lambda_2| \\ge \\dots \\ge |\\lambda_n|\\) and their corresponding linearly independent eigenvectors \\(\\mathbf{v}_\\mathbf{1}, \\dots, \\mathbf{v}_\\mathbf{n}\\) . Original Method Start from any \\(\\mathbf{x}^{(0)} \\ne \\mathbf{0}\\) and \\((\\mathbf{x}^{(0)}, \\mathbf{v}_\\mathbf{1}) \\ne 0\\) , and suppose \\(\\mathbf{x}^{(0)} = \\sum\\limits_{j = 1}^n\\beta_j\\mathbf{v}_\\mathbf{j}\\) , \\(\\beta_1 \\ne 0.\\) \\(\\mathbf{x}^{(k)} = A \\mathbf{x}^{(k - 1)} = \\sum\\limits_{j = 1}^n\\beta_j\\lambda_j^k\\mathbf{v}_\\mathbf{j} = \\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}\\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\lambda_1^k\\beta_1\\mathbf{v}_\\mathbf{1}\\) . \\(A\\mathbf{x}^{(k)} \\approx \\lambda_1^k\\beta_1A\\mathbf{v}_\\mathbf{1} \\approx \\lambda_1\\mathbf{x}^{(k)}\\) , thus \\[ \\lambda_1 = \\frac{\\mathbf{x}^{(k + 1)}_i}{\\mathbf{x}^{(k)}_i}, \\mathbf{v}_\\mathbf{1} = \\frac{\\mathbf{x}^{(k)}}{\\lambda_1^k\\beta_1}. \\] Motivation for Normalization: In the original method, if \\(|\\lambda| > 1\\) then \\(\\mathbf{x}^{(k)}\\) diverges. If \\(|\\lambda| < 1\\) then \\(\\mathbf{x}^{(k)}\\) converges to 0. Both cases are not suitable for actual computing. Thus we need normalization to make sure \\(||\\mathbf{x}||_\\infty = 1\\) at each step to guarantee the stableness . Normalization Let \\(\\mathbf{u}^{(k-1)} = \\frac{\\mathbf{x}^{(k-1)}}{||\\mathbf{x}^{(k-1)}||_\\infty}, \\mathbf{x}^{(k)} = A\\mathbf{u}^{(k - 1)}\\) , then \\[ \\begin{align} & \\mathbf{u}^{(k)} = \\frac{\\mathbf{x}^{(k)}}{||\\mathbf{x}^{(k)}||_\\infty} = \\frac{\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}}{\\left|\\left|\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}\\right|\\right|_\\infty} \\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty} \\\\ & \\mathbf{x}^{(k)} = \\frac{A^k\\mathbf{x}_\\mathbf{0}}{||A^{k - 1}\\mathbf{x}_\\mathbf{0}||} = \\frac{\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}}{\\left|\\left|\\lambda_1^{k - 1} \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^{k - 1}\\mathbf{v}_\\mathbf{j}\\right|\\right|_\\infty} \\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\lambda_1 \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty}=\\lambda_1 \\mathbf{u}^{(k)}. \\end{align} \\] Thus \\[ \\lambda_1 = \\frac{\\mathbf{x}_i^{(k)}}{\\mathbf{u}_i^{(k)}}, \\hat{\\mathbf{v}_\\mathbf{1}} = \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty}= \\mathbf{u}^{(k)}. \\] Remark For multiple eigenvalues \\(\\lambda_1 = \\lambda_2 = \\dots = \\lambda_r\\) , \\[ \\mathbf{x}^{(k)} = \\lambda_1^k\\left( \\sum_{j = 1}^r\\beta_j\\mathbf{v}_\\mathbf{j} + \\sum_{j = r + 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j} \\right)\\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\lambda_1^k\\sum_{j = 1}^r\\beta_j\\mathbf{v}_\\mathbf{j} \\] The method fails to converge if \\(\\lambda_1 = -\\lambda_2\\) . Aitken's \\(\\Delta^2\\) procedure can be used to speed up the convergence. Rate of Convergence \u00b6 Examine \\(\\textbf{x}^{(k)} = \\lambda_1^k \\sum_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\textbf{v}_\\textbf{j}\\) > We find that \\(\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)\\) determines the rate of convergence, especially \\(\\left|\\frac{\\lambda_2}{\\lambda_1}\\right|\\) . Target: Make \\(\\left|\\frac{\\lambda_2}{\\lambda_1}\\right|\\) as small as possible. Example Let \\(p = \\frac12 (\\lambda_2 + \\lambda_n)\\) , and \\(B = A - pI\\) , then \\(B\\) has the eigenvalues \\[ \\lambda_1 - p, \\lambda_2 - p, \\dots, \\lambda_n - p. \\] And since \\[ \\left|\\frac{\\lambda_2 - p}{\\lambda_1 - p}\\right| < \\left|\\frac{\\lambda_2}{\\lambda_1}\\right|, \\] the iteration for finding the eigenvalues of \\(B\\) converges much faster than that of \\(A\\) . But how to find \\(p\\) ? ... Inverse Power Method \u00b6 If \\(A\\) has eigenvalues satisfying \\(|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\gt |\\lambda_n|\\) , then \\(A^{-1}\\) has \\[ \\left|\\frac{1}{\\lambda_n}\\right| \\gt \\left|\\frac{1}{\\lambda_{n-1}}\\right| \\ge \\cdots \\ge \\left|\\frac{1}{\\lambda_1}\\right|. \\] The dominant eigenvalue of \\(A^{-1}\\) is the eigenvalue with the smallest magnitude of \\(A\\) . A way to find eigenvalue \\(\\lambda\\) closest to a given value \\(p\\) Use power method for matrix \\((A - qI)^{-1}\\) , which has the eigenvalues \\[ \\left|\\frac{1}{\\lambda_1 - q}\\right| \\gt \\left|\\frac{1}{\\lambda_2 - q}\\right| \\ge \\cdots \\ge \\left|\\frac{1}{\\lambda_n - q}\\right|. \\] where \\(\\lambda_1\\) is the closest to \\(p\\) .","title":"Chap 9"},{"location":"Mathematics_Basis/NA/Chap_9/#chapter-9-approximating-eigenvalues","text":"","title":"Chapter 9 | Approximating Eigenvalues"},{"location":"Mathematics_Basis/NA/Chap_9/#power-method","text":"The Power Method is an iterative technique used to determine the dominant eigenvalue of a matrix (the eigenvalue with the largest magnitude). Suppose \\(A \\in M_n(\\mathbb{R})\\) with eigenvalues satisfying \\(|\\lambda_1| \\gt |\\lambda_2| \\ge \\dots \\ge |\\lambda_n|\\) and their corresponding linearly independent eigenvectors \\(\\mathbf{v}_\\mathbf{1}, \\dots, \\mathbf{v}_\\mathbf{n}\\) . Original Method Start from any \\(\\mathbf{x}^{(0)} \\ne \\mathbf{0}\\) and \\((\\mathbf{x}^{(0)}, \\mathbf{v}_\\mathbf{1}) \\ne 0\\) , and suppose \\(\\mathbf{x}^{(0)} = \\sum\\limits_{j = 1}^n\\beta_j\\mathbf{v}_\\mathbf{j}\\) , \\(\\beta_1 \\ne 0.\\) \\(\\mathbf{x}^{(k)} = A \\mathbf{x}^{(k - 1)} = \\sum\\limits_{j = 1}^n\\beta_j\\lambda_j^k\\mathbf{v}_\\mathbf{j} = \\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}\\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\lambda_1^k\\beta_1\\mathbf{v}_\\mathbf{1}\\) . \\(A\\mathbf{x}^{(k)} \\approx \\lambda_1^k\\beta_1A\\mathbf{v}_\\mathbf{1} \\approx \\lambda_1\\mathbf{x}^{(k)}\\) , thus \\[ \\lambda_1 = \\frac{\\mathbf{x}^{(k + 1)}_i}{\\mathbf{x}^{(k)}_i}, \\mathbf{v}_\\mathbf{1} = \\frac{\\mathbf{x}^{(k)}}{\\lambda_1^k\\beta_1}. \\] Motivation for Normalization: In the original method, if \\(|\\lambda| > 1\\) then \\(\\mathbf{x}^{(k)}\\) diverges. If \\(|\\lambda| < 1\\) then \\(\\mathbf{x}^{(k)}\\) converges to 0. Both cases are not suitable for actual computing. Thus we need normalization to make sure \\(||\\mathbf{x}||_\\infty = 1\\) at each step to guarantee the stableness . Normalization Let \\(\\mathbf{u}^{(k-1)} = \\frac{\\mathbf{x}^{(k-1)}}{||\\mathbf{x}^{(k-1)}||_\\infty}, \\mathbf{x}^{(k)} = A\\mathbf{u}^{(k - 1)}\\) , then \\[ \\begin{align} & \\mathbf{u}^{(k)} = \\frac{\\mathbf{x}^{(k)}}{||\\mathbf{x}^{(k)}||_\\infty} = \\frac{\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}}{\\left|\\left|\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}\\right|\\right|_\\infty} \\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty} \\\\ & \\mathbf{x}^{(k)} = \\frac{A^k\\mathbf{x}_\\mathbf{0}}{||A^{k - 1}\\mathbf{x}_\\mathbf{0}||} = \\frac{\\lambda_1^k \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j}}{\\left|\\left|\\lambda_1^{k - 1} \\sum\\limits_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^{k - 1}\\mathbf{v}_\\mathbf{j}\\right|\\right|_\\infty} \\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\lambda_1 \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty}=\\lambda_1 \\mathbf{u}^{(k)}. \\end{align} \\] Thus \\[ \\lambda_1 = \\frac{\\mathbf{x}_i^{(k)}}{\\mathbf{u}_i^{(k)}}, \\hat{\\mathbf{v}_\\mathbf{1}} = \\frac{\\mathbf{v}_\\mathbf{1}}{||\\mathbf{v}_\\mathbf{1}||_\\infty}= \\mathbf{u}^{(k)}. \\] Remark For multiple eigenvalues \\(\\lambda_1 = \\lambda_2 = \\dots = \\lambda_r\\) , \\[ \\mathbf{x}^{(k)} = \\lambda_1^k\\left( \\sum_{j = 1}^r\\beta_j\\mathbf{v}_\\mathbf{j} + \\sum_{j = r + 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\mathbf{v}_\\mathbf{j} \\right)\\ \\overset{k \\rightarrow \\infty}{=\\!=\\!=\\!=}\\ \\lambda_1^k\\sum_{j = 1}^r\\beta_j\\mathbf{v}_\\mathbf{j} \\] The method fails to converge if \\(\\lambda_1 = -\\lambda_2\\) . Aitken's \\(\\Delta^2\\) procedure can be used to speed up the convergence.","title":"Power Method"},{"location":"Mathematics_Basis/NA/Chap_9/#rate-of-convergence","text":"Examine \\(\\textbf{x}^{(k)} = \\lambda_1^k \\sum_{j = 1}^n\\beta_j\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k\\textbf{v}_\\textbf{j}\\) > We find that \\(\\left(\\frac{\\lambda_j}{\\lambda_1}\\right)\\) determines the rate of convergence, especially \\(\\left|\\frac{\\lambda_2}{\\lambda_1}\\right|\\) . Target: Make \\(\\left|\\frac{\\lambda_2}{\\lambda_1}\\right|\\) as small as possible. Example Let \\(p = \\frac12 (\\lambda_2 + \\lambda_n)\\) , and \\(B = A - pI\\) , then \\(B\\) has the eigenvalues \\[ \\lambda_1 - p, \\lambda_2 - p, \\dots, \\lambda_n - p. \\] And since \\[ \\left|\\frac{\\lambda_2 - p}{\\lambda_1 - p}\\right| < \\left|\\frac{\\lambda_2}{\\lambda_1}\\right|, \\] the iteration for finding the eigenvalues of \\(B\\) converges much faster than that of \\(A\\) . But how to find \\(p\\) ? ...","title":"Rate of Convergence"},{"location":"Mathematics_Basis/NA/Chap_9/#inverse-power-method","text":"If \\(A\\) has eigenvalues satisfying \\(|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\gt |\\lambda_n|\\) , then \\(A^{-1}\\) has \\[ \\left|\\frac{1}{\\lambda_n}\\right| \\gt \\left|\\frac{1}{\\lambda_{n-1}}\\right| \\ge \\cdots \\ge \\left|\\frac{1}{\\lambda_1}\\right|. \\] The dominant eigenvalue of \\(A^{-1}\\) is the eigenvalue with the smallest magnitude of \\(A\\) . A way to find eigenvalue \\(\\lambda\\) closest to a given value \\(p\\) Use power method for matrix \\((A - qI)^{-1}\\) , which has the eigenvalues \\[ \\left|\\frac{1}{\\lambda_1 - q}\\right| \\gt \\left|\\frac{1}{\\lambda_2 - q}\\right| \\ge \\cdots \\ge \\left|\\frac{1}{\\lambda_n - q}\\right|. \\] where \\(\\lambda_1\\) is the closest to \\(p\\) .","title":"Inverse Power Method"},{"location":"Mathematics_Basis/NA/class_notes/","text":"Class Notes \u00b6 Lecturer \u8bb8\u5a01\u5a01 Textbook \u00b6 Lecture Grade (100 %) \u00b6 Laboratory Projects (36%) 8 labs. Release at a time. Classroom Quiz (4%) Twice, 2% for each test Can also answer questions in class (1% for each) Research Topics (15%) Done in groups 18 topics to choose from In class presentations (5~10 minutes) Homework (5%) Final exam (40%)","title":"Class Notes"},{"location":"Mathematics_Basis/NA/class_notes/#class-notes","text":"Lecturer \u8bb8\u5a01\u5a01","title":"Class Notes"},{"location":"Mathematics_Basis/NA/class_notes/#textbook","text":"","title":"Textbook"},{"location":"Mathematics_Basis/NA/class_notes/#lecture-grade-100","text":"Laboratory Projects (36%) 8 labs. Release at a time. Classroom Quiz (4%) Twice, 2% for each test Can also answer questions in class (1% for each) Research Topics (15%) Done in groups 18 topics to choose from In class presentations (5~10 minutes) Homework (5%) Final exam (40%)","title":"Lecture Grade (100 %)"}]}